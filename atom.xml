<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Karl的博客</title>
  
  <subtitle>日拱一卒，功不唐捐</subtitle>
  <link href="http://blog.karltan.com/atom.xml" rel="self"/>
  
  <link href="http://blog.karltan.com/"/>
  <updated>2024-04-20T14:00:00.000Z</updated>
  <id>http://blog.karltan.com/</id>
  
  <author>
    <name>Karl</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/"/>
    <id>http://blog.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/</id>
    <published>2024-04-15T07:00:00.000Z</published>
    <updated>2024-04-20T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE"><a href="#AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="headerlink" title="AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"></a>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/">论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/138014340">论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》-CSDN博客</a></p><p>论文链接：<a href="https://arxiv.org/abs/2010.11929">[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (arxiv.org)</a></p><p>代码链接：<a href="https://github.com/google-research/vision_transformer">google-research/vision_transformer (github.com)</a></p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a>ABSTRACT</h2><p>虽然Transformer架构已经成为自然语言处理任务的事实上的标准，但它在计算机视觉上的应用仍然有限。在视觉方面，注意力要么与卷积网络结合使用，要么用于替换卷积网络的某些组件，同时保持其整体结构不变。我们证明这种对CNNs的依赖是不必要的，直接应用于图像patch序列的纯Transformer可以很好地完成图像分类任务。当对大量数据进行预训练并转移到多个中型或小型图像识别基准（ImageNet，CIFAR-100，VTAB等）时，Vision Transformer（ViT）与最先进的卷积网络相比获得了出色的结果，同时需要更少的计算资源来训练。</p><blockquote><p>Fine-tuning code and pre-trained models are available at <a href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a>.</p></blockquote><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h2><p>基于自注意力的架构，特别是Transformers（Vaswani et al., 2017），已经成为自然语言处理（NLP）的首选模型。主要的方法是在大型文本语料库上进行预训练，然后在较小的特定任务数据集上进行微调（Devlin et al., 2019）。由于Transformers的计算效率和可扩展性，它可以训练前所未有的规模，超过100B个参数的模型（Brown et al., 2020; Lepikhin et al., 2020）。随着模型和数据集的增长，仍然没有出现性能饱和的迹象。</p><p>然而，在计算机视觉领域，卷积架构仍然占主导地位（LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016）。受NLP成功的启发，许多工作尝试将类CNN架构与自注意力相结合（Wang et al., 2018; Carion et al., 2020），有些完全取代了卷积（Ramachandran et al., 2019; Wang et al., 2020a）。后一种模型虽然在理论上是有效的，但由于使用了专门的注意力模式，还没有在现代硬件加速器上有效地扩展。因此，在大规模图像识别中，经典的类ResNet架构仍然是最先进的（Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020）。</p><p>受NLP中Transformer缩放成功的启发，我们尝试将标准Transformer直接应用于图像，并尽可能减少修改。为此，我们将图像分割成小块，并提供这些小块的线性嵌入序列作为Transformer的输入。在NLP应用程序中，图像patch的处理方式与令牌（单词）相同。我们以监督的方式对模型进行图像分类训练。</p><p>当在中等规模的数据集（如ImageNet）上进行训练时，没有进行强正则化，这些模型产生的精度比同等规模的ResNets低几个百分点。这个看似令人沮丧的结果是可以预料的：Transformers缺乏CNNs固有的一些归纳偏差，例如平移不变性和局部性，因此在数据量不足的情况下训练时不能很好地泛化。</p><p>然而，如果模型在更大的数据集（14M-300M图像）上训练，情况就会发生变化。我们发现大规模训练胜过归纳偏见。我们的Vision Transformer（ViT）在足够的规模上进行预训练并转移到具有更少数据点的任务时获得了出色的结果。当在公共ImageNet-21k数据集或室内JFT-300M数据集上进行预训练时，ViT在多个图像识别基准上接近或超过了最先进的水平。特别是，最佳模型在ImageNet上达到88.55%的准确率，在ImageNet- real上达到90.72%的准确率，在CIFAR-100上达到94.55%的准确率，在VTAB套件的19个任务上达到77.63%的准确率。</p><h2 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2 RELATED WORK"></a>2 RELATED WORK</h2><p>Transformers是由Vaswani等人（2017）提出的，用于机器翻译，并且已经成为许多NLP任务中最先进的方法。基于大型Transformer的模型通常在大型语料库上进行预训练，然后针对手头的任务进行微调：BERT（Devlin et al., 2019）使用去噪自监督预训练任务，而GPT系列则使用语言建模作为其预训练任务（Radford et al., 2018; 2019; Brown et al., 2020）。</p><p>对图像进行注意力的朴素应用会要求每个像素都注意其他像素。由于像素数量的代价是二次的，因此不能按实际的输入大小进行缩放。因此，为了在图像处理的背景下应用Transformers，过去已经尝试了几种近似方法。Parmar等人（2018）仅在局部邻域对每个查询像素应用自注意力，而不是全局。这种局部多头点积自注意块可以完全取代卷积（Hu et al., 2019; Ramachandran et al., 2019; Zhao et a., 2020）。在不同的工作中，稀疏Transformers（Child et al., 2019）采用可扩展的全局自注意力近似，以便适用于图像。另一种扩展注意力的方法是将其应用于不同大小的块（Weissenborn et al., 2019），在极端情况下，仅沿着单个轴（Ho et al., 2019; Wang et al., 2020a）。许多这些专门的注意力架构在计算机视觉任务上显示出有希望的结果，但需要复杂的工程才能在硬件加速器上有效地实现。</p><p>与我们最相关的是Cordonnier等人（2020）的模型，该模型从输入图像中提取大小为$2 \times 2$的patch，并在其上应用完全的自注意力。该模型与ViT非常相似，但我们的工作进一步证明了大规模预训练能够使普通的Transformers与最先进的CNNs竞争（甚至更好）。此外，Cordonnier等人（2020）使用了$2 \times 2$像素的小patch尺寸，这使得该模型仅适用于小分辨率图像，而我们也可以处理中分辨率图像。</p><p>人们对将卷积神经网络（CNNs）与自注意力形式相结合也很感兴趣，例如通过增强图像分类的特征映射（Bello et al., 2019）或通过使用自注意进一步处理CNN的输出，例如用于目标检测（Hu et al., 2018; Carion et al., 2020），视频处理（Wang et al., 2018; Sun et al., 2019）、图像分类（Wu et al., 2020）、无监督对象发现（Locatello et al., 2020）或统一文本视觉任务（Chen et al., 2020c; Lu et al., 2019; Li et al., 2019）。</p><p>另一个最近的相关模型是图像GPT（iGPT）（Chen et al., 2020a），它在降低图像分辨率和色彩空间后，将Transformers应用于图像像素。该模型以无监督的方式作为生成模型进行训练，然后可以对结果表示进行微调或线性探测以获得分类性能，在ImageNet上实现72%的最大准确率。</p><p>我们的工作增加了越来越多的论文，这些论文探索了比标准ImageNet数据集更大规模的图像识别。使用额外的数据源可以在标准基准上获得最先进的结果（Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020）。此外，Sun等人（2017）研究了CNN性能如何随数据集大小而扩展，Kolesnikov等人（2020）；Djolonga等人（2020）在ImageNet-21k和JFT-300M等大规模数据集上对CNN迁移学习进行了实证探索。我们也关注后两个数据集，但是训练的是Transformers而不是在以前的工作中使用的基于ResNet的模型。</p><h2 id="3-METHOD"><a href="#3-METHOD" class="headerlink" title="3 METHOD"></a>3 METHOD</h2><p>在模型设计中，我们尽可能地遵循原始Transformer（Vaswani et al., 2017）。这种故意简单设置的一个优点是，可扩展的NLP Transformer架构——以及它们的高效实现——几乎可以开箱即用。</p><h3 id="3-1-VISION-TRANSFORMER-VIT"><a href="#3-1-VISION-TRANSFORMER-VIT" class="headerlink" title="3.1 VISION TRANSFORMER (VIT)"></a>3.1 VISION TRANSFORMER (VIT)</h3><p>该模型的概述如图1所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416090340705.png" alt="image-20240416090340705"></p><p>标准Transformer接收标记嵌入的1D序列作为输入。为了处理2D图像，我们将图像$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$重塑为一系列平坦的2D patch $\mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$，其中$(H, W)$为原始图像的分辨率，$C$为通道数，$(P, P)$为每个图像patch的分辨率，$N = \frac{HW}{P^2}$为得到的patch数，它也作为Transformer的有效输入序列长度。Transformer在其所有层中使用恒定的潜在向量大小$D$，因此我们用可训练的线性投影：</p><script type="math/tex; mode=display">\mathbf{z}_0 = \left[\mathbf{x}_\text{class};\ \mathbf{x}_p^1\mathbf{E};\ \mathbf{x}_p^2\mathbf{E};\ \cdots;\ \mathbf{x}_p^N\mathbf{E}\right] + \mathbf{E}_{pos}, \quad \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D},\ \mathbf{E}_{pos} \in \mathbb{R}^{(N + 1) \times D}</script><p>将patch展平并映射到$D$维。我们将该投影的输出称为patch embedding。</p><p>与BERT的<code>[class]</code>令牌类似，我们在嵌入的patch序列（$\mathbf{z}_0^0 = \mathbf{x}_\text{class}$）上添加一个可学习的嵌入，其在Transformer编码器（$\mathbf{z}_L^0$）输出处的状态作为图像表示$\mathbf{y}$：</p><script type="math/tex; mode=display">\mathbf{y} = \text{LN}(\mathbf{z}_L^0)</script><p>在预训练和微调过程中，$\mathbf{z}_L^0$都附加了一个分类头。分类头在预训练时由一个隐藏层的MLP实现，在微调时由一个线性层实现。</p><p>将位置嵌入添加到patch嵌入中以保留位置信息。我们使用标准的可学习的1D位置嵌入，因为我们没有观察到使用更先进的2D感知位置嵌入的显著性能提升（附录D.4）。得到的嵌入向量序列作为编码器的输入。</p><p>Transformer编码器（Vaswani et al., 2017）由多头自注意（MSA，见附录A）和MLP块交替层组成：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{z}^\prime_\ell &= \text{MSA}(\text{LN}(\mathbf{z}_{\ell - 1})) + \mathbf{z}_{\ell - 1} & & \ell = 1 \dots L \\\mathbf{z}_\ell &= \text{MSA}(\text{LN}(\mathbf{z}^\prime_\ell)) + \mathbf{z}^\prime_\ell & & \ell = 1 \dots L\end{aligned}</script><p>在每个区块之前应用层Layernorm（LN），在每个区块之后应用残差连接（Wang et al., 2019; Baevski &amp; Auli, 2019）。</p><p>MLP包含两个具有GELU非线性的层：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{z}_0 &= \left[\mathbf{x}_\text{class};\ \mathbf{x}_p^1\mathbf{E};\ \mathbf{x}_p^2\mathbf{E};\cdots\ ;\ \mathbf{x}_p^N\mathbf{E}\right] + \mathbf{E}_{pos}, & & \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}, \mathbf{E}_{pos} \in \mathbb{R}^{(N + 1) \times D} \\\mathbf{z}^\prime_\ell &= \text{MSA}(\text{LN}(\mathbf{z}_{\ell - 1})) + \mathbf{z}_{\ell - 1}, & & \ell = 1 \dots L \\\mathbf{z}_\ell &= \text{MLP}(\text{LN}(\mathbf{z}^\prime_\ell)) + \mathbf{z}^\prime_\ell, & & \ell = 1 \dots L \\\mathbf{y} &= \text{LN}(\mathbf{z}^0_L)\end{aligned}</script><p><strong>Inductive bias.</strong> 我们注意到Vision Transformer具有比CNNs少得多的图像特定的感应偏置。在CNNs中，局部性、二维邻域结构和平移等方差被嵌入到整个模型的每一层中。在ViT中，只有MLP层是局部和平移不变的，而自注意力层是全局的。二维邻域结构的使用非常少：在模型开始时通过将图像切割成小块，以及在微调时用于调整不同分辨率图像的位置嵌入（如下所述）。除此之外，初始化时的位置嵌入不携带关于patch的二维位置信息，所有patch之间的空间关系都需要从头学习。</p><p><strong>Hybrid Architecture.</strong> 作为原始图像patch的替代方案，输入序列可以由CNN的特征映射形成（LeCun et al., 1989）。在该混合模型中，将patch embedding投影$\mathbf{E}$应用于从CNN feature map中提取的patch。作为一种特殊情况，patch的空间大小可以是$1 \times 1$，这意味着输入序列是通过简单地将feature map的空间维度平坦化并投影到Transformer维度得到的。如上所述添加分类输入嵌入和位置嵌入。</p><h3 id="3-2-FINE-TUNING-AND-HIGHER-RESOLUTION"><a href="#3-2-FINE-TUNING-AND-HIGHER-RESOLUTION" class="headerlink" title="3.2 FINE-TUNING AND HIGHER RESOLUTION"></a>3.2 FINE-TUNING AND HIGHER RESOLUTION</h3><p>通常，我们在大型数据集上预训练ViT，并对（较小的）下游任务进行微调。为此，我们移除预训练的预测头，并附加一个零初始化的$D \times K$前传层，其中$K$是下游类的数量。与预训练相比，在更高分辨率下进行微调通常是有益的（Touvron et al., 2019; Kolesnikov et al., 2020）。当输入更高分辨率的图像时，我们保持patch大小不变，从而获得更大的有效序列长度。Vision Transformer可以处理任意序列长度（直到内存限制），但是，预训练的位置嵌入可能不再有意义。因此，我们根据预训练的位置嵌入在原始图像中的位置对其进行二维插值。请注意，此分辨率调整和patch提取是将图像的二维结构的感应偏置手动注入Vision Transformer的唯一点。</p><h2 id="4-EXPERIMENTS"><a href="#4-EXPERIMENTS" class="headerlink" title="4 EXPERIMENTS"></a>4 EXPERIMENTS</h2><p>我们评估了ResNet、Vision Transformer（ViT）和混合架构的表示学习能力。为了了解每个模型的数据需求，我们在不同大小的数据集上进行预训练，并评估许多基准任务。当考虑到预训练模型的计算成本时，ViT表现非常好，以较低的预训练成本在大多数识别基准上达到最先进的水平。最后，我们使用自监督进行了一个小实验，并表明自监督的ViT在未来是有希望的。</p><h3 id="4-1-SETUP"><a href="#4-1-SETUP" class="headerlink" title="4.1 SETUP"></a>4.1 SETUP</h3><p><strong>Datasets.</strong> 为了探索模型的可扩展性，我们使用了ILSVRC-2012 ImageNet数据集，其中包含1k个类和130万张图像（我们将其称为ImageNet），其超集ImageNet-21k包含21k个类和14M张图像（Deng et al., 2009），以及JFT（Sun et al., 2017）包含18k个类和303M张高分辨率图像。我们在Kolesnikov等人（2020）之后，对下游任务的测试集进行了预训练数据集的去重复处理。我们将在这些数据集上训练的模型迁移到几个基准任务：原始验证标签和清理后的ReaL标签上的ImageNet（Beyer et al., 2020）、CIFAR-10/100 （Krizhevsky, 2009）、Oxford-IIIT Pets（Parkhi et al., 2012）和Oxford Flowers-102（Nilsback &amp; Zisserman, 2008）。对于这些数据集，预处理遵循Kolesnikov等人（2020）。</p><p>我们还对19个任务的VTAB分类套件进行了评估（Zhai et al., 2019b）。VTAB评估小样本迁移学习到不同任务，每个任务使用1000个训练示例。任务分为三组：<em>自然</em>任务，如上述任务，宠物，CIFAR等；<em>专业</em>任务——医学和卫星图像；以及<em>结构</em>任务——需要几何理解的任务，比如定位。</p><p><strong>Model Variants.</strong> 我们基于BERT使用的ViT配置（Devlin et al., 2019），如表1所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416095317427.png" alt="image-20240416095317427"></p><p>“Base”和“Large”模型直接取自BERT，我们加入了较大的“Huge”模型。在接下来的内容中，我们使用简短的符号来表示模型大小和输入patch大小：例如，ViT-L/16表示具有$16 \times 16$输入patch大小的“Large”变体。请注意，Transformer的序列长度与patch大小的平方成反比，因此patch大小较小的模型在计算上更昂贵。</p><p>对于基线CNNs，我们使用ResNet（He et al., 2016），但将批归一化层（Ioffe &amp; Szegedy, 2015）替换为组归一化层（Wu &amp; He, 2018），并使用标准化卷积（Qiao et al., 2019）。这些修改改善了迁移（Kolesnikov et al., 2020），我们将修改后的模型命名为“ResNet (BiT)”。对于混合架构，我们将中间特征映射以一个“像素”的patch大小馈送到ViT中。为了实验不同的序列长度，我们：</p><ol><li>要么获取普通ResNet50的阶段4的输出；</li><li>要么删除阶段4，在阶段3中放置相同数量的层（保持层总数），并获取这个扩展阶段3的输出。</li></ol><p>选项2的结果是4倍长的序列长度和更昂贵的ViT模型。</p><p><strong>Training &amp; Fine-tuning.</strong> 我们使用Adam（Kingma &amp; Ba, 2015）训练所有模型，包括ResNets，其中$\beta_1 = 0.9, \beta_2 = 0.999$，批大小为4096，并应用0.1的高权重衰减，我们发现这对所有模型的转移都很有用（附录D.1显示，与常规做法相比，Adam在我们的设置中比SGD对ResNets的效果略好）。我们使用线性学习率预热和衰减，详见附录B.1。对于微调，我们使用SGD动量，批大小512，对于所有模型，见附录B.1.1。对于表2中的ImageNet结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416100725539.png" alt="image-20240416100725539"></p><p>我们在更高的分辨率下进行了微调：ViT-L/16为512，ViT-H/14为518，并且还使用了Polyak &amp; Juditsky（1992）平均系数为0.9999（Ramachandran et al., 2019; Wang et al., 2020b）。</p><p><strong>Metrics.</strong> 我们通过小样本精度或微调精度报告下游数据集的结果。微调精度捕获每个模型在各自的数据集上进行微调后的性能。通过求解正则化最小二乘回归问题，将训练图像子集的（冻结）表示映射到$\{-1, 1\}^K$个目标向量，获得小样本精度。这个公式使我们可以得到封闭形式的精确解。虽然我们主要关注微调性能，但我们有时会使用线性的小样本精度来进行快速的动态评估，因为微调的成本太高。</p><h3 id="4-2-COMPARISON-TO-STATE-OF-THE-ART"><a href="#4-2-COMPARISON-TO-STATE-OF-THE-ART" class="headerlink" title="4.2 COMPARISON TO STATE OF THE ART"></a>4.2 COMPARISON TO STATE OF THE ART</h3><p>我们首先将我们最大的模型——ViT-H/14和ViT-L/16，与文献中最先进的CNNs进行比较。第一个比较点是Big Transfer（BiT）（Kolesnikov et al., 2020），它使用大型ResNets执行监督迁移学习。第二个是Noisy Student（Xie et al., 2020），它是一个大型的高效网络，使用半监督学习在ImageNet和JFT300M上进行训练，去掉了标签。目前，Noisy Student在ImageNet上是最先进的，BiT-L在这里报道的其他数据集上是最先进的。所有模型都在TPUv3硬件上进行训练，我们报告预训练每个模型所需的TPUv3-core-days，即用于训练的TPUv3核数（每个芯片2个）乘以训练时间（天）。</p><p>表2显示了结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416100725539.png" alt="image-20240416100725539"></p><p>在JFT-300M上预训练的较小的ViT-L/16模型在所有任务上都优于BiT-L（在相同的数据集上预训练），同时需要更少的计算资源来训练。更大的模型ViT-H/14进一步提高了性能，特别是在更具挑战性的数据集——ImageNet，CIFAR-100和VTAB套件上。有趣的是，与之前的技术相比，这个模型仍然需要更少的计算来进行预训练。然而，我们注意到预训练效率可能不仅受到体系结构选择的影响，还受到其他参数的影响，如训练计划、优化器、权值衰减等。在第4.4节中，我们对不同架构的性能与计算进行了对照研究。最后，在公共ImageNet-21k数据集上预训练的ViT-L/16模型在大多数数据集上也表现良好，同时需要较少的资源进行预训练：它可以使用标准的8核云TPUv3在大约30天内进行训练。</p><p>图2将VTAB任务分解为各自的组：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416102513943.png" alt="image-20240416102513943"></p><p>并在此基准上与之前的SOTA方法进行比较：BiT，VIVI在ImageNet和Youtube上共同训练的ResNet（Tschannen et al., 2020），以及S4L在ImageNet上监督和半监督学习（Zhai et al., 2019a）。ViT-H/14在<em>自然</em>任务和<em>结构</em>任务上优于BiT-R152x4和其他方法。 在<em>专业</em>任务上，前两个型号的性能相似。</p><h3 id="4-3-PRE-TRAINING-DATA-REQUIREMENTS"><a href="#4-3-PRE-TRAINING-DATA-REQUIREMENTS" class="headerlink" title="4.3 PRE-TRAINING DATA REQUIREMENTS"></a>4.3 PRE-TRAINING DATA REQUIREMENTS</h3><p>Vision Transformer在大型JFT-300M数据集上进行预训练时表现良好。与ResNets相比，对于视觉的归纳偏差更少，那么数据集的大小有多重要呢？我们进行了两个系列的实验。</p><p>首先，我们在越来越大的数据集上预训练ViT模型：ImageNet、ImageNet-21k和JFT300M。为了提高在较小数据集上的性能，我们优化了三个基本的正则化参数——权重衰减、dropout和标签平滑。图3显示了调优到ImageNet后的结果（表5显示了其他数据集上的结果）：</p><blockquote><p>注意，ImageNet预训练的模型也进行了微调，但还是在ImageNet上。这是因为在微调过程中分辨率的提高提高了性能。</p></blockquote><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416104401140.png" alt="image-20240416104401140"></p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416104427137.png" alt="image-20240416104427137"></p><p>当在最小的数据集ImageNet上进行预训练时，尽管（适度）正则化，ViT-Large模型的表现仍不如ViT-Base模型。使用ImageNet-21k预训练，它们的性能是相似的。只有使用JFT-300M，我们才能看到更大型号的全部好处。图3还显示了不同大小的BiT模型所跨越的性能区域：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416104401140.png" alt="image-20240416104401140"></p><p>BiT CNNs在ImageNet上的表现优于ViT，但在更大的数据集上，ViT超越了它。</p><p>其次，我们在9M、30M和90M的随机子集以及完整的JFT300M数据集上训练我们的模型。我们没有对较小的子集执行额外的正则化，并对所有设置使用相同的超参数。这样，我们评估的是模型的内在属性，而不是正则化的影响。然而，我们确实使用了早期停止，并报告了在训练期间实现的最佳验证准确性。为了节省计算，我们报告了小样本线性精度而不是全微调精度。图4包含了结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416104452907.png" alt="image-20240416104452907"></p><p>在较小的数据集上，Vision Transformers比ResNets过拟合得多，计算成本相当。例如，ViT-B/32略快于ResNet50；它在9M的子集上表现得更差，但在90M以上的子集上表现得更好。ResNet152x2和ViT-L/16也是如此。这个结果强化了卷积归纳偏差对较小数据集有用的直觉，但对于较大的数据集，直接从数据中学习相关模式是足够的，甚至是有益的。</p><p>总的来说，ImageNet上的小样本结果（图4）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416104452907.png" alt="image-20240416104452907"></p><p>以及VTAB上的小样本结果（表2）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416100725539.png" alt="image-20240416100725539"></p><p>似乎表明小样本迁移学习很有希望。进一步分析ViT的小样本特性是未来工作的一个令人兴奋的方向。</p><h3 id="4-4-SCALING-STUDY"><a href="#4-4-SCALING-STUDY" class="headerlink" title="4.4 SCALING STUDY"></a>4.4 SCALING STUDY</h3><p>我们通过评估JFT-300M的迁移学习性能，对不同模型进行了控制缩放研究。在这种情况下，数据大小不会成为模型性能的瓶颈，我们根据每个模型的预训练成本来评估性能。模型集包括：</p><ul><li>7个ResNets：R50x1、R50x2、R101x1、R152x1、R152x2，预训练7个epoch，加上R152x2和R200x3预训练14个epoch；</li><li>6个Vision Transformers，ViT-B/32、B/16、L/32、L/16，预训练7个epoch，加上L/16和H/14预训练14个epoch；</li><li>和5个混合架构，R50+ViT-B/32、B/16、L/32、L/16 预训练7个epoch，加上R50+ViT-L/16预训练14个epoch（对于混合架构，数量为模型名称的末尾不代表patch大小，而是代表ResNet主干中的总下采样率）。</li></ul><p>图5包含了迁移学习性能与总预训练计算的对比（参见附录D.5有关计算成本的详细信息）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416110147363.png" alt="image-20240416110147363"></p><p>每个模型的详细结果见附录表6：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416110255797.png" alt="image-20240416110255797"></p><p>可以观察到一些模式。首先，Vision Transformers在性能/计算权衡方面比ResNets有优势。ViT节省了大约2-4倍的计算来获得相同的性能（平均超过5个数据集）。其次，混合架构在较小的计算预算下略优于ViT，但对于较大的模型，这种差异就消失了。这个结果有些令人惊讶，因为人们可能期望卷积局部特征处理能够帮助任何规模的ViT。第三，Vision Transformers在尝试的范围内似乎没有饱和，这激励了未来的扩展努力。</p><h3 id="4-5-INSPECTING-VISION-TRANSFORMER"><a href="#4-5-INSPECTING-VISION-TRANSFORMER" class="headerlink" title="4.5 INSPECTING VISION TRANSFORMER"></a>4.5 INSPECTING VISION TRANSFORMER</h3><p>为了开始理解Vision Transformer如何处理图像数据，我们分析其内部表示。Vision Transformer的第一层将展平后的patch线性投影到较低维空间：</p><script type="math/tex; mode=display">\mathbf{z}_0 = \left[\mathbf{x}_\text{class};\ \mathbf{x}_p^1\mathbf{E};\ \mathbf{x}_p^2\mathbf{E};\ \cdots;\ \mathbf{x}_p^N\mathbf{E}\right] + \mathbf{E}_{pos}, \quad \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D},\ \mathbf{E}_{pos} \in \mathbb{R}^{(N + 1) \times D}</script><p>图7（左）显示了学习到的嵌入过滤器的顶部主成分：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416110823259.png" alt="image-20240416110823259"></p><p>这些成分类似于每个patch内精细结构的低维表示的可信基函数。</p><p>投影后，将学习到的位置嵌入添加到patch表示中。图7（中）显示，该模型在位置嵌入的相似性中学习对图像内的距离进行编码，即距离越近的patch往往有更多相似的位置嵌入。此外，出现了行-列结构：同一行/列中的patch具有相似的嵌入。最后，对于较大的网格，有时会出现正弦结构（附录D）。位置嵌入学习表示二维图像拓扑解释了为什么手工制作的二维感知嵌入变体没有产生改进（附录D.4）。</p><p>自注意力允许ViT在整个图像中集成信息，即使是在最低层。我们研究了网络在多大程度上利用了这种能力。具体来说，我们计算图像空间中信息被整合的平均距离，这是基于注意力权重的（图7，右）。这种“注意距离”类似于CNNs的感受野大小。我们发现，一些人的大脑已经注意到了最低层的大部分图像，这表明该模型确实使用了整体整合信息的能力。其他注意头在低层的注意距离一直很小。在Transformer之前应用ResNet的混合模型中，这种高度局部化的注意力不那么明显（图7，右），这表明它可能与CNN中的早期卷积层具有类似的功能。注意距离随网络深度的增加而增加。从全局来看，我们发现该模型关注与分类在语义上相关的图像区域（图6）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416112342509.png" alt="image-20240416112342509"></p><h3 id="4-6-SELF-SUPERVISION"><a href="#4-6-SELF-SUPERVISION" class="headerlink" title="4.6 SELF-SUPERVISION"></a>4.6 SELF-SUPERVISION</h3><p>Transformers在NLP任务上表现出色。然而，它们的成功不仅源于出色的可扩展性，还源于大规模的自监督预训练（Devlin et al., 2019; Radford et al., 2018）。我们还模拟BERT中使用的掩码语言建模任务，对自我监督的掩码patch预测进行了初步探索。通过自我监督预训练，我们较小的ViT-B/16模型在ImageNet上达到了79.9%的准确率，与从头开始训练相比显著提高了2%，但仍比监督预训练低4%。附录B.1.2载有进一步的细节。我们留下对比预训练的探索（Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; Henaff et al., 2020）对未来工作的影响。</p><h2 id="5-CONCLUSION"><a href="#5-CONCLUSION" class="headerlink" title="5 CONCLUSION"></a>5 CONCLUSION</h2><p>我们探索了Transformers在图像识别中的直接应用。与之前在计算机视觉中使用自注意的工作不同，除了初始patch提取步骤外，我们没有将特定于图像的归纳偏差引入体系结构。相反，我们将图像解释为一系列patch，并通过NLP中使用的标准Transformer编码器对其进行处理。当与大型数据集的预训练相结合时，这种简单但可扩展的策略效果出奇地好。因此，Vision Transformer在许多图像分类数据集上达到或超过了最先进的水平，同时预训练相对便宜。</p><p>虽然这些初步结果令人鼓舞，但仍存在许多挑战。一是将ViT应用于其他计算机视觉任务，如检测和分割。我们的结果，加上Carion等人（2020）的结果，表明了这种方法的前景。另一个挑战是继续探索自我监督的预训练方法。我们的初步实验显示了自监督预训练的改进，但自监督预训练与大规模监督预训练之间仍有很大差距。最后，进一步扩展ViT可能会提高性能。</p><h2 id="ACKNOWLEDGEMENTS"><a href="#ACKNOWLEDGEMENTS" class="headerlink" title="ACKNOWLEDGEMENTS"></a>ACKNOWLEDGEMENTS</h2><p>这项工作在柏林、苏黎世和阿姆斯特丹执行。我们感谢谷歌的许多同事的帮助，特别是Andreas Steiner，他在基础设施和代码的开源发布方面提供了至关重要的帮助；Joan Puigcerver和Maxim Neumann提供大规模训练基础设施的帮助；Dmitry Lepikhin，Aravindh Mahendran，Daniel Keysers，Mario Lucic，Noam Shazeer，Ashish Vaswani和Colin Raffel进行了有益的讨论。</p><h2 id="APPENDIX"><a href="#APPENDIX" class="headerlink" title="APPENDIX"></a>APPENDIX</h2><h3 id="A-MULTIHEAD-SELF-ATTENTION"><a href="#A-MULTIHEAD-SELF-ATTENTION" class="headerlink" title="A MULTIHEAD SELF-ATTENTION"></a>A MULTIHEAD SELF-ATTENTION</h3><p>标准$\mathbf{qkv}$自注意（SA, Vaswani et al.(2017)）是神经架构的流行构建块。对于输入序列$\mathbf{z} \in \mathbb{R}^{N \times D}$，中的每个元素，我们计算序列中所有值$\mathbf{v}$的加权和。注意力权重$A_{ij}$基于序列中两个元素的成对相似度及其各自的查询$\mathbf{q}^i$和键$\mathbf{k}^j$表示。</p><script type="math/tex; mode=display">\begin{aligned}\left[\mathbf{q}, \mathbf{k}, \mathbf{v}\right] &= \mathbf{z}\mathbf{U}_{qkv} & & \mathbf{U}_{qkv} \in \mathbb{R}^{D \times 3D_h}, \\A &= \text{softmax}(\frac{\mathbf{qk}^\mathrm{T}}{\sqrt{D_h}}) & & A \in \mathbb{R}^{N \times N}, \\\text{SA}(\mathbf{z}) &= A\mathbf{v}.\end{aligned}</script><p>多头自注意（MSA）是自注意的扩展，其中我们并行运行$k$个自注意操作，称为“头”，并投影它们的连接输出。为了在改变$k$时保持计算量和参数数量不变，通常将$D_h$设为$\frac{D}{k}$。</p><script type="math/tex; mode=display">\text{MSA}(\mathbf{z}) = \left[\text{SA}_1(z); \text{SA}_2(z);\cdots\ ; \text{SA}_k(z)\right]\mathbf{U}_{msa} \quad \mathbf{U}_{msa} \in \mathbb{R}^{k \cdot D_k \times D}</script><h3 id="B-EXPERIMENT-DETAILS"><a href="#B-EXPERIMENT-DETAILS" class="headerlink" title="B EXPERIMENT DETAILS"></a>B EXPERIMENT DETAILS</h3><h4 id="B-1-TRAINING"><a href="#B-1-TRAINING" class="headerlink" title="B.1 TRAINING"></a>B.1 TRAINING</h4><p>表3总结了不同模型的训练设置：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416150315182.png" alt="image-20240416150315182"></p><p>我们发现在ImageNet上从头开始训练模型时，强正则化是关键。 使用时，Dropout应用于除qkv投影之外的每个密集层之后，以及直接在添加位置到patch嵌入之后应用。 混合模型使用与ViT模型完全相同的设置进行训练。最后，所有的训练都是在分辨率224上完成的。</p><h5 id="B-1-1-FINE-TUNING"><a href="#B-1-1-FINE-TUNING" class="headerlink" title="B.1.1 FINE-TUNING"></a>B.1.1 FINE-TUNING</h5><p>我们使用动量为0.9的SGD对所有ViT模型进行微调。我们在学习率上运行一个小的网格搜索，参见表4中的学习率范围。为此，我们使用训练集中的小分支（宠物和鲜花为10%，CIFAR为2%，ImageNet为1%）作为开发集，并在剩余数据上进行训练。对于最终结果，我们对整个训练集进行训练，并对各自的测试数据进行评估。对于微调ResNets和混合模型，我们使用完全相同的设置，唯一的例外是ImageNet，我们在学习率扫描中添加另一个值0.06。此外，对于ResNets，我们还运行Kolesnikov等人（2020）的设置，并在本次运行和扫描中选择最佳结果。最后，如果没有特别提到，所有的微调实验都在384分辨率下运行（在不同于训练的分辨率下运行微调是常见的做法（Kolesnikov et al., 2020））。</p><p>当将ViT模型转移到另一个数据集时，我们删除整个头部（两个线性层）并将其替换为单个，零初始化的线性层，输出目标数据集所需的类数。我们发现这比简单地重新初始化最后一层要健壮一些。</p><p>对于VTAB，我们遵循Kolesnikov等人（2020）的协议，并对所有任务使用相同的超参数设置。我们使用学习率为0.01，训练2500步（表4）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416152204680.png" alt="image-20240416152204680"></p><p>我们通过在两个学习率和两个时间表上运行一个小扫描来选择这个设置，并在200个示例验证集中选择VTAB得分最高的设置。我们遵循Kolesnikov等人（2020）使用的预处理，除了我们不使用特定于任务的输入分辨率。相反，我们发现Vision Transformer受益于所有任务的高分辨率（$384 \times 384$）。</p><h5 id="B-1-2-SELF-SUPERVISION"><a href="#B-1-2-SELF-SUPERVISION" class="headerlink" title="B.1.2 SELF-SUPERVISION"></a>B.1.2 SELF-SUPERVISION</h5><p>我们采用掩模patch预测目标进行初步的自我监督实验。为了做到这一点，我们通过用可学习的<code>[mask]</code>嵌入（80%），随机的其他patch嵌入（10%）或保持原样（10%）替换它们的嵌入来破坏50%的patch嵌入。这种设置与Devlin等人（2019）使用的语言非常相似。最后，我们使用各自的patch表示来预测每个损坏patch的3位平均颜色（即总共512种颜色）。</p><p>我们在JFT上训练了1M步（约14个epoch）的自监督模型，批大小为4096。我们使用Adam，其基本学习率为$2 \cdot 10^{−4}$，预热为10k步，余弦学习率衰减。作为预训练的预测目标，我们尝试了以下设置：</p><ol><li>仅预测平均的3位颜色（即512种颜色的1种预测）；</li><li>预测$16 \times 16$ patch的$4 \times 4$缩小版本，并行使用3位颜色（即512种颜色的16种预测）；</li><li>使用L2在完整patch上进行回归（即在3个RGB通道上进行256种回归）。</li></ol><p>令人惊讶的是，我们发现所有这些都运行得很好，尽管L2稍微差一些。我们只报告选项1的最终结果，因为它显示了最佳的少数镜头性能。我们还用Devlin等人（2019）使用的15%腐败率进行了实验，但在我们的少数几个指标上，结果也略差。</p><p>最后，我们想指出的是，我们的掩码patch预测实例化不需要如此大量的预训练，也不需要像JFT这样的大型数据集，以便在ImageNet分类上获得类似的性能提升。也就是说，我们观察到在10万步预训练后下游性能的收益递减，并且在ImageNet上预训练时看到类似的收益。</p><h3 id="C-ADDITIONAL-RESULTS"><a href="#C-ADDITIONAL-RESULTS" class="headerlink" title="C ADDITIONAL RESULTS"></a>C ADDITIONAL RESULTS</h3><p>我们报告了与论文中给出的数字相对应的详细结果。表5对应于本文的图3：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416153934345.png" alt="image-20240416153934345"></p><p>显示了不同ViT模型在ImageNet、ImageNet-21k和JFT-300M数据集上预训练的迁移训练性能。表6对应于本文的图5：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416155028868.png" alt="image-20240416155028868"></p><p>显示了不同大小的ViT、ResNet和混合模型的迁移学习性能，以及预训练的估计计算成本。</p><h3 id="D-ADDITIONAL-ANALYSES"><a href="#D-ADDITIONAL-ANALYSES" class="headerlink" title="D ADDITIONAL ANALYSES"></a>D ADDITIONAL ANALYSES</h3><h4 id="D-1-SGD-VS-ADAM-FOR-RESNETS"><a href="#D-1-SGD-VS-ADAM-FOR-RESNETS" class="headerlink" title="D.1 SGD VS. ADAM FOR RESNETS"></a>D.1 SGD VS. ADAM FOR RESNETS</h4><p>ResNets通常使用SGD进行训练，我们使用Adam作为优化器是非常非常规的。在这里，我们展示了激发这一选择的实验。也就是说，我们比较了SGD和Adam在JFT上预训练的两个ResNets-50x1和152x2的微调性能。对于SGD，我们使用Kolesnikov等人（2020）推荐的超参数。结果如表7所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416155220564.png" alt="image-20240416155220564"></p><p>Adam预训练在大多数数据集和平均水平上优于SGD预训练。这证明了选择Adam作为在JFT上预训练ResNets的优化器是合理的。请注意，绝对数字低于Kolesnikov等人（2020）报告的数字，因为我们只预训练了7个epoch，而不是30个epoch。</p><h4 id="D-2-TRANSFORMER-SHAPE"><a href="#D-2-TRANSFORMER-SHAPE" class="headerlink" title="D.2 TRANSFORMER SHAPE"></a>D.2 TRANSFORMER SHAPE</h4><p>我们对Transformer架构的不同维度进行了扩展，以找出最适合扩展到非常大的模型的维度。图8显示了不同配置下ImageNet上的5次性能测试：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416155723027.png" alt="image-20240416155723027"></p><p>所有配置都基于8层的ViT模型，$D = 1024, D_{MLP} = 2048$，patch大小为32，所有线相交。我们可以看到，缩放深度带来了最大的改进，这一点在64层之前非常明显。然而，在16层之后，收益递减已经很明显了。有趣的是，扩展网络的宽度似乎只会导致最小的变化。在不引入参数的情况下，减小patch大小从而增加有效序列长度显示出惊人的鲁棒性改进。这些发现表明，计算可能比参数数量更好地预测性能，并且缩放应该强调深度而不是宽度（如果有的话）。总体而言，我们发现按比例缩放所有维度会产生稳健的改进。</p><h4 id="D-3-HEAD-TYPE-AND-CLASS-TOKEN"><a href="#D-3-HEAD-TYPE-AND-CLASS-TOKEN" class="headerlink" title="D.3 HEAD TYPE AND CLASS TOKEN"></a>D.3 HEAD TYPE AND <code>CLASS</code> TOKEN</h4><p>为了尽可能接近原始的Transformer模型，我们使用了一个额外的<code>[class]</code>令牌，它被用作图像表示。然后，该令牌的输出通过一个小型多层感知器（MLP）转换为类预测，其中$\tanh$为单个隐藏层中的非线性。</p><p>这种设计继承自文本的Transformer模型，我们在整个主要论文中都使用它。最初的尝试是只使用图像patch嵌入，全局平均池化（GAP），然后是线性分类器——就像ResNet的最终特征图一样——表现非常糟糕。然而，我们发现这既不是由于额外的令牌，也不是由于GAP操作。相反，性能上的差异完全可以通过对不同学习率的需求来解释，参见图9：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416160136254.png" alt="image-20240416160136254"></p><h4 id="D-4-POSITIONAL-EMBEDDING"><a href="#D-4-POSITIONAL-EMBEDDING" class="headerlink" title="D.4 POSITIONAL EMBEDDING"></a>D.4 POSITIONAL EMBEDDING</h4><p>我们使用位置嵌入对不同的空间信息编码方式进行了消融。我们尝试了以下情况：</p><ul><li>不提供位置信息：将输入视为a bag of patchs；</li><li>一维位置嵌入：将输入视为栅格顺序的patch序列（本文中所有其他实验的默认值）；</li><li>二维位置嵌入：将输入视为二维的patch网格。在这种情况下，学习了两组嵌入，每组用于一个轴，$X$嵌入和$Y$嵌入，每个轴的大小为$\frac{D}{2}$；然后，基于输入路径上的坐标，我们将$X$嵌入和$Y$嵌入连接起来，得到该patch的最终位置嵌入。</li><li>相对位置嵌入：考虑patch之间的相对距离来编码空间信息，而不是它们的绝对位置。为此，我们使用一维相对注意，其中我们定义了所有可能的patch对的相对距离。因此，对于每个给定的对（一个作为查询，另一个作为注意机制中的键/值），我们有一个偏移量$p_q − p_k$，其中每个偏移量都与嵌入相关联。然后，在应用softmax之前，我们使用相对注意的logit作为偏差项，并将其添加到主要注意（基于内容的注意）的logit中。</li></ul><p>除了不同的空间信息编码方式，我们还尝试了不同的方式将这些信息整合到我们的模型中。对于一维和二维位置嵌入，我们尝试了三种不同的情况：</p><ol><li>在它们模型的主干之后和将输入馈给Transformer编码器之前向输入添加位置嵌入（本文中所有其他实验的默认值）；</li><li>学习并在每层开始的输入中添加位置嵌入；</li><li>在每层开始的输入中添加一个学习到的位置嵌入（层与层之间共享）。</li></ol><p>表8总结了在ViT-B/16模型上的消融研究结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416215253321.png" alt="image-20240416215253321"></p><p>我们可以看到，虽然没有位置嵌入的模型和有位置嵌入的模型在性能上有很大的差距，但是不同的位置信息编码方式之间几乎没有差别。我们推测，由于我们的Transformer编码器在patch级输入上操作，而不是像素级输入，因此如何编码空间信息的差异不太重要。更准确地说，在patch级输入中，空间维度比原始像素级输入小得多，例如，$14 \times 14$而不是$224 \times 224$，并且对于这些不同的位置编码策略来说，学习在这种分辨率下表示空间关系同样容易。即便如此，网络学习到的位置嵌入相似度的具体模式取决于训练超参数（图10）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416215549761.png" alt="image-20240416215549761"></p><h4 id="D-5-EMPIRICAL-COMPUTATIONAL-COSTS"><a href="#D-5-EMPIRICAL-COMPUTATIONAL-COSTS" class="headerlink" title="D.5 EMPIRICAL COMPUTATIONAL COSTS"></a>D.5 EMPIRICAL COMPUTATIONAL COSTS</h4><p>我们还对硬件上架构的实际速度感兴趣，由于通道宽度和缓存大小等细节，理论FLOPs并不总是很好地预测。为此，我们在TPUv3加速器上对感兴趣的主要模型执行推理速度计时；推理速度和反传速度之间的差异是一个恒定的模型无关因素。</p><p>图12（左）显示了在不同的输入大小下，一个核每秒可以处理多少图像：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416215738272.png" alt="image-20240416215738272"></p><p>每个单点都是指在广泛的批大小范围内测量的峰值性能。可以看到，ViT与图像大小的理论双二次缩放仅在最大分辨率下的最大模型中才刚刚开始发生。</p><p>另一个感兴趣的量是每个模型可以容纳在一个核心上的最大批处理大小，更大的批处理更适合扩展到大型数据集。图12（右）显示了同一组模型的数量。这表明大型ViT模型在内存效率方面比ResNet模型有明显的优势。</p><h4 id="D-6-AXIAL-ATTENTION"><a href="#D-6-AXIAL-ATTENTION" class="headerlink" title="D.6 AXIAL ATTENTION"></a>D.6 AXIAL ATTENTION</h4><p>轴向注意力（Huang et al., 2020; Ho et al., 2019）是一种简单而有效的技术，可以在组织为多维张量的大输入上运行自注意力。轴向注意力的一般思想是执行多个注意力操作，每个操作沿着输入张量的单个轴进行，而不是将一维注意力应用于展平后的输入。在轴向注意力中，每个注意力沿特定轴混合信息，同时保持沿其他轴的信息独立。沿着这条路线，Wang等人（2020b）提出了AxialResNet模型，其中ResNet50中所有内核大小为$3 \times 3$的卷积都被轴向自注意力（即行和列注意力）取代，并通过相对位置编码增强。我们已经实现了AxialResNet作为一个基线模型。</p><blockquote><p>我们的实现基于<a href="https://github.com/csrhddlam/axial-deeplab">https://github.com/csrhddlam/axial-deeplab</a>中的开源PyTorch实现。在我们的实验中，我们在准确性方面复制了（Wang et al., 2020b）中报告的分数，然而，我们的实现与开源实现类似，在TPUs上非常慢。因此，我们无法将其用于广泛的大规模实验。这些可以通过精心优化的实现来解锁。</p></blockquote><p>此外，我们修改了ViT以处理二维形状的输入，而不是一维序列的patch，并合并了轴向Transformer块，其中不是自注意力后面跟着MLP，而是行自注意力加上MLP，然后是列自注意力加上MLP。</p><p>图13展示了在JFT数据集上进行预训练时，轴向ResNet、轴向ViT-B/32和轴向ViT-B/16在ImageNet 5shot线性上与预训练计算的性能，包括FLOPs数和推理时间（每秒示例数）。正如我们所看到的，就性能而言，轴向ViT-B/32和轴向ViT-B/16都比它们的ViT-B对应物做得更好，但这是以更多的计算为代价的。这是因为在轴向ViT模型中，每个具有全局自注意力的Transformer块被两个轴向Transformer块取代，一个具有行自注意力，一个具有列自注意力，尽管自注意力操作的序列长度在轴向情况下较小，但每个轴向ViT块都有一个额外的MLP。对于AxialResNet，尽管它在精度/计算权衡方面看起来是合理的（图13，左），但是这种朴素的实现在TPUs上非常慢（图13，右）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416221154910.png" alt="image-20240416221154910"></p><h4 id="D-7-ATTENTION-DISTANCE"><a href="#D-7-ATTENTION-DISTANCE" class="headerlink" title="D.7 ATTENTION DISTANCE"></a>D.7 ATTENTION DISTANCE</h4><p>为了理解ViT如何使用自注意力来整合图像上的信息，我们分析了不同层的注意力权重所跨越的平均距离（图11）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416221306823.png" alt="image-20240416221306823"></p><p>这种“注意距离”类似于CNNs的感受野大小。在较低的层中，注意力头之间的平均注意距离变化很大，一些注意力头注意图像的大部分，而另一些则注意查询位置或附近的小区域。随着深度的增加，所有注意力头的注意距离都会增加。在网络的后半段，大多数注意力头都能够注意大部分令牌。</p><h4 id="D-8-ATTENTION-MAPS"><a href="#D-8-ATTENTION-MAPS" class="headerlink" title="D.8 ATTENTION MAPS"></a>D.8 ATTENTION MAPS</h4><p>为了计算从输出标记到输入空间的注意力映射（图6和14）：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416112342509.png" alt="image-20240416112342509"></p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416221926761.png" alt="image-20240416221926761"></p><p>我们使用了注意力Rollout（Abnar &amp; Zuidema, 2020）。简单地说，我们在所有头部上平均ViT-L/16的注意力权重，然后递归地乘以所有层的权重矩阵。这解释了所有层的令牌之间的注意力混合。</p><h4 id="D-9-OBJECTNET-RESULTS"><a href="#D-9-OBJECTNET-RESULTS" class="headerlink" title="D.9 OBJECTNET RESULTS"></a>D.9 OBJECTNET RESULTS</h4><p>我们还根据Kolesnikov等人（2020）的评估设置，在ObjectNet基准上评估了我们的旗舰ViT-H/14模型，得到了82.1%的前5准确率和61.7%的前1准确率。</p><h4 id="D-10-VTAB-BREAKDOWN"><a href="#D-10-VTAB-BREAKDOWN" class="headerlink" title="D.10 VTAB BREAKDOWN"></a>D.10 VTAB BREAKDOWN</h4><p>表9显示了每个VTAB-1k任务的得分。</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416222304154.png" alt="image-20240416222304154"></p>]]></content>
    
    
    <summary type="html">AN IMAGE IS WORTH 16X16 WORDS：用于大规模图像识别的Transformers。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2021" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2021/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="ICLR" scheme="http://blog.karltan.com/tags/ICLR/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/"/>
    <id>http://blog.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/</id>
    <published>2024-04-04T05:00:00.000Z</published>
    <updated>2024-04-04T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ConvNeXt-V2-Co-designing-and-Scaling-ConvNets-with-Masked-Autoencoders"><a href="#ConvNeXt-V2-Co-designing-and-Scaling-ConvNets-with-Masked-Autoencoders" class="headerlink" title="ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"></a>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/">论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/137381266">论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》-CSDN博客</a></p><p>论文链接：<a href="https://arxiv.org/abs/2301.00808">[2301.00808] ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders (arxiv.org)</a></p><p>代码链接：<a href="https://github.com/facebookresearch/ConvNeXt-V2">facebookresearch/ConvNeXt-V2: Code release for ConvNeXt V2 model (github.com)</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在改进的架构和更好的表示学习框架的推动下，视觉识别领域在本世纪20年代初经历了快速的现代化和性能提升。例如，以ConvNeXt[52]为代表的现代卷积神经网络已经在各种场景中展示了强大的性能。虽然这些模型最初是为使用ImageNet标签进行监督学习而设计的，但它们也可能受益于自监督学习技术，如掩码自动编码器（MAE）[31]。然而，我们发现简单地结合这两种方法会导致表现欠佳。在本文中，我们提出了一个全卷积掩码自编码器框架和一个新的全局响应归一化（GRN）层，该层可以添加到ConvNeXt架构中以增强信道间特征竞争。这种自监督学习技术和架构改进的共同设计产生了一个名为ConvNeXt V2的新模型家族，它显著提高了纯ConvNets在各种识别基准上的性能，包括ImageNet分类、COCO检测和ADE20K分割。我们还提供了各种大小的预训练ConvNeXt V2模型，从在ImageNet上具有76.7% top-1精度的高效3.7 M参数Atto模型，到仅使用公共训练数据即可实现最先进的88.9%精度的650M Huge模型。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>在前几十年研究突破的基础上[34, 44, 47, 60, 68]，视觉识别领域迎来了大规模视觉表征学习的新时代。预训练的大规模视觉模型已经成为特征学习和实现广泛视觉应用的基本工具。视觉表示学习系统的性能在很大程度上受三个主要因素的影响：选择的神经网络架构、用于训练网络的方法和用于训练的数据。在视觉识别领域，这些领域的进步都有助于整体性能的提高。</p><p>神经网络架构设计的创新一直在表征学习领域发挥着重要作用。卷积神经网络架构（ConvNets）[34, 44, 47]对计算机视觉研究产生了重大影响，因为它允许在各种视觉识别任务中使用通用的特征学习方法[25, 33]，而不是依赖于人工特征工程。近年来，最初为自然语言处理而开发的transformer架构[68]也因其在模型和数据集大小方面的强大缩放行为[21]而受到欢迎。最近，ConvNeXt[52]架构使传统的卷积网络现代化，并证明纯卷积模型也可以是可扩展的架构。然而，探索神经网络架构设计空间的最常见方法仍然是通过对ImageNet上的监督学习性能进行基准测试。</p><p>在另一项研究中，视觉表征学习的重点已经从有标签的监督学习转向有预训练目标的自我监督预训练。在许多不同的自监督算法中，掩码自编码器（mask autoencoders, MAE）[31]最近将掩码语言建模成功地引入了视觉领域，并迅速成为视觉表征学习的一种流行方法。然而，自监督学习中的一个常见做法是使用为监督学习设计的预定架构，并假设该设计是固定的。例如，MAE是使用视觉transformer[21]架构开发的。</p><p>将架构的设计元素和自监督学习框架结合起来是可能的，但是这样做可能会在使用带有掩码自编码器的ConvNeXt时带来挑战。一个问题是，MAE有一个特定的编解码器设计，该设计针对transformers的序列处理能力进行了优化，这使得计算量大的编码器能够专注于可见的小块，从而减少预训练成本。这种设计可能与使用密集滑动窗口的标准卷积神经网络不兼容。此外，如果不考虑体系结构和训练目标之间的关系，那么是否能够实现最佳性能可能是不清楚的。事实上，之前的研究表明，用基于掩码的自监督学习训练卷积神经网络可能很困难，而且经验证据表明，transformers和卷积神经网络可能具有不同的特征学习行为，从而影响表征质量。</p><p>为此，我们提出在同一框架下共同设计网络架构和掩码自编码器，目的是使基于掩码的自监督学习对ConvNeXt模型有效，并获得与使用transformers相似的结果。</p><p>在设计掩码自编码器时，我们将掩码输入视为一组稀疏小块，并使用稀疏卷积[28]只处理可见部分。这个想法的灵感来自于在处理大规模3D点云时使用稀疏卷积[15, 76]。在实践中，我们可以用稀疏卷积实现ConvNeXt，在微调时，权重被转换回标准的密集层，而不需要特殊处理。为了进一步提高预训练效率，我们将transformer解码器替换为单个ConvNeXt块，使整个设计完全卷积。我们观察到这些变化的混合结果：学习到的特征是有用的，并且在基线结果上有所改进，但是微调性能仍然不如基于transformer的模型好。</p><p>然后，我们对ConvNeXt的不同训练配置进行特征空间分析。当直接在掩码输入上训练ConvNeXt时，我们发现了MLP层特征崩溃的潜在问题。为了解决这个问题，我们建议增加一个全局响应规范化层来增强通道间的特征竞争。当模型用掩码自编码器预训练时，这种变化是最有效的，这表明重用来自监督学习的固定架构设计可能是次优的。</p><p>总之，我们介绍了ConvNeXt V2，当与掩码自编码器结合使用时，它展示了改进的性能。我们发现，该模型显著提高了纯卷积神经网络在各种下游任务上的性能，包括ImageNet分类[60]、COCO对象检测[49]和ADE20K分割[81]。ConvNeXt V2模型可用于各种计算机制，包括不同复杂性的模型：从3.7M参数的Atto模型，在ImageNet上达到76.7%的top-1精度，到使用IN-22K标签时达到最先进的88.9%精度的650M Huge模型。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p><strong>ConvNets.</strong> 卷积神经网络的设计于20世纪80年代首次引入，并使用反向传播进行训练，多年来在优化、准确性和效率方面经历了许多改进[35, 36, 39, 44, 58, 61, 63, 75]。这些创新主要是通过在ImageNet数据集上使用监督训练发现的。近年来，人们在使用自监督的预训练任务（如旋转预测和着色）来执行架构搜索方面做出了一些努力，例如UnNAS[50]。最近，ConvNeXt[52]对设计空间进行了全面的回顾，并证明纯ConvNets可以像视觉transformers一样具有可扩展性[21, 51]，这已经成为许多应用中的主导架构。ConvNeXt在需要较低复杂性的场景中表现尤为出色[7, 70, 71]。我们的ConvNeXt V2模型由自监督学习驱动，提供了一种简单的方法来升级现有模型，并在广泛的用例中实现性能的显著提升。</p><p><strong>Masked Autoencoders.</strong> 以掩码自编码器[31]为代表的掩码图像建模是一种最新的自监督学习策略。掩码自编码器作为一种神经网络预训练框架，在视觉识别领域显示出广泛的影响。然而，原始的掩码自编码器由于其不对称的编解码器设计而不能直接应用于卷积神经网络。替代框架，如[3, 77]已经尝试将该方法用于卷积神经网络，但结果好坏参半。MCMAE[23]使用一些卷积块作为输入标记器。据我们所知，没有预训练的模型表明自监督学习可以提高最佳的ConvNeXt监督结果。</p><h2 id="3-Fully-Convolutional-Masked-Autoencoder"><a href="#3-Fully-Convolutional-Masked-Autoencoder" class="headerlink" title="3. Fully Convolutional Masked Autoencoder"></a>3. Fully Convolutional Masked Autoencoder</h2><p>我们的方法在概念上很简单，并且以完全卷积的方式运行。学习信号是通过以高掩蔽比例随机掩蔽原始输入视觉效果并让模型在给定剩余上下文的情况下预测缺失部分来生成的。我们的框架如图2所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404145621488.png" alt="image-20240404145621488"></p><p>现在我们将更详细地描述它的主要组件。</p><p><strong>Masking.</strong> 我们使用随机掩蔽策略，掩蔽比例为0.6。由于卷积模型具有分层设计，其中在不同阶段对特征进行下采样，在最后阶段生成掩码并递归上采样以达到最佳分辨率。为了在实践中实现这一点，我们从原始输入图像中随机去除60%的$32 \times 32$块。我们使用最小的数据增强，只包括随机调整大小的裁剪。</p><p><strong>Encoder design.</strong> 在我们的方法中，我们使用ConvNeXt[52]模型作为编码器。使掩码图像建模有效的一个挑战是防止模型学习允许它从掩码区域复制和粘贴信息的快捷方式。在基于transformer的模型中，这相对容易防止，因为它可以将可见的小块作为编码器的唯一输入。然而，使用卷积神经网络实现这一点更加困难，因为必须保留二维图像结构。虽然朴素的解决方案涉及在输入端引入可学习的掩码令牌[3, 77]，但这些方法降低了预训练的效率，并导致训练和测试时间不一致，因为在测试时没有掩码令牌。当掩蔽比例很高时，这变得特别成问题。</p><p>为了解决这个问题，我们的新见解是从“稀疏数据视角”来看待被掩盖的图像，这是受到3D任务中稀疏点云学习的启发[15, 76]。我们的关键观察是，被遮挡的图像可以表示为二维稀疏的像素数组。基于这一见解，将稀疏卷积合并到我们的框架中以促进掩码自编码器的预训练是很自然的。在实践中，在预训练过程中，我们建议将编码器中的标准卷积层转换为子流形稀疏卷积，使模型仅对可见数据点进行操作[15, 27, 28]。我们注意到稀疏卷积层可以在微调阶段转换回标准卷积，而不需要额外的处理。作为一种替代方法，也可以在密集卷积操作之前和之后应用二进制掩蔽操作。该操作在数值上与稀疏卷积具有相同的效果，理论上计算强度更高，但在TPU等AI加速器上可能更友好。</p><p><strong>Decoder design.</strong> 我们使用轻量级的、普通的ConvNeXt块作为解码器。这在整体上形成了一个不对称的编码器-解码器架构，因为编码器更重并且具有层次结构。我们还考虑了更复杂的解码器，如分层解码器[48, 59]或transformers[21, 31]，但更简单的单个ConvNeXt块解码器在微调精度和显著减少预训练时间方面表现良好，如表1所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404151132387.png" alt="image-20240404151132387"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404151211498.png" alt="image-20240404151211498"></p><p>我们将解码器的尺寸设置为512。</p><p><strong>Reconstruction target.</strong> 我们计算重建图像与目标图像之间的均方误差（MSE）。与MAE[31]类似，目标是原始输入的逐块归一化图像，并且损失仅应用于被掩蔽的补丁。</p><p><strong>FCMAE.</strong> 我们现在结合上述建议提出了一个全卷积掩码自动编码器（FCMAE）。为了评估该框架的有效性，我们使用ConvNeXt-Base模型作为编码器，并进行了一系列消融研究。在整篇论文中，我们关注端到端微调性能，因为它在迁移学习中具有实际意义，并使用它来评估学习表征的质量。</p><p>我们使用ImageNet-1K（IN-1K）数据集分别进行800次和100次预训练和微调，并报告单个$224 \times 224$中心裁剪的前1个IN-1K验证精度。关于实验设置的更多细节可以在附录中找到。</p><p>为了理解在我们的FCMAE框架中使用稀疏卷积的影响，我们首先研究了它如何影响掩码图像预训练期间学习到的表示的质量。我们的实证研究结果表明，为了达到良好的效果，防止掩码区域的信息泄露是至关重要的。</p><p>接下来，我们将自我监督方法与监督学习方法进行比较。具体来说，我们获得了两个基线实验结果：使用相同配方的监督100 epoch基线和原始ConvNeXt论文[52]中提供的300 epoch监督训练基线。我们发现我们的FCMAE预训练提供了比随机基线更好的初始化（即$82.7 \to 83.7$），但它仍然需要赶上在原始监督设置中获得的最佳性能。</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404152152273.png" alt="image-20240404152152273"></p><p>这与最近使用基于transformer的模型的掩码图像建模的成功形成对比[3, 31, 77]，其中预训练的模型显著优于有监督的模型。这促使我们研究ConvNeXt编码器在掩码自动编码器预训练期间面临的独特挑战，我们接下来将讨论。</p><h2 id="4-Global-Response-Normalization"><a href="#4-Global-Response-Normalization" class="headerlink" title="4. Global Response Normalization"></a>4. Global Response Normalization</h2><p>在本节中，我们将介绍一种新的全局响应归一化（GRN）技术，以使FCMAE预训练与ConvNeXt架构相结合更加有效。我们首先通过定性和定量特征分析来激励我们的方法。</p><p><strong>Feature collapse.</strong> 为了更深入地了解学习行为，我们首先在特征空间中进行定性分析。我们可视化了FCMAE预训练的ConvNeXt-Base模型的激活，并注意到一个有趣的“特征崩溃”现象：有许多死的或饱和的特征图，并且激活在通道之间变得冗余。我们在图3中展示了一些可视化效果：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404152752530.png" alt="image-20240404152752530"></p><p>这种行为主要在ConvNeXt块[52]的维度扩展MLP层中观察到。</p><p><strong>Feature cosine distance analysis.</strong> 为了进一步定量验证我们的观察结果，我们进行了特征余弦距离分析。给定一个激活张量$X \in R^{H \times W \times C}$，$X_i \in R^{H \times W}$是第$i$个通道的特征图。我们将其重塑为$HW$维向量，并通过$\frac{1}{C^2}\sum_i^C\sum_j^C\frac{1 - \cos(X_i, X_h)}{2}$计算通道上的平均成对余弦距离。距离值越高表示特征越多样，距离值越低表示特征冗余。</p><p>为了进行分析，我们从ImageNet-1K验证集中随机选择1000张不同类别的图像，并从不同模型的每一层提取高维特征，包括FCMAE模型、ConvNeXt监督模型[52]和MAE预训练的ViT模型[31]。然后，我们计算每个图像的每层距离，并将所有图像的值平均。结果如图4所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404153255459.png" alt="image-20240404153255459"></p><p>FCMAE预训练的ConvNeXt模型显示出明显的特征崩溃趋势，这与我们从之前的激活可视化中观察到的结果一致。这促使我们在学习过程中考虑如何使特征多样化，防止特征崩溃。</p><p><strong>Approach.</strong> 大脑中有许多促进神经元多样性的机制。例如，侧抑制[6, 30]可以帮助增强激活神经元的反应，增加单个神经元对刺激的对比和选择性，同时也增加了神经元群体反应的多样性。在深度学习中，这种形式的横向抑制可以通过响应归一化[45]来实现。在这项工作中，我们引入了一个新的响应归一化层，称为全局响应归一化（GRN），旨在增加通道的对比度和选择性。给定输入特征$X \in R^{H \times W \times C}$，本文提出的GRN单元包括三个步骤：</p><ol><li>全局特征聚合；</li><li>特征归一化；</li><li>特征校准。</li></ol><p>首先，我们将空间特征映射$X_i$聚合为具有全局函数$\mathcal{G}(\cdot)$的向量$gx$：</p><script type="math/tex; mode=display">\mathcal{G}(X) := X \in \mathcal{R}^{H \times W \times C} \to gx \in \mathcal{R}^C</script><p>这可以看作是一个简单的池化层。我们在表2a中实验了不同的函数：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404154320071.png" alt="image-20240404154320071"></p><p>有趣的是，广泛使用的特征聚合器global average pooling[37, 72]在我们的案例中表现不佳。相反，我们发现使用基于范数的特征聚合，特别是使用L2范数，会产生更好的性能。这为我们提供了一组聚合值$\mathcal{G}(X) = gx = \{\Vert X_1 \Vert, \Vert X_2 \Vert, \cdots, \Vert X_C \Vert\} \in \mathcal{R}^C$，其中$\mathcal{G}(X)_i = \Vert X_i \Vert$是聚合第$i$个通道统计信息的标量。</p><p>接下来，我们对聚合值应用响应归一化函数$\mathcal{N}(\cdot)$。具体来说，我们使用标准的分裂归一化如下：</p><script type="math/tex; mode=display">\mathcal{N}(\Vert X_i \Vert) := \Vert X_i \Vert \in \mathcal{R} \to \frac{\Vert X_i \Vert}{\sum_{j = 1, 2, \cdots, C} \Vert X_j \Vert} \in \mathcal{R}</script><p>其中$\Vert X_i \Vert$是第$i$通道的L2范数（为了考虑更深层中通道数量的增加，在实践中，我们还通过通道计数$C$缩放规范化值）。直观地，对于第$i$个通道，$\mathcal{N}(\Vert X_i \Vert) := \Vert X_i \Vert \in \mathcal{R} \to \frac{\Vert X_i \Vert}{\sum_{j = 1, 2, \cdots, C} \Vert X_j \Vert} \in \mathcal{R}$计算其相对于所有其他通道的相对重要性。与其他形式的归一化类似[42, 45, 68]，这一步通过相互抑制在渠道之间产生特征竞争。在表2b中，我们还研究了其他归一化函数的使用：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404154320071.png" alt="image-20240404154320071"></p><p>发现简单的分裂归一化效果最好，尽管标准化$\frac{\Vert X_i \Vert - \mu}{\sigma}$在应用于相同的L2范数聚合值时产生类似的结果。</p><p>最后，我们使用计算的特征归一化分数校准原始输入响应：</p><script type="math/tex; mode=display">X_i = X_i * \mathcal{N}(\mathcal{G}(X)_i) \in \mathcal{R}^{H \times W}</script><p>核心GRN单元非常容易实现，只需要三行代码，并且没有可学习的参数。GRN单元的伪码在算法1中：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404155317540.png" alt="image-20240404155317540"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gamma, beta: learnable affine transform parameters</span></span><br><span class="line"><span class="comment"># X: input of shape (N, H, W, C)</span></span><br><span class="line"></span><br><span class="line">gx = torch.norm(X, p=<span class="number">2</span>, dim=(<span class="number">1</span>,<span class="number">2</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">nx = gx / (gx.mean(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="number">1e-6</span>)</span><br><span class="line"><span class="keyword">return</span> gamma * (X * nx) + beta + X</span><br></pre></td></tr></table></figure><p>为了简化优化，我们添加了两个额外的可学习参数，$\gamma$和$\beta$，并将它们初始化为零。我们还在GRN层的输入和输出之间添加了残差连接。最终得到的GRN块为$X_i = \gamma <em> X_i </em> \mathcal{N}(\mathcal{G}(X)_i) + \beta + X_i$。这种设置允许GRN层最初执行恒等函数，并在训练期间逐渐适应。残差连接的重要性如表2c所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404154320071.png" alt="image-20240404154320071"></p><p><strong>ConvNeXt V2.</strong> 我们将GRN层合并到原始的ConvNeXt块中，如图5所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404155853899.png" alt="image-20240404155853899"></p><p>我们通过经验发现，当应用GRN时，LayerScale[65]变得不必要，并且可以删除。使用这种新的模块设计，我们创建了具有不同效率和容量的各种模型，我们将其称为ConvNeXt V2模型家族。这些模型的范围从轻量级（例如Atto[70]）到计算密集型（例如Huge）。详细的模型配置可以在附录中找到。</p><p><strong>Impact of GRN.</strong> 我们现在使用FCMAE框架对ConvNeXt V2进行预训练，并评估GRN的影响。从图3的可视化和图4的余弦距离分析中，我们可以观察到ConvNeXt V2有效地缓解了特征折叠问题。余弦距离值一直很高，表明特征多样性在各层之间保持不变。这种行为类似于MAE预训练的ViT模型[31]。总的来说，这表明ConvNeXt V2的学习行为可以类似于ViT，在类似的掩码图像预训练框架下。</p><p>接下来，我们评估微调性能。</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404160212348.png" alt="image-20240404160212348"></p><p>当配备GRN时，FCMAE预训练模型可以显著优于300 epoch监督模型。GRN通过增强特征多样性来提高表征质量，这在V1模型中是不存在的，但事实证明对于基于掩码的预训练至关重要。注意，这种改进是在不增加额外的参数开销或增加FLOPS的情况下实现的（附加的仿射参数$\frac{\gamma}{\beta}$可以忽略不计）。</p><p><strong>Relation to feature normalization methods.</strong> 其他归一化层[2, 41, 45, 67, 73]的表现是否与全局响应归一化（GRN）层一样好？在表2d中：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404154320071.png" alt="image-20240404154320071"></p><p>我们将GRN与三种广泛使用的归一化层进行了比较：局部响应归一化（LRN）[45]，批归一化（BN）[41]和层归一化（LN）[2]。我们观察到只有GRN可以显著优于监督基线。LRN缺乏全局上下文，因为它仅对比附近通道。BN沿批量轴进行空间归一化，这不适合屏蔽输入。LN通过全局均值和方差标准化隐式鼓励特征竞争，但效果不如GRN。</p><p><strong>Relation to feature gating methods.</strong> 另一种增强神经元间竞争的方法是使用动态特征门控方法[37, 56, 69, 72, 78]。在表2e中：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404154320071.png" alt="image-20240404154320071"></p><p>我们将GRN与两个经典的门控层进行了比较：squeeze-and-excite（SE）[37]和convolutional block attention module（CBAM）[72]。SE侧重于通道门控，而CBAM侧重于空间门控。这两个模块都可以增加单个通道的对比度，类似于GRN的功能。GRN更简单、更有效，因为它不需要额外的参数层（如MLP）。</p><p><strong>The role of GRN in pre-training/fine-tuning.</strong> 最后，我们研究了GRN在预训练和微调中的重要性。我们在表2f中给出了结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404154320071.png" alt="image-20240404154320071"></p><p>其中我们从微调中删除GRN，或者仅在微调时添加新初始化的GRN。无论哪种方式，我们都观察到显著的性能下降，这表明在预训练和微调中保持GRN是重要的。</p><h2 id="5-ImageNet-Experiments"><a href="#5-ImageNet-Experiments" class="headerlink" title="5. ImageNet Experiments"></a>5. ImageNet Experiments</h2><p>在本节中，我们提出并分析了两个关键的建议，FCMAE预训练框架和ConvNeXt V2架构，它们被共同设计以使基于掩码的自监督预训练成功。我们展示了这些设计协同良好，并为将模型缩放到各种尺寸提供了坚实的基础。此外，我们通过实验将我们的方法与以前的掩码图像建模方法进行了比较。不仅如此，我们展示了我们最大的ConvNeXt V2 Huge模型，使用FCMAE框架进行预训练并在ImageNet-22K数据集上进行微调，可以在ImageNet-1K数据集上实现88.9%的top-1精度，仅使用公开可用的数据。</p><p><strong>Co-design matters.</strong> 在本文中，我们进行了一项独特的研究，包括共同设计自监督学习框架（FCMAE）和模型架构改进（GRN层），通过实证研究他们的学习行为。表3中的结果表明了这种方法的重要性：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404170509595.png" alt="image-20240404170509595"></p><p>我们发现，在不修改模型架构的情况下使用FCMAE框架对表示学习质量的影响有限。同样，在监督设置下，新的GRN层对性能的影响相当小。然而，两者的结合会显著改善微调性能。这支持了模型和学习框架应该一起考虑的观点，特别是当涉及到自我监督学习时。</p><p><strong>Model scaling.</strong> 在本研究中，我们评估了8种不同尺寸的模型，从低容量3.7M Atto模型到高容量650M Huge模型。我们使用提出的FCMAE框架对这些模型进行预训练，并将微调结果与完全监督的模型进行比较。</p><p>结果如图1所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404170927174.png" alt="image-20240404170927174"></p><p>展示了强大的模型缩放行为，在所有模型大小的监督基线上始终如一地提高了性能。这是第一次在如此广泛的模型范围内证明掩码图像建模的好处，无论是在有效性还是效率方面。完整的表格结果可在附录中找到。</p><p><strong>Comparisons with previous methods.</strong> 我们将我们的方法与之前的掩码自编码器方法[3, 31, 77]进行了比较，这些方法都是为基于transformer的模型设计的。结果总结在表4中：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404171108800.png" alt="image-20240404171108800"></p><p>我们的框架在所有模型尺寸上都优于用SimMIM[77]预训练的Swintransformer。与使用MAE[31]预训练的普通ViT相比，尽管使用的参数少得多(198M vs 307M)，但我们的方法在大型模型体系中的表现相似。然而，在庞大的模型体系中，我们的方法略显落后。这可能是因为一个巨大的ViT模型可以从自我监督的预训练中获益更多。正如我们接下来将看到的，这个差距可以通过额外的中间微调来缩小。</p><p><strong>ImageNet-22K intermediate fine-tuning.</strong> 我们还展示了ImageNet-22K中间微调结果[3]。训练过程包括三个步骤：</p><ol><li>FCMAE预训练；</li><li>ImageNet-22K微调；</li><li>ImageNet-1K微调。</li></ol><p>我们使用$384 \times 384$分辨率的图像进行预训练和微调[38]。我们将我们的结果与最先进的架构设计进行了比较，包括基于卷积的[52, 64]，基于transformer的[22]和混合设计[20, 66]。所有这些结果都使用ImageNet-22K监督标签进行训练。结果总结在表5中：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404171500940.png" alt="image-20240404171500940"></p><p>我们的方法使用基于卷积的架构，仅使用公开可用的数据（即ImageNet-1K和ImageNet-22K）达到了新的最先进的精度。</p><h2 id="6-Transfer-Learning-Experiments"><a href="#6-Transfer-Learning-Experiments" class="headerlink" title="6. Transfer Learning Experiments"></a>6. Transfer Learning Experiments</h2><p>我们现在对迁移学习性能进行基准测试。首先，我们评估共同设计的影响，即比较ConvNeXt V1 + supervised与ConvNeXt V2 + FCMAE。我们还直接将我们的方法与SimMIM预训练的Swintransformer模型进行了比较[77]。训练和测试的细节在附录中提供。</p><p><strong>Object detection and segmentation on COCO.</strong> 我们在COCO数据集[49]上微调Mask R-CNN[33]，并在COCO val2017数据集上报告检测$\text{mAP}^\text{box}$和分割$\text{mAP}^\text{mask}$。结果如表6所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404172848277.png" alt="image-20240404172848277"></p><p>随着我们的建议得到实施，我们看到情况在逐步改善。从V1到V2，新引入了GRN层，增强了性能。在此基础上，当从监督学习过渡到基于FCMAE的自监督学习时，模型进一步受益于更好的初始化。当两者同时应用时，可以获得最佳性能。此外，我们的最终提案，在FCMAE上预训练的ConvNeXt V2，在所有模型尺寸上都优于Swintransformer，在巨大的模型范围内实现了最大的差距。</p><p><strong>Semantic segmentation on ADE20K.</strong> 综上所述，我们使用UperNet框架[74]对ADE20K[82]语义分割任务进行了实验。我们的结果显示出与目标检测实验相似的趋势，并且我们的最终模型比V1监督的对应模型显著改进。它在base和large模型系统中的性能也与Swintransformer相当，但在大型模型系统中优于Swin。</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404190657722.png" alt="image-20240404190657722"></p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>在本文中，我们介绍了一个新的ConvNet模型家族，称为ConvNeXt V2，它涵盖了更广泛的复杂性。虽然体系结构的变化很小，但它是专门为更适合自监督学习而设计的。使用我们的全卷积掩码自动编码器进行预训练，我们可以显著提高纯卷积神经网络在各种下游任务中的性能，包括ImageNet分类、COCO对象检测和ADE20K分割。</p><h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>本附录提供了实现细节，包括模型配置，预训练和微调配方，以及FCMAE预训练的稀疏和密集编码方法（参见$\S$A）。在$\S$B中，我们在ImageNet 1K和22K上对ConvNeXt V1和V2进行了完整的微调精度比较。在$\S$C中，我们使用类选择性指数对稀疏编码和一般特征分析的效率进行了分析。最后，在$\S$D中，我们对掩蔽比和GRN成分分析进行了额外的消融研究。我们还比较了FCMAE（掩码图像建模）和MoCo V3（对比学习）。</p><h3 id="A-Implementation-Details"><a href="#A-Implementation-Details" class="headerlink" title="A. Implementation Details"></a>A. Implementation Details</h3><h4 id="A-1-ConvNeXt-V2-model-configurations"><a href="#A-1-ConvNeXt-V2-model-configurations" class="headerlink" title="A.1. ConvNeXt V2 model configurations"></a>A.1. ConvNeXt V2 model configurations</h4><p>基本模型，即Tiny（28M）， Base（89M）和Large（198M），遵循与ConvNeXt V1[52]相同的stage、block（B）和通道（C）设置配置。</p><ul><li>ConvNeXt V2-T: C = 96, B = (3, 3, 9, 3)</li><li>ConvNeXt V2-B: C = 128, B = (3, 3, 27, 3)</li><li>ConvNeXt V2-L: C = 192, B = (3, 3, 27, 3)</li></ul><p>给定上述相同的定义，我们缩放模型以提供广泛的模型大小范围，针对多种场景。首先，为了得到有效的模型，我们按如下比例缩小：</p><ul><li>ConvNeXt V2-A: C = 40, B = (2, 2, 6, 2)</li><li>ConvNeXt V2-F: C = 48, B = (2, 2, 6, 2)</li><li>ConvNeXt V2-P: C = 64, B = (2, 2, 6, 2)</li><li>ConvNeXt V2-N: C = 80, B = (2, 2, 8, 2)</li></ul><p>A、F、P、N分别表示Atto（3.7M）、Femto（5.2M）、Pico（9.1M）和Nano（15.6M）模型，最初设计于[70]。接下来，为了引入大容量的变体，我们按以下方式进行扩展：</p><ul><li>ConvNeXt V2-H: C = 352, B = (3, 3, 27, 3)</li></ul><p>H为本文新提出的Huge（659M）模型。</p><h4 id="A-2-ImageNet-Experiments"><a href="#A-2-ImageNet-Experiments" class="headerlink" title="A.2. ImageNet Experiments"></a>A.2. ImageNet Experiments</h4><p><strong>Pre-training.</strong> 所有模型共享相同的预训练设置，如表8所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404191554554.png" alt="image-20240404191554554"></p><p>我们使用线性$lr$缩放规则[26]：$lr = \frac{base_lr \times \text{batchsize}}{256}$。</p><p><strong>ImageNet-1K fine-tuning.</strong> 由于学习能力因模型大小而异，我们对每个模型采用不同的微调配方。我们将它们总结在表9、10和11中：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404191829977.png" alt="image-20240404191829977"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404191847071.png" alt="image-20240404191847071"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404191907357.png" alt="image-20240404191907357"></p><p>我们看到更长的微调周期有助于小模型。在这项工作中，我们采用了两种不同的学习率层衰减策略：</p><ol><li>group-wise[52]，我们将三个连续的层视为单个“层”，并对它们使用相同的衰减值；</li><li>layer-wise[3]，我们为每层分配不同的值，两者都遵循标准衰减规则。</li></ol><p>默认是layer-wise策略，但是我们将group-wise策略应用于Base和Large模型。</p><p><strong>ImageNet-22K intermediate fine-tuning.</strong> 我们使用FCMAE预训练的ConvNeXt模型进行ImageNet-22K中间微调。我们使用nano，tiny，base，large和huge模型。表12和表13总结了这些设置：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404191930499.png" alt="image-20240404191930499"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404191945731.png" alt="image-20240404191945731"></p><p>类似地，对于小模型使用更大的layer-wise学习率衰减值是有帮助的。</p><p><strong>Sparse encoding implementations.</strong> 我们提出了两种可能的实现来实现FCMAE预训练：</p><ol><li>使用外部库支持的稀疏卷积[15, 27, 28]进行稀疏编码[15, 18]；</li><li>用掩码密集卷积模拟稀疏编码，通过在标准卷积操作前后分别应用二进制掩码可以很容易地实现。</li></ol><p>由于它们产生数字上相同的输出，因此可以根据不同的用例采用两者。在这项工作中，我们在GPU环境下采用稀疏编码，其中我们使用MinkowskiEngine库[15]和PyTorch框架[57]；我们使用Jax[5]在TPU加速器上使用基于密集掩码转换的编码。主论文中的实验都是在TPU（v3-256）pods上进行的，但是我们发布了一个PyTorch的复制。</p><h4 id="A-3-Object-detection-and-segmentation-on-COCO"><a href="#A-3-Object-detection-and-segmentation-on-COCO" class="headerlink" title="A.3. Object detection and segmentation on COCO"></a>A.3. Object detection and segmentation on COCO</h4><p>对于COCO实验，我们使用MMDetection[10]工具箱和ImageNet-1K预训练的最终模型权重作为网络初始化。所有模型都以3倍的时间表（36个epoch）和batch size为<code>32</code>进行训练。我们使用一个AdamW优化器[54]，其学习率为<code>1e-4</code>，权值衰减为<code>0.05</code>，扫描层学习率衰减为$\{0.9, 0.95\}$，随机深度率衰减为$\{0.2, 0.3, 0.4, 0.5\}$。我们采用大规模抖动增强[24]（$1024 \times 1024$分辨率，尺度范围$[0.1, 2.0]$）。我们在推理期间使用soft-NMS[4]进行单尺度测试。</p><h4 id="A-4-Semantic-segmentation-in-ADE20K"><a href="#A-4-Semantic-segmentation-in-ADE20K" class="headerlink" title="A.4. Semantic segmentation in ADE20K"></a>A.4. Semantic segmentation in ADE20K</h4><p>对于ADE20K实验，我们使用MMSegmentation[17]工具箱。我们使用具有以下超参数的AdamW优化器[54]：权重衰减为<code>0.05</code>，batch size为<code>16</code>，layer-wise衰减率为$\{0.8, 0.9\}$，学习率为$\{1\text{e-}4, 2\text{e-}4, 3\text{e-}4\}$，随机深度率$\{0.1, 0.2, 0.3, 0.4\}$。所有模型都训练了160K次迭代，输入分辨率为$512 \times 512$。在推理中，使用$512 \times 2048$的分辨率为$[0.75, 0.875, 1.0, 1.125, 1.25]$的多尺度检验。</p><p>与[77]类似，我们在ImageNet-1K上进行监督微调后使用模型权值初始化分割模型，因为我们发现它的性能优于直接使用自监督预训练权值。</p><h3 id="B-Complete-comparisons-with-V1"><a href="#B-Complete-comparisons-with-V1" class="headerlink" title="B. Complete comparisons with V1"></a>B. Complete comparisons with V1</h3><p>在表14和表15中，我们详细介绍了ConvNeXt V1[52, 70]和V2之间的实验水平比较：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404193157266.png" alt="image-20240404193157266"></p><p>特别是，表14显示了使用八个模型的ImageNet-1K微调结果：Atto、Femto、Nano、Pico、Tiny、Base、Large和Huge，范围从低计算（Atto，3.7M）到大容量模型（Huge，660M）。我们在所有模型中都看到了一致和显著的改进。当结构从V1升级到V2并使用自监督学习框架FCMAE时，性能达到最佳，证明了协同设计的有效性。</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404193233745.png" alt="image-20240404193233745"></p><p>在表15中，我们给出了ImageNet-22K中间微调结果。预训练和微调过程包括三个步骤：</p><ol><li>FCMAE预训练；</li><li>ImageNet-22K微调；</li><li>ImageNet-1K微调。</li></ol><p>在这里，我们专注于五种V2模型：Nano、Tiny、Base、Large和Huge。相比V1版本，我们看到了持续的改进。特别是，V2 Base（86.8%/87.7%）和Large（87.3%/88.2%）模型的性能优于V1的下一级模型尺寸，即Large（86.6%/87.5%）和XLarge（87.0%/87.8%）模型。V2 Huge模型也实现了一个新的最先进的性能为88.9%。我们的建议表明，纯卷积模型也可以通过基于掩码的预训练成为强大的、可扩展的视觉学习器。</p><h3 id="C-Further-Analyses"><a href="#C-Further-Analyses" class="headerlink" title="C. Further Analyses"></a>C. Further Analyses</h3><p><strong>Sparse encoding efficiency.</strong> 我们的FCMAE框架的关键设计选择之一是在预训练期间使用稀疏卷积[15, 27, 28]。其主要目的是阻断来自掩码区域的信息流，方便掩码自编码器的预训练。作为副产品，它还在预训练期间提供了改进的计算和内存效率，因为内核仅适用于可见像素。然而，我们注意到稀疏卷积库[15, 18]并没有针对现代硬件进行高度优化，并且实现的效率通常取决于实践中使用的框架[1, 5, 57]。</p><p>为了更好地理解使用稀疏卷积实现的实际预训练效率，我们使用Minkowski Engine v0.5.4[15]和PyTorch[57]进行了基准测试。我们模拟了预训练掩码输入（图像大小$224 \times 224$，掩码比<code>0.6</code>，掩码大小$32 \times 32$），并比较了基于稀疏卷积和基于密集掩码卷积编码器之间的训练吞吐量（图像/s）和最大GPU内存使用量（G）。虽然结果可能因实验环境而有所不同（我们使用PyTorch V1.8.0，CUDA 11.1，CuDNN 8.2和NVIDIA RTX A6000 GPU），但我们观察到预训练效率适度增加，吞吐量平均增加1.3倍，最大内存使用量减少2倍。随着模型尺寸的增加，这种差距变得更加明显。</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404193902909.png" alt="image-20240404193902909"></p><p><strong>Class Selectivity Index.</strong> FCMAE预训练的ConvNeXt V2与V1相比具有鲜明的特征特征。为了理解这一点，我们对ConvNeXt V1和V2的FCMAE预训练权值进行了类选择性指数分析。类选择指数是衡量最高类条件平均活动与所有其他类条件平均活动之间差异的度量标准。最终的规范化值介于0和1之间，1表示过滤器仅对单个类激活，0表示过滤器对所有类统一激活。在图7中，我们使用每个残差块的输出绘制了模型中所有中间层的类选择性指数分布：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404194014921.png" alt="image-20240404194014921"></p><p>前期V1和V2的分布紧密匹配，但在较深层，如第3阶段第12层开始出现发散。随着层的加深，该图显示V2（双峰）倾向于包含比V1（单峰）更多的类通用特征。由于与类无关的特性更具可移植性，这将在下游任务中带来更好的微调性能。我们将更多的探索留给未来的研究。</p><h3 id="D-Additional-Experiments"><a href="#D-Additional-Experiments" class="headerlink" title="D. Additional Experiments"></a>D. Additional Experiments</h3><p><strong>GRN component analysis.</strong> 提出的全局关系网络（GRN）包括三个步骤：</p><ol><li>全局特征聚合；</li><li>特征归一化；</li><li>特征校准。</li></ol><p>本文主要论证了基于L2范数的聚合和分裂归一化的结合在实践中效果良好。表16使用ConvNeXt V2-Base作为编码器验证了这些组件的各自贡献：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404194210952.png" alt="image-20240404194210952"></p><p>当任何一个组件被丢弃时，性能都会显著下降，并且如果在特征归一化之前没有进行全局聚合，训练将变得不稳定。这支持了这两种操作协同工作以使GRN有效的想法。</p><p><strong>Masking ratios.</strong> 我们对掩码尺寸为$32 \times 32$的掩模比进行了超参数分析。如图8所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404194329282.png" alt="image-20240404194329282"></p><p>结果表明，掩蔽比在<code>0.5</code>到<code>0.7</code>范围内会产生最好的结果，掩蔽比为<code>0.6</code>可以提供最高的性能。</p><p><strong>Comparison with contrastive SSL.</strong> 在这项工作中，我们比较了两种主流的自监督学习（SSL）方法的性能：对比学习[8, 9, 12, 13, 14, 29, 32]和掩码图像建模[3, 31, 77]。具体来说，我们比较了MoCoV3[14]的端到端微调性能，MoCoV3[14]是目前最先进的对比学习方法，与我们提出的FCMAE框架使用相同的ConvNeXt V2-Base作为编码器。我们遵循每种方法的默认预训练和微调配方，并给出如下结果。</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404194537499.png" alt="image-20240404194537499"></p><p>我们使用300 epoch的监督学习基线作为参考。上表表明，FCMAE的表示质量优于MoCo V3，并且优于监督基线。这与最近的观察结果一致，即在端到端微调方面，掩膜图像建模比基于对比学习的SSL提供了更好的结果。在这项工作中，纯卷积神经网络也使这一成功成为可能。</p>]]></content>
    
    
    <summary type="html">ConvNeXt V2：使用掩码自动编码器共同设计和扩展ConvNet。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2023" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2023/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="CVPR" scheme="http://blog.karltan.com/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《NOPE：Novel Object Pose Estimation from a Single Image》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/"/>
    <id>http://blog.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/</id>
    <published>2024-03-28T04:00:00.000Z</published>
    <updated>2024-03-28T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NOPE-Novel-Object-Pose-Estimation-from-a-Single-Image"><a href="#NOPE-Novel-Object-Pose-Estimation-from-a-Single-Image" class="headerlink" title="NOPE: Novel Object Pose Estimation from a Single Image"></a>NOPE: Novel Object Pose Estimation from a Single Image</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/">论文笔记《NOPE：Novel Object Pose Estimation from a Single Image》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/137119067">论文笔记《NOPE: Novel Object Pose Estimation from a Single Image》-CSDN博客</a></p><p>论文链接：<a href="https://arxiv.org/abs/2303.13612">[2303.13612] NOPE: Novel Object Pose Estimation from a Single Image (arxiv.org)</a></p><p>代码链接：<a href="https://github.com/nv-nguyen/nope">nv-nguyen/nope: [CVPR 2024] PyTorch implementation of NOPE: Novel Object Pose Estimation from a Single Image (github.com)</a></p><p>项目链接：<a href="https://nv-nguyen.github.io/nope/">NOPE: Novel Object Pose Estimation from a Single Image (nv-nguyen.github.io)</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>由于需要3D模型的先验知识，且在面对新对象时，需要额外的训练时间，3D对象位姿估计的实用性在许多应用中仍然受到限制。为了解决这一限制，我们提出了一种方法，该方法将新对象的单个图像作为输入（在已知另一张图片的基础上），并在不事先了解对象的3D模型的情况下预测该对象在新图像中的相对位姿，并且不需要对新对象和类别进行训练。我们通过训练一个模型来直接预测物体周围视角的判别嵌入来实现这一点。这种预测是使用一个简单的U-Net架构来完成的，它的注意力和条件取决于期望的位姿，这产生了极快的推断。我们将我们的方法与最先进的方法进行比较，并表明它在准确性和鲁棒性方面都优于它们。我们的源代码可以在<a href="https://github.com/nv-nguyen/nope">https://github.com/nv-nguyen/nope</a>上公开获得。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>在过去十年中，物体的三维位姿估计在鲁棒性和准确性方面都取得了重大进展[12, 16, 33, 47, 57]。具体来说，对部分遮挡的鲁棒性已经有了相当大的提高[8, 29, 30]，并且通过使用域转移[1]、域随机化[14, 44, 48]和利用合成图像进行训练的自监督学习技术[46]，对大量真实带注释的训练图像的需求已经放松。</p><p>不幸的是，3D物体位姿估计的实用性在许多应用中仍然有限，包括机器人和增强现实。通常，现有的方法需要一个3D模型[15]，一个视频序列[6, 43]，或目标物体[58]的稀疏多幅图像（仅参考最近的作品），以及一个训练阶段。有几种技术旨在通过假设新对象属于已识别的类别[5, 51]，与之前训练过的示例（如T-LESS数据集[44]）有相似之处，或者表现出明显的角点[31]，从而避免重新训练的需要。</p><p>在本文中，我们介绍了一种称为NOPE(<strong>N</strong>ovel <strong>O</strong>bject <strong>P</strong>ose <strong>E</strong>stimation)的新对象位姿估计方法，该方法只需要新对象的单个图像即可预测该对象在任何新图像中的相对位姿，而不需要对象的3D模型并且没有对新对象进行训练。这是一项非常具有挑战性的任务，因为与[43, 58]中使用的多个视图相比，单个视图仅提供有关对象几何形状的有限信息。</p><p>为了实现这一目标，我们训练NOPE来生成物体的新视图。这些新视图可以看作带有相应位姿注释的“模板”。将这些模板与新的输入视图相<strong>匹配</strong>，可以让我们估计物体相对于初始视图的相对位姿。我们的方法与新视图合成的最新发展有关。然而，我们更进一步：受最近用于位姿估计的模板匹配工作的良好性能[28, 40]的激励，我们不是预测纯色图像，而是直接预测视图的判别嵌入。具体来说，我们的嵌入是通过U-Net架构将输入图像传递给新视图，并以新视图的所需位姿为条件。</p><p>本质上，与现有的新视图合成工作[24, 32]相比，对于我们的位姿估计任务，我们不需要创建对象的3D模型。这节省了大量的计算时间，因为使用单视图优化3D模型至少需要一个小时[62]或更长时间。相比之下，我们基于视图嵌入的直接推理方法非常快，在不到2秒的时间内处理图像。并且，它对未见过的实例或类别产生准确的位姿估计，平均准确率超过65%。我们的工作也与两幅图像之间的运动预测有关，如[27, 49]，它预测了两幅图像之间的相机位移。我们与[27]最接近的工作进行了比较，结果表明我们明显优于这种方法。</p><p>此外，我们表明，我们的方法在没有得到对象的3D模型，且只有一个单视图的情况下，可以识别位姿的模糊性，例如，对称[20]。</p><p>为此，我们估计查询的所有位姿的分布，随着位姿歧义越来越多，其峰值越来越少。图1描述了各种模糊和非模糊情况及其位姿分布：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328151442993.png" alt="image-20240328151442993"></p><p>总之，我们的主要贡献是，在新视图中，且只给出该物体的单一视图的情况下，我们可以有效和可靠地恢复一个未见过的物体的相对位姿。据我们所知，我们的方法是第一个仅从单一视图预测由于对称性和不可见物体的部分遮挡而导致的模糊性的方法。</p><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><p>在本节中，我们首先回顾实现新视图合成的各种方法。然后，我们将注意力转移到旨在实现泛化的位姿估计技术上。</p><h3 id="2-1-Novel-view-synthesis-from-a-single-image"><a href="#2-1-Novel-view-synthesis-from-a-single-image" class="headerlink" title="2.1. Novel view synthesis from a single image"></a>2.1. Novel view synthesis from a single image</h3><p>我们的方法生成判别特征视图，该视图以参考视图和视图之间的相对位姿为条件。这与NeRFs[25]的开创性工作有关，因为它执行新视图合成。虽然最近的进展已经提高了NeRFs的速度[26, 39, 55]，但我们的方法仍然要快几个数量级，因为它不需要创建完整的3D体积模型。此外，我们的方法只需要一个输入视图，而典型的NeRF设置需要大约50个视图。减少NeRF重建所需的视图数量仍然是一个活跃的研究领域，特别是在单视图场景中[24, 56]。</p><p>最近的研究[24, 62]通过利用二维扩散模型，利用稀疏视图集作为输入，通过NeRFs成功地生成了新的视图。对于图像，扩散模型的突破[7, 41]已经解锁了几个工作流程[35, 37, 38]。对于3D应用程序，DreamFusion[32]开创了分数蒸馏采样技术，允许使用2D扩散模型作为基于图像的损失，并通过可微分渲染在3D应用程序中进行利用。这使得以前使用基于CLIP的图像损失训练的任务得到了显著的改进[9, 10, 13, 17, 34, 50]。通过在分数蒸馏采样的基础上构建，在使用至少两个视图的基础上，SparseFusion[62]重构了一个NeRF场景，而同时期的一项工作RealFusion[24]从单个输入视图进行重构，尽管重构时间对于实时应用来说是不切实际的。我们的方法要快得多，因为我们不需要创建对象的3D表示。</p><p>与我们的工作最接近的是3DiM[52]，这项工作通过在位姿上调节扩散模型来生成物体的新视图。他们没有像DreamFusion那样利用2D基础扩散模型，而是专门针对此任务重新训练扩散模型。虽然他们还没有将他们的方法应用到基于模板的位姿估计中，但我们设计了这样一个基线并与之进行比较。我们发现扩散模型总是倾向于生成清晰的图像，即使有时会改变纹理或产生错误的细节，这也会阻碍基于模板的方法的性能。相比之下，我们的方法直接在嵌入空间而不是像素空间中生成新的视图，正如我们将在3.1节中演示的那样，这要有效得多。</p><p>最后，几种方法[22, 23]通过调节3D位姿的前馈神经网络来生成新颖的视图，我们也使用U-Net来实现这一点。我们与这些方法在速度上有一个共同的优势：这种前馈神经网络比目前的扩散模型快一到两个数量级。然而，我们执行位姿估计的方式是完全不同的。我们在基于模板的匹配方法[28]中使用新视图合成，而他们在基于回归的优化中使用它。在实践中，我们发现这些方法在有限数量的对象类别上工作得很好，并且我们观察到它们在评估新类别时的性能显著下降。</p><h3 id="2-2-Generalizable-object-pose-estimation"><a href="#2-2-Generalizable-object-pose-estimation" class="headerlink" title="2.2. Generalizable object pose estimation"></a>2.2. Generalizable object pose estimation</h3><p>尽管最近许多从图像中估计物体三维位姿的方法在效率和精度方面都取得了显着进步[2, 14, 30, 42, 44, 45, 53]，但大多数方法只能适用于已知物体，这意味着它们需要在每个新物体上重新训练，从而限制了它们的应用范围。</p><p>人们已经探索了几种技术来更好地推广到未见过物体的位姿估计，例如通用2D-3D对应关系[31]、基于能量的策略[58]、关键点匹配[43]或模板匹配[15, 18, 28, 40, 59]。尽管取得了重大进展，但这些方法要么需要精确的目标三维模型，要么依赖于来自不同视角的多幅带注释的参考图像。这些3D注释在实践中很难获得。相比之下，我们提出了一种既不使用目标的3D模型也不使用多视图注释的策略。更重要的是，我们的方法只需要一个参考图像就能预测出准确的位姿，并且不需要再训练就能推广到新的对象。</p><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><h3 id="3-1-Motivation"><a href="#3-1-Motivation" class="headerlink" title="3.1. Motivation"></a>3.1. Motivation</h3><p>为了激励我们的方法，我们评估了最先进的基于3D模型的图像生成方法（3DiM）[52]在位姿估计中的使用。3DiM是一种基于扩散的像素空间视图合成方法。为了将其应用于给定对象参考图像的位姿预测，我们尝试使用它从许多不同的视角生成一组新视图，并将对象的查询图像与这些视图在像素空间中进行匹配。由于生成的视图被标注了相应的位姿，这给了我们一个位姿估计。</p><p>如图2所示，3DiM生成的图像看起来非常逼真：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328155434429.png" alt="image-20240328155434429"></p><p>然而，恢复的位姿不是很准确。这可以部分解释为扩散可以“发明”干扰图像匹配的细节。这种方法的局限性将在我们的实验中得到进一步的实证证明（表1）：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328155647483.png" alt="image-20240328155647483"></p><p>这促使我们学习直接生成视图的判别表示。</p><p>在本节中，我们首先用一个基于新视图生成的实验来激励我们的方法。然后我们描述我们的架构，我们如何训练它，以及我们如何使用它进行位姿预测和识别位姿歧义。</p><h3 id="3-2-Framework"><a href="#3-2-Framework" class="headerlink" title="3.2. Framework"></a>3.2. Framework</h3><p>图3概述了我们的方法：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328160810385.png" alt="image-20240328160810385"></p><p>我们训练一个深度架构，使用第一组对象类别中的对象图像对来生成对象的新视图嵌入。在实践中，我们考虑用[28]的网络计算嵌入，因为它被证明可以产生模板匹配的鲁棒表示。为了生成这些嵌入，我们使用类似U-Net的网络，其位姿调节机制与3DiM[52]中引入的机制非常接近，并且与[36]中的文本到图像机制相关。</p><p>更准确地说，我们首先使用多层感知机（MLP）将参考视图中相对于物体位姿的期望相对位姿$\Delta R$转换为位姿嵌入。然后，我们使用交叉注意力将这个位姿嵌入到U-Net的每个阶段的特征图中，如[36]所述。</p><p><strong>Training.</strong> 在每次训练迭代中，我们构建一个由$N$对图像组成的批，其中包括一个参考图像和具有已知相对位姿的同一物体的另一个图像。U-Net模型以参考图像的嵌入为输入，并以相对位姿的嵌入为条件来预测第二图像的嵌入。我们通过最小化预测嵌入与查询图像嵌入之间的欧氏距离来共同优化U-Net和MLP。注意，我们冻结了[28]计算嵌入的网络，因为它已经被训练过了。</p><p>通过在不同的对象数据集上进行训练，这种架构可以很好地推广到新的未知对象类别，正如我们的结果所示。</p><h3 id="3-3-Pose-prediction"><a href="#3-3-Pose-prediction" class="headerlink" title="3.3. Pose prediction"></a>3.3. Pose prediction</h3><p><strong>Template matching.</strong> 一旦我们的架构得到训练，我们可以用它来生成新视图的嵌入：给定参考图像$\mathbf{I}_\mathrm{r}$和一组$N$个相对位姿$\mathcal{P} = (\Delta R_1, \Delta R_2, \cdots, \Delta R_N)$，我们可以得到一组相应的预测嵌入$(\mathbf{e}_1, \mathbf{e}_2, \cdots, \mathbf{e}_N)$。为了定义这些视角，我们遵循[28]中使用的方法：我们从一个正二十面体开始，将每个三角形递归地细分为四个较小的三角形两次，以获得342个最终视角。</p><p>最后，我们简单地执行最近邻搜索来确定嵌入最接近查询图像嵌入的参考点。请注意，这可以有效地完成[28]。</p><p><strong>Detecting pose ambiguities.</strong> 当物体具有对称性时，或者当可以消除模糊性的物体部分不可见时，就会出现位姿模糊性，如图1中的马克杯。通过考虑查询图像的嵌入与生成的嵌入之间的距离，我们不仅可以预测单个位姿，还可以识别给定参考视图和查询视图的所有其他可能的位姿。</p><p>更正式地，我们估计给定参考图像$I_r$和查询图像$I_q$的相对位姿$\Delta R$的概率为：</p><script type="math/tex; mode=display">p(\Delta R \mid I_r, I_q) = \frac{1}{Z}\frac{\mathbf{e}_{\Delta R} \cdot \mathbf{e}_q}{\Vert\mathbf{e}_{\Delta R}\Vert\Vert\mathbf{e}_q\Vert}</script><p>其中$\mathbf{e}_{\Delta R}$为U-Net根据参考图像$I_r$和相对位姿$\Delta R$预测的嵌入，$\mathbf{e}_q$为查询图像$I_q$的嵌入，$Z$为归一化因子。算子$\cdot$表示点积。</p><p>为了说明这一点，我们在图4中展示了三种不同类型的对称：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328164505558.png" alt="image-20240328164505558"></p><p>并可视化了相应的参考和查询图像对的位姿分布（未显示）。相似度得分高的区域数量与对称和构成歧义的数量是一致的：如果一个物体没有对称，则概率分布有一个明确的模式。对称物体的概率分布在旋转对称的情况下通常具有几个模态，甚至是一个连续的高概率区域。我们在第4节中提供了额外的定性结果。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><p>在本节中，我们首先在4.1节中描述我们的实验设置，然后在4.2节中将我们的方法与其他方法[21, 22, 27, 52]在可见和不可见对象类别上进行比较。第4.3节报告了对部分遮挡的鲁棒性的评估。在第4.4节中，我们进行了消融实验，以研究我们的方法在不同环境下的有效性，最后在第4.6节中讨论了我们的方法的失败案例。</p><h3 id="4-1-Experimental-setup"><a href="#4-1-Experimental-setup" class="headerlink" title="4.1. Experimental setup"></a>4.1. Experimental setup</h3><p><strong>Dataset.</strong> 据我们所知，我们是第一个解决当对象属于训练期间未见的类别时，从单个图像中估计对象位姿问题的方法：PIZZA[27]在DeepIM改进基准上进行评估，该基准由具有较小相对位姿的图像对组成；SSVE[22]和ViewNet[21]仅对训练期间看到的类别中的对象进行评估。</p><p>因此，我们必须创建一个新的基准来评估我们的方法。我们使用ShapeNet[3]中与FORGE[11]中相同的对象类别创建了一个数据集。对于训练集，我们从FORGE中所做的13个类别（飞机、长凳、橱柜、汽车、椅子、显示器、灯、扬声器、步枪、沙发、桌子、电话和船只）中的每个类别中随机选择1000个对象实例，结果总共有13,000个实例。我们构建两个独立的测试集进行评估。第一个测试集是“新实例”集，它包含50个新实例，这些实例在训练期间没有出现，但仍然来自用于训练的对象类别。第二个测试集是“新类别”集，包括从FORGE选出的10个未见过的类别（公共汽车，吉他，时钟，瓶子，火车，杯子，洗衣机，滑板，洗碗机和手枪）中选出的100个模型。对于每个3D模型，我们随机选择相机位姿生成五张参考图像和五张查询图像。我们使用BlenderProc[4]作为合成渲染引擎。</p><p>图5说明了用于训练我们架构的类别和用于测试它的类别：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328165217458.png" alt="image-20240328165217458"></p><p>测试集中类别的形状和外观与训练集中类别的形状和外观有很大的不同，因此构成了一个很好的测试集，可以泛化到未见过的类别。</p><p><strong>Metrics.</strong> 我们根据相对相机位姿误差报告了两个不同的指标，如[22]中所做的那样。具体来说，我们为测试集中的每个类别提供了跨实例的中位数位姿误差，以及当位姿误差$\le 30^\circ$时预测被视为正确的精度Acc 30。此外，我们给出了我们的方法对模板匹配检索到的前3和5个最近邻的结果。</p><p><strong>Baselines.</strong> 我们将我们的工作与之前所有旨在从单一视图预测位姿的方法（据我们所知）进行比较：PIZZA[27]是一种直接预测相对位姿的基于回归的方法，SSVE[22]和ViewNet[21]采用半监督和自监督技术，将视角估计视为使用条件生成的图像重建问题。</p><p>我们还将我们的方法与最近基于扩散的方法3DiM[52]进行了比较，后者生成像素级视图合成。由于3DiM最初只针对视图合成，而不是为3D对象位姿设计的，我们使用它来生成模板并执行最近邻搜索来估计3D对象位姿，如3.1节所述。为了使3DiM在与我们相同的环境中工作，我们重新训练它相对位姿条件调节而不是规范位姿条件调节。</p><p>只有PIZZA的代码可用。在撰写本文时，其他方法没有发布它们的代码，但是我们重新实现了它们。我们对“新实例”情况的数值结果与相应论文中报告的结果相似，这验证了我们的实现。我们使用[27]中的ResNet18主干网络来处理PIZZA、SSVE和ViewNet。</p><p>我们在分辨率为$256 \times 256$的输入图像上训练所有模型，但3DiM除外，我们使用$128 \times 128$的分辨率，因为3DiM在像素空间中执行视图合成，这需要更多的内存。我们的重新实现在对相同的数据进行评估时获得了与原始论文相似的性能，如表1所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328155647483.png" alt="image-20240328155647483"></p><p>它验证了我们的重新实现。我们的方法还使用[28]中的冻结编码器将输入图像编码为大小为$32 \times 32 \times 8$的嵌入。对于所有的方法，我们使用AdamW[19]，初始学习率为<code>1e-4</code>。每种方法平均需要在4GPU V100上进行10小时的训练。我们建议读者参阅补充材料以了解更多实现细节。</p><h3 id="4-2-Comparison-with-the-state-of-the-art"><a href="#4-2-Comparison-with-the-state-of-the-art" class="headerlink" title="4.2. Comparison with the state of the art"></a>4.2. Comparison with the state of the art</h3><p>表1总结了我们的方法与上述基线的比较结果。在Acc30和Median指标下，我们的方法始终达到最佳的整体性能，在Acc30和Median指标上的表现分别超过基线的10%和10%。特别是，虽然其他工作能在未见过的训练类别中的实例上产生合理的结果，但他们经常难以从未见过的类别中估计物体的3D位姿。相比之下，我们的方法在这种情况下效果很好，在未见过的类别上表现出更好的泛化能力。</p><p>图6显示了我们的方法在有对称性和没有对称性的未见类别上的一些可视化结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328171121023.png" alt="image-20240328171121023"></p><p>当存在对称轴时，我们的方法比基线产生更精确的3D位姿。</p><h3 id="4-3-Robustness-to-occlusions"><a href="#4-3-Robustness-to-occlusions" class="headerlink" title="4.3. Robustness to occlusions"></a>4.3. Robustness to occlusions</h3><p>为了评估我们的方法对遮挡的鲁棒性，我们在对象上的查询图像中添加了填充高斯噪声的随机矩形，以类似于随机擦除[60]的方式。我们改变矩形的大小，以覆盖对象bounding box的0%到25%的范围。图1和图6显示了几个示例。</p><p>表2比较了我们之前评估中表现最好的第二种方法PIZZA与我们的方法在不同遮挡率下的性能：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328171442950.png" alt="image-20240328171442950"></p><p>我们的方法即使在大遮挡下也保持鲁棒性，这要归功于用于匹配图像的鲁棒性判别嵌入。</p><p>在图6中，请注意我们的位姿概率保持在正确的最大值上，并且当它们存在时仍然显示出清晰的对称性。</p><h3 id="4-4-Ablation-study"><a href="#4-4-Ablation-study" class="headerlink" title="4.4. Ablation study"></a>4.4. Ablation study</h3><p><strong>Pose conditioning.</strong> 我们尝试了两种不同的位姿表示。3DiM首先从给定的3D位姿创建相机光线，并使用[25]的位置编码来获得逐像素的位姿条件。在我们的方法中，该表示在集成到U-Net的特征图之前，由MLP处理。我们对这种方法进行了实验，但发现与直接对旋转表示应用MLP相比，它并没有提高我们问题的性能。我们喜欢第二种方法，因为它简单。</p><p><strong>Rotation representation for the relative pose.</strong> 同样，我们尝试了不同的旋转表示：轴角、四元数和旋转-6D[61]。我们观察到所有这些表示都会产生类似的性能，因此我们在所有实验中都使用旋转-6D[61]。</p><h3 id="4-5-Runtime-analysis"><a href="#4-5-Runtime-analysis" class="headerlink" title="4.5. Runtime analysis"></a>4.5. Runtime analysis</h3><p>我们在表3中报告了NOPE和3DiM的运行时间：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328171754129.png" alt="image-20240328171754129"></p><p>我们的方法比3DiM要快得多，这要归功于我们的策略是用一个步骤而不是多个扩散步骤来预测新视点的嵌入。</p><h3 id="4-6-Failure-cases"><a href="#4-6-Failure-cases" class="headerlink" title="4.6. Failure cases"></a>4.6. Failure cases</h3><p>当对“公共汽车”和“吉他”类别进行评估时，所有方法都不能产生准确的结果，正如高中位数误差所表明的那样。经过目视检查（参见图7）：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328171915360.png" alt="image-20240328171915360"></p><p>似乎吉他类别的3D模型在某些视点下可能非常薄。此外，吉他和巴士类别都是“几乎对称的”，在某种意义上，只有小细节才能使位姿不模棱两可。</p><p>为了检查这是否真的是这两个类别表现不佳的原因，我们重新运行评估，将“吉他”类别的参考视图作为具有最大轮廓的视图，并将这两个类别视为具有180度对称。表4给出了这项新评价的结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328172225284.png" alt="image-20240328172225284"></p><p>度量标准明显更好。这表明，失败确实是由物体看起来非常薄的视图和“准对称性”引起的。</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>我们提出了NOPE，一种从单个图像中估计新物体位姿的方法。我们的实验表明，从单个视图中直接推断视图嵌入可以准确地估计物体位姿，甚至对于来自未知类别的物体，同时既不需要重新训练也不需要3D模型。此外，我们已经证明，我们遵循的模板匹配方法可以让我们估计许多对象出现的位姿歧义。</p>]]></content>
    
    
    <summary type="html">NOPE：一种新的单图像目标位姿估计方法。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2023" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2023/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="CVPR" scheme="http://blog.karltan.com/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Attention Is All You Need》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/"/>
    <id>http://blog.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/</id>
    <published>2024-02-09T02:00:00.000Z</published>
    <updated>2024-02-09T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/">论文笔记《Attention Is All You Need》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/136089101">论文笔记《Attention Is All You Need》-CSDN博客</a></p><p>论文链接：<a href="https://arxiv.org/abs/1706.03762">[1706.03762] Attention Is All You Need (arxiv.org)</a></p><p>代码链接：<a href="https://github.com/tensorflow/tensor2tensor">tensorflow/tensor2tensor: Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research. (github.com)</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>现阶段，占主导地位的序列转换模型基于复杂的循环或卷积神经网络，包括编码器和解码器。性能最好的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，它完全基于注意力机制，完全摒弃了递归和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优，同时具有更强的并行性，需要的训练时间显著减少。我们的模型在WMT 2014英德翻译任务中的BLEU值为28.4，比现有的最佳BLEU值（包括集合）提高了2。在WMT 2014英法翻译任务中，我们的模型在8个GPU上经过3.5天的训练后，建立了一个新的SOTA单模型，其BLEU得分为41.0，这只是文献中最好的模型的一小部分。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>循环神经网络，特别是长短期记忆[12]和门控循环[7]神经网络，已被牢固地确立为序列建模和转换问题（例如语言建模和机器翻译）中最先进的方法[29, 2, 5]。此后，人们做出了许多努力，不断突破循环语言模型和编码器-解码器架构的边界[31, 21, 13]。</p><p>循环模型通常根据输入和输出序列的符号位置进行计算。将位置与计算时间中的步骤对齐，它们生成一个隐藏状态序列$h_t$，作为前一个隐藏状态$h_{t − 1}$和位置$t$的输入的函数。这种固有的顺序性妨碍了训练样例中的并行化，这在较长的序列长度时变得非常关键，因为内存限制限制了样例之间的批处理。最近的工作通过分解技巧[18]和条件计算[26]在计算效率上取得了显著的提高，同时也提高了后者的模型性能。然而，顺序计算的基本约束仍然存在。</p><p>注意力机制已经成为各种任务中引人注目的序列建模和转换模型的组成部分，允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离[2, 16]。然而，除了少数情况外[22]，这种注意力机制都是与循环网络结合使用的。</p><p>在本工作中，我们提出了Transformer，它是一种模型架构，避免了递归，完全依赖于注意力机制来绘制输入和输出之间的全局依赖关系。Transformer支持更多的并行化，在8个P100 GPU上经过12个小时的训练后，可以在翻译质量上达到一个新的水平。</p><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><p>减少顺序计算的目标也构成了Extended Neural GPU[20]、ByteNet[15]和ConvS2S[8]的基础，它们都使用卷积神经网络作为基本的构建块，并行计算所有输入和输出位置的隐藏表示。在这些模型中，将来自两个任意输入或输出位置的信号关联起来所需的操作数量，按照位置之间的距离增长，对ConvS2S来说是线性增长，对ByteNet来说是对数增长。这使得学习遥远位置[11]之间的依赖关系变得更加困难。在Transformer中，这被减少为一个固定数量的操作，尽管因为平均注意力加权位置的影响，有效分辨率会降低，所以我们会用3.2节中描述的多头注意力抵消这一影响。</p><p>自注意力，有时也被称为内部注意力，是一种将单个序列的不同位置联系起来以计算序列的表示形式的注意力机制。自注意力已成功地应用于各种任务，包括阅读理解、抽象摘要、文本蕴涵和学习任务独立的句子表征[4, 22, 23, 19]。</p><p>端到端记忆网络基于循环注意力机制而不是序列对齐循环，并且在简单语言问答和语言建模任务[28]中表现良好。</p><p>然而，据我们所知，Transformer是第一个完全依赖自注意力来计算其输入和输出表示而不使用序列对齐RNN或卷积的转换模型。在接下来的章节中，我们将描述Transformer，自注意力的动机并讨论其相对于[14, 15]和[8]等模型的优势。</p><h2 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3 Model Architecture"></a>3 Model Architecture</h2><p>大多数具有竞争力的神经序列转换模型都有编码器-解码器结构[5, 2, 29]。这里，编码器将符号表示的输入序列$(x_1, \cdots, x_n)$映射到连续表示序列$\mathbf{z} = (z_1, \cdots, z_n)$，给定$\mathbf{z}$，解码器每次生成一个元素的符号输出序列$(y_1, \cdots, y_m)$。在每个步骤中，模型都是自回归的[9]，在生成下一个步骤时，使用之前生成的符号作为附加输入。</p><p>Transformer遵循这种整体架构，为编码器和解码器使用堆叠的自注意力层和逐点完全连接的层，分别如图1的左、右两部分所示。</p><p><img src="https://img.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/image-20240209112650024.png" alt="image-20240209112650024"></p><h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><p><strong>Encoder：</strong>编码器由$N = 6$个相同的层堆叠而成。每个层有两个子层。第一个子层是多头自注意力机制，第二个子层是简单的、位置明智的全连接前馈网络。我们在每个子层都使用了一个残差连接[10]，然后是层归一化[1]。也就是说，每个子层的输出都是$\text{LayerNorm}(x + \text{Sublayer}(x))$，其中$\text{Sublayer}(x)$是子层本身实现的函数。为了便于这些残差连接，模型中的所有子层以及嵌入层都产生了维度为$d_{\text{model}} = 512$的输出。</p><p><strong>Decoder：</strong>解码器也由$N = 6$个相同的层堆叠而成。除了每个编码器层中的两个子层外，解码器还插入了第三个子层，该子层对编码器的输出执行多头注意力。与编码器类似，我们在每个子层周围使用残差连接，然后进行层归一化。我们还修改了解码器中的自注意力子层，以防止位置对后续位置的影响。这种掩蔽，结合输出嵌入偏移一个位置的事实，确保了位置$i$的预测只能依赖于位置小于$i$的已知输出。</p><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。输出是作为值的加权和计算的，其中分配给每个值的权重是通过查询与相应键的兼容性函数计算的。</p><h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p>我们称我们的特别注意力为“缩放的点积注意力”（图2）。</p><p><img src="https://img.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/image-20240209115434068.png" alt="image-20240209115434068"></p><p>输入由维度$d_k$的查询、键和维度$d_v$的值组成。我们计算查询与所有键的点积，每个键除以$\sqrt{d_k}$，并应用softmax函数来获得值的权重。</p><p>在实践中，我们同时计算一组查询的注意力函数，这些查询被打包成一个矩阵$Q$。键和值也打包成矩阵$K$和$V$，我们计算输出矩阵如下：</p><script type="math/tex; mode=display">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</script><p>最常用的两个注意力函数是加性注意力[2]和点积（乘法）注意力。点积注意力与我们的算法相同，除了比例因子为$\frac{1}{\sqrt{d_k}}$。加性注意力使用带有单个隐藏层的前馈网络计算兼容性函数。虽然两者在理论上的复杂性相似，但点积注意力在实践中要快得多，而且空间效率更高，因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>当$d_k$值较小时，两种机制表现相似，当$d_k$值较大时，不需缩放的加性注意力优于点积注意力[3]。我们猜想，当$d_k$值较大时，点积的幅度会变大，从而使softmax函数进入其梯度极小的区域。为了抵消这种影响，我们将点积乘以$\frac{1}{\sqrt{d_k}}$。</p><h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>我们发现，将查询、键和值分别用不同的、学习过的线性投影投影到$d_k$、$d_k$和$d_v$维$h$次后，比用单个维度为$d_{\text{model}}$的注意力函数更有效。然后，在查询、键和值的每一个投影版本上，我们并行执行注意力函数，产生$d_v$维的输出值。这些输出被连接起来并再次投影，最终得到图2所示的值。</p><p>多头注意力允许模型在不同位置共同注意来自不同表示子空间的信息。对于单一注意力头，后续的平均操作就会抑制这一点。</p><script type="math/tex; mode=display">\begin{aligned}\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \cdots, \text{head}_h)W^O \\\text{where } \text{head}_\text{i} &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\end{aligned}</script><p>其中投影是参数矩阵$W_i^Q \in \mathbb{R}^{d_\text{model} \times d_k}, W_i^K \in \mathbb{R}^{d_\text{model} \times d_k}, W_i^V \in \mathbb{R}^{d_\text{model} \times d_v}$，$W^O \in \mathbb{R}^{hd_v \times d_\text{model}}$。</p><p>在这项工作中，我们采用了$h = 8$个并行注意力层或头。对于每一个注意力层或头，我们使用$d_k = d_v = \frac{d_\text{model}}{h} = 64$。由于每个头的维度减小了，总的计算成本与全维度的单头注意力相似。</p><h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><p>Transformer以三种不同的方式使用多头注意力：</p><ul><li>在“编码器-解码器注意力”层中，查询来自先前的解码器层，而记忆键和值来自编码器的输出。这使得解码器中的每个位置都能处理输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意力机制，如[31, 2, 8]。</li><li>编码器包含自注意力层。在自注意力层中，所有的键、值和查询都来自同一个地方，在本例中，即编码器中前一层的输出。编码器中的每个位置可以处理所述编码器的前一层中的所有位置。</li><li>类似地，解码器中的自注意力层允许解码器中的每个位置注意到解码器中的所有位置直至并包括该位置。我们需要防止信息在解码器中向左流动，以保持自回归特性。我们通过屏蔽（设置为$-\infty$）softmax输入中与非法连接对应的所有值来实现缩放点乘注意力。参见图2。</li></ul><h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed-Forward Networks"></a>3.3 Position-wise Feed-Forward Networks</h3><p>除了注意力子层外，我们的编码器和解码器中的每一层都包含一个全连接的前馈网络，该网络分别和相同地应用于每个位置。这包括两个线性转换，中间有一个ReLU激活。</p><script type="math/tex; mode=display">\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2</script><p>虽然在不同的位置上线性转换是相同的，但它们在不同的层中使用不同的参数。另一种描述它的方法是两个卷积，卷积核大小为1。输入输出维数为$d_{model} = 512$，内层维数为$d_{ff} = 2048$。</p><h3 id="3-4-Embeddings-and-Softmax"><a href="#3-4-Embeddings-and-Softmax" class="headerlink" title="3.4 Embeddings and Softmax"></a>3.4 Embeddings and Softmax</h3><p>与其他序列转换模型类似，我们使用学习过的嵌入将输入token和输出token转换为维数为$d_\text{model}$的向量。我们还使用通常学到的线性变换和softmax函数将解码器输出转换为预测的下一token概率。在我们的模型中，我们共享两个嵌入层之间的权值矩阵和pre-softmax线性变换，类似于[24]。在嵌入层中，我们将这些权重乘以$\sqrt{d_\text{model}}$。</p><h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a>3.5 Positional Encoding</h3><p>由于我们的模型不包含递归和卷积，为了让模型利用序列的顺序，我们必须注入一些关于序列中token的相对或绝对位置的信息。为此，我们将“位置编码”添加到编码器和解码器底部的输入嵌入中。位置编码与嵌入具有相同的维度$d_\text{model}$，因此可以将两者相加。有许多位置编码的选择，比如可学习的位置编码和固定的位置编码[8]。</p><p>在这项工作中，我们使用不同频率的正弦和余弦函数：</p><script type="math/tex; mode=display">\begin{aligned}PE_{(pos, 2i)} &= \sin(\frac{pos}{10000^{\frac{2i}{d_\text{model}}}}) \\PE_{(pos, 2i + 1)} &= \cos(\frac{pos}{10000^{\frac{2i}{d_\text{model}}}})\end{aligned}</script><p>其中$pos$是位置，$i$是维数。也就是说，位置编码的每个维度都对应一个正弦信号。波长从$2\pi$到$10000 \cdot 2\pi$，呈几何级数。我们选择这个函数是因为我们假设它可以让模型很容易通过相对位置进行学习，因为对于任何固定偏移量$k$，$PE_{pos + k}$可以表示为$PE_{pos}$的线性函数。</p><p>我们还使用学习过的位置嵌入[8]进行了实验，发现两个版本产生了几乎相同的结果（见表3行(E)）。我们选择正弦版本是因为它可以让模型外推到比训练中遇到的序列更长的序列长度。</p><h2 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4 Why Self-Attention"></a>4 Why Self-Attention</h2><p>在本节中，我们将自注意力层的各个方面与循环层和卷积层进行比较，这些层通常用于将符号表示的一个可变长度序列$(x_1, \cdots, x_n)$映射到另一个等长度序列$(z_1, \cdots, z_n)$，其中$x_i, z_i \in \mathbb{R}^d$，例如典型序列转换编码器或解码器中的隐藏层。关于使用自注意力的动机，我们考虑三个需求。</p><p>一个是每一层的总计算复杂度。另一个是可以并行化的计算量，由所需的最小顺序操作数来衡量。</p><p>第三个是网络中远程依赖关系之间的路径长度。在许多序列转换任务中，学习长期依赖是一个关键的挑战。影响学习这种依赖关系的能力的一个关键因素是信号在网络中必须穿越的前向和后向路径的长度。输入和输出序列中任意位置组合之间的路径越短，学习远程依赖关系[11]就越容易。因此，我们也比较了由不同层类型组成的网络中，任意两个输入和输出位置之间的最大路径长度。</p><p>如表1所示，自注意力层用固定数量的顺序执行操作连接所有位置，而循环层需要$O(n)$个顺序操作。在计算复杂度方面，当序列长度$n$小于表示维数$d$时，自注意力层比循环层更快，这是最常用的情况，在最先进的机器翻译模型中使用的句子表示，如词块[31]和字节对[25]表示。</p><p><img src="https://img.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/image-20240209132432948.png" alt="image-20240209132432948"></p><p>为了提高涉及非常长的序列的任务的计算性能，可以将自注意力限制在只考虑输入序列中大小为$r$的以各自输出位置为中心的邻域。这将增加最大路径长度到$O(\frac{n}{r})$。我们计划在未来的工作中进一步研究这种方法。</p><p>核宽度为$k &lt; n$的单个卷积层并不连接所有的输入和输出位置对。在相邻核的情况下，这样做需要$O(\frac{n}{k})$个卷积层的堆叠，在扩张卷积[15]的情况下，需要$O(\log_k(n))$个卷积层的堆叠，增加网络中任意两个位置之间的最长路径的长度。卷积层通常比递归层贵$k$倍。而可分离卷积[6]则大大降低了卷积的复杂度，达到$O(k \cdot n \cdot d + n \cdot d^2)$。然而，即使在$k = n$的情况下，可分离卷积的复杂性也等于自注意力层和点前馈层的结合，这是我们在模型中采用的方法。</p><p>作为附加的好处，自注意力可以产生更多可解释的模型。我们检查了我们模型中的注意力分布，并在附录中展示和讨论了一些例子。不仅个别注意力头清楚地学会了执行不同的任务，许多注意力头似乎表现出与句子的句法和语义结构有关的行为。</p><h2 id="5-Training"><a href="#5-Training" class="headerlink" title="5 Training"></a>5 Training</h2><p>本节描述了我们模型的训练方法。</p><h3 id="5-1-Training-Data-and-Batching"><a href="#5-1-Training-Data-and-Batching" class="headerlink" title="5.1 Training Data and Batching"></a>5.1 Training Data and Batching</h3><p>我们使用由450万对句子组成的WMT 2014英德标准数据集进行训练。使用[3]编码的字节对对句子进行编码，该编码具有大约37000个标记的共享源目标词汇表。对于英法两种语言，我们使用更大的WMT 2014英法数据集，其中包含3600万个句子，并将token分解为32000个单词块词汇[31]。句子对按照近似的序列长度进行分组。每个训练批包含一组句子对，其中包含大约25000个源标记和25000个目标标记。</p><h3 id="5-2-Hardware-and-Schedule"><a href="#5-2-Hardware-and-Schedule" class="headerlink" title="5.2 Hardware and Schedule"></a>5.2 Hardware and Schedule</h3><p>我们在一台装有8个NVIDIA P100 GPU的机器上训练我们的模型。对于使用本文中描述的超参数的基本模型，每个训练步骤大约需要0.4秒。我们对基本模型进行了总共10万步或12小时的训练。对于我们的大型模型（如表3所示），步长为1.0秒。这些大型模型被训练了30万步（3.5天）。</p><h3 id="5-3-Optimizer"><a href="#5-3-Optimizer" class="headerlink" title="5.3 Optimizer"></a>5.3 Optimizer</h3><p>我们使用了Adam优化器[17]，其中$\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-9}$。根据公式，我们在训练过程中改变学习率：</p><script type="math/tex; mode=display">lrate = d_{\text{model}}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})</script><p>这对应于第一个$warmup_steps$训练步骤的学习速率线性增加，然后与步骤数的平方根倒数成比例减少。我们使用$warmup_steps = 4000$。</p><h3 id="5-4-Regularization"><a href="#5-4-Regularization" class="headerlink" title="5.4 Regularization"></a>5.4 Regularization</h3><p>在训练过程中，我们采用了三种正则化方法：</p><p><strong>Residual Dropout</strong>，我们对每个子层的输出应用dropout[27]，然后将其添加到子层输入并进行规范化。此外，我们在编码器和解码器中对嵌入和位置编码的和应用了dropout。对于基础模型，我们使用$P_{drop} = 0.1$的速率。</p><p><strong>Label Smoothing</strong>，在训练过程中，我们使用了$\epsilon_{ls} = 0.1$[30]的标签平滑。这会降低模型的困惑度，因为模型会变得更加不确定，但会提高准确性和BLEU分数。</p><h2 id="6-Results"><a href="#6-Results" class="headerlink" title="6 Results"></a>6 Results</h2><h3 id="6-1-Machine-Translation"><a href="#6-1-Machine-Translation" class="headerlink" title="6.1 Machine Translation"></a>6.1 Machine Translation</h3><p>在2014年WMT英德翻译任务中，big transformer模型（表2中的Transformer (big)）的性能比之前报告的最佳模型（包括集成模型）在BLEU评分上高出了2.0分以上，从而达到了一个SOTA的BLEU评分28.4。</p><p><img src="https://img.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/image-20240209170517873.png" alt="image-20240209170517873"></p><p>该模型的配置列于表3的底部。在8个P100 GPU上训练了3.5天。</p><p><img src="https://img.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/image-20240209170606266.png" alt="image-20240209170606266"></p><p>甚至我们的基础模型超过了所有以前发表的模型和集成模型，而训练成本只是任何竞争模型的一小部分。</p><p>在WMT 2014英法翻译任务中，我们的大模型获得了41.0的BLEU分数，超过了之前发布的所有单个模型，而训练成本不到之前最先进的模型的$\frac{1}{4}$。Transformer (big)模型的英语-法语训练的dropout概率为$P_{drop} = 0.1$，而不是$0.3$。</p><p>对于基本模型，我们使用了通过平均最后5个检查点获得的单个模型，这些检查点每10分钟记录一次。对于大型模型，我们计算了最后20个检查点的平均值。我们采用了束大小为4，长度惩罚$\alpha = 0.6$[31]的束搜索。这些超参数是在开发集上经过实验后选定的。在推理期间，我们将最大输出长度设置为输入长度$+50$，但在可能的情况下提前终止[31]。</p><p>表2总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们通过将训练时间、所使用的GPU数量和每个GPU的持续单精度浮点容量相乘，来估计用于训练一个模型的浮点运算的数量。</p><h3 id="6-2-Model-Variations"><a href="#6-2-Model-Variations" class="headerlink" title="6.2 Model Variations"></a>6.2 Model Variations</h3><p>为了评估Transformer不同组件的重要性，我们以不同的方式改变了我们的基本模型，并在开发集（newstest2013）上测量了英语到德语翻译的性能变化。我们使用了前一节所描述的束搜索，但没有使用检查点平均。我们在表3中展示了这些结果。</p><p>在表3行(A)中，我们改变注意力头的数量以及注意力键和值维度，保持计算量不变，如章节3.2.2所述。虽然单头注意力比最好的设置差0.9 BLEU，但过多的头也会降低质量。</p><p>在表3行(B)中，我们观察到降低注意力键大小$d_k$会影响模型质量。这表明确定兼容性并不容易，一个比点积更复杂的兼容性函数可能是有益的。我们在(C)和(D)行进一步观察到，正如预期的那样，更大的模型更好，并且dropout在避免过拟合方面非常有帮助。在行(E)中，我们将正弦位置编码替换为学习过的位置嵌入[8]，并观察到与基础模型几乎相同的结果。</p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7 Conclusion"></a>7 Conclusion</h2><p>在本工作中，我们提出了Transformer，这是第一个完全基于注意力的序列转换模型，用多头自注意力替换了编码器-解码器架构中最常用的循环层。</p><p>对于翻译任务，Transformer的训练速度比基于循环或卷积层的体系结构快得多。在WMT 2014英语到德语和WMT 2014英语到法语的翻译任务中，我们都达到了一个新的水平。在前一个任务中，我们的最佳模型甚至优于所有之前报告模型的集成。</p><p>我们对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。我们计划将Transformer扩展到涉及文本以外的输入和输出模式的问题，并研究局部受限的注意力机制，以有效地处理图像、音频和视频等大型输入和输出。使生成过程更少地依赖于序列是我们的另一个研究目标。</p><p>我们用来训练和评估模型的代码可以在<a href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a>中找到。</p>]]></content>
    
    
    <summary type="html">Attention Is All You Need：Transformer开山之作。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2017" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2017/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="NIPS" scheme="http://blog.karltan.com/tags/NIPS/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5199. 现代艺术</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5199/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5199/</id>
    <published>2024-01-18T08:00:00.000Z</published>
    <updated>2024-01-18T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5199-现代艺术"><a href="#AcWing-5199-现代艺术" class="headerlink" title="AcWing 5199. 现代艺术"></a>AcWing 5199. 现代艺术</h1><p><a href="https://www.acwing.com/problem/content/5202/">5199. 现代艺术 - AcWing题库</a></p><p>给定一个$M$行$N$列的方格矩阵，行从上到下依次编号为$1 \sim M$，列从左到右依次编号为$1 \sim N$。</p><p>初始时，所有方格都是黑色的。</p><p>一个艺术家将对矩阵依次进行$K$次涂鸦操作，操作分为以下两种：</p><ul><li><code>R i</code>，表示将第$i$行的所有方格改变颜色；</li><li><code>C j</code>，表示将第$j$列的所有方格改变颜色。</li></ul><p>变色规则：黑色方格改变颜色会变成金色方格，金色方格改变颜色会变成黑色方格。</p><p>请你计算在所有操作完成以后，矩阵中有多少金色方格。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$M$。</p><p>第二行包含整数$N$。</p><p>第三行包含整数$K$。</p><p>接下来$K$行，每行包含一个操作指令，格式如题面描述。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示金色方格的数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le M \times N \le 5 \times 10^6, \\1 &\le K \le 10^6, \\1 &\le i \le M, \\1 &\le j \le N。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">R 1</span><br><span class="line">C 1</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>第一次操作，将第一行所有方格改变颜色，矩阵变为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">金金金</span><br><span class="line">黑黑黑</span><br><span class="line">黑黑黑</span><br></pre></td></tr></table></figure><p>第二次操作，将第一列所有方格改变颜色，矩阵变为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">黑金金</span><br><span class="line">金黑黑</span><br><span class="line">金黑黑</span><br></pre></td></tr></table></figure><p>一共有$4$个金色方格。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">5</span><br><span class="line">7</span><br><span class="line">R 3</span><br><span class="line">C 1</span><br><span class="line">C 2</span><br><span class="line">R 2</span><br><span class="line">R 2</span><br><span class="line">C 1</span><br><span class="line">R 4</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>所有操作完成以后，最终得到矩阵：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">黑金黑黑黑</span><br><span class="line">黑金黑黑黑</span><br><span class="line">金黑金金金</span><br><span class="line">金黑金金金</span><br></pre></td></tr></table></figure><p>一共有$10$个金色方格。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">5000010</span>;</span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line"><span class="comment">// r[i]存储第i行是否被翻转过</span></span><br><span class="line"><span class="comment">// c[i]存储第i列是否被翻转过</span></span><br><span class="line"><span class="type">bool</span> r[N], c[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m &gt;&gt; k;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; k; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">char</span> op;</span><br><span class="line">        <span class="type">int</span> num;</span><br><span class="line">        cin &gt;&gt; op &gt;&gt; num;</span><br><span class="line">        <span class="keyword">if</span> (op == <span class="string">&#x27;R&#x27;</span>)</span><br><span class="line">            r[num] = !r[num];</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (op == <span class="string">&#x27;C&#x27;</span>)</span><br><span class="line">            c[num] = !c[num];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 直接暴力遍历整个矩阵</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j ++)</span><br><span class="line">            <span class="keyword">if</span> (r[i] != c[j])</span><br><span class="line">                res ++;</span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    m = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    k = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    r = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">    c = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        opnum = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">        op = opnum[<span class="number">0</span>]</span><br><span class="line">        num = <span class="built_in">int</span>(opnum[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> op == <span class="string">&#x27;R&#x27;</span>:</span><br><span class="line">            r[num] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> op == <span class="string">&#x27;C&#x27;</span>:</span><br><span class="line">            c[num] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> (r[i] + c[j]) &amp; <span class="number">1</span>:</span><br><span class="line">                res += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">5000010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line"><span class="type">int</span> r[N], c[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;n, &amp;m, &amp;k);</span><br><span class="line">    <span class="keyword">while</span> (k -- )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">char</span> op[<span class="number">2</span>];</span><br><span class="line">        <span class="type">int</span> x;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%s%d&quot;</span>, op, &amp;x);</span><br><span class="line">        <span class="keyword">if</span> (*op == <span class="string">&#x27;R&#x27;</span>) r[x] ++ ;</span><br><span class="line">        <span class="keyword">else</span> c[x] ++ ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j ++ )</span><br><span class="line">            <span class="keyword">if</span> ((r[i] + c[j]) % <span class="number">2</span>)</span><br><span class="line">                res ++ ;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, res);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5199. 现代艺术。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5198. 整理书籍</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5198/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5198/</id>
    <published>2024-01-18T05:00:00.000Z</published>
    <updated>2024-01-18T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5198-整理书籍"><a href="#AcWing-5198-整理书籍" class="headerlink" title="AcWing 5198. 整理书籍"></a>AcWing 5198. 整理书籍</h1><p><a href="https://www.acwing.com/problem/content/5201/">5198. 整理书籍 - AcWing题库</a></p><p>书架上有若干本书排成一排。</p><p>每本书要么是大型书（用<code>L</code>表示），要么是中型书（用<code>M</code>表示），要么是小型书（用<code>S</code>表示）。</p><p>我们希望所有书能够从大到小有序排列，也就是说，所有大型书都在左侧，所有中型书都在中间，所有小型书都在右侧。</p><p>为此，你可以进行任意次交换操作，每次可以任选两本书并交换它们的位置。</p><p>请你计算，为了让所有书按要求有序排列，至少需要进行多少次交换操作。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>共一行，包含一个由<code>L</code>、<code>M</code>、<code>S</code>构成的字符串，表示初始时每个位置上的书的类型。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示所需要的最少交换操作次数。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><p>输入字符串的长度范围$[1, 5 \times 10^5]$。</p><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LMMMS</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>无需任何操作，初始排列已经符合要求。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LLSLM</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>一种最佳方案如下：</p><ul><li>第一步，交换<code>S</code>和<code>M</code>，序列变为<code>LLMLS</code>；</li><li>第二步，交换<code>M</code>和和它右边的<code>L</code>，序列变为<code>LLLMS</code>。</li></ul><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">5000010</span>;</span><br><span class="line"><span class="comment">// a[N]存储原始序列</span></span><br><span class="line"><span class="comment">// b[N]存储目标序列</span></span><br><span class="line"><span class="type">int</span> a[N], b[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string s;</span><br><span class="line">    cin &gt;&gt; s;</span><br><span class="line">    <span class="type">int</span> n = s.<span class="built_in">size</span>();</span><br><span class="line">    <span class="comment">// cnt[i]存储i的个数</span></span><br><span class="line">    <span class="type">int</span> cnt[<span class="number">3</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="comment">// 将原字符串转换为数字</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 由于要按先L，再M，最后S的顺序排序</span></span><br><span class="line">        <span class="comment">// 所以将L变为0，M变为1，S变为2</span></span><br><span class="line">        <span class="keyword">if</span> (s[i] == <span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">            a[i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (s[i] == <span class="string">&#x27;M&#x27;</span>)</span><br><span class="line">            a[i] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (s[i] == <span class="string">&#x27;S&#x27;</span>)</span><br><span class="line">            a[i] = <span class="number">2</span>;</span><br><span class="line">        <span class="comment">// 个数++</span></span><br><span class="line">        cnt[a[i]] ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 初始化b[N]</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, k = <span class="number">0</span>; i &lt; <span class="number">3</span>; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; cnt[i]; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            b[k] = i;</span><br><span class="line">            k ++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// e[i][j]是邻接矩阵</span></span><br><span class="line">    <span class="type">int</span> e[<span class="number">3</span>][<span class="number">3</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="comment">// 初始化邻接矩阵</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        e[a[i]][b[i]] ++;</span><br><span class="line">    <span class="comment">// m存储当前环的个数</span></span><br><span class="line">    <span class="type">int</span> m = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 先计算自环</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i ++)</span><br><span class="line">        m += e[i][i];</span><br><span class="line">    <span class="comment">// 再计算小环</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; <span class="number">3</span>; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 这里需要取小值</span></span><br><span class="line">            <span class="comment">// 只有小值所代表的边才全部构小环</span></span><br><span class="line">            <span class="comment">// 较大值中必还含有大环中的边</span></span><br><span class="line">            <span class="type">int</span> t = <span class="built_in">min</span>(e[i][j], e[j][i]);</span><br><span class="line">            <span class="comment">// 将结果统计至m中</span></span><br><span class="line">            m += t;</span><br><span class="line">            <span class="comment">// 减去边</span></span><br><span class="line">            e[i][j] -= t;</span><br><span class="line">            e[j][i] -= t;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 最后计算大环</span></span><br><span class="line">    <span class="comment">// 最后剩下的边就全部在大环中了</span></span><br><span class="line">    <span class="comment">// 且只会有顺时针大环或逆时针大环中的一种</span></span><br><span class="line">    m += (e[<span class="number">0</span>][<span class="number">1</span>] + e[<span class="number">1</span>][<span class="number">0</span>]);</span><br><span class="line">    <span class="comment">// 输出结果</span></span><br><span class="line">    cout &lt;&lt; n - m &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">5000010</span></span><br><span class="line">a = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">b = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    s = <span class="built_in">input</span>()</span><br><span class="line">    n = <span class="built_in">len</span>(s)</span><br><span class="line">    cnt = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> s[i] == <span class="string">&#x27;L&#x27;</span>:</span><br><span class="line">            a[i] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> s[i] == <span class="string">&#x27;M&#x27;</span>:</span><br><span class="line">            a[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> s[i] == <span class="string">&#x27;S&#x27;</span>:</span><br><span class="line">            a[i] = <span class="number">2</span></span><br><span class="line">        cnt[a[i]] += <span class="number">1</span></span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(cnt[i]):</span><br><span class="line">            b[k] = i</span><br><span class="line">            k += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    e = [[<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        e[a[i]][b[i]] += <span class="number">1</span></span><br><span class="line">    m = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        m += e[i][i]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">            t = <span class="built_in">min</span>(e[i][j], e[j][i])</span><br><span class="line">            m += t</span><br><span class="line">            e[i][j] -= t</span><br><span class="line">            e[j][i] -= t</span><br><span class="line">    m += (e[<span class="number">0</span>][<span class="number">1</span>] + e[<span class="number">1</span>][<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(n - m)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5198. 整理书籍。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 3724. 街灯</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing3724/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing3724/</id>
    <published>2024-01-17T13:00:00.000Z</published>
    <updated>2024-01-17T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-3724-街灯"><a href="#AcWing-3724-街灯" class="headerlink" title="AcWing 3724. 街灯"></a>AcWing 3724. 街灯</h1><p><a href="https://www.acwing.com/problem/content/3727/">3724. 街灯 - AcWing题库</a></p><p>现在是基督降临节，有$N$个位置的街道上有$M$个街灯。</p><p>每个灯的照明范围是$K$。</p><p>也就是说，在第$X$个位置的灯，可以照亮第$X - K$到第$X + K$个位置。</p><p>当然，街道某处可能被多个灯照亮。</p><p>所有灯位于不同的位置。</p><p>问题在于有可能这些灯没法照亮整条街道。</p><p>你的任务是，确定最少还要加多少灯，使得整条街道都被照亮。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>输入包含多组测试数据。</p><p>每组数据第一行包含三个整数$N, M, K$。</p><p>第二行包含$M$个升序的整数，表示每个灯的位置。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一行，一个整数，表示答案。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le N \le 1000, \\1 &\le M \le N, \\0 &\le K \le N。\end{aligned}</script><p>输入最多包含$100$组数据。</p><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5 2 2</span><br><span class="line">1 5</span><br><span class="line">5 1 2</span><br><span class="line">2</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1010</span>;</span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line"><span class="type">int</span> pos;</span><br><span class="line"><span class="type">int</span> res;</span><br><span class="line"><span class="comment">// is_light[i]记录第i个位置是否是亮的</span></span><br><span class="line"><span class="type">bool</span> is_light[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (cin &gt;&gt; n &gt;&gt; m &gt;&gt; k)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 将所有位置置为黑</span></span><br><span class="line">        <span class="built_in">memset</span>(is_light, <span class="literal">false</span>, <span class="keyword">sizeof</span> is_light);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 输入路灯的位置</span></span><br><span class="line">            cin &gt;&gt; pos;</span><br><span class="line">            <span class="comment">// 向这个位置的两边各延伸k个位置</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = (pos - k &gt;= <span class="number">1</span> ? pos - k : <span class="number">1</span>); j &lt;= (pos + k &lt;= n ? pos + k : n); j ++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 将该位置置为亮</span></span><br><span class="line">                is_light[j] = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果有黑的地方，说明需要加路灯</span></span><br><span class="line">            <span class="keyword">if</span> (is_light[i] == <span class="literal">false</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 加路灯</span></span><br><span class="line">                res ++;</span><br><span class="line">                <span class="comment">// 在照亮这个位置的基础上，还可以向后延伸2k个位置</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> j = i; j &lt;= (i + <span class="number">2</span> * k &lt;= n ? i + <span class="number">2</span> * k : n); j ++)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 照亮</span></span><br><span class="line">                    is_light[j] = <span class="literal">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">1010</span></span><br><span class="line">is_light = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            nmk = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        n = nmk[<span class="number">0</span>]</span><br><span class="line">        m = nmk[<span class="number">1</span>]</span><br><span class="line">        k = nmk[<span class="number">2</span>]</span><br><span class="line">        poses = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">        <span class="keyword">for</span> pos <span class="keyword">in</span> poses:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(pos - k <span class="keyword">if</span> pos - k &gt;= <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span>, pos + k + <span class="number">1</span> <span class="keyword">if</span> pos + k + <span class="number">1</span> &lt;= n + <span class="number">1</span> <span class="keyword">else</span> n + <span class="number">1</span>):</span><br><span class="line">                is_light[i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> is_light[i] == <span class="number">0</span>:</span><br><span class="line">                res += <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i, i + <span class="number">2</span> * k + <span class="number">1</span> <span class="keyword">if</span> i + <span class="number">2</span> * k + <span class="number">1</span> &lt;= n + <span class="number">1</span> <span class="keyword">else</span> n + <span class="number">1</span>):</span><br><span class="line">                    is_light[j] = <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(res)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            is_light[i] = <span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line"><span class="type">int</span> b[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;n, &amp;m, &amp;k) != <span class="number">-1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">memset</span>(b, <span class="number">0</span>, <span class="keyword">sizeof</span> b);</span><br><span class="line">        <span class="keyword">while</span> (m -- )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> x;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;x);</span><br><span class="line">            <span class="type">int</span> l = <span class="built_in">max</span>(<span class="number">1</span>, x - k), r = <span class="built_in">min</span>(n, x + k);</span><br><span class="line">            b[l] ++, b[r + <span class="number">1</span>] -- ;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ ) b[i] += b[i - <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        b[n + <span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>, len = k * <span class="number">2</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>, c = <span class="number">0</span>; i &lt;= n + <span class="number">1</span>; i ++ )</span><br><span class="line">            <span class="keyword">if</span> (b[i])</span><br><span class="line">            &#123;</span><br><span class="line">                res += (c + len - <span class="number">1</span>) / len;</span><br><span class="line">                c = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> c ++ ;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 3724. 街灯。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5183. 好三元组</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5183/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5183/</id>
    <published>2024-01-16T12:00:00.000Z</published>
    <updated>2024-01-16T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5183-好三元组"><a href="#AcWing-5183-好三元组" class="headerlink" title="AcWing 5183. 好三元组"></a>AcWing 5183. 好三元组</h1><p><a href="https://www.acwing.com/problem/content/5186/">5183. 好三元组 - AcWing题库</a></p><p>平面中有一个圆，圆心坐标为$(0, 0)$，周长为$C$。</p><p>圆周上有$C$个整数位置点，编号$0 \sim C - 1$。</p><p>其中：</p><ul><li>$0$号点：圆上的最右点。</li><li>$1 \sim C - 1$号点：从$0$号点到自身位置的<strong>逆时针</strong>弧长恰好等于自身编号。</li></ul><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5183.png" alt="acwing5183"></p><p>给定一个长度为$N$的整数序列$P_1, P_2, \cdots, P_N(0 \le P_i \le C - 1)$。</p><p>请你计算，一共有多少个不同的整数三元组$(a, b, c)$同时满足：</p><ol><li>$1 \le a &lt; b &lt; c \le N$；</li><li>原点$(0, 0)$严格位于以点$P_a, P_b, P_c$为顶点的三角形内部。注意，三角形边上的点不在三角形内部。</li></ol><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含两个整数$N, C$。</p><p>第二行包含$N$个整数$P_1, P_2, \cdots, P_N$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示满足条件的三元组数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}3& \le N \le 10^6, \\3& \le N \le 10^6, \\0& \le P_i \le C - 1。\end{aligned}</script><p>注意，$P_i$<strong>不一定</strong>各不相同。</p><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">8 10</span><br><span class="line">0 2 5 5 6 9 0 0</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>样例示意图如下：</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5183_case1.png" alt="acwing5183_case1"></p><p>所以满足条件的三元组为：$(1, 2, 5), (2, 3, 6), (2, 4, 6), (2, 5, 6), (2, 5, 7), (2, 5, 8)$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">2000010</span>;</span><br><span class="line"><span class="type">int</span> n, c;</span><br><span class="line"><span class="comment">// cnt[i]存储Pi处点的数量</span></span><br><span class="line"><span class="comment">// s[N]为前缀和数组</span></span><br><span class="line"><span class="type">int</span> cnt[N], s[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算组合数</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="type">long</span> <span class="title">C</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = a, j = <span class="number">1</span>; j &lt;= b; i --, j ++)</span><br><span class="line">        res = res * i / j;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; c;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> p;</span><br><span class="line">        cin &gt;&gt; p;</span><br><span class="line">        cnt[p] ++;</span><br><span class="line">        <span class="comment">// 破环成链</span></span><br><span class="line">        <span class="comment">// 将原来基础的圆变为一个链后，再在这个链的后面增加一个相同的链</span></span><br><span class="line">        cnt[p + c] ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 前缀和</span></span><br><span class="line">    s[<span class="number">0</span>] = cnt[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; <span class="number">2</span> * c; i ++)</span><br><span class="line">        s[i] = s[i - <span class="number">1</span>] + cnt[i];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里采用的是求不成立方案数的做法</span></span><br><span class="line">    <span class="comment">// 求出不成立方案数后，将总方案数 - 不成立方案数即为答案</span></span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> res = <span class="built_in">C</span>(n, <span class="number">3</span>);</span><br><span class="line">    <span class="comment">// 求需要枚举的长度，此时只会处理圆心在选定三角形一侧时的情况</span></span><br><span class="line">    <span class="type">int</span> half_len = (c - <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; c; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// x是在half_len这段弧中有多少个点</span></span><br><span class="line">        <span class="type">int</span> x = s[i + half_len] - s[i];</span><br><span class="line">        <span class="comment">// y是现在枚举的地方有几个点</span></span><br><span class="line">        <span class="type">int</span> y = cnt[i];</span><br><span class="line">        <span class="comment">// 有三种不成立的方案：</span></span><br><span class="line">        <span class="comment">// 1. 在现在枚举的地方取3个点</span></span><br><span class="line">        <span class="comment">// 2. 在现在枚举的地方取2个点，剩下1个在弧中取</span></span><br><span class="line">        <span class="comment">// 3. 在现在枚举的地方取1个点，剩下2个在弧中取</span></span><br><span class="line">        res -= (<span class="built_in">C</span>(y, <span class="number">3</span>) + <span class="built_in">C</span>(y, <span class="number">2</span>) * <span class="built_in">C</span>(x, <span class="number">1</span>) + <span class="built_in">C</span>(y, <span class="number">1</span>) * <span class="built_in">C</span>(x, <span class="number">2</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 当圆能够被平分为2段弧时，此时可以处理圆心在选定三角形边上的情况</span></span><br><span class="line">    <span class="keyword">if</span> (c % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; c; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 下面的情况是在直径两端各选1个点，然后在直径所对的弧上再选1点</span></span><br><span class="line">            <span class="comment">// cnt[i]直径一端点的数量</span></span><br><span class="line">            <span class="comment">// cnt[i + c / 2]直径另一端点的数量</span></span><br><span class="line">            <span class="comment">// (s[i + c / 2 - 1] - s[i])直径所对的弧上点的数量</span></span><br><span class="line">            res -= ((<span class="type">long</span> <span class="type">long</span>)cnt[i] * cnt[i + c / <span class="number">2</span>] * (s[i + c / <span class="number">2</span> - <span class="number">1</span>] - s[i]));</span><br><span class="line">            <span class="comment">// 下面的情况是在直径一端选一个点，在另一端选两个点的情况</span></span><br><span class="line">            res -= (cnt[i] * <span class="built_in">C</span>(cnt[i + c / <span class="number">2</span>], <span class="number">2</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">2000010</span></span><br><span class="line">cnt = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">s = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">C</span>(<span class="params">a:<span class="built_in">int</span>, b:<span class="built_in">int</span></span>):</span><br><span class="line">    res = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(a, a - b, -<span class="number">1</span>), <span class="built_in">range</span>(<span class="number">1</span>, b + <span class="number">1</span>)):</span><br><span class="line">        res = res * i // j</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    nc = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    n = nc[<span class="number">0</span>]</span><br><span class="line">    c = nc[<span class="number">1</span>]</span><br><span class="line">    l = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    <span class="keyword">for</span> pos <span class="keyword">in</span> l:</span><br><span class="line">        cnt[pos] += <span class="number">1</span></span><br><span class="line">        cnt[pos + c] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        cnt[i + c] = cnt[i]</span><br><span class="line">    s[<span class="number">0</span>] = cnt[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">2</span> * c):</span><br><span class="line">        s[i] = s[i - <span class="number">1</span>] + cnt[i]</span><br><span class="line"></span><br><span class="line">    res = C(n, <span class="number">3</span>)</span><br><span class="line">    half_len = (c - <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(c):</span><br><span class="line">        x = s[i + half_len] - s[i]</span><br><span class="line">        y = cnt[i]</span><br><span class="line">        res -= (C(y, <span class="number">3</span>) + C(y, <span class="number">2</span>) * C(x, <span class="number">1</span>) + C(y, <span class="number">1</span>) * C(x, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">if</span> c % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(c):</span><br><span class="line">            res -= (cnt[i] * cnt[i + c // <span class="number">2</span>] * (s[i + c // <span class="number">2</span> - <span class="number">1</span>] - s[i]))</span><br><span class="line">            res -= (cnt[i] * C(cnt[i + c // <span class="number">2</span>], <span class="number">2</span>))</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5183. 好三元组。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5181. 好四和好五</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5181/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5181/</id>
    <published>2024-01-16T09:00:00.000Z</published>
    <updated>2024-01-16T10:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5181-好四和好五"><a href="#AcWing-5181-好四和好五" class="headerlink" title="AcWing 5181. 好四和好五"></a>AcWing 5181. 好四和好五</h1><p><a href="https://www.acwing.com/problem/content/5184/">5181. 好四和好五 - AcWing题库</a></p><p>给定一个正整数$N$，请你计算一共有多少个不同的可重复正整数集合满足：</p><ul><li>集合中只包含$4$和$5$。</li><li>集合中所有元素之和恰好等于$N$。</li></ul><p>例如，当$N = 14$时，满足条件的集合只有一个：$\{4, 5, 5\}$；当$N = 20$时，满足条件的集合有两个：$\{4, 4, 4, 4, 4\}$和$\{5, 5, 5, 5\}$；当$N = 40$时，满足条件的集合有三个：$\{4, 4, 4, 4, 4, 4, 4, 4, 4, 4\}$、$\{4, 4, 4, 4, 4, 5, 5, 5, 5\}$以及$\{5, 5, 5, 5, 5, 5, 5, 5\}$。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>一个正整数$N$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示满足条件的集合数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">1 \le N \le 10^6。</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">14</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">40</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><p><strong>输入样例3：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6</span><br></pre></td></tr></table></figure><p><strong>输出样例3：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>枚举：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt;= n / <span class="number">4</span>; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> temp = n - <span class="number">4</span> * i;</span><br><span class="line">        <span class="keyword">if</span> (temp % <span class="number">5</span> == <span class="number">0</span>)</span><br><span class="line">            res ++;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n // <span class="number">4</span> + <span class="number">1</span>):</span><br><span class="line">        temp = n - <span class="number">4</span> * i</span><br><span class="line">        <span class="keyword">if</span> temp % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            res += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><p>扩展欧几里得算法：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    cout &lt;&lt; <span class="built_in">floor</span>(n / <span class="number">4.0</span>) - <span class="built_in">ceil</span>(n / <span class="number">5.0</span>) + <span class="number">1</span> &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> floor, ceil</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    <span class="built_in">print</span>(floor(n / <span class="number">4.0</span>) - ceil(n / <span class="number">5.0</span>) + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5181. 好四和好五。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5180. 正方形泳池</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5180/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5180/</id>
    <published>2024-01-15T04:00:00.000Z</published>
    <updated>2024-01-15T05:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5180-正方形泳池"><a href="#AcWing-5180-正方形泳池" class="headerlink" title="AcWing 5180. 正方形泳池"></a>AcWing 5180. 正方形泳池</h1><p><a href="https://www.acwing.com/problem/content/5183/">5180. 正方形泳池 - AcWing题库</a></p><p>给定一个$N \times N$的方格矩阵。</p><p>左上角方格坐标为$(1, 1)$，右下角方格坐标为$(N, N)$。</p><p>有$T$个方格内有树，这些方格的具体坐标已知。</p><p>我们希望建立一个正方形的泳池。</p><p>你的任务是找到一个尽可能大的<strong>正方形</strong>子矩阵，要求子矩阵内没有包含树的方格。</p><p>输出满足条件的子矩阵的最大可能边长。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$N$。</p><p>第二行包含整数$T$。</p><p>接下来$T$行，每行包含两个整数$r, c$，表示方格$(r, c)$内有树。</p><p>输入保证，这$T$个方格坐标两两不同。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示满足条件的子矩阵的最大可能边长。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&2 \le N \le 5 \times 10^5, \\&1 \le T \le 100, \\&T < N^2, \\&1 \le r, c \le N。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">5</span><br><span class="line">1</span><br><span class="line">2 4</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>样例矩阵以及最大泳池示意图如下所示：</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5180_case1.png" alt="acwing5180_case1"></p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">15</span><br><span class="line">8</span><br><span class="line">4 7</span><br><span class="line">4 1</span><br><span class="line">14 11</span><br><span class="line">10 6</span><br><span class="line">13 4</span><br><span class="line">4 10</span><br><span class="line">10 3</span><br><span class="line">9 14</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">7</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>样例矩阵以及最大泳池示意图如下所示：</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5180_case2.png" alt="acwing5180_case2"></p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">110</span>;</span><br><span class="line"><span class="type">int</span> n, t;</span><br><span class="line">pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; trees[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">work</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">sort</span>(trees, trees + t);</span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; t; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 存储y方向上的上下界</span></span><br><span class="line">        <span class="type">int</span> up = <span class="number">0</span>, down = n + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; t; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果第i棵树和第j棵树在同一列，则continue</span></span><br><span class="line">            <span class="keyword">if</span> (trees[i].first == trees[j].first)</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            <span class="comment">// 如果在此次遍历中，x方向上的最大长度被y方向上的最大长度限制，则break</span></span><br><span class="line">            <span class="keyword">if</span> (trees[j].first - trees[i].first &gt; down - up)</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">// 否则正常更新</span></span><br><span class="line">            res = <span class="built_in">max</span>(res, trees[j].first - trees[i].first - <span class="number">1</span>);</span><br><span class="line">            <span class="comment">// 根据当前遍历到的下界更新下界</span></span><br><span class="line">            <span class="keyword">if</span> (trees[i].second &lt;= trees[j].second)</span><br><span class="line">                down = <span class="built_in">min</span>(down, trees[j].second);</span><br><span class="line">            <span class="comment">// 根据当前遍历到的上界更新上界</span></span><br><span class="line">            <span class="keyword">if</span> (trees[i].second &gt;= trees[j].second)</span><br><span class="line">                up = <span class="built_in">max</span>(up, trees[j].second);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 返回答案</span></span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    cin &gt;&gt; t;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; t; i ++)</span><br><span class="line">        cin &gt;&gt; trees[i].first &gt;&gt; trees[i].second;</span><br><span class="line">    <span class="comment">// 在四周添加四棵树，以框定范围</span></span><br><span class="line">    trees[t ++] = &#123;<span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    trees[t ++] = &#123;<span class="number">0</span>, n + <span class="number">1</span>&#125;;</span><br><span class="line">    trees[t ++] = &#123;n + <span class="number">1</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    trees[t ++] = &#123;n + <span class="number">1</span>, n + <span class="number">1</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> res = <span class="built_in">work</span>();</span><br><span class="line">    <span class="comment">// 将已有树的xy坐标翻转，再遍历一次</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; t; i ++)</span><br><span class="line">        <span class="built_in">swap</span>(trees[i].first, trees[i].second);</span><br><span class="line">    res = <span class="built_in">max</span>(res, <span class="built_in">work</span>());</span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">work</span>():</span><br><span class="line">    trees.sort()</span><br><span class="line">    res = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t):</span><br><span class="line">        up = <span class="number">0</span></span><br><span class="line">        down = n + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, t):</span><br><span class="line">            <span class="keyword">if</span> trees[i][<span class="number">0</span>] == trees[j][<span class="number">0</span>]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> trees[j][<span class="number">0</span>] - trees[i][<span class="number">0</span>] &gt; down - up:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            res = <span class="built_in">max</span>(res, trees[j][<span class="number">0</span>] - trees[i][<span class="number">0</span>] - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> trees[i][<span class="number">1</span>] &lt;= trees[j][<span class="number">1</span>]:</span><br><span class="line">                down = <span class="built_in">min</span>(down, trees[j][<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> trees[i][<span class="number">1</span>] &gt;= trees[j][<span class="number">1</span>]:</span><br><span class="line">                up = <span class="built_in">max</span>(up, trees[j][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    t = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    trees = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t):</span><br><span class="line">        trees.append(<span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>))))</span><br><span class="line">    trees.append((<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    trees.append((<span class="number">0</span>, n + <span class="number">1</span>))</span><br><span class="line">    trees.append((n + <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line">    trees.append((n + <span class="number">1</span>, n + <span class="number">1</span>))</span><br><span class="line">    t += <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    res = work()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(t):</span><br><span class="line">        trees[i] = (trees[i][<span class="number">1</span>], trees[i][<span class="number">0</span>])</span><br><span class="line">    res = <span class="built_in">max</span>(res, work())</span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5180. 正方形泳池。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5179. 分组</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5179/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5179/</id>
    <published>2024-01-14T15:00:00.000Z</published>
    <updated>2024-01-14T15:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5179-分组"><a href="#AcWing-5179-分组" class="headerlink" title="AcWing 5179. 分组"></a>AcWing 5179. 分组</h1><p><a href="https://www.acwing.com/problem/content/5182/">5179. 分组 - AcWing题库</a></p><p>某班一共有$3G$个学生，按照三人一组的规则，被分成了$G$个小组。</p><p>同学之间关系有好有坏，分组结果可能会违背一些同学的意愿。</p><p>已知，同学两两之间一共有$X$个同组意愿和$Y$个不同组意愿。</p><p>每个同组意愿涉及两个同学，表示这两个同学强烈要求必须分在同一组。</p><p>每个不同组意愿涉及两个同学，表示这两个同学强烈要求必须分在不同组。</p><p>请你计算，所有$X + Y$个意愿当中，一共有多少个意愿<strong>没有得到满足</strong>。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$X$，表示一共有$X$个同组意愿。</p><p>接下来$X$行，每行包含两个同学的姓名，表示这两个同学强烈要求必须分在同一组。</p><p>再一行包含整数$Y$，表示一共有$Y$个不同组意愿。</p><p>接下来$Y$行，每行包含两个同学的姓名，表示这两个同学强烈要求必须分在不同组。</p><p>输入保证，不含重复信息或矛盾信息，即同一对同学不会出现两次。</p><p>再一行包含一个整数$G$，表示一共分成了$G$组。</p><p>最后$G$行，每行包含三个同学的姓名，表示这三个同学被分在了一组。</p><p>此班级的所有同学的名字都在最后$G$行给出了，输入中不会出现不在班级内的名字。</p><p>输入保证，每个同学只被分在一组。</p><p>每个名字由$1 \sim 10$个大写字母组成，不同学生的名字不同。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示没有得到满足的意愿数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le G \le 10^5, \\0 &\le X, Y \le 10^5。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">ELODIE CHI</span><br><span class="line">0</span><br><span class="line">2</span><br><span class="line">DWAYNE BEN ANJALI</span><br><span class="line">CHI FRANCOIS ELODIE</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>唯一的一个同组意愿得到了满足，所以没有得到满足的意愿数量为$0$。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">A B</span><br><span class="line">G L</span><br><span class="line">J K</span><br><span class="line">2</span><br><span class="line">D F</span><br><span class="line">D G</span><br><span class="line">4</span><br><span class="line">A C G</span><br><span class="line">B D F</span><br><span class="line">E H I</span><br><span class="line">J K L</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>一共有$5$个意愿，逐一进行分析：</p><ul><li>第一个意愿，$A, B$同组，没有被满足。</li><li>第二个意愿，$G, L$同组，没有被满足。</li><li>第三个意愿，$J, K$同组，得到满足。</li><li>第四个意愿，$D, F$不同组，没有被满足。</li><li>第五个意愿，$D, G$不同组，得到满足。</li></ul><p>综上，没有得到满足的意愿数量为$3$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> x, y, g, res;</span><br><span class="line">string X[N][<span class="number">2</span>], Y[N][<span class="number">2</span>];</span><br><span class="line">unordered_map&lt;string, <span class="type">int</span>&gt; m;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 输入x和X</span></span><br><span class="line">    cin &gt;&gt; x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; x; i ++)</span><br><span class="line">        cin &gt;&gt; X[i][<span class="number">0</span>] &gt;&gt; X[i][<span class="number">1</span>];</span><br><span class="line">    <span class="comment">// 输入y和Y</span></span><br><span class="line">    cin &gt;&gt; y;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; y; i ++)</span><br><span class="line">        cin &gt;&gt; Y[i][<span class="number">0</span>] &gt;&gt; Y[i][<span class="number">1</span>];</span><br><span class="line">    <span class="comment">// 输入g</span></span><br><span class="line">    cin &gt;&gt; g;</span><br><span class="line">    string name;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; g; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 每组有3个同学</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">3</span>; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; name;</span><br><span class="line">            <span class="comment">// 将同组同学的id置为i</span></span><br><span class="line">            m[name] = i;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断是否满足同组需求</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; x; i ++)</span><br><span class="line">        <span class="comment">// 如果不同组，则++</span></span><br><span class="line">        <span class="keyword">if</span> (m[X[i][<span class="number">0</span>]] != m[X[i][<span class="number">1</span>]])</span><br><span class="line">            res ++;</span><br><span class="line">    <span class="comment">// 判断是否满足不同组需求</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; y; i ++)</span><br><span class="line">        <span class="comment">// 如果同组，则++</span></span><br><span class="line">        <span class="keyword">if</span> (m[Y[i][<span class="number">0</span>]] == m[Y[i][<span class="number">1</span>]])</span><br><span class="line">            res ++;</span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">100010</span></span><br><span class="line">res = <span class="number">0</span></span><br><span class="line">X = [[<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">Y = [[<span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">m = <span class="built_in">dict</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x):</span><br><span class="line">        X[i] = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    y = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y):</span><br><span class="line">        Y[i] = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    g = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(g):</span><br><span class="line">        names = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">str</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> names:</span><br><span class="line">            m[name] = i</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x):</span><br><span class="line">        <span class="keyword">if</span> m[X[i][<span class="number">0</span>]] != m[X[i][<span class="number">1</span>]]:</span><br><span class="line">            res += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y):</span><br><span class="line">        <span class="keyword">if</span> m[Y[i][<span class="number">0</span>]] == m[Y[i][<span class="number">1</span>]]:</span><br><span class="line">            res += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5179. 分组。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5164. 所有三角形</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5164/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5164/</id>
    <published>2024-01-14T11:00:00.000Z</published>
    <updated>2024-01-14T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5164-所有三角形"><a href="#AcWing-5164-所有三角形" class="headerlink" title="AcWing 5164. 所有三角形"></a>AcWing 5164. 所有三角形</h1><p><a href="https://www.acwing.com/problem/content/5167/">5164. 所有三角形 - AcWing题库</a></p><p>建筑商波奇刚刚完成了她的最新作品：一条精美的巷道。</p><p>此巷道由两排瓷砖组成，每排都恰好包含$C$个边长为$1$的白色等边三角形瓷砖。</p><p>其中，上排左起第一个三角形瓷砖指向上方，每对相邻三角形瓷砖（即包含公共边的三角形瓷砖）的指向都相反（可参照图例）。</p><p>不幸的是，她不小心打翻了一桶黑色油漆，使得其中一些三角形瓷砖被染黑了。</p><p>由于被染黑的瓷砖油漆未干，她计划使用胶带将所有染黑区域的边缘围住，以防别人误踩。</p><p>请你计算，她需要使用多少米的胶带。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$C$。</p><p>第二行包含$C$个整数$0$或$1$，表示第一排每个瓷砖的颜色。如果第$i$个整数为$1$，则表示第$i$个瓷砖（左起）为黑色，如果第$i$个整数为$0$，则表示第$i$个瓷砖（左起）为白色。</p><p>第三行包含$C$个整数$0$或$1$，表示第二排每个瓷砖的颜色。如果第$i$个整数为$1$，则表示第$i$个瓷砖（左起）为黑色，如果第$i$个整数为$0$，则表示第$i$个瓷砖（左起）为白色。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示所需使用的胶带长度。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">1 \le C \le 2 \times 10^5。</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">5</span><br><span class="line">1 0 1 0 1</span><br><span class="line">0 0 0 0 0</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>瓷砖颜色分布以及胶带张贴情况如下图所示，一共需要$9$米胶带。</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5164_case1.png" alt="acwing5164_case1"></p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">7</span><br><span class="line">0 0 1 1 0 1 0</span><br><span class="line">0 0 1 0 1 0 0</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">11</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>瓷砖颜色分布以及胶带张贴情况如下图所示，一共需要$11$米胶带。</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5164_case2.png" alt="acwing5164_case2"></p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">200010</span>;</span><br><span class="line"><span class="type">int</span> n, res;</span><br><span class="line"><span class="type">int</span> a[N], b[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="comment">// 输入</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; a[i];</span><br><span class="line">        <span class="comment">// 如果这块砖为黑色，假设其没有相邻的黑色砖块，直接+3</span></span><br><span class="line">        <span class="keyword">if</span> (a[i] == <span class="number">1</span>)</span><br><span class="line">            res += <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; b[i];</span><br><span class="line">        <span class="comment">// 同理</span></span><br><span class="line">        <span class="keyword">if</span> (b[i] == <span class="number">1</span>)</span><br><span class="line">            res += <span class="number">3</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 如果上排砖中，这一块和下一块都是黑色，则-2</span></span><br><span class="line">        <span class="keyword">if</span> (a[i] == <span class="number">1</span> &amp;&amp; a[i + <span class="number">1</span>] == <span class="number">1</span>)</span><br><span class="line">            res -= <span class="number">2</span>;</span><br><span class="line">        <span class="comment">// 如果下排砖中，这一块和下一块都是黑色，则-2</span></span><br><span class="line">        <span class="keyword">if</span> (b[i] == <span class="number">1</span> &amp;&amp; b[i + <span class="number">1</span>] == <span class="number">1</span>)</span><br><span class="line">            res -= <span class="number">2</span>;</span><br><span class="line">        <span class="comment">// 如果当前位置上下都为黑色，且是奇数列，则-2</span></span><br><span class="line">        <span class="keyword">if</span> (a[i] == <span class="number">1</span> &amp;&amp; b[i] == <span class="number">1</span> &amp;&amp; i % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">            res -= <span class="number">2</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">200010</span></span><br><span class="line">res = <span class="number">0</span></span><br><span class="line">a = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">b = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    a[<span class="number">1</span>:n + <span class="number">1</span>] = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    b[<span class="number">1</span>:n + <span class="number">1</span>] = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> a[i] == <span class="number">1</span>:</span><br><span class="line">            res += <span class="number">3</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> b[i] == <span class="number">1</span>:</span><br><span class="line">            res += <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> a[i] == <span class="number">1</span> <span class="keyword">and</span> a[i + <span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">            res -= <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> b[i] == <span class="number">1</span> <span class="keyword">and</span> b[i + <span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">            res -= <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> a[i] == <span class="number">1</span> <span class="keyword">and</span> b[i] == <span class="number">1</span> <span class="keyword">and</span> i % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">            res -= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(res)</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">200010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> w[<span class="number">2</span>][N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i ++ )</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;w[i][j]);</span><br><span class="line">            <span class="keyword">if</span> (w[i][j])</span><br><span class="line">            &#123;</span><br><span class="line">                res += <span class="number">3</span>;</span><br><span class="line">                <span class="keyword">if</span> (j &amp;&amp; w[i][j - <span class="number">1</span>]) res -= <span class="number">2</span>;</span><br><span class="line">                <span class="keyword">if</span> (i &amp;&amp; j % <span class="number">2</span> == <span class="number">0</span> &amp;&amp; w[i - <span class="number">1</span>][j]) res -= <span class="number">2</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, res);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5164. 所有三角形。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5170. 二进制</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5170/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5170/</id>
    <published>2024-01-04T07:00:00.000Z</published>
    <updated>2024-01-04T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5170-二进制"><a href="#AcWing-5170-二进制" class="headerlink" title="AcWing 5170. 二进制"></a>AcWing 5170. 二进制</h1><p><a href="https://www.acwing.com/problem/content/5173/">5170. 二进制 - AcWing题库</a></p><p>给定一个长度为$N$的二进制串（$01$串）以及一个正整数$K$。</p><p>按照从左到右的顺序，依次遍历给定二进制串的$N - K + 1$个长度为$K$的子串，并计算每个遍历子串的各位数字之和。</p><p>将这$N - K + 1$个子串数字和按照子串的遍历顺序进行排列，得到的序列就是给定二进制串的K-子串数字和序列。</p><p>注意，所有子串数字和均用十进制表示。</p><p>例如，当$K = 4$时，二进制串<code>110010</code>的K-子串数字和序列为<code>2 2 1</code>，分析过程如下：</p><ul><li>依次遍历<code>110010</code>的所有长度为$4$的子串：<code>1100</code>、<code>1001</code>、<code>0010</code>。</li><li>计算每个遍历子串的各位数字之和：$1 + 1 + 0 + 0 = 2$、$1 + 0 + 0 + 1 = 2$、$0 + 0 + 1 + 0 = 1$。</li><li>将所有子串数字和按顺序排列，最终得到<code>2 2 1</code>。</li></ul><p>现在，给定$N, K$以及一个K-子串数字和序列，请你计算一共有多少个不同的长度为$N$的二进制串可以得到该K-子串数字和序列。</p><p><strong>数据保证：至少存在一个长度为$N$的二进制串可以得到该K-子串数字和序列。</strong></p><p>由于结果可能很大，你只需要输出对$10^6 + 3$取模后的结果。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含两个整数$N, K$。</p><p>第二行包含$N - K + 1$个整数，表示给定的K-子串数字和序列。</p><p>输入保证给定的K-子串数字和序列一定合法，即至少存在一个满足条件的二进制串与之对应。</p><p>不难发现，长度为$K$的子串的各位数字之和一定不小于$0$且不大于$K$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示满足条件的二进制串数量对$10^6 + 3$取模后的结果。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">1 \le K \le N \le 10^6。</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7 4</span><br><span class="line">3 2 2 2</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>满足条件的二进制串一共有$3$个，分别为<code>1011001</code>，<code>1101010</code>，<code>1110011</code>。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><p><del>这就是困难题的含金量吗…</del></p><p>首先分析样例：</p><script type="math/tex; mode=display">\begin{aligned}\underbrace{\underline{}\ \underline{}\ \underline{}\ \underline{}}_3\ \underline{}\ \underline{}\ \underline{} \quad \cdots \quad \mathrm{1} \\\underline{}\ \underbrace{\underline{}\ \underline{}\ \underline{}\ \underline{}}_2\ \underline{}\ \underline{} \quad \cdots \quad \mathrm{2} \\\underline{}\ \underline{}\ \underbrace{\underline{}\ \underline{}\ \underline{}\ \underline{}}_2\ \underline{} \quad \cdots \quad \mathrm{3} \\\underline{}\ \underline{}\ \underline{}\ \underbrace{\underline{}\ \underline{}\ \underline{}\ \underline{}}_2 \quad \cdots \quad \mathrm{4}\end{aligned}</script><p>首先看第一行和第二行，可以看出2、3和4号位是相同的，但是1234的和却比2345的和多1，那么可以确定1号位为1，5号位为0：</p><script type="math/tex; mode=display">\begin{aligned}\underbrace{\underline{1}\ \underline{}\ \underline{}\ \underline{}}_3\ \underline{0}\ \underline{}\ \underline{} \quad \cdots \quad \mathrm{1} \\\underline{1}\ \underbrace{\underline{}\ \underline{}\ \underline{}\ \underline{0}}_2\ \underline{}\ \underline{} \quad \cdots \quad \mathrm{2} \\\underline{1}\ \underline{}\ \underbrace{\underline{}\ \underline{}\ \underline{0}\ \underline{}}_2\ \underline{} \quad \cdots \quad \mathrm{3} \\\underline{1}\ \underline{}\ \underline{}\ \underbrace{\underline{}\ \underline{0}\ \underline{}\ \underline{}}_2 \quad \cdots \quad \mathrm{4}\end{aligned}</script><p>然后看第二行和第三行，2345位和3456位上的和是相同的，那么可以推出2号位的值和6号位上的值相等：</p><script type="math/tex; mode=display">\begin{aligned}\underbrace{\underline{1}\ \underline{a}\ \underline{}\ \underline{}}_3\ \underline{0}\ \underline{a}\ \underline{} \quad \cdots \quad \mathrm{1} \\\underline{1}\ \underbrace{\underline{a}\ \underline{}\ \underline{}\ \underline{0}}_2\ \underline{a}\ \underline{} \quad \cdots \quad \mathrm{2} \\\underline{1}\ \underline{a}\ \underbrace{\underline{}\ \underline{}\ \underline{0}\ \underline{a}}_2\ \underline{} \quad \cdots \quad \mathrm{3} \\\underline{1}\ \underline{a}\ \underline{}\ \underbrace{\underline{}\ \underline{0}\ \underline{a}\ \underline{}}_2 \quad \cdots \quad \mathrm{4}\end{aligned}</script><p>再看第三行和第四行，同理，3号位和7号位的值是相同的：</p><script type="math/tex; mode=display">\begin{aligned}\underbrace{\underline{1}\ \underline{a}\ \underline{b}\ \underline{}}_3\ \underline{0}\ \underline{a}\ \underline{b} \quad \cdots \quad \mathrm{1} \\\underline{1}\ \underbrace{\underline{a}\ \underline{b}\ \underline{}\ \underline{0}}_2\ \underline{a}\ \underline{b} \quad \cdots \quad \mathrm{2} \\\underline{1}\ \underline{a}\ \underbrace{\underline{b}\ \underline{}\ \underline{0}\ \underline{a}}_2\ \underline{b} \quad \cdots \quad \mathrm{3} \\\underline{1}\ \underline{a}\ \underline{b}\ \underbrace{\underline{}\ \underline{0}\ \underline{a}\ \underline{b}}_2 \quad \cdots \quad \mathrm{4}\end{aligned}</script><p>那么此时，整个式子只有4号位是没有被确定的了，且只要前$K$位被确定后，整个串就被唯一确定了。</p><p>那么易知，只要在2、3和4三个位上选取两位为1即可满足条件，那么最终的结果就是$C_3^2 = 3$。</p><p>那么如果将当前字串的和称为$S_i$，$S_i$第一个元素的取值为$x$，下一个字串的和称为$S_{i + 1}$，$S_{i + 1}$最后一个元素的取值为$y$，有如下三种情况：</p><ol><li>$S_i = S_{i + 1}$，那么有$x = y$；</li><li>$S_i &gt; S_{i + 1}$，那么有$x = 1, y = 0$；</li><li>$S_i &lt; S_{i + 1}$，那么有$x = 0, y = 1$。</li></ol><p>对于第一种情况，我们需要并查集来维护“值相等”的共同属性。</p><p>除此之外，注意到第一个字串共有$K$个位置，其中有$c_0$个0，$c_1$个1，那么未确定位的个数为$K - c_0  - c_1$，此时需要从这$K - c_0  - c_1$个位中选出$S_0 - c_1$个位置来放1，那么总共有$C_{K - c_0 - c_1}^{S_0 - c_1}$种选法，并且由于这里的结果较大，需要使用优化后的方法来求组合数。</p><h4 id="并查集"><a href="#并查集" class="headerlink" title="并查集"></a>并查集</h4><p><a href="https://www.acwing.com/problem/content/838/">836. 合并集合 - AcWing题库</a></p><p>核心代码为：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(p[x] != x) p[x] = <span class="built_in">find</span>(p[x]);</span><br><span class="line">    <span class="comment">// p[x]是自己的父节点</span></span><br><span class="line">    <span class="comment">// find(p[x])找到自己父节点的根节点</span></span><br><span class="line">    <span class="comment">// p[x] = find(p[x])</span></span><br><span class="line">    <span class="comment">// 令自己的父节点等于自己父节点的根节点</span></span><br><span class="line">    <span class="keyword">return</span> p[x];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="快速幂"><a href="#快速幂" class="headerlink" title="快速幂"></a>快速幂</h4><p><a href="https://www.acwing.com/problem/content/878/">876. 快速幂求逆元 - AcWing题库</a></p><h4 id="组合数"><a href="#组合数" class="headerlink" title="组合数"></a>组合数</h4><p><a href="https://www.acwing.com/problem/content/888/">886. 求组合数 II - AcWing题库</a></p><p>最后的代码为：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1000010</span>, MOD = <span class="number">1e6</span> + <span class="number">3</span>;</span><br><span class="line"><span class="type">int</span> n, k;</span><br><span class="line"><span class="type">int</span> s[N], p[N], v[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">find</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (p[x] != x) p[x] = <span class="built_in">find</span>(p[x]);</span><br><span class="line">    <span class="keyword">return</span> p[x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">qmi</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (b)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (b &amp; <span class="number">1</span>)</span><br><span class="line">            res = (<span class="type">long</span> <span class="type">long</span>)res * a % MOD;</span><br><span class="line">        a = (<span class="type">long</span> <span class="type">long</span>)a * a % MOD;</span><br><span class="line">        b &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">C</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> fa = <span class="number">1</span>, fb = <span class="number">1</span>, fab = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= a; i ++)</span><br><span class="line">        fa = (<span class="type">long</span> <span class="type">long</span>)fa * i % MOD;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= b; i ++)</span><br><span class="line">        fb = (<span class="type">long</span> <span class="type">long</span>)fb * i % MOD;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= a - b; i ++)</span><br><span class="line">        fab = (<span class="type">long</span> <span class="type">long</span>)fab * i % MOD;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (<span class="type">long</span> <span class="type">long</span>)fa * <span class="built_in">qmi</span>(fb, MOD - <span class="number">2</span>) * <span class="built_in">qmi</span>(fab, MOD - <span class="number">2</span>) % MOD;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        p[i] = i, v[i] = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n - k + <span class="number">1</span>; i ++)</span><br><span class="line">        cin &gt;&gt; s[i];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, j = k; i &lt; n - k; i ++, j ++)</span><br><span class="line">        <span class="keyword">if</span> (s[i] == s[i + <span class="number">1</span>])</span><br><span class="line">            p[<span class="built_in">find</span>(j)] = <span class="built_in">find</span>(i);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, j = k; i &lt; n - k; i ++, j ++)</span><br><span class="line">        <span class="keyword">if</span> (s[i] &gt; s[i + <span class="number">1</span>])</span><br><span class="line">            v[<span class="built_in">find</span>(i)] = <span class="number">1</span>, v[<span class="built_in">find</span>(j)] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (s[i] &lt; s[i + <span class="number">1</span>])</span><br><span class="line">            v[<span class="built_in">find</span>(i)] = <span class="number">0</span>, v[<span class="built_in">find</span>(j)] = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> c0 = <span class="number">0</span>, c1 = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; k; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> t = v[<span class="built_in">find</span>(i)];</span><br><span class="line">        <span class="keyword">if</span> (t == <span class="number">0</span>)</span><br><span class="line">            c0 ++;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (t == <span class="number">1</span>)</span><br><span class="line">            c1 ++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; <span class="built_in">C</span>(k - c0 - c1, s[<span class="number">0</span>] - c1) &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">1000010</span></span><br><span class="line">MOD = <span class="number">1000003</span> <span class="comment"># 这里不能写1e6 + 3，会被认为是浮点数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find</span>(<span class="params">x:<span class="built_in">int</span></span>):</span><br><span class="line">    <span class="keyword">if</span> p[x] != x:</span><br><span class="line">        p[x] = find(p[x])</span><br><span class="line">    <span class="keyword">return</span> p[x]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">qmi</span>(<span class="params">a:<span class="built_in">int</span>, b:<span class="built_in">int</span></span>):</span><br><span class="line">    res = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> b:</span><br><span class="line">        <span class="keyword">if</span> b &amp; <span class="number">1</span>:</span><br><span class="line">            res = res * a % MOD</span><br><span class="line">        a = a * a % MOD</span><br><span class="line">        b &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">C</span>(<span class="params">a:<span class="built_in">int</span>, b:<span class="built_in">int</span></span>):</span><br><span class="line">    fa = <span class="number">1</span></span><br><span class="line">    fb = <span class="number">1</span></span><br><span class="line">    fab = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, a + <span class="number">1</span>):</span><br><span class="line">        fa = fa * i % MOD</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, b + <span class="number">1</span>):</span><br><span class="line">        fb = fb * i % MOD;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, a - b + <span class="number">1</span>):</span><br><span class="line">        fab = fab * i % MOD;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> fa * qmi(fb, MOD - <span class="number">2</span>) * qmi(fab, MOD - <span class="number">2</span>) % MOD</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    nk = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    n = nk[<span class="number">0</span>]</span><br><span class="line">    k = nk[<span class="number">1</span>]</span><br><span class="line">    p = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    v = [-<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    s = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">0</span>, n - k), <span class="built_in">range</span>(k, n)):</span><br><span class="line">        <span class="keyword">if</span> s[i] == s[i + <span class="number">1</span>]:</span><br><span class="line">            p[find(j)] = find(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="built_in">range</span>(<span class="number">0</span>, n - k), <span class="built_in">range</span>(k, n)):</span><br><span class="line">        <span class="keyword">if</span> s[i] &gt; s[i + <span class="number">1</span>]:</span><br><span class="line">            v[find(i)] = <span class="number">1</span></span><br><span class="line">            v[find(j)] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> s[i] &lt; s[i + <span class="number">1</span>]:</span><br><span class="line">            v[find(i)] = <span class="number">0</span></span><br><span class="line">            v[find(j)] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    c0 = <span class="number">0</span></span><br><span class="line">    c1 = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        t = v[find(i)]</span><br><span class="line">        <span class="keyword">if</span> t == <span class="number">0</span>:</span><br><span class="line">            c0 += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> t == <span class="number">1</span>:</span><br><span class="line">            c1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">int</span>(C(k - c0 - c1, s[<span class="number">0</span>] - c1)))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5170. 二进制。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5166. 对称山脉</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5166/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5166/</id>
    <published>2024-01-04T02:00:00.000Z</published>
    <updated>2024-01-04T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5166-对称山脉"><a href="#AcWing-5166-对称山脉" class="headerlink" title="AcWing 5166. 对称山脉"></a>AcWing 5166. 对称山脉</h1><p><a href="https://www.acwing.com/problem/content/5169/">5166. 对称山脉 - AcWing题库</a></p><p>有$N$座山排成一排，从左到右依次编号为$1 \sim N$。</p><p>其中，第$i$座山的高度为$h_i$。</p><p>对于一段<strong>连续的</strong>山脉，我们使用如下方法定义该段山脉的不对称值。</p><p>如果一段连续的山脉由第$l \sim r$（$1 \le l \le r \le N$）座山组成，那么该段山脉的不对称值为$\sum_{0 \le i \le \frac{r - l}{2}}\left|h_{l + i} - h_{r - i}\right|$。</p><p>现在，你需要回答$N$个问题，问题编号$1 \sim N$。</p><p>其中，第$i$个问题的内容是：请计算，一段恰好包含$i$座山的<strong>连续</strong>山脉（即长度为$i$的连续区间）的不对称值的最小可能值。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含一个整数$N$。</p><p>第二行包含$N$个整数$h_1, h_2, \cdots, h_N$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>输出一行$N$个整数，其中第$i$个数表示第$i$个问题的答案。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le N \le 5000, \\0 &\le h_i \le 10^5。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">7</span><br><span class="line">3 1 4 1 5 9 2</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 2 0 5 2 10 10</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>关于第$5$个问题的答案为什么是$2$，见如下解析。</p><p>让我们依次列举所有长度为$5$的连续区间并计算区间不对称值：</p><ul><li>区间$[3, 1, 4, 1, 5]$的不对称值为$\left|3 - 5\right| + \left|1 - 1\right| + \left|4 - 4\right| = 2$。</li><li>区间$[1, 4, 1, 5, 9]$的不对称值为$\left|1 - 9\right| + \left|4 - 5\right| + \left|1 - 1\right| = 9$。</li><li>区间$[4, 1, 5, 9, 2]$的不对称值为$\left|4 - 2\right| + \left|1 - 9\right| + \left|5 - 5\right| = 10$。</li></ul><p>由上可知，不对称值的最小可能值为$2$。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">1 3 5 6</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0 1 3 7</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>注意，长度为$4$的连续区间只有$[1, 3, 5, 6]$，其不对称值为$\left|1 - 6\right| + \left|3 - 5\right| = 7$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p><del>很久没写真是退化的严重…</del></p><p>下面是一个TLE的版本：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">5050</span>;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> h[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">        cin &gt;&gt; h[i];</span><br><span class="line">    <span class="type">int</span> res;</span><br><span class="line">    <span class="comment">// 枚举长度</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        res = <span class="number">2100000000</span>;</span><br><span class="line">        <span class="type">int</span> temp;</span><br><span class="line">        <span class="comment">// 枚举起点</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n - i + <span class="number">1</span>; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            temp = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = j; k &lt;= j + i / <span class="number">2</span> - <span class="number">1</span>; k ++)</span><br><span class="line">                &#123;</span><br><span class="line">                    temp += <span class="built_in">abs</span>(h[k] - h[j + i - <span class="number">1</span> - (k - j)]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = j; k &lt;= j + i / <span class="number">2</span>; k ++)</span><br><span class="line">                &#123;</span><br><span class="line">                    temp += <span class="built_in">abs</span>(h[k] - h[j + i - <span class="number">1</span> - (k - j)]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            res = <span class="built_in">min</span>(res, temp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 每个长度输出一次</span></span><br><span class="line">        cout &lt;&lt; res &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><p><del>但是看了题解发现其实退不退化我都不会…</del></p><p>大概的思路是遍历整个序列，从而遍历所有可能的中心（在奇数情况下选取一个山作为中心，偶数情况则选取两个山作为中心），然后依次向两侧扩展，此时连续序列长度为$i + 2$的不对称值就会依赖于连续序列长度为$i$的不对称值，且由于奇数偶数都考虑过，所以可以将所有可能的长度都进行计算。</p><p>并且将连续序列长度为$i$的不对称值存储在<code>f[i]</code>中。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">5050</span>;</span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> h[N], f[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        cin &gt;&gt; h[i];</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(f, <span class="number">0x3f</span>, <span class="keyword">sizeof</span> f);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 长度为奇数的序列</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> l = i, r = i, s = <span class="number">0</span>; l &gt;= <span class="number">0</span> &amp;&amp; r &lt; n; l --, r ++)</span><br><span class="line">        &#123;</span><br><span class="line">            s += <span class="built_in">abs</span>(h[r] - h[l]);</span><br><span class="line">            f[r - l + <span class="number">1</span>] = <span class="built_in">min</span>(f[r - l + <span class="number">1</span>], s);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 长度为偶数的序列</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> l = i, r = i + <span class="number">1</span>, s = <span class="number">0</span>; l &gt;= <span class="number">0</span> &amp;&amp; r &lt; n; l --, r ++)</span><br><span class="line">        &#123;</span><br><span class="line">            s += <span class="built_in">abs</span>(h[r] - h[l]);</span><br><span class="line">            f[r - l + <span class="number">1</span>] = <span class="built_in">min</span>(f[r - l + <span class="number">1</span>], s);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">        cout &lt;&lt; f[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于最近在学Python语法，所以Python版本如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> inf</span><br><span class="line"></span><br><span class="line">N = <span class="number">5050</span></span><br><span class="line">f = [inf <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    n = <span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    h = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, <span class="built_in">input</span>().split(<span class="string">&#x27; &#x27;</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="comment"># 奇数的情况</span></span><br><span class="line">        l = i</span><br><span class="line">        r = i</span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> l &gt;= <span class="number">0</span> <span class="keyword">and</span> r &lt; n:</span><br><span class="line">                s += <span class="built_in">abs</span>(h[r] - h[l])</span><br><span class="line">                f[r - l + <span class="number">1</span>] = <span class="built_in">min</span>(f[r - l + <span class="number">1</span>], s)</span><br><span class="line">                l -= <span class="number">1</span></span><br><span class="line">                r += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 偶数的情况</span></span><br><span class="line">        l = i</span><br><span class="line">        r = i + <span class="number">1</span></span><br><span class="line">        s = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> l &gt;= <span class="number">0</span> <span class="keyword">and</span> r &lt; n:</span><br><span class="line">                s += <span class="built_in">abs</span>(h[r] - h[l])</span><br><span class="line">                f[r - l + <span class="number">1</span>] = <span class="built_in">min</span>(f[r - l + <span class="number">1</span>], s)</span><br><span class="line">                l -= <span class="number">1</span></span><br><span class="line">                r += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(*f[<span class="number">1</span>:n + <span class="number">1</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5166. 对称山脉。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Triton：An Intermediate Language and Compiler for Tiled Neural Network Computations》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/"/>
    <id>http://blog.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/</id>
    <published>2023-12-26T11:00:00.000Z</published>
    <updated>2023-12-26T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Triton-An-Intermediate-Language-and-Compiler-for-Tiled-Neural-Network-Computations"><a href="#Triton-An-Intermediate-Language-and-Compiler-for-Tiled-Neural-Network-Computations" class="headerlink" title="Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations"></a>Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>深度学习领域新颖研究想法的验证和部署通常受到某些基本原语高效计算内核的可用性限制。特别是，无法利用现有供应商库（例如cuBLAS、cuDNN）的操作面临着设备利用率不佳的风险，除非由专家编写自定义实现——通常以牺牲可移植性为代价。因此，开发新的编程抽象来以最小的性能成本指定自定义深度学习工作负载变得至关重要。</p><p>我们提出了Triton，一种以图块（tile）概念为中心的语言和编译器，即静态形状的多维子数组。我们的方法围绕：</p><ol><li>基于C语言和基于LLVM的中间表示（IR），用于根据参数图块变量的操作来表达张量程序；</li><li>以及一组新颖的图块级优化过程，用于将这些程序编译成高效的GPU代码。</li></ol><p>我们将演示如何使用Triton来构建矩阵乘法和卷积核的可移植实现，与手工调优的供应商库（cuBLAS/cuDNN）相当，或者高效地实现最近的研究想法，如移位卷积。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>深度神经网络（DNN）最近的复兴很大程度上得益于可编程并行计算设备的广泛使用[24]。特别是，多核架构（例如GPU）性能的持续改进发挥了基础性作用，使研究人员和工程师能够使用越来越多的数据来探索越来越多、越来越大的模型。这项工作得到了一系列供应商库（cuBLAS、cuDNN）的支持，旨在尽快为从业者带来最新的硬件创新。不幸的是，这些库仅支持一组有限的张量运算，将新原语的实现留给专家[13, 17, 25]。</p><p>这一观察导致了基于多面体机制（例如张量理解[43]）和/或循环合成技术（例如Halide[37]、TVM[10]和PlaidML[22]）的DNN的各种特定领域语言（Domain-Specific Languages, DSLs）的开发。但是，虽然这些系统通常对于某些类别的问题表现良好，例如深度可分离卷积（例如MobileNet[20]），但在实践中它们通常比供应商库慢得多（参见图1），并且缺乏实现结构化稀疏模式所需的表达能力[28, 31, 47]，无法在嵌套循环中使用仿射数组索引直接指定。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226200619856.png" alt="image-20231226200619856"></p><p>这些问题通常通过使用微内核来解决[11, 21]——即手写的图块级内在函数——但这种解决方案需要大量的体力劳动并且缺乏可移植性。尽管最近提出了几种用于图块的高级编程抽象[23, 41]，但底层编译器后端仍然缺乏对图块级操作和优化的支持。为此，我们推出Triton（图2），这是一种<a href="https://triton-lang.org">开源的</a>中间语言和编译器，用于指定图块程序并将其编译为高效的GPU代码。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226200639026.png" alt="image-20231226200639026"></p><p>本文的主要贡献总结如下：</p><ul><li><p><strong>Triton-C</strong>（第3节）：一种类似C的语言，用于根据参数图块变量表达张量程序。该语言的目的是为现有的DNN转编译器（例如PlaidML、Tensor Compressive）和熟悉CUDA的程序员提供稳定的接口。Listing 1显示了与简单矩阵乘法任务相关的Triton-C源代码。</p><p>  <img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226202413994.png" alt="image-20231226202413994"></p>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Tile shapes are parametric and can be optimized</span></span><br><span class="line"><span class="comment">// by compilation backends</span></span><br><span class="line"><span class="type">const</span> tunable <span class="type">int</span> TM = &#123;<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>&#125;</span><br><span class="line"><span class="type">const</span> tunable <span class="type">int</span> TN = &#123;<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>&#125;</span><br><span class="line"><span class="type">const</span> tunable <span class="type">int</span> TK = &#123;<span class="number">8</span>, <span class="number">16</span>&#125;</span><br><span class="line"><span class="comment">// C = A * B.T</span></span><br><span class="line">kernel <span class="type">void</span> <span class="built_in">matmul_nt</span>(<span class="type">float</span>* a, <span class="type">float</span>* b, <span class="type">float</span>* c,</span><br><span class="line">                     <span class="type">int</span> M, <span class="type">int</span> N, <span class="type">int</span> K)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 1D tile of indices</span></span><br><span class="line">    <span class="type">int</span> rm[TM] = <span class="built_in">get_global_range</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> rn[TN] = <span class="built_in">get_global_range</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="type">int</span> rk[TK] = <span class="number">0</span> ... TK;</span><br><span class="line">    <span class="comment">// 2D tile of pointers</span></span><br><span class="line">    <span class="type">float</span>* pa[TM, TK] = a + rm[:, newaxis] + rk * M;</span><br><span class="line">    <span class="type">float</span>* pb[TN, TK] = b + rn[:, newaxis] + rk * K;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = K; k &gt;= <span class="number">0</span>; k -= TK)&#123;</span><br><span class="line">        <span class="type">bool</span> check_k[TK] = rk &lt; k;</span><br><span class="line">        <span class="type">bool</span> check_a[TM, TK] = (rm &lt; M)[:, newaxis] &amp;&amp; check_k;</span><br><span class="line">        <span class="type">bool</span> check_b[TN, TK] = (rn &lt; N)[:, newaxis] &amp;&amp; check_k;</span><br><span class="line">        <span class="comment">// load tile operands</span></span><br><span class="line">        <span class="type">float</span> A[TM, TK] = check_a ? *pa : <span class="number">0</span>;</span><br><span class="line">        <span class="type">float</span> B[TN, TK] = check_b ? *pb : <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// accumulate</span></span><br><span class="line">        C += <span class="built_in">dot</span>(A, <span class="built_in">trans</span>(B));</span><br><span class="line">        <span class="comment">// update pointers</span></span><br><span class="line">        pa = pa + TK * M;</span><br><span class="line">        pb = pb + TK * N;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// write-back accumulators</span></span><br><span class="line">    <span class="type">float</span>* pc[TM, TN] = c + rm[:, newaxis] + rn * M;</span><br><span class="line">    <span class="type">bool</span> check_c[TM, TN] = (rm &lt; M)[:, newaxis] &amp;&amp; (rn &lt; N);</span><br><span class="line">    @check_c *pc = C;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>Triton-IR</strong>（第4节）：基于LLVM的中间表示（IR），提供适合切片级程序分析、转换和优化的环境。Listing 5显示了修正线性单元（ReLU）函数的Triton-IR代码。这里，Triton-IR程序是在解析过程中直接从Triton-C构建的，但未来也可以探索从嵌入式DSL或更高级别的DNN编译器（例如TVM）自动生成。</p><p>  <img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226202744799.png" alt="image-20231226202744799"></p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">define kernel void @relu(float* %A, i32 %M, i32 %N) &#123;</span><br><span class="line">prologue:</span><br><span class="line">    %rm = call i32&lt;8&gt; get_global_range(0);</span><br><span class="line">    %rn = call i32&lt;8&gt; get_global_range(1);</span><br><span class="line">    ; broadcast shapes</span><br><span class="line">    %1 = reshape i32&lt;8, 8&gt; %M;</span><br><span class="line">    %M0 = broadcast i32&lt;8, 8&gt; %1;</span><br><span class="line">    %2 = reshape i32&lt;8, 8&gt; %N;</span><br><span class="line">    %N0 = broadcast i32&lt;8, 8&gt; %2;</span><br><span class="line">    ; broadcast global ranges</span><br><span class="line">    %3 = reshape i32&lt;8, 1&gt; %rm;</span><br><span class="line">    %rm_bc = broadcast i32&lt;8, 8&gt; %3;</span><br><span class="line">    %4 = reshape i32&lt;1, 8&gt; %rn;</span><br><span class="line">    %rn_bc = broadcast i32&lt;8, 8&gt; %4;</span><br><span class="line">    ; compute pointer</span><br><span class="line">    %A0 = splat float*&lt;8, 8&gt; %A;</span><br><span class="line">    %5 = getelementptr %A0, %rm_bc;</span><br><span class="line">    %6 = mul %rn_bc, %M0;</span><br><span class="line">    %pa = getelementptr %5, %6;</span><br><span class="line">    ; compute result</span><br><span class="line">    %a = load %pa;</span><br><span class="line">    %_0 = splat float&lt;8, 8&gt; 0;</span><br><span class="line">    %result = max %float %a, %_0;</span><br><span class="line">    ; write back</span><br><span class="line">    store fp32&lt;8, 8&gt; %pa, %result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>Triton-JIT</strong>（第5节）：即时（Just-In-Time, JIT）编译器和代码生成后端，用于将Triton-IR程序编译为高效的LLVM位代码。这包括：</p><ol><li>一组图块级、独立于机器的通道，旨在独立于任何编译目标简化输入计算内核；</li><li>一组图块级机器相关通道，用于生成高效的GPU就绪LLVM-IR；</li><li>一个自动调节器，用于优化与上述过程相关的任何元参数。</li></ol></li><li><p><strong>Numerical Experiments</strong>（第6节）：对Triton的数值评估证明了它的能力：</p><ol><li>在循环神经网络和Transformer上生成与cuBLAS相当的矩阵乘法实现，并且比替代的DSL快3倍；</li><li>重新实现了cuDNN中用于密集卷积的<code>IMPLICIT_GEMM</code>算法，而不损失性能；</li><li>创建新颖的研究思想的有效实现，例如shift-conv[47]模块。</li></ol></li></ul><p>本文将以对现有相关文献的简要分析作为序言（第2节），并以总结和未来工作方向作为总结（第7节）。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><p>深度学习框架[1, 9, 36]和库的存在对于新型神经网络架构和算法的出现至关重要。但是，尽管线性代数编译器的分析[5, 48]和经验[6, 30]启发式方法取得了进步，这些软件仍然总是依赖于手动优化的子例程（例如cuBLAS和cuDNN）。这导致了各种DSL和DNN编译器的开发，通常基于三种不同的方法之一：</p><ul><li><strong>Tensor-level IRs</strong>：XLA[16]和Glow[38]已经使用张量级IRs来使用模式匹配将张量程序转换为预定义的LLVM-IR和CUDA-C操作模板(例如，张量收缩、元素级操作等)。</li><li><strong>The polyhedral model</strong>：张量理解（TC）[43]和Diesel[14]使用多面体模型[18]，将一个或多个DNN层参数化并自动编译成LLVM-IR和CUDA-C程序。</li><li><strong>Loop synthesizers</strong>：Halide[37]和TVM[10]已经使用循环合成器来将张量计算转换为可以使用用户定义（尽管可能是参数化的[11]）调度手动优化的环路嵌套式。</li></ul><p>相比之下，Triton依赖于在传统编译管道中添加图块级操作和优化。这种方法提供了：</p><ol><li>较XLA和Glow更多的灵活性；</li><li>支持非仿射张量下标，与TC和Diesel相反；</li><li>自动推断可能的执行计划，否则必须手动指定到Halide或TVM。</li></ol><p>Triton的好处是以增加编程工作为代价的——参见Listing 2，了解这些DSL中矩阵乘法的实现。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226203345242.png" alt="image-20231226203345242"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">C = tf.<span class="built_in">matmul</span>(A, tf.<span class="built_in">transpose</span>(B))<span class="comment">// TF</span></span><br><span class="line">C[i, j: I, J] = +(A[i, k] * B[j, k]);<span class="comment">// PlaidML</span></span><br><span class="line"><span class="built_in">C</span>(i, j) +=! <span class="built_in">A</span>(i, k) * <span class="built_in">B</span>(j, k)<span class="comment">// TC</span></span><br><span class="line">tvm.<span class="built_in">sum</span>(A[i, k] * B[j, k], axis=k)<span class="comment">// TVM</span></span><br></pre></td></tr></table></figure><h2 id="3-The-Triton-C-Language"><a href="#3-The-Triton-C-Language" class="headerlink" title="3 The Triton-C Language"></a>3 The Triton-C Language</h2><p>Triton-C的目的是为现有（和未来）DNN转编译器以及熟悉低级GPU编程的程序员提供稳定的前端。在本节中，我们将描述Triton-C的类似CUDA的语法（第3.1节）、类似Numpy[35]的语义（第3.2节）及其“单程序、多数据”（SPMD）的编程模型（第3.3节） 。</p><h3 id="3-1-Syntax"><a href="#3-1-Syntax" class="headerlink" title="3.1 Syntax"></a>3.1 Syntax</h3><p>Triton-C的语法基于ANSI C（更具体地说是CUDA-C）的语法，但进行了修改和扩展（参见Listing 3）以适应接下来两小节中描述的语义和编程模型。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226204252233.png" alt="image-20231226204252233"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Broadcasting semantics</span></span><br><span class="line">slice: <span class="string">&#x27;:&#x27;</span> | <span class="string">&#x27;newaxis&#x27;</span></span><br><span class="line">slice_list: slice | slice_list <span class="string">&#x27;,&#x27;</span> slice</span><br><span class="line">slice_expr: postfix_expr | expr <span class="string">&#x27;[&#x27;</span> slice_list <span class="string">&#x27;]&#x27;</span></span><br><span class="line"><span class="comment">// Range initialization</span></span><br><span class="line">constant_range: expr <span class="string">&#x27;...&#x27;</span> expr</span><br><span class="line"><span class="comment">// Intrinsics</span></span><br><span class="line">global_range: <span class="string">&#x27;get_global_range&#x27;</span> <span class="string">&#x27;(&#x27;</span> constant <span class="string">&#x27;get_global_range&#x27;</span></span><br><span class="line">dot: <span class="string">&#x27;dot&#x27;</span> <span class="string">&#x27;(&#x27;</span> expr <span class="string">&#x27;,&#x27;</span> expr <span class="string">&#x27;)&#x27;</span></span><br><span class="line">trans: <span class="string">&#x27;trans&#x27;</span> <span class="string">&#x27;(&#x27;</span> expr <span class="string">&#x27;,&#x27;</span> expr <span class="string">&#x27;)&#x27;</span></span><br><span class="line">intrinsic_expr: global_range | dot | trans</span><br><span class="line"><span class="comment">// Predication</span></span><br><span class="line">predicate_expr: <span class="string">&#x27;@&#x27;</span> expr</span><br><span class="line"><span class="comment">// Tile extensions for abstract declarators</span></span><br><span class="line">abstract_decl: abstract_decl | <span class="string">&#x27;[&#x27;</span> constant_list <span class="string">&#x27;]&#x27;</span></span><br><span class="line"><span class="comment">// Extensions of C expressions</span></span><br><span class="line">expr: expr | constant_range | slice_expr | intrinsic_expr</span><br><span class="line"><span class="comment">// Extensions of C specifiers</span></span><br><span class="line">storage_spec: storage_spec | <span class="string">&#x27;kernel&#x27;</span></span><br><span class="line">type_spec: type_spec | <span class="string">&#x27;tunable&#x27;</span></span><br><span class="line"><span class="comment">// Extensions of C statements</span></span><br><span class="line">statement: statement | predicate_expr statement</span><br></pre></td></tr></table></figure><p>这些变化分为以下几类：</p><ul><li><strong>Tile declarations</strong>：我们添加了用于声明多维数组的特殊语法（例如<code>int tile[16, 16]</code>）以强调它们与ANSI C中的嵌套数组（例如<code>int tile[16][16]</code>）的语义差异。图块形状必须恒定，但也可以使用<code>tunable</code>关键字进行参数化。一维整数图块可以使用省略号来初始化（例如<code>int range[8] = 0 ... 8</code>）。</li><li><strong>Built-in function</strong>：虽然保留了常见的C语法用于逐元素数组操作（<code>+, -, &amp;&amp;, *</code>等等），但添加了各种内置函数（<code>dot, trans, get_global_range</code>）以支持图块语义（第3.2.1节）和SPMD编程模型。</li><li><strong>Broadcasting</strong>：N维图块可以使用<code>newaxis</code>关键字和常用切片语法沿任何特定轴进行广播（例如，用于堆叠列的<code>int Broadcast [8, 8] = range[:, newaxis]</code>）。注意，在其他情况下，禁止将图块切片来检索标量或子数组。</li><li><strong>Predication</strong>：图块操作（第4.3节）中的基本控制流是通过使用“@”前缀来实现的。</li></ul><h3 id="3-2-Semantics"><a href="#3-2-Semantics" class="headerlink" title="3.2 Semantics"></a>3.2 Semantics</h3><h4 id="3-2-1-Tile-Semantics"><a href="#3-2-1-Tile-Semantics" class="headerlink" title="3.2.1 Tile Semantics"></a>3.2.1 Tile Semantics</h4><p>Triton-C中内置图块类型和操作（即图块语义）的存在提供了两个主要好处。首先，它通过隐藏与块内内存合并[12]、缓存管理[32]和专用硬件利用率[27]相关的重要性能细节来简化张量程序的结构。其次，它为编译器自动执行这些优化打开了大门，如第5节所述。</p><h4 id="3-2-2-Broadcasting-Semantics"><a href="#3-2-2-Broadcasting-Semantics" class="headerlink" title="3.2.2 Broadcasting Semantics"></a>3.2.2 Broadcasting Semantics</h4><p>Triton-C中的图块是强类型的，因为某些指令静态地要求其操作数遵守严格的形状约束。例如，除非首先适当地广播标量，否则不能将标量添加到数组中。广播语义[35]提供了一组规则来隐式执行这些转换（参见Listing 4的示例）：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226205354247.png" alt="image-20231226205354247"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> a[<span class="number">16</span>], b[<span class="number">32</span>, <span class="number">16</span>], c[<span class="number">16</span>, <span class="number">1</span>];</span><br><span class="line"><span class="comment">// a is first reshaped to [1, 16]</span></span><br><span class="line"><span class="comment">// and then broadcast to [32, 16]</span></span><br><span class="line"><span class="type">int</span> x_l[<span class="number">32</span>, <span class="number">16</span>] = a[newaxis, :] + b;</span><br><span class="line"><span class="comment">// Same as above but implicitly</span></span><br><span class="line"><span class="type">int</span> x_2[<span class="number">32</span>, <span class="number">16</span>] = a + b;</span><br><span class="line"><span class="comment">// a is first reshaped to [1, 16]</span></span><br><span class="line"><span class="comment">// a is broadcast to [16, 16]</span></span><br><span class="line"><span class="comment">// c is broadcast to [16, 16]</span></span><br><span class="line"><span class="type">int</span> y[<span class="number">16</span>, <span class="number">16</span>] = a + c;</span><br></pre></td></tr></table></figure><ol><li><strong>Padding</strong>：最短操作数的形状用1进行左填充，直到两个操作数具有相同的维度。</li><li><strong>Broadcasting</strong>：根据需要，多次复制两个操作数的内容，直到它们的形状相同；如果无法完成此操作，则会发出错误。</li></ol><h3 id="3-3-Programming-Model"><a href="#3-3-Programming-Model" class="headerlink" title="3.3 Programming Model"></a>3.3 Programming Model</h3><p>GPU上CUDA[33]代码的执行由SPMD[4]编程模型支持，其中每个内核与所谓的启动网格中的可识别线程块相关联。Triton编程模型类似，但每个内核都是单线程的（尽管自动并行化），并且与一组因实例而异的全局范围相关联（参见图3）。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226205549047.png" alt="image-20231226205549047"></p><p>这种方法导致内核更简单，其中不存在类似CUDA的并发原语（共享内存同步、线程间通信等）。</p><p>可以使用<code>get_global_range(axis)</code>内置函数查询与内核关联的全局范围，以便创建指针图块，如Listing 1所示。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226202413994.png" alt="image-20231226202413994"></p><h2 id="4-The-Triton-IR"><a href="#4-The-Triton-IR" class="headerlink" title="4 The Triton IR"></a>4 The Triton IR</h2><p>Triton-IR是一种基于LLVM的中间表示（IR），其目的是提供适合图块级程序分析、转换和优化的环境。在这项工作中，Triton-IR程序是在解析过程中直接从Triton-C构建的，尽管将来它们也可以直接从更高级别的DSL生成。</p><p>Triton-IR和LLVM-IR程序共享相同的高级结构（第4.1节中回顾），但前者还包括图块级数据流（第4.2节）和控制流（第4.3节）分析所需的许多扩展。这些新颖的扩展对于执行第5节中概述的优化以及安全访问第6节中所示的任意形状的张量至关重要。</p><h3 id="4-1-Structure"><a href="#4-1-Structure" class="headerlink" title="4.1 Structure"></a>4.1 Structure</h3><h4 id="4-1-1-Modules"><a href="#4-1-1-Modules" class="headerlink" title="4.1.1 Modules"></a>4.1.1 Modules</h4><p>在最高级别，Triton-IR程序由一个或多个称为模块的基本编译单元组成。这些模块彼此独立编译，并最终由链接器聚合，链接器的作用是解析前向声明并充分合并全局定义。</p><p>每个模块本身都由函数、全局变量、常量和其他杂项符号（例如元数据、函数属性）组成。</p><h4 id="4-1-2-Functions"><a href="#4-1-2-Functions" class="headerlink" title="4.1.2 Functions"></a>4.1.2 Functions</h4><p>Triton-IR函数定义由返回类型、名称和可能为空的参数列表组成。如果需要，可以添加额外的可见性、对齐和链接说明符。还可以指定函数属性（例如内联提示）和参数属性（例如只读、别名提示），从而允许编译器后端执行更积极的优化，例如更好地利用只读内存缓存。</p><p>该标头后面是由基本块列表组成的主体，这些基本块的相互依赖性形成了函数的控制流图（Control Flow Graph, CFG）。</p><h4 id="4-1-3-Basic-Blocks"><a href="#4-1-3-Basic-Blocks" class="headerlink" title="4.1.3 Basic Blocks"></a>4.1.3 Basic Blocks</h4><p>根据定义，基本块是直线代码序列，其末尾可能仅包含所谓的终止符指令（即分支、返回）。</p><p>Triton-IR使用静态单赋值（Static Single Assignment, SSA）形式，这意味着每个基本块中的每个变量都必须是：</p><ol><li>仅分配一次；</li><li>在使用前定义。</li></ol><p>这样做时，每个基本块隐式定义了一个数据流图（Data-Flow Graph, DFG），其不同路径对应于程序SSA表示中的use-def链。这种形式可以直接从抽象语法树（Abstract Syntax Trees, AST）创建，如[7]所示。</p><h3 id="4-2-Support-for-Tile-Level-Data-Flow-Analysis"><a href="#4-2-Support-for-Tile-Level-Data-Flow-Analysis" class="headerlink" title="4.2 Support for Tile-Level Data-Flow Analysis"></a>4.2 Support for Tile-Level Data-Flow Analysis</h3><h4 id="4-2-1-Types"><a href="#4-2-1-Types" class="headerlink" title="4.2.1 Types"></a>4.2.1 Types</h4><p>多维图块是Triton-IR中数据流分析的中心，可以使用类似于LLVM-IR中向量声明的语法进行声明。例如，<code>i32&lt;8, 8&gt;</code>是对应于$8 \times 8$ 32位整数图块的类型。请注意，Triton-IR中没有<code>tunable</code>关键字，因此必须在生成程序之前解析参数形状值。在我们的例子中，这是由Triton-JIT的自动调节器完成的（第5.3节）。</p><h4 id="4-2-2-Instructions"><a href="#4-2-2-Instructions" class="headerlink" title="4.2.2 Instructions"></a>4.2.2 Instructions</h4><p>Triton-IR引入了一组重绘指令，其目的是支持广播语义，如第3.2.2节中所述：</p><ul><li><code>reshape</code>指令使用来自其输入参数的数据创建指定形状的图块。这对于通过用变量填充输入形状以准备隐式或显式广播来将变量重新解释为高维数组特别有用。</li><li><code>broadcast</code>指令通过沿大小为1的维度根据需要多次复制其输入参数来创建指定形状的图块——如图4所示。</li></ul><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226210454984.png" alt="image-20231226210454984"></p><p>保留并扩展了常用的标量指令（<code>cmp, getelementptr, add, load...</code>），以表示对图块操作数进行逐元素操作。最后，Triton-IR还公开了用于转置（<code>trans</code>）和矩阵乘法（<code>dot</code>）的专用算术指令。</p><h3 id="4-3-Support-for-Tile-Level-Control-Flow-Analysis"><a href="#4-3-Support-for-Tile-Level-Control-Flow-Analysis" class="headerlink" title="4.3 Support for Tile-Level Control-Flow Analysis"></a>4.3 Support for Tile-Level Control-Flow Analysis</h3><p>Triton-IR中存在图块级操作而产生的一个问题是图块内发散控制流的不可表达性。例如，程序可能需要部分保护图块级加载免受内存访问冲突，但这无法使用分支来实现，因为无法单独访问图块元素。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226210823092.png" alt="image-20231226210823092"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">;pt[i, j], pf[i, j] = (true, false) if x[i, j] &lt; 5</span><br><span class="line">;pt[i, j], pf[i, j] = (false, true) if x[i, j] &gt;= 5</span><br><span class="line">%pt, %pf = icmpp slt %x, 5</span><br><span class="line">@%pt %x1 = add &amp;y, 1</span><br><span class="line">@%pf %x2 = sub %y, 1</span><br><span class="line">; merge values from different predicates</span><br><span class="line">%x = psi i32&lt;8, 8&gt; [%pt, %x1], [%pf, %x2]</span><br><span class="line">%z = mul i32&lt;8, 8&gt; %x, 2</span><br></pre></td></tr></table></figure><p>我们建议通过使用谓词SSA（Predicated SSA, PSSA）形式[8]和$\psi$函数[39]来解决这个问题。这需要向Triton-IR添加两个指令类（参见Listing 6）：</p><ul><li><code>cmpp</code>指令[8]与通常的比较（<code>cmp</code>）指令类似，除了它们返回两个相反的谓词而不是一个。</li><li><code>psi</code>指令合并来自不同谓词指令流的指令。</li></ul><h2 id="5-The-Triton-JIT-Compiler"><a href="#5-The-Triton-JIT-Compiler" class="headerlink" title="5 The Triton-JIT Compiler"></a>5 The Triton-JIT Compiler</h2><p>Triton-JIT的目标是通过由自动调整引擎（第5.3节）支持的一组机器独立（第5.1节）和机器相关（第5.2节）传递，将Triton-IR程序简化并编译为高效的机器代码。</p><h3 id="5-1-Machine-Independent-Passes"><a href="#5-1-Machine-Independent-Passes" class="headerlink" title="5.1 Machine-Independent Passes"></a>5.1 Machine-Independent Passes</h3><h4 id="5-1-1-Pre-Fetching"><a href="#5-1-1-Pre-Fetching" class="headerlink" title="5.1.1 Pre-Fetching"></a>5.1.1 Pre-Fetching</h4><p>循环内的分块级内存操作可能会出现问题，因为它们可能会导致严重的延迟，而在没有足够的独立指令的情况下，这种延迟是无法隐藏的。不过，可以通过检测循环并在必要时添加足够的预取代码来直接缓解Triton-IR中的这个问题（参见Listing 7）。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226211028623.png" alt="image-20231226211028623"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">B0:</span><br><span class="line">%p0 = getelementptr %1, %2</span><br><span class="line">B1:</span><br><span class="line">%p = phi [%p0, B0], [%p1, B1]</span><br><span class="line">%x = load %p</span><br><span class="line">; increment pointer</span><br><span class="line">%p1 = getelementptr %p, %3</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">B0:</span><br><span class="line">%p0 = getelementptr %1, %2</span><br><span class="line">%x0 = load %p0</span><br><span class="line">B1:</span><br><span class="line">%p = phi [%p0, B0], [%p1, B1]</span><br><span class="line">%x = phi [%x0, B0], [%p1, B1]</span><br><span class="line">; increment pointer</span><br><span class="line">%p1 = getelementptr %p, %3</span><br><span class="line">; prefetching</span><br><span class="line">%x1 = load %p</span><br></pre></td></tr></table></figure><h4 id="5-1-2-Tile-Level-Peephole-Optimization"><a href="#5-1-2-Tile-Level-Peephole-Optimization" class="headerlink" title="5.1.2 Tile-Level Peephole Optimization"></a>5.1.2 Tile-Level Peephole Optimization</h4><p>Triton-IR中图块级操作的存在为窥视孔[29]优化器提供了新的机会。例如，对于任何图块$X$，可以使用恒等式$X = (X^T)^T$来简化转置链。我们相信，与对角线图块相关的其他代数性质在未来也可以被利用。</p><h3 id="5-2-Machine-Dependent-Passes"><a href="#5-2-Machine-Dependent-Passes" class="headerlink" title="5.2 Machine-Dependent Passes"></a>5.2 Machine-Dependent Passes</h3><p>现在，我们提出了一组遵循图5所示高级模型的机器优化过程。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226211223843.png" alt="image-20231226211223843"></p><p>具体来说，Triton-JIT执行的优化包括：</p><ol><li>分层图块；</li><li>内存合并；</li><li>共享内存分配；</li><li>共享内存同步。</li></ol><h4 id="5-2-1-Hierarchical-Tiling"><a href="#5-2-1-Hierarchical-Tiling" class="headerlink" title="5.2.1 Hierarchical Tiling"></a>5.2.1 Hierarchical Tiling</h4><p>嵌套切片策略（见图5）旨在将图块分解为micro图块并最终分解为nano图块，以便尽可能紧密地适应机器的计算能力和内存层次结构。虽然这种技术通常用于自动调整框架[34, 40]，但Triton-IR的结构使得可以自动枚举和优化任何可表达程序的有效嵌套图块配置（并且不需要多面体机器）。</p><h4 id="5-2-2-Memory-Coalescing"><a href="#5-2-2-Memory-Coalescing" class="headerlink" title="5.2.2 Memory Coalescing"></a>5.2.2 Memory Coalescing</h4><p>当相邻线程同时访问附近的内存位置时，内存访问被称为合并。这很重要，因为内存通常是从DRAM中以大块的形式检索的。</p><p>由于Triton-IR程序是单线程且自动并行化，因此我们的编译器后端能够在每个微块内部对线程进行排序，以便尽可能避免未合并的内存访问。此策略减少了加载图块列所需的内存事务数量（参见图6）。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226211606560.png" alt="image-20231226211606560"></p><h4 id="5-2-3-Shared-Memory-Allocation"><a href="#5-2-3-Shared-Memory-Allocation" class="headerlink" title="5.2.3 Shared Memory Allocation"></a>5.2.3 Shared Memory Allocation</h4><p>具有高算术强度（例如<code>dot</code>）的图块级操作可以受益于将其操作数临时存储在快速共享内存中。共享内存分配过程的目的是确定应将图块存储到此空间的时间和位置。如图7所示，这可以通过首先计算每个感兴趣变量的生存范围，然后使用[15]中提出的线性时间存储分配算法来完成。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226211657655.png" alt="image-20231226211657655"></p><h4 id="5-2-4-Shared-Memory-Synchronization"><a href="#5-2-4-Shared-Memory-Synchronization" class="headerlink" title="5.2.4 Shared Memory Synchronization"></a>5.2.4 Shared Memory Synchronization</h4><p>在我们的机器模型中，共享内存的读取和写入是异步的。共享内存同步过程的目标是在生成的GPU源代码中自动插入屏障，以保持程序的正确性。这是通过使用前向数据流分析和以下数据流方程来检测写后读（RAW）和读后写（WAR）危险来完成的：</p><script type="math/tex; mode=display">\begin{aligned}in_s^{(RAW)} &= \bigcup_{p \in \text{pred}(s)}out_p^{(RAW)} \\in_s^{(WAR)} &= \bigcup_{p \in \text{pred}(s)}out_p^{(WAR)} \\out_s^{(RAW)} &=\begin{cases}\begin{array}{ll}\emptyset & \mathbf{if}\ in_s^{(RAW)} \cap read(s) \ne \emptyset\ (\text{barrier}) \\in_s^{(RAW)} \cup write(s) & \mathbf{otherwise}\end{array}\end{cases} \\out_s^{(WAR)} &=\begin{cases}\begin{array}{ll}\emptyset & \mathbf{if}\ in_s^{(WAR)} \cap write(s) \ne \emptyset\ (\text{barrier}) \\in_s^{(WAR)} \cup read(s) & \mathbf{otherwise}\end{array}\end{cases}\end{aligned}</script><h3 id="5-3-Auto-tuner"><a href="#5-3-Auto-tuner" class="headerlink" title="5.3 Auto-tuner"></a>5.3 Auto-tuner</h3><p>传统的自动调优器[42, 45]通常依靠手写的参数化代码模板来在预定义的工作负载上实现良好的性能。相比之下，Triton-JIT可以通过简单地连接与上述每个优化过程相关的元参数来直接从Triton-IR程序中提取优化空间。</p><p>在这项工作中，仅考虑分层图块通道，导致每个图块每个维度的图块参数不超过3个。然后使用对：</p><ol><li>32和128之间的2的幂进行穷举搜索来优化这些参数，以获取图块大小；</li><li>micro图块尺寸为8和32；</li><li>1和4为nano图块尺寸。</li></ol><p>将来可以使用更好的自动调整方法。</p><h2 id="6-Numerical-Experiments"><a href="#6-Numerical-Experiments" class="headerlink" title="6 Numerical Experiments"></a>6 Numerical Experiments</h2><p>在本节中，我们将根据深度学习文献评估Triton在各种工作负载上的性能。我们使用NVIDIA GeForce GTX1070，并将我们的系统与最新的供应商库（cuBLAS 10.0、cuDNN 7.0）以及相关编译器技术（AutoTVM、TC、PlaidML）进行比较。如果适用，我们会按照官方文档指南针对每个单独的问题大小自动调整这些DSL。</p><h3 id="6-1-Matrix-Multiplication"><a href="#6-1-Matrix-Multiplication" class="headerlink" title="6.1 Matrix Multiplication"></a>6.1 Matrix Multiplication</h3><p>矩阵乘法任务的形式为：$A = D \times W^T(D \in \mathbb{R}^{M \times K}, W \in \mathbb{R}^{N \times K})$是神经网络计算的核心。在这里，我们考虑了来自循环神经网络（DeepSpeech2[3]）和Transformer[44]的各种任务；我们在图8中报告了它们的性能。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226212751497.png" alt="image-20231226212751497"></p><p>Triton和cuBLAS总体上不相上下，在某些任务上达到了设备峰值性能90%以上。然而，由于使用了3D算法[2]，cuBLAS在浅层Transformer上仍然比Triton更快，该算法将深度归约分割为独立的块，以便在M和N太小时提供更多并行性。否则，现有的DSL比我们的解决方案慢2-3倍——除了当输入形状是32的倍数时的TVM（慢$&lt; 2$倍）。</p><h3 id="6-2-Convolutions"><a href="#6-2-Convolutions" class="headerlink" title="6.2 Convolutions"></a>6.2 Convolutions</h3><p>卷积神经网络（CNN）是一类重要的机器学习模型，应该得到DSL和编译器的良好支持。它们基于卷积层（图9a），其实现为矩阵乘法（图9b），这对于使用专用张量处理硬件是必要的——但现有DSL不支持。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226213137298.png" alt="image-20231226213137298"></p><p>在这里，我们对cuDNN的“IMPLICIT_GEMM”算法（第6.2.1节）的Triton重新实现进行了基准测试，并提供了第一个可用于移位卷积的融合内核（第6.2.2节）。我们使用指针增量查找表来实现这些例程，如Listing 8所示。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226213310752.png" alt="image-20231226213310752"></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> tunable <span class="type">int</span> TM = &#123;<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>&#125;;</span><br><span class="line"><span class="type">const</span> tunable <span class="type">int</span> TN = &#123;<span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>&#125;;</span><br><span class="line"><span class="type">const</span> tunable <span class="type">int</span> TK = &#123;<span class="number">8</span>&#125;;</span><br><span class="line"></span><br><span class="line">__constant__ <span class="type">int</span>* delta = alloc_const <span class="type">int</span>[<span class="number">512</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; C; c ++)</span><br><span class="line">    delta[c] = c * H * W + shift_h[c] * W + shift_w[c]</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="built_in">shift_conv</span>(restrict read_only <span class="type">float</span> *a,</span><br><span class="line">                restrict read_only <span class="type">float</span> *b, <span class="type">float</span> *c,</span><br><span class="line">                <span class="type">int</span> M, <span class="type">int</span> N, <span class="type">int</span> K)&#123;</span><br><span class="line">    <span class="type">int</span> rxa[TM] = get_global_range[TM](<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> rxb[TN] = get_global_range[TM](<span class="number">1</span>);</span><br><span class="line">    <span class="type">int</span> rka[TK] = <span class="number">0</span> ... TK;</span><br><span class="line">    <span class="type">int</span> rkb[TK] = <span class="number">0</span> ... TK;</span><br><span class="line">    <span class="type">float</span> C[TM, TN] = <span class="number">0</span>;</span><br><span class="line">    <span class="type">float</span>* pxa[TM, TN] = a + rxa[:, newaxis];</span><br><span class="line">    <span class="type">float</span>* pb[TN, TK] = b + ryb[:, newaxis] + rkb * N;</span><br><span class="line">    __constant__ <span class="type">int</span>* pd[TK] = delta + rka;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = K; k &gt; <span class="number">0</span>; k = k - TK)&#123;</span><br><span class="line">        <span class="type">int</span> delta[TK] = *pd;</span><br><span class="line">        <span class="type">float</span> *pa[TM, TK] = pxa + delta[newaxis, :];</span><br><span class="line">        <span class="type">float</span> a[TM, TK] = *pa;</span><br><span class="line">        <span class="type">float</span> b[TN, TK] = *pb;</span><br><span class="line">        C = <span class="built_in">dot</span>(a, <span class="built_in">trans</span>(b), C);</span><br><span class="line">        pb = pb + TK * N;</span><br><span class="line">        pd = pd + TK;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> rxc[TM] = get_global_range[TM](<span class="number">0</span>);</span><br><span class="line">    <span class="type">int</span> ryx[TN] = get_global_range[TM](<span class="number">1</span>);</span><br><span class="line">    <span class="type">float</span>* pc[TM, TN] = c + rxc[:, newaxis] + ryc * M;</span><br><span class="line">    <span class="type">bool</span> checkc0[TM] = rxc &lt; M;</span><br><span class="line">    <span class="type">bool</span> checkc1[TN] = ryc &lt; N;</span><br><span class="line">    <span class="type">bool</span> checkc[TM, TN] = checkc0[:, newaxis] &amp;&amp; checkc1;</span><br><span class="line">    @checkc *pc = C;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="6-2-1-Dense-Convolutions"><a href="#6-2-1-Dense-Convolutions" class="headerlink" title="6.2.1 Dense Convolutions"></a>6.2.1 Dense Convolutions</h4><p>本小节中考虑的卷积层来自深度学习文献，如表1所示。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226213353580.png" alt="image-20231226213353580"></p><p>如图10所示，Triton的性能优于cuDNN对ResNet的IMPLICIT_GEMM实现。这可能是因为cuDNN还为$3 \times 3$卷积维护了更好的算法（即 Winograd[25]），从而几乎没有留下多少工程资源来优化不太重要的内核。当快速算法不可用时（例如DeepSpeech2），cuDNN和Triton不相上下。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226213517661.png" alt="image-20231226213517661"></p><h4 id="6-2-2-Shift-Convolutions"><a href="#6-2-2-Shift-Convolutions" class="headerlink" title="6.2.2 Shift Convolutions"></a>6.2.2 Shift Convolutions</h4><p>最后，我们将表1中的Task1-5的实现视为移位卷积——一种新颖的CNN方法（见图9a）。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226213137298.png" alt="image-20231226213137298"></p><p>我们将Triton中融合移位转换模块的实现（Listing 8）与依赖手写移位内核和单独调用cuBLAS的简单实现进行比较。我们还报告了未完成移位（即$1 \times 1$卷积）时可达到的最大性能。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226213310752.png" alt="image-20231226213310752"></p><p>正如我们在图11中看到的，我们的Triton实现几乎完全隐藏了转移成本。</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Tillet_et_al-2019-Triton/image-20231226213715793.png" alt="image-20231226213715793"></p><h2 id="7-Conclusions"><a href="#7-Conclusions" class="headerlink" title="7 Conclusions"></a>7 Conclusions</h2><p>在本文中，我们介绍了Triton，一种开源语言和编译器，用于将平铺神经网络计算表达和编译为高效的机器代码。我们表明，只需向LLVM-IR添加一些数据流和控制流扩展就可以实现各种图块级优化过程，从而共同实现与供应商库相当的性能。我们还提出了Triton-C，这是一种高级语言，我们能够在其中为CNN的新型神经网络架构简洁地实现高效的内核。</p><p>未来的工作方向包括对张量核心的支持、量化内核的实现[26]以及集成到更高级别的DSL中。</p>]]></content>
    
    
    <summary type="html">Triton：用于图块神经网络计算的中间语言和编译器。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2019" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2019/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="MAPL" scheme="http://blog.karltan.com/tags/MAPL/"/>
    
  </entry>
  
  <entry>
    <title>跑代码中的bug</title>
    <link href="http://blog.karltan.com/bugs/bugs-while-running-code/"/>
    <id>http://blog.karltan.com/bugs/bugs-while-running-code/</id>
    <published>2023-12-25T11:00:00.000Z</published>
    <updated>2023-12-25T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="跑代码中的bug"><a href="#跑代码中的bug" class="headerlink" title="跑代码中的bug"></a>跑代码中的bug</h1><h2 id="关于PyTorch模型权重的数据类型问题"><a href="#关于PyTorch模型权重的数据类型问题" class="headerlink" title="关于PyTorch模型权重的数据类型问题"></a>关于PyTorch模型权重的数据类型问题</h2><p>这段时间在写毕设代码，在训练的时候，遇到了一个报错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: mat1 and mat2 must have the same dtype</span><br></pre></td></tr></table></figure><p>遂去询问GitHub Copilot，GitHub Copilot告诉我：<strong>这个错误是由于在进行矩阵乘法时，两个矩阵的数据类型不一致导致的</strong>。</p><p>但是我输入网络中的两个参数都是相同类型的，我就把整个报错信息都复制下来输入到GitHub Copilot中，此时GitHub Copilot再次告诉我：<strong>在PyTorch中，模型的权重默认是<code>float32</code>类型。如果你的输入数据不是这个类型，就会出现这个错误</strong>。并且GitHub Copilot说可以将模型的权重类型转换为与我输入（假设我的输入数据类型是<code>float64</code>）相同的数据类型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = net.double()</span><br></pre></td></tr></table></figure><p>我恍然大悟，再次询问GitHub Copilot，可以使用张量的<code>.float()</code>方法将张量的数据类型转换为<code>float32</code>。</p><p>以上，将这个问题记录下来。</p><p><strong>未完待续…</strong></p>]]></content>
    
    
    <summary type="html">记录跑代码时一些奇奇怪怪的bug。</summary>
    
    
    
    <category term="修bug实录" scheme="http://blog.karltan.com/categories/%E4%BF%AEbug%E5%AE%9E%E5%BD%95/"/>
    
    
    <category term="PyTorch" scheme="http://blog.karltan.com/tags/PyTorch/"/>
    
    <category term="代码" scheme="http://blog.karltan.com/tags/%E4%BB%A3%E7%A0%81/"/>
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用命令</title>
    <link href="http://blog.karltan.com/notes-out-class/common-linux-commands/"/>
    <id>http://blog.karltan.com/notes-out-class/common-linux-commands/</id>
    <published>2023-12-21T07:00:00.000Z</published>
    <updated>2023-12-23T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux常用命令"><a href="#Linux常用命令" class="headerlink" title="Linux常用命令"></a>Linux常用命令</h1><p>由于所有命令的用法均可以直接查找到，所以这里只记录具体的情况及对应的用法。</p><h2 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h2><h3 id="删除指定文件"><a href="#删除指定文件" class="headerlink" title="删除指定文件"></a>删除指定文件</h3><p><code>.DS_Store</code>文件是macOS自动创建的文件，同时也会占用一定的空间，可以用下面的命令搜索当前目录下的<code>.DS_Store</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find <span class="string">&quot;.&quot;</span> -name .DS_Store</span><br></pre></td></tr></table></figure><p>使用管道删除当前目录下的<code>.DS_Store</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find <span class="string">&quot;.&quot;</span> -name .DS_Store | xargs <span class="built_in">rm</span></span><br></pre></td></tr></table></figure><h3 id="查看当前目录下所有文件夹的大小"><a href="#查看当前目录下所有文件夹的大小" class="headerlink" title="查看当前目录下所有文件夹的大小"></a>查看当前目录下所有文件夹的大小</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">du</span> -h . --max-depth=1</span><br></pre></td></tr></table></figure><h3 id="压缩-解压"><a href="#压缩-解压" class="headerlink" title="压缩/解压"></a>压缩/解压</h3><h4 id="zip文件"><a href="#zip文件" class="headerlink" title=".zip文件"></a><code>.zip</code>文件</h4><ol><li><p>压缩</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zip -r filename.zip /the/dir/you/want/to/compress</span><br></pre></td></tr></table></figure></li><li><p>解压</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip filename.zip -d /the/dir/you/want/to/decompress</span><br></pre></td></tr></table></figure></li></ol><h4 id="tar文件"><a href="#tar文件" class="headerlink" title=".tar文件"></a><code>.tar</code>文件</h4><h4 id="tar-gz文件"><a href="#tar-gz文件" class="headerlink" title=".tar.gz文件"></a><code>.tar.gz</code>文件</h4><p><strong>未完待续…</strong></p>]]></content>
    
    
    <summary type="html">本文记录Linux常见发行版中的常用命令。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Ubuntu" scheme="http://blog.karltan.com/tags/Ubuntu/"/>
    
    <category term="Linux" scheme="http://blog.karltan.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>魔法文档</title>
    <link href="http://blog.karltan.com/environment/climb-over-the-great-wall/"/>
    <id>http://blog.karltan.com/environment/climb-over-the-great-wall/</id>
    <published>2023-12-16T08:00:00.000Z</published>
    <updated>2024-01-03T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="魔法文档"><a href="#魔法文档" class="headerlink" title="魔法文档"></a>魔法文档</h1><h2 id="软件"><a href="#软件" class="headerlink" title="软件"></a>软件</h2><h3 id="在Windows上"><a href="#在Windows上" class="headerlink" title="在Windows上"></a>在Windows上</h3><h4 id="Clash-for-Windows"><a href="#Clash-for-Windows" class="headerlink" title="Clash for Windows"></a>Clash for Windows</h4><p>在<a href="https://down.clashcn.com/soft/clashcn.com_Clash.for.Windows.Setup.0.20.39.exe">安装包下载链接</a>下载Clash for Windows安装包，在<a href="https://down.clashcn.com/soft/clashcn.com_cfw-cn-app_0.20.39.zip">汉化包下载链接</a>下载Clash for Windows汉化包，用汉化包中的<code>app.asar</code>文件替换掉<code>Clash for Windows安装地址/resources/app.asar</code>文件，重启Clash for Windows，即可看到汉化效果。</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103000427326.png" alt="image-20240103000427326"></p><h3 id="在macOS上"><a href="#在macOS上" class="headerlink" title="在macOS上"></a>在macOS上</h3><p><del>等我有钱换mac了就更新</del></p><h2 id="服务"><a href="#服务" class="headerlink" title="服务"></a>服务</h2><h3 id="91Merry"><a href="#91Merry" class="headerlink" title="91Merry"></a>91Merry</h3><p>打开<a href="https://thesixshadow.com/">91Merry网址导航 (thesixshadow.com)</a>，注册并登录，本人使用的是老平台：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103000427378.png" alt="image-20240103000427378"></p><p>在“我的钱包”页面进行充值：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103000732613.png" alt="image-20240103000732613"></p><p>这里建议将余额充值到￥30，购买青铜套餐中的季度类别：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103000854171.png" alt="image-20240103000854171"></p><p>购买完成后会在首页显示会员时长、剩余流量、在线设备和钱包余额等信息：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103001009036.png" alt="image-20240103001009036"></p><p>依次点击“Clash订阅”-“2. 点此一键导入Clash”：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103001319500.png" alt="image-20240103001319500"></p><p>选继续导入：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103001352693.png" alt="image-20240103001352693"></p><p>在浏览器上方的弹窗处选择打开：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103001444386.png" alt="image-20240103001444386"></p><p>打开Clash for Windows，在“配置”界面，回看到你导入的配置文件：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103002006689.png" alt="image-20240103002006689"></p><p>上面会显示已用流量、套餐更新时间和配置文件上次更新时间等。</p><p>最后打开系统代理，就可以愉快的使用魔法了：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103002130723.png" alt="image-20240103002130723"></p><p>不要忘记在使用之前更新一下配置文件：</p><p><img src="https://img.karltan.com/environment/climb-over-the-great-wall/image-20240103002230723.png" alt="image-20240103002230723"></p><h3 id="魔戒"><a href="#魔戒" class="headerlink" title="魔戒"></a>魔戒</h3><p>一般情况下，91Merry就够用了，后续有机会再更新魔戒的相关使用方法。</p>]]></content>
    
    
    <summary type="html">如何在Windows上使用魔法。</summary>
    
    
    
    <category term="环境配置" scheme="http://blog.karltan.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="魔法" scheme="http://blog.karltan.com/tags/%E9%AD%94%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Python深度学习包安装指南</title>
    <link href="http://blog.karltan.com/environment/python-dl-package-installation-tutorial/"/>
    <id>http://blog.karltan.com/environment/python-dl-package-installation-tutorial/</id>
    <published>2023-12-16T07:00:00.000Z</published>
    <updated>2023-12-20T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python深度学习包安装指南"><a href="#Python深度学习包安装指南" class="headerlink" title="Python深度学习包安装指南"></a>Python深度学习包安装指南</h1><h2 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h2><p>首先是总的下载网址：<a href="https://download.pytorch.org/whl/torch_stable.html">download.pytorch.org/whl/torch_stable.html</a>。</p><p>由于使用<code>pip3 install</code>命令下载的时候，下载速度会很慢，所以这里建议先将对应版本的<code>.whl</code>包下载好后直接<code>pip3 install filename</code>进行离线安装。</p><h3 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h3><p>PyTorch可以直接选一个新的安装，安装后根据安装的版本去下面的torchaudio，torchvision和torchtext</p><h3 id="torchaudio"><a href="#torchaudio" class="headerlink" title="torchaudio"></a>torchaudio</h3><p>下面是一部分对应表，如果下表中没有你需要的版本，可以去<a href="https://pytorch.org/audio/main/installation.html">Installing pre-built binaries — Torchaudio 2.2.0.dev20231215 documentation (pytorch.org)</a>自行寻找。</p><div class="table-container"><table><thead><tr><th><code>torch</code></th><th><code>torchaudio</code></th><th><code>Python</code></th></tr></thead><tbody><tr><td><code>2.1.0</code></td><td><code>2.1.0</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.0.1</code></td><td><code>2.0.2</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.0.0</code></td><td><code>2.0.1</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>1.13.1</code></td><td><code>0.13.1</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.13.0</code></td><td><code>0.13.0</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.12.1</code></td><td><code>0.12.1</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.12.0</code></td><td><code>0.12.0</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.11.0</code></td><td><code>0.11.0</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.10.0</code></td><td><code>0.10.0</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.9.1</code></td><td><code>0.9.1</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.8.1</code></td><td><code>0.8.1</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.7.1</code></td><td><code>0.7.2</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.7.0</code></td><td><code>0.7.0</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.6.0</code></td><td><code>0.6.0</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.5.0</code></td><td><code>0.5.0</code></td><td><code>&gt;=3.5</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.4.0</code></td><td><code>0.4.0</code></td><td><code>==2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.8</code></td></tr></tbody></table></div><h3 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h3><p>下面是一部分对应表，如果下表中没有你需要的版本，可以去<a href="https://github.com/pytorch/vision">pytorch/vision: Datasets, Transforms and Models specific to Computer Vision (github.com)</a>自行寻找。</p><div class="table-container"><table><thead><tr><th><code>torch</code></th><th><code>torchvision</code></th><th><code>Python</code></th></tr></thead><tbody><tr><td><code>main</code> / <code>nightly</code></td><td><code>main</code> / <code>nightly</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.2</code></td><td><code>0.17</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.1</code></td><td><code>0.16</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.0</code></td><td><code>0.15</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>1.13</code></td><td><code>0.14</code></td><td><code>&gt;=3.7.2</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.12</code></td><td><code>0.13</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.11</code></td><td><code>0.12</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.10</code></td><td><code>0.11</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.9</code></td><td><code>0.10</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.8</code></td><td><code>0.9</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.7</code></td><td><code>0.8</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.6</code></td><td><code>0.7</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.5</code></td><td><code>0.6</code></td><td><code>&gt;=3.5</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.4</code></td><td><code>0.5</code></td><td><code>==2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.3</code></td><td><code>0.4.2</code> / <code>0.4.3</code></td><td><code>==2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.7</code></td></tr><tr><td><code>1.2</code></td><td><code>0.4.1</code></td><td><code>==2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.7</code></td></tr><tr><td><code>1.1</code></td><td><code>0.3</code></td><td><code>==2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.7</code></td></tr><tr><td><code>&lt;=1.0</code></td><td><code>0.2</code></td><td><code>==2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.7</code></td></tr></tbody></table></div><h3 id="torchtext"><a href="#torchtext" class="headerlink" title="torchtext"></a>torchtext</h3><p>下面是一部分对应表，如果下表中没有你需要的版本，可以去<a href="https://github.com/pytorch/text">pytorch/text: Models, data loaders and abstractions for language processing, powered by PyTorch (github.com)</a>自行寻找。</p><div class="table-container"><table><thead><tr><th><code>torch</code></th><th><code>torchtext</code></th><th><code>Python</code></th></tr></thead><tbody><tr><td><code>nightly build</code></td><td><code>main</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.2.0</code></td><td><code>0.17.0</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.1.0</code></td><td><code>0.16.0</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>2.0.0</code></td><td><code>0.15.0</code></td><td><code>&gt;=3.8</code>, <code>&lt;=3.11</code></td></tr><tr><td><code>1.13.0</code></td><td><code>0.14.0</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.12.0</code></td><td><code>0.13.0</code></td><td><code>&gt;=3.7</code>, <code>&lt;=3.10</code></td></tr><tr><td><code>1.11.0</code></td><td><code>0.12.0</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.10.0</code></td><td><code>0.11.0</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.9.1</code></td><td><code>0.10.1</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.9</code></td><td><code>0.10</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.8.1</code></td><td><code>0.9.1</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.8</code></td><td><code>0.9</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.7.1</code></td><td><code>0.8.1</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.9</code></td></tr><tr><td><code>1.7</code></td><td><code>0.8</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.6</code></td><td><code>0.7</code></td><td><code>&gt;=3.6</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.5</code></td><td><code>0.6</code></td><td><code>&gt;=3.5</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>1.4</code></td><td><code>0.5</code></td><td><code>2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.8</code></td></tr><tr><td><code>&lt;=0.4</code></td><td><code>0.2.3</code></td><td><code>2.7</code>, <code>&gt;=3.5</code>, <code>&lt;=3.8</code></td></tr></tbody></table></div><h2 id="xformers"><a href="#xformers" class="headerlink" title="xformers"></a>xformers</h2><p>在<a href="https://anaconda.org/xformers/xformers/files">https://anaconda.org/xformers/xformers/files</a>中，所有的文件会以<code>linux-64/xformers-0.0.23-py310_cu11.8.0_pyt2.1.1.tar.bz2</code>的格式作为文件名，可以从其中捕捉到版本号为<code>0.0.23</code>，适配的Python版本为<code>3.10</code>，适配的CUDA版本为<code>11.8.0</code>，可以根据这些信息自行修改pip3命令进行安装（比如要安装<code>0.0.23</code>版，直接输入命令<code>pip3 install xformers==0.0.23</code>即可）。</p><p>但是需要注意的是，同一个版本号对应了两个CUDA版本（CUDA11.8和CUDA12.1），如果你使用上面的命令安装遇到了版本不匹配的问题，此时需要参照官方GitHub链接<a href="https://github.com/facebookresearch/xformers">facebookresearch/xformers: Hackable and optimized Transformers building blocks, supporting a composable construction. (github.com)</a>中的安装命令进行重新安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cuda 11.8 version</span></span><br><span class="line">pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu118</span><br><span class="line"><span class="comment"># cuda 12.1 version</span></span><br><span class="line">pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure><h2 id="import与pip3对应"><a href="#import与pip3对应" class="headerlink" title="import与pip3对应"></a>import与pip3对应</h2><p><code>import</code>的名称和<code>pip3 install</code>的名称不一致的会加粗。</p><div class="table-container"><table><thead><tr><th style="text-align:center">import</th><th style="text-align:center">pip3 install</th></tr></thead><tbody><tr><td style="text-align:center"><strong>cv2</strong></td><td style="text-align:center"><strong>opencv-python</strong></td></tr><tr><td style="text-align:center"><strong>yaml</strong></td><td style="text-align:center"><strong>pyyaml</strong></td></tr><tr><td style="text-align:center"><strong>skimage</strong></td><td style="text-align:center"><strong>scikit-image</strong></td></tr><tr><td style="text-align:center"><strong>sklearn</strong></td><td style="text-align:center"><strong>scikit-learn</strong></td></tr><tr><td style="text-align:center">pandas</td><td style="text-align:center">pandas</td></tr><tr><td style="text-align:center">loguru</td><td style="text-align:center">loguru</td></tr><tr><td style="text-align:center">tqdm</td><td style="text-align:center">tqdm</td></tr><tr><td style="text-align:center">h5py</td><td style="text-align:center">h5py</td></tr><tr><td style="text-align:center">tabulate</td><td style="text-align:center">tabulate</td></tr><tr><td style="text-align:center">plyfile</td><td style="text-align:center">plyfile</td></tr><tr><td style="text-align:center">transforms3d</td><td style="text-align:center">transforms3d</td></tr><tr><td style="text-align:center">einops</td><td style="text-align:center">einops</td></tr><tr><td style="text-align:center">kornia</td><td style="text-align:center">kornia</td></tr><tr><td style="text-align:center">yacs</td><td style="text-align:center">yacs</td></tr><tr><td style="text-align:center">matplotlib</td><td style="text-align:center">matplotlib</td></tr><tr><td style="text-align:center">easydict</td><td style="text-align:center">easydict</td></tr><tr><td style="text-align:center">omegaconf</td><td style="text-align:center">omegaconf</td></tr><tr><td style="text-align:center">xformers</td><td style="text-align:center">xformers</td></tr><tr><td style="text-align:center"></td></tr></tbody></table></div><p><strong>未完待续…</strong></p>]]></content>
    
    
    <summary type="html">本文记录Python中深度学习包的安装方法。</summary>
    
    
    
    <category term="环境配置" scheme="http://blog.karltan.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Python" scheme="http://blog.karltan.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 【完结】73 - 课程总结和进阶学习</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/73/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/73/</id>
    <published>2023-12-15T08:00:00.000Z</published>
    <updated>2023-12-15T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【完结】73-课程总结和进阶学习"><a href="#【完结】73-课程总结和进阶学习" class="headerlink" title="【完结】73 - 课程总结和进阶学习"></a>【完结】73 - 课程总结和进阶学习</h1><p>好好好，完结撒花！</p><h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><ul><li>从3月到8月一共进行了34节课，共51小时</li><li>讲了1055页幻灯片（一半是代码），回答了934个问题</li><li>直播参与同学稳定在一千人以上<ul><li>倒数第二节课仍有1001，平均观看1.1小时</li></ul></li><li>视频录像在B站观看125万次，1万弹幕+评论，收到了608.95贝壳</li></ul><h2 id="直播设置-v3"><a href="#直播设置-v3" class="headerlink" title="直播设置 v3"></a>直播设置 v3</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/73/image-20231215160348930.png" alt="image-20231215160348930"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/73/image-20231215164524891.png" alt="image-20231215164524891"></p><h2 id="大纲回顾"><a href="#大纲回顾" class="headerlink" title="大纲回顾"></a>大纲回顾</h2><ul><li>深度学习基础<ul><li>线性回归</li><li>Softmax回归</li><li>多层感知机</li><li>模型选择</li><li>过拟合、欠拟合</li><li>权重衰退</li><li>Dropout</li><li>数值稳定性</li><li>模型初始化和激活函数</li></ul></li><li>卷积神经网络<ul><li>LeNet</li><li>AlexNet</li><li>VGG</li><li>NiN</li><li>GoogleNet</li><li>ResNet</li></ul></li><li>计算机视觉<ul><li>图片增广</li><li>微调</li><li>R-CNN，SSD，YOLO</li><li>FCN</li><li>样式迁移</li></ul></li><li>循环神经网络<ul><li>RNN，GRU，LSTM</li><li>深层、双向RNN</li><li>Seq2Seq</li></ul></li><li>注意力机制<ul><li>Seq2Seq + attention</li><li>Transformer</li><li>BERT</li></ul></li><li>性能<ul><li>CPU、GPU、ASIC</li><li>使用（多）GPU</li><li>分布式</li></ul></li><li>应用<ul><li>房价预测</li><li>图片分类</li><li>物体检测</li><li>语义分割</li><li>BERT</li><li>样式迁移</li><li>语言模型</li><li>机器翻译</li><li>自然语言推理</li></ul></li></ul><h2 id="还有很多应用、模型没有讲到"><a href="#还有很多应用、模型没有讲到" class="headerlink" title="还有很多应用、模型没有讲到"></a>还有很多应用、模型没有讲到</h2><ul><li>计算机视觉<ul><li>人脸识别</li><li>体态识别</li><li>无人驾驶</li><li>图片合成</li><li>超分辨率</li><li>医学图片</li></ul></li><li>自然语言处理<ul><li>文本分类</li><li>文本合成</li><li>文本摘要</li><li>实体命名识别</li></ul></li><li>图神经网络</li><li>语音识别</li><li>时序数据</li><li>玩游戏</li><li>代码生成</li><li>音乐</li><li>推荐系统</li></ul><h2 id="斯坦福2021秋季新课：实用机器学习"><a href="#斯坦福2021秋季新课：实用机器学习" class="headerlink" title="斯坦福2021秋季新课：实用机器学习"></a>斯坦福2021秋季新课：实用机器学习</h2><ul><li>课程地址：<a href="https://c.d2l.ai/stanford-cs329p/">Practical Machine Learning — Practical Machine Learning (d2l.ai)</a></li><li>关注机器学习落地的技术，是本课的补充<ul><li>基础建模<ul><li>数据收集</li><li>数据处理</li><li>模型验证</li><li>模型融合</li></ul></li><li>不正确的假设<ul><li>Covariate/Concept/Label偏移</li><li>IID以外的数据（时序、图）</li></ul></li><li>性能调优<ul><li>AutoML</li><li>模型蒸馏</li><li>多模态数据</li></ul></li><li>模型之外<ul><li>模型部署</li><li>公平</li></ul></li></ul></li></ul><h2 id="个人进阶建议-知识"><a href="#个人进阶建议-知识" class="headerlink" title="个人进阶建议 - 知识"></a>个人进阶建议 - 知识</h2><ul><li>可以看朋友圈分享，但更要主动获取信息、建立知识库</li><li>学会读论文<ul><li>经典论文需要读懂每一句话</li><li>结合代码来了解细节（例如<a href="https://paperswithcode.com/">The latest in Machine Learning | Papers With Code</a>）</li><li>可以看openreview上的评（吐）论（槽）</li></ul></li><li>对读过的论文整理<ul><li><a href="https://www.bilibili.com/video/BV1nA41157y4">我是如何快速阅读和整理文献 哔哩哔哩 bilibili</a></li></ul></li></ul><h2 id="个人进阶建议-实践"><a href="#个人进阶建议-实践" class="headerlink" title="个人进阶建议 - 实践"></a>个人进阶建议 - 实践</h2><ul><li>可以参加竞赛，但注意竞赛跟研究和工业界落地都不一样<ul><li>竞赛：调最好的参、模型融合</li><li>研究：新的模型、调还不错的参</li><li>工业界：将应用表达成机器学习问题、收集数据</li></ul></li><li>多研究开源代码，跟开发者多交流，积极贡献<ul><li>你可以从修文档开始</li></ul></li></ul><h2 id="感谢"><a href="#感谢" class="headerlink" title="感谢"></a>感谢</h2><p>下次再见！</p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 【完结】73 - 课程总结和进阶学习。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 72 优化算法</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/72/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/72/</id>
    <published>2023-12-15T07:00:00.000Z</published>
    <updated>2023-12-15T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="72-优化算法"><a href="#72-优化算法" class="headerlink" title="72 优化算法"></a>72 优化算法</h1><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><ul><li><p>一般形式</p><script type="math/tex; mode=display">  \text{minimize }f(\mathbf{x}) \quad \text{subject to }\mathbf{x} \in C</script><ul><li><p>目标函数$f$：$\mathbb{R}^n \to \mathbb{R}$</p></li><li><p>限制集合例子</p><script type="math/tex; mode=display">  C = \{\mathbf{x} | h_1(\mathbf{x}) = 0, \cdots, h_m(\mathbf{x}) = 0, g_1(x) \le 0, \cdots, g_r(\mathbf{x}) \le 0\}</script></li><li><p>如果$c = \mathbb{R}^n$那就是不受限</p></li></ul></li></ul><h2 id="局部最小-vs-全局最小"><a href="#局部最小-vs-全局最小" class="headerlink" title="局部最小 vs 全局最小"></a>局部最小 vs 全局最小</h2><ul><li>全局最小$\mathbf{x}^\ast$：$f(\mathbf{x}^\ast) \le f(\mathbf{x}) \quad \forall\mathbf{x} \in C$</li><li>局部最小$\mathbf{x}^\ast$：存在$\varepsilon$，使得$f(\mathbf{x}^\ast) \le f(\mathbf{x}) \quad \forall\mathbf{x}:\Vert\mathbf{x} - \mathbf{x}^\ast\Vert \le \varepsilon$</li><li>使用迭代优化算法来求解，一般只能找到局部最小值</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215143032804.png" alt="image-20231215143032804"></p><h2 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h2><ul><li>一个$\mathbb{R}^n$的子集$C$是凸当且仅当：<script type="math/tex; mode=display">  \begin{aligned}  &\alpha\mathbf{x} + (1 - \alpha)\mathbf{y} \in C \\  &\forall\alpha \in [0, 1] \\  &\forall\mathbf{x}, \mathbf{y} \in C  \end{aligned}</script></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215143544415.png" alt="image-20231215143544415"></p><p>在集合中任意取两点，该两点的连线在集合内，那么该集合是凸集。</p><h2 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h2><ul><li><p>函数$f:C \to \mathbb{R}$是凸当且仅当：</p><script type="math/tex; mode=display">  \begin{aligned}  &f(\alpha\mathbf{x} + (1 - \alpha)\mathbf{y}) \le \alpha f(\mathbf{x}) + (1 - \alpha)f(\mathbf{y}) \\  &\forall\alpha \in [0, 1] \\  &\forall\mathbf{x}, \mathbf{y} \in C  \end{aligned}</script></li><li><p>如果$\mathbf{x} \ne \mathbf{y}, \alpha \in (0, 1)$时不等式严格成立，那么叫严格凸函数</p></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215144028820.png" alt="image-20231215144028820"></p><p>在函数上任意取两点，该两点的连线在函数的上方，那么该函数是凸函数。</p><h2 id="凸函数优化"><a href="#凸函数优化" class="headerlink" title="凸函数优化"></a>凸函数优化</h2><ul><li>如果代价函数$f$是凸的，且限制集合$C$是凸的，那么就是凸优化问题，那么局部最小一定是全局最小</li><li>严格凸优化问题有唯一的全局最小</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215144244862.png" alt="image-20231215144244862"></p><h2 id="凸和非凸例子"><a href="#凸和非凸例子" class="headerlink" title="凸和非凸例子"></a>凸和非凸例子</h2><p>但是很遗憾，机器学习中绝大多是都是非凸的（激活函数引入了非线性）。</p><ul><li>凸<ul><li>线性回归$f(\mathbf{x}) = \Vert\mathbf{W}\mathbf{x} - \mathbf{b}\Vert^2_2$</li><li>Softmax回归</li></ul></li><li>非凸：其他<ul><li>MLP，CNN，RNN，attention，…</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215144642366.png" alt="image-20231215144642366"></p><h2 id="梯度下降（Gradient-Descent，GD）"><a href="#梯度下降（Gradient-Descent，GD）" class="headerlink" title="梯度下降（Gradient Descent，GD）"></a>梯度下降（Gradient Descent，GD）</h2><ul><li>最简单的迭代求解算法</li><li>选取开始点$\mathbf{x}_0$</li><li>对$t = 1, \cdots, \mathrm{T}$<ul><li>$\mathbf{x}_t = \mathbf{x}_{t - 1} - \eta\nabla f(\mathbf{x}_{t - 1})$</li></ul></li><li>$\eta$叫做学习率</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215144936384.png" alt="image-20231215144936384"></p><h2 id="随机梯度下降（Stochastic-Gradient-Descent，SGD）"><a href="#随机梯度下降（Stochastic-Gradient-Descent，SGD）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent，SGD）"></a>随机梯度下降（Stochastic Gradient Descent，SGD）</h2><ul><li><p>有$n$个样本时，计算$f(\mathbf{x}) = \frac{1}{n}\sum_{i = 0}^n\ell_i(\mathbf{x})$的导数太贵</p></li><li><p>随机梯度下降在时间$t$随机选项样本$t_i$来近似$f(x)$</p><script type="math/tex; mode=display">  \begin{aligned}  &\mathbf{x}_t = \mathbf{x}_{t - 1} - \eta_t\nabla\ell_{t_i}(\mathbf{x}_{t - 1}) \\  &\mathbb{E}\left[\nabla\ell_{t_i}(\mathbf{x})\right] = \mathbb{E}\left[\nabla f(\mathbf{x})\right]  \end{aligned}</script></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215145711574.png" alt="image-20231215145711574"></p><h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><ul><li><p>计算单样本的梯度难以完全利用硬件资源</p></li><li><p>小批量随机梯度下降在时间$t$采样一个随机子集$I_t \subset \{1, \cdots, n\}$使得$\left|I_t\right| = b$</p><script type="math/tex; mode=display">  \mathbf{x}_t = \mathbf{x}_{t - 1} - \frac{\eta_t}{b}\sum_{i \in I_t}\nabla\ell_i(\mathbf{x}_{t - 1})</script></li><li><p>同样，这是一个无偏的近似，但降低了方差</p><script type="math/tex; mode=display">  \mathbb{E}\left[\frac{1}{b}\sum_{i \in I_t}\nabla\ell_i(\mathbf{x})\right] = \nabla f(\mathbf{x})</script></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215145817760.png" alt="image-20231215145817760"></p><h2 id="冲量法（Momentum）"><a href="#冲量法（Momentum）" class="headerlink" title="冲量法（Momentum）"></a>冲量法（Momentum）</h2><ul><li><p>冲量法使用平滑过的梯度对权重更新</p><script type="math/tex; mode=display">  \begin{aligned}  \mathbf{g}_t &= \frac{1}{b}\sum_{i \in I_t}\nabla\ell_i(\mathbf{x}_{t - 1}) \\  \mathbf{v}_t &= \beta\mathbf{v}_{t - 1} + \mathbf{g}_t \\  \mathbf{w}_t &= \mathbf{w}_{t - 1} - \eta\mathbf{v}_t  \end{aligned}</script><blockquote><p>梯度平滑：</p><script type="math/tex; mode=display">\mathbf{v}_t = \mathbf{g}_t + \beta\mathbf{g}_{t - 1} + \beta^2\mathbf{g}_{t - 2} + \beta^3\mathbf{g}_{t - 3} + \cdots</script></blockquote><ul><li>$\beta$常见取值$[0.5, 0.9, 0.95, 0.99]$</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/72/image-20231215151213793.png" alt="image-20231215151213793"></p><p>下图使用了momentum，在更少的步数内收敛了。</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>对学习率不敏感，非常平滑的SGD。</p><ul><li>记录$\mathbf{v}_t = \beta_1\mathbf{v}_{t - 1} + (1 - \beta_1)\mathbf{g}_t$，通常$\beta_1 = 0.9$</li><li>展开$\mathbf{v}_t = (1 - \beta_1)(\mathbf{g}_t + \beta_1\mathbf{g}_{t - 1} + \beta_1^2\mathbf{g}_{t - 2} + \beta_1^3\mathbf{g}_{t - 3} + \cdots)$</li><li>因为$\sum_{i = 0}^\infty\beta_1^i = \frac{1}{1 - \beta_1}$，所以权重和为1</li><li>由于$\mathbf{v}_0 = 0$，且$\sum_{i = 0}^t\beta_1^i = \frac{1 - \beta_1^t}{1 - \beta_1}$，修正$\hat{\mathbf{v}}_t = \frac{\mathbf{v}_t}{1 - \beta_1^t}$</li><li>类似记录$\mathbf{s}_t = \beta_2\mathbf{s}_{t - 1} + (1 - \beta_2)\mathbf{g}_t^2$，通常$\beta_2 = 0.999$，且修正$\hat{\mathbf{s}}_t = \frac{\mathbf{s}_t}{1 - \beta_2^t}$</li><li>计算重新调整后的梯度$\mathbf{g}_t^\prime = \frac{\hat{\mathbf{v}}_t}{\sqrt{\hat{\mathbf{s}}_t} + \epsilon}$</li><li>最后更新$\mathbf{w}_t = \mathbf{w}_{t - 1} - \eta\mathbf{g}_t^\prime$</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>深度学习模型大多是非凸</li><li>小批量随机梯度下降是最常用的优化算法</li><li>冲量对梯度做平滑</li><li>Adam对梯度做平滑，且对梯度各个维度值做重新调整</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 72 优化算法。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 71 目标检测竞赛总结</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/71/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/71/</id>
    <published>2023-12-15T05:00:00.000Z</published>
    <updated>2023-12-15T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="71-目标检测竞赛总结"><a href="#71-目标检测竞赛总结" class="headerlink" title="71 目标检测竞赛总结"></a>71 目标检测竞赛总结</h1><h2 id="任务回顾"><a href="#任务回顾" class="headerlink" title="任务回顾"></a>任务回顾</h2><ul><li>检测牛仔夹克、墨镜、靴子、牛仔帽、腰带</li><li>6937张训练图片，12660标注框</li><li>数据格式使用<a href="https://cocodataset.org/#home">MS-COCO</a>格式，评测使用mAP<ul><li>均可直接调用pycocotools</li></ul></li><li>挑战：类别不平衡</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/71/image-20231215133615344.png" alt="image-20231215133615344"></p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><ul><li>23名同学提交了300次结果</li><li>15名同学公布了代码<ul><li><a href="https://www.kaggle.com/c/cowboyoutfits/code">CowBoy Outfits Detection | Kaggle</a></li></ul></li><li>请Final Phase前10并提交了代码的同学将微信、Kaggle、CodaLab ID发给mli@amazon.com</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/71/image-20231215134022954.png" alt="image-20231215134022954"></p><h2 id="数据重采样"><a href="#数据重采样" class="headerlink" title="数据重采样"></a>数据重采样</h2><ul><li>当有类别样本严重不足时，可以人工干预提升它们对模型的影响力</li><li>最简单将不足的类别样本重复多次</li><li>在随机采样小批量时对每个类使用不同采样频率</li><li>在计算损失时增大不足类别样本的权重</li><li>有同学使用了SMOTE<ul><li>在不足类样本中选择相近对做插值</li></ul></li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><ul><li>YOLOX：YOLOv3 + anchor free</li><li>YOLOv5：YOLOv3 Pytorch版本的改进版<ul><li>YOLOv4和YOLOv5均是社区改进版，命名有争议</li></ul></li><li>Detectron2：Faster R-CNN</li><li>大都采用了多模型、k折融合</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>目标检测代码实现复杂，训练代价大，上手仍然以找到容易上手的库为主</li><li>因为超参数多，一般需要较长时间探索</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 71 目标检测竞赛总结。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 70 BERT微调</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/70/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/70/</id>
    <published>2023-12-14T14:00:00.000Z</published>
    <updated>2023-12-14T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="70-BERT微调"><a href="#70-BERT微调" class="headerlink" title="70 BERT微调"></a>70 BERT微调</h1><h2 id="微调BERT"><a href="#微调BERT" class="headerlink" title="微调BERT"></a>微调BERT</h2><ul><li>BERT对每一个词元返回抽取了上下文信息的特征向量</li><li>不同的任务使用不同的特征</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/70/image-20231214224941275.png" alt="image-20231214224941275"></p><h2 id="句子分类"><a href="#句子分类" class="headerlink" title="句子分类"></a>句子分类</h2><ul><li>将<code>&lt;cls&gt;</code>对应的向量输入到全连接层分类</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/70/image-20231214225152036.png" alt="image-20231214225152036"></p><h2 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h2><ul><li>识别一个词元是不是命名实体，例如人名、机构、位置</li><li>将非特殊词元放进全连接层分类</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/70/image-20231214225424337.png" alt="image-20231214225424337"></p><h2 id="问题回答"><a href="#问题回答" class="headerlink" title="问题回答"></a>问题回答</h2><ul><li>给定一个问题和描述文字，找出一个片段作为回答</li><li>对片段中的每个词元预测它是不是回答的开头或者结束</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/70/image-20231214225655788.png" alt="image-20231214225655788"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>即使下游任务各有不同，使用BERT微调时均只需要增加输出层</li><li>但根据任务的不同，输入的表示和使用的BERT特征也会不一样</li></ul><h2 id="自然语言推理数据集"><a href="#自然语言推理数据集" class="headerlink" title="自然语言推理数据集"></a>自然语言推理数据集</h2><p>斯坦福自然语言推断语料库（Stanford Natural Language Inference，SNLI）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;SNLI&#x27;</span>] = (</span><br><span class="line">    <span class="string">&#x27;https://nlp.stanford.edu/projects/snli/snli_1.0.zip&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;9fcde07509c7e87ec61c640c1b2753d9041758e4&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># data_dir = d2l.download_extract(&#x27;SNLI&#x27;)</span></span><br><span class="line">data_dir = <span class="string">&#x27;../data/snli_1.0/&#x27;</span></span><br></pre></td></tr></table></figure><p>这里我直接手动下载，手动解压，解压后的<code>snli_1.0</code>文件夹中保留下列几个文件即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">snli_1.0_dev.jsonl</span><br><span class="line">snli_1.0_dev.txt</span><br><span class="line">snli_1.0_test.jsonl</span><br><span class="line">snli_1.0_test.txt</span><br><span class="line">snli_1.0_train.jsonl</span><br><span class="line">snli_1.0_train.txt</span><br></pre></td></tr></table></figure><p>读取数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_snli</span>(<span class="params">data_dir, is_train</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将SNLI数据集解析为前提、假设和标签&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">extract_text</span>(<span class="params">s</span>):</span><br><span class="line">        s = re.sub(<span class="string">&#x27;\\(&#x27;</span>, <span class="string">&#x27;&#x27;</span>, s)</span><br><span class="line">        s = re.sub(<span class="string">&#x27;\\)&#x27;</span>, <span class="string">&#x27;&#x27;</span>, s)</span><br><span class="line">        s = re.sub(<span class="string">&#x27;\\s&#123;2,&#125;&#x27;</span>, <span class="string">&#x27; &#x27;</span>, s)</span><br><span class="line">        <span class="keyword">return</span> s.strip()</span><br><span class="line">    label_set = &#123;<span class="string">&#x27;entailment&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;contradiction&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;neutral&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line">    file_name = os.path.join(data_dir, <span class="string">&#x27;snli_1.0_train.txt&#x27;</span></span><br><span class="line">                             <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;snli_1.0_test.txt&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_name, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        rows = [row.split(<span class="string">&#x27;\t&#x27;</span>) <span class="keyword">for</span> row <span class="keyword">in</span> f.readlines()[<span class="number">1</span>:]]</span><br><span class="line">    premises = [extract_text(row[<span class="number">1</span>]) <span class="keyword">for</span> row <span class="keyword">in</span> rows <span class="keyword">if</span> row[<span class="number">0</span>] <span class="keyword">in</span> label_set]</span><br><span class="line">    hypotheses = [extract_text(row[<span class="number">2</span>]) <span class="keyword">for</span> row <span class="keyword">in</span> rows <span class="keyword">if</span> row[<span class="number">0</span>] \</span><br><span class="line">                <span class="keyword">in</span> label_set]</span><br><span class="line">    labels = [label_set[row[<span class="number">0</span>]] <span class="keyword">for</span> row <span class="keyword">in</span> rows <span class="keyword">if</span> row[<span class="number">0</span>] <span class="keyword">in</span> label_set]</span><br><span class="line">    <span class="keyword">return</span> premises, hypotheses, labels</span><br></pre></td></tr></table></figure><p>打印前3对：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_data = read_snli(data_dir, is_train=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> x0, x1, y <span class="keyword">in</span> <span class="built_in">zip</span>(train_data[<span class="number">0</span>][:<span class="number">3</span>], train_data[<span class="number">1</span>][:<span class="number">3</span>], train_data[<span class="number">2</span>][:<span class="number">3</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;premise:&#x27;</span>, x0)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;hypothesis:&#x27;</span>, x1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;label:&#x27;</span>, y)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">premise: A person on a horse jumps over a broken down airplane .</span><br><span class="line">hypothesis: A person is training his horse for a competition .</span><br><span class="line">label: 2</span><br><span class="line">premise: A person on a horse jumps over a broken down airplane .</span><br><span class="line">hypothesis: A person is at a diner , ordering an omelette .</span><br><span class="line">label: 1</span><br><span class="line">premise: A person on a horse jumps over a broken down airplane .</span><br><span class="line">hypothesis: A person is outdoors , on a horse .</span><br><span class="line">label: 0</span><br></pre></td></tr></table></figure><p>标签“蕴涵”、“矛盾”和“中性”是平衡的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_data = read_snli(data_dir, is_train=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [train_data, test_data]:</span><br><span class="line">    <span class="built_in">print</span>([[row <span class="keyword">for</span> row <span class="keyword">in</span> data[<span class="number">2</span>]].count(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[183416, 183187, 182764]</span><br><span class="line">[3368, 3237, 3219]</span><br></pre></td></tr></table></figure><p>定义一个用于加载数据的类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SNLIDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于加载SNLI数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, num_steps, vocab=<span class="literal">None</span></span>):</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line">        all_premise_tokens = d2l.tokenize(dataset[<span class="number">0</span>])</span><br><span class="line">        all_hypothesis_tokens = d2l.tokenize(dataset[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> vocab <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            self.vocab = d2l.Vocab(all_premise_tokens + \</span><br><span class="line">                all_hypothesis_tokens, min_freq=<span class="number">5</span>, reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.vocab = vocab</span><br><span class="line">        self.premises = self._pad(all_premise_tokens)</span><br><span class="line">        self.hypotheses = self._pad(all_hypothesis_tokens)</span><br><span class="line">        self.labels = torch.tensor(dataset[<span class="number">2</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.premises)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_pad</span>(<span class="params">self, lines</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([d2l.truncate_pad(</span><br><span class="line">            self.vocab[line], self.num_steps, self.vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">                         <span class="keyword">for</span> line <span class="keyword">in</span> lines])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.premises[idx], self.hypotheses[idx]), self.labels[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.premises)</span><br></pre></td></tr></table></figure><p>整合代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_snli</span>(<span class="params">batch_size, num_steps=<span class="number">50</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载SNLI数据集并返回数据迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    num_workers = d2l.get_dataloader_workers()</span><br><span class="line">    <span class="comment"># data_dir = d2l.download_extract(&#x27;SNLI&#x27;)</span></span><br><span class="line">    data_dir = <span class="string">&#x27;../data/snli_1.0/&#x27;</span></span><br><span class="line">    train_data = read_snli(data_dir, <span class="literal">True</span>)</span><br><span class="line">    test_data = read_snli(data_dir, <span class="literal">False</span>)</span><br><span class="line">    train_set = SNLIDataset(train_data, num_steps)</span><br><span class="line">    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(train_set, batch_size,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>,</span><br><span class="line">                                             num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(test_set, batch_size,</span><br><span class="line">                                            shuffle=<span class="literal">False</span>,</span><br><span class="line">                                            num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter, train_set.vocab</span><br><span class="line"></span><br><span class="line">train_iter, test_iter, vocab = load_data_snli(<span class="number">128</span>, <span class="number">50</span>)</span><br><span class="line"><span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 549367 examples</span><br><span class="line">read 9824 examples</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">18678</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X[<span class="number">0</span>].shape)</span><br><span class="line">    <span class="built_in">print</span>(X[<span class="number">1</span>].shape)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([128, 50])</span><br><span class="line">torch.Size([128, 50])</span><br><span class="line">torch.Size([128])</span><br></pre></td></tr></table></figure><h2 id="BERT微调代码"><a href="#BERT微调代码" class="headerlink" title="BERT微调代码"></a>BERT微调代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>加载预训练的BERT：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;bert.base&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;bert.base.torch.zip&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;225d66f04cae318b841a13d32af3acc165f253ac&#x27;</span>)</span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;bert.small&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;bert.small.torch.zip&#x27;</span>,</span><br><span class="line">                              <span class="string">&#x27;c72329e68a732bef0452e4b96a1c341c8910f81f&#x27;</span>)</span><br></pre></td></tr></table></figure><p>加载预训练的BERT参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_pretrained_model</span>(<span class="params">pretrained_model, num_hiddens, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                          num_heads, num_layers, dropout, max_len, devices</span>):</span><br><span class="line">    data_dir = d2l.download_extract(pretrained_model)</span><br><span class="line">    vocab = d2l.Vocab()</span><br><span class="line">    vocab.idx_to_token = json.load(<span class="built_in">open</span>(os.path.join(data_dir,</span><br><span class="line">        <span class="string">&#x27;vocab.json&#x27;</span>)))</span><br><span class="line">    vocab.token_to_idx = &#123;token: idx <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(</span><br><span class="line">        vocab.idx_to_token)&#125;</span><br><span class="line">    bert = d2l.BERTModel(<span class="built_in">len</span>(vocab), num_hiddens, norm_shape=[<span class="number">256</span>],</span><br><span class="line">                         ffn_num_input=<span class="number">256</span>, ffn_num_hiddens=ffn_num_hiddens,</span><br><span class="line">                         num_heads=<span class="number">4</span>, num_layers=<span class="number">2</span>, dropout=<span class="number">0.2</span>,</span><br><span class="line">                         max_len=max_len, key_size=<span class="number">256</span>, query_size=<span class="number">256</span>,</span><br><span class="line">                         value_size=<span class="number">256</span>, hid_in_features=<span class="number">256</span>,</span><br><span class="line">                         mlm_in_features=<span class="number">256</span>, nsp_in_features=<span class="number">256</span>)</span><br><span class="line">    bert.load_state_dict(torch.load(os.path.join(data_dir, <span class="string">&#x27;pretrained.params&#x27;</span>)))</span><br><span class="line">    <span class="keyword">return</span> bert, vocab</span><br><span class="line"></span><br><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line">bert, vocab = load_pretrained_model(</span><br><span class="line">    <span class="string">&#x27;bert.small&#x27;</span>, num_hiddens=<span class="number">256</span>, ffn_num_hiddens=<span class="number">512</span>, num_heads=<span class="number">4</span>,</span><br><span class="line">    num_layers=<span class="number">2</span>, dropout=<span class="number">0.1</span>, max_len=<span class="number">512</span>, devices=devices)</span><br></pre></td></tr></table></figure><p>微调BERT的数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SNLIBERTDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataset, max_len, vocab=<span class="literal">None</span></span>):</span><br><span class="line">        all_premise_hypothesis_tokens = [[</span><br><span class="line">            p_tokens, h_tokens] <span class="keyword">for</span> p_tokens, h_tokens <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">            *[d2l.tokenize([s.lower() <span class="keyword">for</span> s <span class="keyword">in</span> sentences])</span><br><span class="line">              <span class="keyword">for</span> sentences <span class="keyword">in</span> dataset[:<span class="number">2</span>]])]</span><br><span class="line"></span><br><span class="line">        self.labels = torch.tensor(dataset[<span class="number">2</span>])</span><br><span class="line">        self.vocab = vocab</span><br><span class="line">        self.max_len = max_len</span><br><span class="line">        (self.all_token_ids, self.all_segments,</span><br><span class="line">         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.all_token_ids)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_preprocess</span>(<span class="params">self, all_premise_hypothesis_tokens</span>):</span><br><span class="line">        <span class="comment"># pool = multiprocessing.Pool(4)</span></span><br><span class="line">        <span class="comment"># out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)</span></span><br><span class="line">        out = <span class="built_in">map</span>(self._mp_worker, all_premise_hypothesis_tokens)</span><br><span class="line">        out = <span class="built_in">list</span>(out)</span><br><span class="line">        all_token_ids = [</span><br><span class="line">            token_ids <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        all_segments = [segments <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        valid_lens = [valid_len <span class="keyword">for</span> token_ids, segments, valid_len <span class="keyword">in</span> out]</span><br><span class="line">        <span class="keyword">return</span> (torch.tensor(all_token_ids, dtype=torch.long),</span><br><span class="line">                torch.tensor(all_segments, dtype=torch.long),</span><br><span class="line">                torch.tensor(valid_lens))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_mp_worker</span>(<span class="params">self, premise_hypothesis_tokens</span>):</span><br><span class="line">        p_tokens, h_tokens = premise_hypothesis_tokens</span><br><span class="line">        self._truncate_pair_of_tokens(p_tokens, h_tokens)</span><br><span class="line">        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)</span><br><span class="line">        token_ids = self.vocab[tokens] + [self.vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] \</span><br><span class="line">                             * (self.max_len - <span class="built_in">len</span>(tokens))</span><br><span class="line">        segments = segments + [<span class="number">0</span>] * (self.max_len - <span class="built_in">len</span>(segments))</span><br><span class="line">        valid_len = <span class="built_in">len</span>(tokens)</span><br><span class="line">        <span class="keyword">return</span> token_ids, segments, valid_len</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_truncate_pair_of_tokens</span>(<span class="params">self, p_tokens, h_tokens</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(p_tokens) + <span class="built_in">len</span>(h_tokens) &gt; self.max_len - <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(p_tokens) &gt; <span class="built_in">len</span>(h_tokens):</span><br><span class="line">                p_tokens.pop()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                h_tokens.pop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.all_token_ids[idx], self.all_segments[idx],</span><br><span class="line">                self.valid_lens[idx]), self.labels[idx]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.all_token_ids)</span><br></pre></td></tr></table></figure><p>注意，上面的代码与原代码相比有修改。</p><p>生成训练和测试样本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_size, max_len, num_workers = <span class="number">512</span>, <span class="number">128</span>, <span class="number">0</span></span><br><span class="line"><span class="comment"># data_dir = d2l.download_extract(&#x27;SNLI&#x27;)</span></span><br><span class="line">data_dir = <span class="string">&#x27;../data/snli_1.0/&#x27;</span></span><br><span class="line">train_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="literal">True</span>), max_len, vocab)</span><br><span class="line">test_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="literal">False</span>), max_len, vocab)</span><br><span class="line">train_iter = torch.utils.data.DataLoader(</span><br><span class="line">    train_set, batch_size, shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(</span><br><span class="line">    test_set, batch_size, num_workers=num_workers)</span><br></pre></td></tr></table></figure><p>注意，上面的代码中将<code>num_workers</code>直接设为了0。</p><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 549367 examples</span><br><span class="line">read 9824 examples</span><br></pre></td></tr></table></figure><p>这个多层感知机将特殊的<code>&lt;cls&gt;</code>词元转换为了NLP中的三个输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BERTClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, bert</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTClassifier, self).__init__()</span><br><span class="line">        self.encoder = bert.encoder</span><br><span class="line">        self.hidden = bert.hidden</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        tokens_X, segments_X, valid_lens_x = inputs</span><br><span class="line">        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)</span><br><span class="line">        <span class="keyword">return</span> self.output(self.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line"></span><br><span class="line">net = BERTClassifier(bert)</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">1e-4</span>, <span class="number">5</span></span><br><span class="line">trainer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">d2l.train_ch13(</span><br><span class="line">    net, train_iter, test_iter,</span><br><span class="line">    loss, trainer, num_epochs, devices)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.518, train acc 0.791, test acc 0.784</span><br><span class="line">2263.1 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/70/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 70 BERT微调。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 69 BERT预训练</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/69/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/69/</id>
    <published>2023-12-14T11:00:00.000Z</published>
    <updated>2023-12-14T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="69-BERT预训练"><a href="#69-BERT预训练" class="headerlink" title="69 BERT预训练"></a>69 BERT预训练</h1><h2 id="NLP里的迁移学习"><a href="#NLP里的迁移学习" class="headerlink" title="NLP里的迁移学习"></a>NLP里的迁移学习</h2><ul><li>使用预训练好的模型来抽取词、句子的特征<ul><li>例如word2vec或语言模型</li></ul></li><li>不更新预训练好的模型</li><li>需要构建新的网络来抓取新任务需要的信息<ul><li>word2vec忽略了时序信息，语言模型只看了一个方向</li></ul></li></ul><h2 id="BERT的动机"><a href="#BERT的动机" class="headerlink" title="BERT的动机"></a>BERT的动机</h2><ul><li>基于微调的NLP模型</li><li>预训练的模型抽取了足够多的信息</li><li>新的任务只需要增加一个简单的输出层</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/69/image-20231214195219278.png" alt="image-20231214195219278"></p><h2 id="BERT架构"><a href="#BERT架构" class="headerlink" title="BERT架构"></a>BERT架构</h2><ul><li>只有编码器的Transformer</li><li>两个版本：<ul><li>Base: #blocks = 12, hidden size = 768, #heads = 12, #parameters = 110M</li><li>Large: #blocks = 24, hidden size = 1024, #heads = 16, #parameters = 340M</li></ul></li><li>在大规模数据上训练 &gt; 3B词</li></ul><h2 id="对输入的修改"><a href="#对输入的修改" class="headerlink" title="对输入的修改"></a>对输入的修改</h2><ul><li>每个样本是一个句子对</li><li>加入额外的片段嵌入</li><li>位置编码可学习</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/69/image-20231214200048675.png" alt="image-20231214200048675"></p><h2 id="预训练任务1：带掩码的语言模型"><a href="#预训练任务1：带掩码的语言模型" class="headerlink" title="预训练任务1：带掩码的语言模型"></a>预训练任务1：带掩码的语言模型</h2><ul><li>Transformer的编码器是双向的，标准语言模型要求单向</li><li>带掩码的语言模型每次随机（15%概率）将一些词元换成<code>&lt;mask&gt;</code></li><li>因为微调任务中不出现<code>&lt;mask&gt;</code><ul><li>80%概率下，将选中的词元变成<code>&lt;mask&gt;</code></li><li>10%概率下换成一个随机词元</li><li>10%概率下保持原有的词元</li></ul></li></ul><h2 id="预训练任务2：下一句子预测"><a href="#预训练任务2：下一句子预测" class="headerlink" title="预训练任务2：下一句子预测"></a>预训练任务2：下一句子预测</h2><ul><li>预测一个句子对中两个句子是不是相邻</li><li>训练样本中：<ul><li>50%概率选择相邻句子对：<code>&lt;cls&gt;</code> this movie is great <code>&lt;sep&gt;</code> i like it <code>&lt;sep&gt;</code></li><li>50%概率选择随机句子对：<code>&lt;cls&gt;</code> this movie is great <code>&lt;sep&gt;</code> hello world <code>&lt;sep&gt;</code></li></ul></li><li>将<code>&lt;cls&gt;</code>对应的输出放到一个全连接层来预测</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>BERT针对微调设计</li><li>基于Transformer的编码器做了如下修改：<ul><li>模型更大，训练数据更多</li><li>输入句子对，片段嵌入，可学习的位置编码</li><li>训练时使用两个任务：<ul><li>带掩码的语言模型</li><li>下一个句子预测</li></ul></li></ul></li></ul><h2 id="BERT代码"><a href="#BERT代码" class="headerlink" title="BERT代码"></a>BERT代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>输入表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_tokens_and_segments</span>(<span class="params">tokens_a, tokens_b=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取输入序列的词元及其片段索引&quot;&quot;&quot;</span></span><br><span class="line">    tokens = [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>] + tokens_a + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">    segments = [<span class="number">0</span>] * (<span class="built_in">len</span>(tokens_a) + <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> tokens_b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        tokens += tokens_b + [<span class="string">&#x27;&lt;sep&gt;&#x27;</span>]</span><br><span class="line">        segments += [<span class="number">1</span>] * (<span class="built_in">len</span>(tokens_b) + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> tokens, segments</span><br></pre></td></tr></table></figure><p><code>BERTEncoder</code>类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BERTEncoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span></span><br><span class="line"><span class="params">                 max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.segment_embedding = nn.Embedding(<span class="number">2</span>, num_hiddens)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">f&quot;<span class="subst">&#123;i&#125;</span>&quot;</span>, d2l.EncoderBlock(</span><br><span class="line">                key_size, query_size, value_size, num_hiddens, norm_shape,</span><br><span class="line">                ffn_num_input, ffn_num_hiddens, num_heads, dropout, <span class="literal">True</span>))</span><br><span class="line">        self.pos_embedding = nn.Parameter(torch.randn(<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens</span>):</span><br><span class="line">        X = self.token_embedding(tokens) + self.segment_embedding(segments)</span><br><span class="line">        X = X + self.pos_embedding.data[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blks:</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p><code>BERTEncoder</code>的推理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, ffn_num_hiddens, num_heads = <span class="number">10000</span>, <span class="number">768</span>, <span class="number">1024</span>, <span class="number">4</span></span><br><span class="line">norm_shape, ffn_num_input, num_layers, dropout = [<span class="number">768</span>], <span class="number">768</span>, <span class="number">2</span>, <span class="number">0.2</span></span><br><span class="line">encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,</span><br><span class="line">                      ffn_num_hiddens, num_heads, num_layers, dropout)</span><br><span class="line"></span><br><span class="line">tokens = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line">segments = torch.tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">encoded_X = encoder(tokens, segments, <span class="literal">None</span>)</span><br><span class="line">encoded_X.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 8, 768])</span><br></pre></td></tr></table></figure><p>带掩码的句子模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MaskLM</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的掩蔽语言模型任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, num_inputs=<span class="number">768</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MaskLM, self).__init__(**kwargs)</span><br><span class="line">        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.LayerNorm(num_hiddens),</span><br><span class="line">                                 nn.Linear(num_hiddens, vocab_size))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, pred_positions</span>):</span><br><span class="line">        num_pred_positions = pred_positions.shape[<span class="number">1</span>]</span><br><span class="line">        pred_positions = pred_positions.reshape(-<span class="number">1</span>)</span><br><span class="line">        batch_size = X.shape[<span class="number">0</span>]</span><br><span class="line">        batch_idx = torch.arange(<span class="number">0</span>, batch_size)</span><br><span class="line">        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)</span><br><span class="line">        masked_X = X[batch_idx, pred_positions]</span><br><span class="line">        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="number">1</span>))</span><br><span class="line">        mlm_Y_hat = self.mlp(masked_X)</span><br><span class="line">        <span class="keyword">return</span> mlm_Y_hat</span><br></pre></td></tr></table></figure><p><code>MaskLM</code>的前向推理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mlm = MaskLM(vocab_size, num_hiddens)</span><br><span class="line">mlm_positions = torch.tensor([[<span class="number">1</span>, <span class="number">5</span>, <span class="number">2</span>], [<span class="number">6</span>, <span class="number">1</span>, <span class="number">5</span>]])</span><br><span class="line">mlm_Y_hat = mlm(encoded_X, mlm_positions)</span><br><span class="line">mlm_Y_hat.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 10000])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mlm_Y = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>]])</span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">mlm_l = loss(mlm_Y_hat.reshape((-<span class="number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="number">1</span>))</span><br><span class="line">mlm_l.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([6])</span><br></pre></td></tr></table></figure><p>下一句子预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NextSentencePred</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT的下一句预测任务&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(NextSentencePred, self).__init__(**kwargs)</span><br><span class="line">        self.output = nn.Linear(num_inputs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(X)</span><br></pre></td></tr></table></figure><p><code>NextSentencePred</code>的前向推理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoded_X = torch.flatten(encoded_X, start_dim=<span class="number">1</span>)</span><br><span class="line">nsp = NextSentencePred(encoded_X.shape[-<span class="number">1</span>])</span><br><span class="line">nsp_Y_hat = nsp(encoded_X)</span><br><span class="line">nsp_Y_hat.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 2])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nsp_y = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">nsp_l = loss(nsp_Y_hat, nsp_y)</span><br><span class="line">nsp_l.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2])</span><br></pre></td></tr></table></figure><p>整合代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BERTModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;BERT模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span><br><span class="line"><span class="params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span></span><br><span class="line"><span class="params">                 max_len=<span class="number">1000</span>, key_size=<span class="number">768</span>, query_size=<span class="number">768</span>, value_size=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 hid_in_features=<span class="number">768</span>, mlm_in_features=<span class="number">768</span>,</span></span><br><span class="line"><span class="params">                 nsp_in_features=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BERTModel, self).__init__()</span><br><span class="line">        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,</span><br><span class="line">                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span><br><span class="line">                    dropout, max_len=max_len, key_size=key_size,</span><br><span class="line">                    query_size=query_size, value_size=value_size)</span><br><span class="line">        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),</span><br><span class="line">                                    nn.Tanh())</span><br><span class="line">        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)</span><br><span class="line">        self.nsp = NextSentencePred(nsp_in_features)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, tokens, segments, valid_lens=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                pred_positions=<span class="literal">None</span></span>):</span><br><span class="line">        encoded_X = self.encoder(tokens, segments, valid_lens)</span><br><span class="line">        <span class="keyword">if</span> pred_positions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mlm_Y_hat = self.mlm(encoded_X, pred_positions)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mlm_Y_hat = <span class="literal">None</span></span><br><span class="line">        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, <span class="number">0</span>, :]))</span><br><span class="line">        <span class="keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat</span><br></pre></td></tr></table></figure><h2 id="BERT预训练数据代码"><a href="#BERT预训练数据代码" class="headerlink" title="BERT预训练数据代码"></a>BERT预训练数据代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>WikiText-2数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;wikitext-2&#x27;</span>] = (</span><br><span class="line">    <span class="string">&#x27;https://s3.amazonaws.com/research.metamind.io/wikitext/&#x27;</span></span><br><span class="line">    <span class="string">&#x27;wikitext-2-v1.zip&#x27;</span>, <span class="string">&#x27;3c914d17d80b1459be871a5039ac23e752a53cbe&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_read_wiki</span>(<span class="params">data_dir</span>):</span><br><span class="line">    file_name = os.path.join(data_dir, <span class="string">&#x27;wiki.train.tokens&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_name, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    paragraphs = [line.strip().lower().split(<span class="string">&#x27; . &#x27;</span>)</span><br><span class="line">                  <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">if</span> <span class="built_in">len</span>(line.split(<span class="string">&#x27; . &#x27;</span>)) &gt;= <span class="number">2</span>]</span><br><span class="line">    random.shuffle(paragraphs)</span><br><span class="line">    <span class="keyword">return</span> paragraphs</span><br></pre></td></tr></table></figure><p>生成下一句子预测任务的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_next_sentence</span>(<span class="params">sentence, next_sentence, paragraphs</span>):</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">        is_next = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        next_sentence = random.choice(random.choice(paragraphs))</span><br><span class="line">        is_next = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> sentence, next_sentence, is_next</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_nsp_data_from_paragraph</span>(<span class="params">paragraph, paragraphs, vocab, max_len</span>):</span><br><span class="line">    nsp_data_from_paragraph = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(paragraph) - <span class="number">1</span>):</span><br><span class="line">        tokens_a, tokens_b, is_next = _get_next_sentence(</span><br><span class="line">            paragraph[i], paragraph[i + <span class="number">1</span>], paragraphs)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tokens_a) + <span class="built_in">len</span>(tokens_b) + <span class="number">3</span> &gt; max_len:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)</span><br><span class="line">        nsp_data_from_paragraph.append((tokens, segments, is_next))</span><br><span class="line">    <span class="keyword">return</span> nsp_data_from_paragraph</span><br></pre></td></tr></table></figure><p>生成带掩码的语言模型的数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_replace_mlm_tokens</span>(<span class="params">tokens, candidate_pred_positions, num_mlm_preds,</span></span><br><span class="line"><span class="params">                        vocab</span>):</span><br><span class="line">    mlm_input_tokens = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    pred_positions_and_labels = []</span><br><span class="line">    random.shuffle(candidate_pred_positions)</span><br><span class="line">    <span class="keyword">for</span> mlm_pred_position <span class="keyword">in</span> candidate_pred_positions:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(pred_positions_and_labels) &gt;= num_mlm_preds:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        masked_token = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> random.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">            masked_token = <span class="string">&#x27;&lt;mask&gt;&#x27;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">                masked_token = tokens[mlm_pred_position]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                masked_token = random.choice(vocab.idx_to_token)</span><br><span class="line">        mlm_input_tokens[mlm_pred_position] = masked_token</span><br><span class="line">        pred_positions_and_labels.append(</span><br><span class="line">            (mlm_pred_position, tokens[mlm_pred_position]))</span><br><span class="line">    <span class="keyword">return</span> mlm_input_tokens, pred_positions_and_labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_get_mlm_data_from_tokens</span>(<span class="params">tokens, vocab</span>):</span><br><span class="line">    candidate_pred_positions = []</span><br><span class="line">    <span class="keyword">for</span> i, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(tokens):</span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> [<span class="string">&#x27;&lt;cls&gt;&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        candidate_pred_positions.append(i)</span><br><span class="line">    num_mlm_preds = <span class="built_in">max</span>(<span class="number">1</span>, <span class="built_in">round</span>(<span class="built_in">len</span>(tokens) * <span class="number">0.15</span>))</span><br><span class="line">    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(</span><br><span class="line">        tokens, candidate_pred_positions, num_mlm_preds, vocab)</span><br><span class="line">    pred_positions_and_labels = <span class="built_in">sorted</span>(pred_positions_and_labels,</span><br><span class="line">                                       key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    pred_positions = [v[<span class="number">0</span>] <span class="keyword">for</span> v <span class="keyword">in</span> pred_positions_and_labels]</span><br><span class="line">    mlm_pred_labels = [v[<span class="number">1</span>] <span class="keyword">for</span> v <span class="keyword">in</span> pred_positions_and_labels]</span><br><span class="line">    <span class="keyword">return</span> vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]</span><br></pre></td></tr></table></figure><p>将<code>&lt;mask&gt;</code>添加到输入的末尾：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_pad_bert_inputs</span>(<span class="params">examples, max_len, vocab</span>):</span><br><span class="line">    max_num_mlm_preds = <span class="built_in">round</span>(max_len * <span class="number">0.15</span>)</span><br><span class="line">    all_token_ids, all_segments, valid_lens,  = [], [], []</span><br><span class="line">    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []</span><br><span class="line">    nsp_labels = []</span><br><span class="line">    <span class="keyword">for</span> (token_ids, pred_positions, mlm_pred_label_ids, segments,</span><br><span class="line">         is_next) <span class="keyword">in</span> examples:</span><br><span class="line">        all_token_ids.append(torch.tensor(token_ids + [vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (</span><br><span class="line">            max_len - <span class="built_in">len</span>(token_ids)), dtype=torch.long))</span><br><span class="line">        all_segments.append(torch.tensor(segments + [<span class="number">0</span>] * (</span><br><span class="line">            max_len - <span class="built_in">len</span>(segments)), dtype=torch.long))</span><br><span class="line">        valid_lens.append(torch.tensor(<span class="built_in">len</span>(token_ids), dtype=torch.float32))</span><br><span class="line">        all_pred_positions.append(torch.tensor(pred_positions + [<span class="number">0</span>] * (</span><br><span class="line">            max_num_mlm_preds - <span class="built_in">len</span>(pred_positions)), dtype=torch.long))</span><br><span class="line">        all_mlm_weights.append(</span><br><span class="line">            torch.tensor([<span class="number">1.0</span>] * <span class="built_in">len</span>(mlm_pred_label_ids) + [<span class="number">0.0</span>] * (</span><br><span class="line">                max_num_mlm_preds - <span class="built_in">len</span>(pred_positions)),</span><br><span class="line">                dtype=torch.float32))</span><br><span class="line">        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [<span class="number">0</span>] * (</span><br><span class="line">            max_num_mlm_preds - <span class="built_in">len</span>(mlm_pred_label_ids)), dtype=torch.long))</span><br><span class="line">        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))</span><br><span class="line">    <span class="keyword">return</span> (all_token_ids, all_segments, valid_lens, all_pred_positions,</span><br><span class="line">            all_mlm_weights, all_mlm_labels, nsp_labels)</span><br></pre></td></tr></table></figure><p>为预训练BERT准备的WikiText-2数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">_WikiTextDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, paragraphs, max_len</span>):</span><br><span class="line">        paragraphs = [d2l.tokenize(</span><br><span class="line">            paragraph, token=<span class="string">&#x27;word&#x27;</span>) <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs]</span><br><span class="line">        sentences = [sentence <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs</span><br><span class="line">                     <span class="keyword">for</span> sentence <span class="keyword">in</span> paragraph]</span><br><span class="line">        self.vocab = d2l.Vocab(sentences, min_freq=<span class="number">5</span>, reserved_tokens=[</span><br><span class="line">            <span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;mask&gt;&#x27;</span>, <span class="string">&#x27;&lt;cls&gt;&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>])</span><br><span class="line">        examples = []</span><br><span class="line">        <span class="keyword">for</span> paragraph <span class="keyword">in</span> paragraphs:</span><br><span class="line">            examples.extend(_get_nsp_data_from_paragraph(</span><br><span class="line">                paragraph, paragraphs, self.vocab, max_len))</span><br><span class="line">        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)</span><br><span class="line">                      + (segments, is_next))</span><br><span class="line">                     <span class="keyword">for</span> tokens, segments, is_next <span class="keyword">in</span> examples]</span><br><span class="line">        (self.all_token_ids, self.all_segments, self.valid_lens,</span><br><span class="line">         self.all_pred_positions, self.all_mlm_weights,</span><br><span class="line">         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(</span><br><span class="line">            examples, max_len, self.vocab)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.all_token_ids[idx], self.all_segments[idx],</span><br><span class="line">                self.valid_lens[idx], self.all_pred_positions[idx],</span><br><span class="line">                self.all_mlm_weights[idx], self.all_mlm_labels[idx],</span><br><span class="line">                self.nsp_labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.all_token_ids)</span><br></pre></td></tr></table></figure><p>下载WikiText-2数据集并生成预训练例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_wiki</span>(<span class="params">batch_size, max_len</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载WikiText-2数据集&quot;&quot;&quot;</span></span><br><span class="line">    num_workers = d2l.get_dataloader_workers()</span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;wikitext-2&#x27;</span>, <span class="string">&#x27;wikitext-2&#x27;</span>)</span><br><span class="line">    paragraphs = _read_wiki(data_dir)</span><br><span class="line">    train_set = _WikiTextDataset(paragraphs, max_len)</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(train_set, batch_size,</span><br><span class="line">                                        shuffle=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, train_set.vocab</span><br></pre></td></tr></table></figure><p>打印BERT预训练minibatch例子的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">batch_size, max_len = <span class="number">512</span>, <span class="number">64</span></span><br><span class="line">train_iter, vocab = load_data_wiki(batch_size, max_len)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,</span><br><span class="line">     mlm_Y, nsp_y) <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(tokens_X.shape, segments_X.shape, valid_lens_x.shape,</span><br><span class="line">          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,</span><br><span class="line">          nsp_y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])</span><br></pre></td></tr></table></figure><p>词量为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20256</span><br></pre></td></tr></table></figure><h2 id="BERT预训练代码"><a href="#BERT预训练代码" class="headerlink" title="BERT预训练代码"></a>BERT预训练代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, max_len = <span class="number">512</span>, <span class="number">64</span></span><br><span class="line">train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)</span><br></pre></td></tr></table></figure><p>使用2层，128隐藏单元和2个自注意力头来构建小BERT：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = d2l.BERTModel(<span class="built_in">len</span>(vocab), num_hiddens=<span class="number">128</span>, norm_shape=[<span class="number">128</span>],</span><br><span class="line">                    ffn_num_input=<span class="number">128</span>, ffn_num_hiddens=<span class="number">256</span>, num_heads=<span class="number">2</span>,</span><br><span class="line">                    num_layers=<span class="number">2</span>, dropout=<span class="number">0.2</span>, key_size=<span class="number">128</span>, query_size=<span class="number">128</span>,</span><br><span class="line">                    value_size=<span class="number">128</span>, hid_in_features=<span class="number">128</span>, mlm_in_features=<span class="number">128</span>,</span><br><span class="line">                    nsp_in_features=<span class="number">128</span>)</span><br><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><p>为带掩码的语言模型任务和下一句子预测任务计算损失：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_get_batch_loss_bert</span>(<span class="params">net, loss, vocab_size, tokens_X,</span></span><br><span class="line"><span class="params">                         segments_X, valid_lens_x,</span></span><br><span class="line"><span class="params">                         pred_positions_X, mlm_weights_X,</span></span><br><span class="line"><span class="params">                         mlm_Y, nsp_y</span>):</span><br><span class="line">    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,</span><br><span class="line">                                  valid_lens_x.reshape(-<span class="number">1</span>),</span><br><span class="line">                                  pred_positions_X)</span><br><span class="line">    mlm_l = loss(mlm_Y_hat.reshape(-<span class="number">1</span>, vocab_size), mlm_Y.reshape(-<span class="number">1</span>)) *\</span><br><span class="line">    mlm_weights_X.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    mlm_l = mlm_l.<span class="built_in">sum</span>() / (mlm_weights_X.<span class="built_in">sum</span>() + <span class="number">1e-8</span>)</span><br><span class="line">    nsp_l = loss(nsp_Y_hat, nsp_y)</span><br><span class="line">    l = mlm_l + nsp_l</span><br><span class="line">    <span class="keyword">return</span> mlm_l, nsp_l, l</span><br></pre></td></tr></table></figure><p>在WikiText-2数据集上预训练BERT：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_bert</span>(<span class="params">train_iter, net, loss, vocab_size, devices, num_steps</span>):</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    step, timer = <span class="number">0</span>, d2l.Timer()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;step&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">1</span>, num_steps], legend=[<span class="string">&#x27;mlm&#x27;</span>, <span class="string">&#x27;nsp&#x27;</span>])</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">4</span>)</span><br><span class="line">    num_steps_reached = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> step &lt; num_steps <span class="keyword">and</span> <span class="keyword">not</span> num_steps_reached:</span><br><span class="line">        <span class="keyword">for</span> tokens_X, segments_X, valid_lens_x, pred_positions_X,\</span><br><span class="line">            mlm_weights_X, mlm_Y, nsp_y <span class="keyword">in</span> train_iter:</span><br><span class="line">            tokens_X = tokens_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            segments_X = segments_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            valid_lens_x = valid_lens_x.to(devices[<span class="number">0</span>])</span><br><span class="line">            pred_positions_X = pred_positions_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            mlm_weights_X = mlm_weights_X.to(devices[<span class="number">0</span>])</span><br><span class="line">            mlm_Y, nsp_y = mlm_Y.to(devices[<span class="number">0</span>]), nsp_y.to(devices[<span class="number">0</span>])</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            timer.start()</span><br><span class="line">            mlm_l, nsp_l, l = _get_batch_loss_bert(</span><br><span class="line">                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,</span><br><span class="line">                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">            metric.add(mlm_l, nsp_l, tokens_X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">            timer.stop()</span><br><span class="line">            animator.add(step + <span class="number">1</span>,</span><br><span class="line">                         (metric[<span class="number">0</span>] / metric[<span class="number">3</span>], metric[<span class="number">1</span>] / metric[<span class="number">3</span>]))</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> step == num_steps:</span><br><span class="line">                num_steps_reached = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;MLM loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;NSP loss <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> sentence pairs/sec on &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_bert(train_iter, net, loss, <span class="built_in">len</span>(vocab), devices, <span class="number">50</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MLM loss 5.603, NSP loss 0.804</span><br><span class="line">3926.7 sentence pairs/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/69/output.svg" alt="output"></p><blockquote><p>笔记本2060 6G跑了半小时，大家慎重。</p></blockquote><p>使用BERT表示文本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_bert_encoding</span>(<span class="params">net, tokens_a, tokens_b=<span class="literal">None</span></span>):</span><br><span class="line">    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)</span><br><span class="line">    token_ids = torch.tensor(vocab[tokens], device=devices[<span class="number">0</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    segments = torch.tensor(segments, device=devices[<span class="number">0</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    valid_len = torch.tensor(<span class="built_in">len</span>(tokens), device=devices[<span class="number">0</span>]).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    encoded_X, _, _ = net(token_ids, segments, valid_len)</span><br><span class="line">    <span class="keyword">return</span> encoded_X</span><br></pre></td></tr></table></figure><p>考虑“a crane is flying”这句话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokens_a = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;crane&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;flying&#x27;</span>]</span><br><span class="line">encoded_text = get_bert_encoding(net, tokens_a)</span><br><span class="line">encoded_text_cls = encoded_text[:, <span class="number">0</span>, :]</span><br><span class="line">encoded_text_crane = encoded_text[:, <span class="number">2</span>, :]</span><br><span class="line">encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[<span class="number">0</span>][:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([1, 6, 128]),</span><br><span class="line"> torch.Size([1, 128]),</span><br><span class="line"> tensor([ 0.0039, -0.4272,  1.2977], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;))</span><br></pre></td></tr></table></figure><p>现在考虑一个句子对，“a crane driver came”和“he just left”：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokens_a, tokens_b = [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;crane&#x27;</span>, <span class="string">&#x27;driver&#x27;</span>, <span class="string">&#x27;came&#x27;</span>], [<span class="string">&#x27;he&#x27;</span>, <span class="string">&#x27;just&#x27;</span>, <span class="string">&#x27;left&#x27;</span>]</span><br><span class="line">encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)</span><br><span class="line">encoded_pair_cls = encoded_pair[:, <span class="number">0</span>, :]</span><br><span class="line">encoded_pair_crane = encoded_pair[:, <span class="number">2</span>, :]</span><br><span class="line">encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[<span class="number">0</span>][:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([1, 10, 128]),</span><br><span class="line"> torch.Size([1, 128]),</span><br><span class="line"> tensor([ 0.0607, -0.3982,  1.4001], device=&#x27;cuda:0&#x27;, grad_fn=&lt;SliceBackward0&gt;))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 69 BERT预训练。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 68 Transformer</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/68/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/68/</id>
    <published>2023-12-14T08:00:00.000Z</published>
    <updated>2023-12-14T10:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="68-Transformer"><a href="#68-Transformer" class="headerlink" title="68 Transformer"></a>68 Transformer</h1><h2 id="Transformer架构"><a href="#Transformer架构" class="headerlink" title="Transformer架构"></a>Transformer架构</h2><ul><li>基于编码器-解码器架构来处理序列对</li><li>跟使用注意力的seq2seq不同，Transformer是纯基于注意力</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214162215722.png" alt="image-20231214162215722"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214162338629.png" alt="image-20231214162338629"></p><h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214163117330.png" alt="image-20231214163117330"></p><ul><li>对同一key，value，query，希望抽取不同的信息<ul><li>例如短距离关系和长距离关系</li></ul></li><li>多头注意力使用$h$个独立的注意力池化<ul><li>合并各个头（head）输出得到最终输出</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214163216956.png" alt="image-20231214163216956"></p><ul><li><p>query $\mathbf{q} \in \mathbb{R}^{d_q}$，key $\mathbf{k} \in \mathbb{R}^{d_k}$，value $\mathbf{v} \in \mathbb{R}^{d_v}$</p></li><li><p>头$i$的可学习参数$\mathbf{W}_i^{(q)} \in \mathbb{R}^{p_q \times d_q}$，$\mathbf{W}_i^{(k)} \in \mathbf{R}^{p_k \times d_k}$，$\mathbf{W}_i^{(v)} \in \mathbf{R}^{p_v \times d_v}$</p></li><li><p>头$i$的输出$\mathbf{h}_i = f(\mathbf{W}_i^{(q)}\mathbf{q}, \mathbf{W}_i^{(k)}\mathbf{k}, \mathbf{W}_i^{(v)}\mathbf{v}) \in \mathbb{R}^{p_v}$</p></li><li><p>输出的可学习参数$\mathbf{W}_o \in \mathbb{R}^{p_o \times hp_v}$</p></li><li><p>多头注意力的输出</p><script type="math/tex; mode=display">  \mathbf{W}_o\begin{bmatrix}  \mathbf{h}_1 \\  \vdots \\  \mathbf{h}_h  \end{bmatrix} \in \mathbb{R}^{p_o}</script></li></ul><h2 id="有掩码的多头注意力"><a href="#有掩码的多头注意力" class="headerlink" title="有掩码的多头注意力"></a>有掩码的多头注意力</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214164447538.png" alt="image-20231214164447538"></p><ul><li>解码器对序列中的一个元素输出时，不应该考虑该元素之后的元素</li><li>可以通过掩码来实现<ul><li>也就是计算$\mathbf{x}_i$输出时，假装当前序列长度为$i$</li></ul></li></ul><h2 id="基于位置的前馈网络"><a href="#基于位置的前馈网络" class="headerlink" title="基于位置的前馈网络"></a>基于位置的前馈网络</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214164757348.png" alt="image-20231214164757348"></p><ul><li>将输入形状由$(b, n, d)$变换成$(bn, d)$</li><li>作用两个全连接层</li><li>输出形状由$(bn, d)$变化回$(b, n, d)$</li><li>等价于两层核窗口为1的一维卷积层</li></ul><h2 id="层归一化"><a href="#层归一化" class="headerlink" title="层归一化"></a>层归一化</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214165025004.png" alt="image-20231214165025004"></p><ul><li>批量归一化对每个特征/通道里元素进行归一化<ul><li>不适合序列长度会变的NLP应用</li></ul></li><li>层归一化对每个样本里的元素进行归一化</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214165150948.png" alt="image-20231214165150948"></p><h2 id="信息传递"><a href="#信息传递" class="headerlink" title="信息传递"></a>信息传递</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214170101058.png" alt="image-20231214170101058"></p><ul><li>编码器中的输出$\mathbf{y}_1, \cdots, \mathbf{y}_n$</li><li>将其作为解码中第$i$个Transformer块中多头注意力的key核value<ul><li>它的query来自目标序列</li></ul></li><li>意味着编码器核解码器中块的个数和输出维度都是一样的</li></ul><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><ul><li>预测第$t + 1$个输出时</li><li>解码器中输入前$t$个预测值<ul><li>在自注意力中，前$t$个预测值作为key和value，第$t$个预测值还作为query</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214170404922.png" alt="image-20231214170404922"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>Transformer是一个纯使用注意力的编码-解码器</li><li>编码器和解码器都有$n$个Transformer块</li><li>每个块里使用多头（自）注意力，基于位置的前馈网络和层归一化</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/68/image-20231214170738324.png" alt="image-20231214170738324"></p><h2 id="多头注意力代码"><a href="#多头注意力代码" class="headerlink" title="多头注意力代码"></a>多头注意力代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>选择缩放点积注意力作为每一个注意力头：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><p>使多个头并行计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_output</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                               num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_k): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_v): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_o): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">num_kvpairs, valid_lens =  <span class="number">6</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">Y = torch.ones((batch_size, num_kvpairs, num_hiddens))</span><br><span class="line">attention(X, Y, Y, valid_lens).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 100])</span><br></pre></td></tr></table></figure><h2 id="Transformer代码"><a href="#Transformer代码" class="headerlink" title="Transformer代码"></a>Transformer代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>基于位置的前馈网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure><p>改变张量的最里层维度的尺寸：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0841,  0.5425, -0.5535, -0.5576,  0.2484, -0.0762, -0.3432, -0.3082],</span><br><span class="line">        [ 0.0841,  0.5425, -0.5535, -0.5576,  0.2484, -0.0762, -0.3432, -0.3082],</span><br><span class="line">        [ 0.0841,  0.5425, -0.5535, -0.5576,  0.2484, -0.0762, -0.3432, -0.3082]],</span><br><span class="line">       grad_fn=&lt;SelectBackward0&gt;)</span><br></pre></td></tr></table></figure><p>对比不同维度的层归一化和批量归一化的效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">layer norm: tensor([[-1.0000,  1.0000],</span><br><span class="line">        [-1.0000,  1.0000]], grad_fn=&lt;NativeLayerNormBackward0&gt;) </span><br><span class="line">batch norm: tensor([[-1.0000, -1.0000],</span><br><span class="line">        [ 1.0000,  1.0000]], grad_fn=&lt;NativeBatchNormBackward0&gt;)</span><br></pre></td></tr></table></figure><p>使用残差连接和层归一化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><p>加法操作后输出张量的形状相同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 3, 4])</span><br></pre></td></tr></table></figure><p>实现编码器中的一个层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens</span>):</span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><p>Transformer编码器中的任何层都不会改变其输入的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure><p>Transformer编码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span> + <span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>创建一个两层的Transformer编码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure><p>Transformer解码器也是由多个相同的层组成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, i, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>编码器和解码器的特征维度都是<code>num_hiddens</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 100, 24])</span><br></pre></td></tr></table></figure><p>Transformer解码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(d2l.AttentionDecoder):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span> + <span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            self._attention_weights[<span class="number">0</span>][i] = blk.attention1.attention.attention_weights</span><br><span class="line">            self._attention_weights[<span class="number">1</span>][i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">200</span>, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span></span><br><span class="line">key_size, query_size, value_size = <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span><br><span class="line">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="line">    num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss 0.033, 5020.7 tokens/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/68/output.svg" alt="output"></p><p>将一些英语句子翻译成法语：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">go . =&gt; va !,  bleu 1.000</span><br><span class="line">i lost . =&gt; j&#x27;ai perdu .,  bleu 1.000</span><br><span class="line">he&#x27;s calm . =&gt; il est calme .,  bleu 1.000</span><br><span class="line">i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span><br></pre></td></tr></table></figure><p>可视化Transformer的注意力权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="number">0</span>).reshape((num_layers, num_heads,</span><br><span class="line">    -<span class="number">1</span>, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 10, 10])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    enc_attention_weights.cpu(), xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/68/heatmap1.svg" alt="heatmap1"></p><p>为了可视化解码器的自注意力权重和“编码器-解码器”的注意力权重，我们需要完成更多的数据操作工作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [head[<span class="number">0</span>].tolist()</span><br><span class="line">                            <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq</span><br><span class="line">                            <span class="keyword">for</span> attn <span class="keyword">in</span> step <span class="keyword">for</span> blk <span class="keyword">in</span> attn <span class="keyword">for</span> head <span class="keyword">in</span> blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="number">0.0</span>).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape((-<span class="number">1</span>, <span class="number">2</span>, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([2, 4, 6, 10]), torch.Size([2, 4, 6, 10]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :<span class="built_in">len</span>(translation.split()) + <span class="number">1</span>],</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">    titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/68/heatmap2.svg" alt="heatmap2"></p><p>输出序列的查询不会与输入序列中填充位置的词元进行注意力计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_inter_attention_weights, xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;Query positions&#x27;</span>, titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)],</span><br><span class="line">    figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/68/heatmap3.svg" alt="heatmap3"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 68 Transformer。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 67 自注意力</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/67/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/67/</id>
    <published>2023-12-14T07:00:00.000Z</published>
    <updated>2023-12-14T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="67-自注意力"><a href="#67-自注意力" class="headerlink" title="67 自注意力"></a>67 自注意力</h1><h2 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h2><ul><li><p>给定序列$\mathbf{x}_1, \cdots, \mathbf{x}_n, \forall\mathbf{x}_i \in \mathbb{R}^d$</p></li><li><p>自注意力池化层将$\mathbf{x}_i$当作key，value，query来对序列抽取特征得到$\mathbf{y}_1, \cdots, \mathbf{y}_n$，这里</p><script type="math/tex; mode=display">  \mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \cdots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d</script></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/67/image-20231214150855429.png" alt="image-20231214150855429"></p><h2 id="跟CNN，RNN对比"><a href="#跟CNN，RNN对比" class="headerlink" title="跟CNN，RNN对比"></a>跟CNN，RNN对比</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/67/image-20231214151210420.png" alt="image-20231214151210420"></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">CNN</th><th style="text-align:center">RNN</th><th style="text-align:center">自注意力</th></tr></thead><tbody><tr><td style="text-align:center"><strong>计算复杂度</strong></td><td style="text-align:center">$O(knd^2)$</td><td style="text-align:center">$O(nd^2)$</td><td style="text-align:center">$O(n^2d)$</td></tr><tr><td style="text-align:center"><strong>并行度</strong></td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$O(1)$</td><td style="text-align:center">$O(n)$</td></tr><tr><td style="text-align:center"><strong>最长路径</strong></td><td style="text-align:center">$O(\frac{n}{k})$</td><td style="text-align:center">$O(n)$</td><td style="text-align:center">$O(1)$</td></tr></tbody></table></div><p>其中：</p><ul><li>$k$是窗口大小</li><li>$n$是长度</li><li>$d$是维度</li></ul><p>自注意力比较适合看较长的文本。</p><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><ul><li><p>跟CNN/RNN不同，自注意力并没有记录位置信息</p></li><li><p>位置编码将位置信息注入到输入里</p><ul><li>假设长度为$n$的序列$\mathbf{X} \in \mathbb{R}^{n \times d}$，那么使用位置编码矩阵$\mathbf{P} \in \mathbb{R}^{n \times d}$来输出$\mathbf{X} + \mathbf{P}$作为自编码输入</li></ul></li><li><p>$\mathbf{P}$的元素如下计算：</p><script type="math/tex; mode=display">  \begin{aligned}  &p_{i, 2j} = \sin\left(\frac{i}{10000^{\frac{2j}{d}}}\right), \\  &p_{i, 2j + 1} = \cos\left(\frac{i}{10000^{\frac{2j}{d}}}\right)  \end{aligned}</script></li></ul><h2 id="位置编码矩阵"><a href="#位置编码矩阵" class="headerlink" title="位置编码矩阵"></a>位置编码矩阵</h2><ul><li>$\mathbf{P} \in \mathbb{R}^{n \times d}$：<script type="math/tex; mode=display">  \begin{aligned}  &p_{i, 2j} = \sin\left(\frac{i}{10000^{\frac{2j}{d}}}\right), \\  &p_{i, 2j + 1} = \cos\left(\frac{i}{10000^{\frac{2j}{d}}}\right)  \end{aligned}</script></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/67/self-attention-and-positional-encoding-curve.svg" alt="self-attention-and-positional-encoding-curve"></p><h2 id="绝对位置信息"><a href="#绝对位置信息" class="headerlink" title="绝对位置信息"></a>绝对位置信息</h2><ul><li>计算机使用的二进制编码</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0 in binary is 000</span><br><span class="line">1 in binary is 001</span><br><span class="line">2 in binary is 010</span><br><span class="line">3 in binary is 011</span><br><span class="line">4 in binary is 100</span><br><span class="line">5 in binary is 101</span><br><span class="line">6 in binary is 110</span><br><span class="line">7 in binary is 111</span><br></pre></td></tr></table></figure><ul><li>位置编码矩阵</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/67/self-attention-and-positional-encoding-matrix.svg" alt="self-attention-and-positional-encoding-matrix"></p><h2 id="相对位置信息"><a href="#相对位置信息" class="headerlink" title="相对位置信息"></a>相对位置信息</h2><ul><li><p>位置于$i + \delta$处的位置编码可以线性投影位置$i$处的位置编码来表示</p></li><li><p>记$\omega_j = \frac{1}{10000^{\frac{2j}{d}}}$，那么</p><script type="math/tex; mode=display">  \begin{bmatrix}  \cos(\delta\omega_j) & \sin(\delta\omega_j) \\  -\sin(\delta\omega_j) & \cos(\delta\omega_j)  \end{bmatrix}  \begin{bmatrix}  p_{i, 2j} \\  p_{i, 2j + 1}  \end{bmatrix}  =  \begin{bmatrix}  p_{i + \delta, 2j} \\  p_{i + \delta, 2j + 1}  \end{bmatrix}</script><p>  注意，投影矩阵$\begin{bmatrix}\cos(\delta\omega_j) &amp; \sin(\delta\omega_j) \\ -\sin(\delta\omega_j) &amp; \cos(\delta\omega_j)\end{bmatrix}$跟$i$无关。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>自注意力池化层将$\mathbf{x}_i$当作key，value，query来对序列抽取特征</li><li>完全并行，最长序列为1，但对长序列计算复杂度高</li><li>位置编码在输入中加入位置信息，使得自注意力能够记忆位置信息</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>自注意力：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=0.5, inplace=False)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_k): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_v): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">  (W_o): Linear(in_features=100, out_features=100, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 100])</span><br></pre></td></tr></table></figure><p>位置编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;位置编码&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(<span class="number">10000</span>, torch.arange(</span><br><span class="line">            <span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) / num_hiddens)</span><br><span class="line">        self.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        self.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = X + self.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(X)</span><br></pre></td></tr></table></figure><p>行代表标记在序列中的位置，列代表位置编码的不同维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/67/pos-encoding.svg" alt="pos-encoding"></p><p>二进制表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span> in binary is <span class="subst">&#123;i:&gt;03b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0 in binary is 000</span><br><span class="line">1 in binary is 001</span><br><span class="line">2 in binary is 010</span><br><span class="line">3 in binary is 011</span><br><span class="line">4 in binary is 100</span><br><span class="line">5 in binary is 101</span><br><span class="line">6 in binary is 110</span><br><span class="line">7 in binary is 111</span><br></pre></td></tr></table></figure><p>在编码维度上降低频率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">&#x27;Column (encoding dimension)&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Row (position)&#x27;</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/67/heatmap.svg" alt="heatmap"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 67 自注意力。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 66 使用注意力机制的seq2seq</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/66/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/66/</id>
    <published>2023-12-11T13:00:00.000Z</published>
    <updated>2023-12-11T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="66-使用注意力机制的seq2seq"><a href="#66-使用注意力机制的seq2seq" class="headerlink" title="66 使用注意力机制的seq2seq"></a>66 使用注意力机制的seq2seq</h1><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><ul><li><p>机器翻译中，每个生成的词可能相关于源句子中不同的词</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/66/image-20231211212741624.png" alt="image-20231211212741624"></p></li><li><p>seq2seq模型中不能对此直接建模</p></li></ul><h2 id="加入注意力"><a href="#加入注意力" class="headerlink" title="加入注意力"></a>加入注意力</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/66/image-20231211212842812.png" alt="image-20231211212842812"></p><ul><li>编码器对每次词的输出作为key和value（它们是同样的）</li><li>解码器RNN对上一个词的输出是query</li><li>注意力的输出和下一个词的词嵌入合并进入FC</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>Seq2seq中通过隐状态在编码器和解码器中传递信息</li><li>注意力机制可以根据解码器RNN的输出来匹配到合适的编码器RNN的输出来更有效的传递信息</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>带有注意力机制的解码器基本接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>实现带有Bahdanau注意力的循环神经网络解码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttentionDecoder</span>(<span class="title class_ inherited__">AttentionDecoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><p>测试Bahdanau注意力解码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                             num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                                  num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, <span class="built_in">len</span>(state), state[<span class="number">0</span>].shape, <span class="built_in">len</span>(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16]))</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(</span><br><span class="line">    <span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(</span><br><span class="line">    <span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss 0.019, 4382.6 tokens/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/66/output.svg" alt="output"></p><p>将几个英语句子翻译成法语：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">go . =&gt; va !,  bleu 1.000</span><br><span class="line">i lost . =&gt; j&#x27;ai perdu .,  bleu 1.000</span><br><span class="line">he&#x27;s calm . =&gt; il est paresseux .,  bleu 0.658</span><br><span class="line">i&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.cat([step[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq], <span class="number">0</span>).reshape((</span><br><span class="line">    <span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, num_steps))</span><br></pre></td></tr></table></figure><p>可视化注意力权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :<span class="built_in">len</span>(engs[-<span class="number">1</span>].split()) + <span class="number">1</span>].cpu(),</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/66/heatmap.svg" alt="heatmap"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 66 使用注意力机制的seq2seq。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 65 注意力分数</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/65/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/65/</id>
    <published>2023-12-11T10:00:00.000Z</published>
    <updated>2023-12-11T11:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="65-注意力分数"><a href="#65-注意力分数" class="headerlink" title="65 注意力分数"></a>65 注意力分数</h1><h2 id="注意力分数"><a href="#注意力分数" class="headerlink" title="注意力分数"></a>注意力分数</h2><ul><li><p>回顾：</p><script type="math/tex; mode=display">  f(x) = \sum_i\alpha(x, x_i)y_i = \sum_{i = 1}^n\text{softmax}\left(-\frac{1}{2}(x - x_i)^2\right)y_i</script><ul><li>其中：<ul><li>$\alpha(x, x_i)$为注意力权重</li><li>$-\frac{1}{2}(x - x_i)^2$是注意力分数</li></ul></li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/65/image-20231211185951329.png" alt="image-20231211185951329"></p><h2 id="拓展到高维度"><a href="#拓展到高维度" class="headerlink" title="拓展到高维度"></a>拓展到高维度</h2><ul><li><p>假设query $\mathbf{q} \in \mathbb{R}^q$，$m$对key-value $(\mathbf{k}_1, \mathbf{v}_1), \cdots, (\mathbf{k}_m, \mathbf{v}_m)$这里$\mathbf{k}_i \in \mathbb{R}^k, \mathbf{v}_i \in \mathbb{R}^v$</p></li><li><p>注意力池化层：</p><script type="math/tex; mode=display">  \begin{aligned}  f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \cdots, (\mathbf{k}_m, \mathbf{v}_m)) &= \sum_{i = 1}^m\alpha(\mathbf{q}, \mathbf{k}_i)\mathbf{v}_i \in \mathbb{R}^v \\  \alpha(\mathbf{q}, \mathbf{k}_i) = \text{softmax}(a(\mathbf{q}, \mathbf{k}_i)) &= \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j = 1}^m\exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}  \end{aligned}</script></li></ul><h2 id="Additive-Attention"><a href="#Additive-Attention" class="headerlink" title="Additive Attention"></a>Additive Attention</h2><ul><li>可学参数：<ul><li>$\mathbf{W}_k \in \mathbb{R}^{h \times k}, \mathbf{W}_q \in \mathbb{R}^{h \times q}, \mathbf{v} \in \mathbb{R}^h$</li><li>$a(\mathbf{k}, \mathbf{q}) = \mathbf{v}^\mathrm{T}\tanh(\mathbf{W}_k\mathbf{k} + \mathbf{W}_q\mathbf{q})$</li></ul></li><li>等价于将key和query合并起来后放入到一个隐藏大小为$h$，输出大小为$1$的单隐藏层MLP</li></ul><h2 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h2><ul><li><p>如果query和key都是同样的长度$\mathbf{q}, \mathbf{k} \in \mathbb{R}^d$，那么可以</p><script type="math/tex; mode=display">  a(\mathbf{q}, \mathbf{k}_i) = \frac{\left\langle\mathbf{q}, \mathbf{k}_i\right\rangle}{\sqrt{d}}</script></li><li><p>向量化版本</p><ul><li>$\mathbf{Q} \in \mathbb{R}^{n \times d}, \mathbf{K} \in \mathbb{R}^{m \times d}, \mathbf{V} \in \mathbb{R}^{m \times v}$</li><li>注意力分数：$a(\mathbf{Q}, \mathbf{K}) = \frac{\mathbf{Q}\mathbf{K}^\mathrm{T}}{\sqrt{d}} \in \mathbb{R}^{n \times m}$</li><li>注意力池化：$f = \text{softmax}(a(\mathbf{Q}, \mathbf{K}))\mathbf{V} \in \mathbb{R}^{n \times v}$</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>注意力分数是query和key的相似度，注意力权重是分数的softmax结果</li><li>两种常见的分数计算：<ul><li>将query和key合并起来进入一个单输出单隐藏层的MLP</li><li>直接将query和key做内积</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>遮蔽softmax操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>演示此函数是如何工作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[0.5971, 0.4029, 0.0000, 0.0000],</span><br><span class="line">         [0.5368, 0.4632, 0.0000, 0.0000]],</span><br><span class="line"></span><br><span class="line">        [[0.3622, 0.3615, 0.2763, 0.0000],</span><br><span class="line">         [0.5435, 0.2094, 0.2471, 0.0000]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>]]))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[1.0000, 0.0000, 0.0000, 0.0000],</span><br><span class="line">         [0.3441, 0.2273, 0.4286, 0.0000]],</span><br><span class="line"></span><br><span class="line">        [[0.3945, 0.6055, 0.0000, 0.0000],</span><br><span class="line">         [0.1446, 0.2679, 0.3367, 0.2507]]])</span><br></pre></td></tr></table></figure><p>加性注意力：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>演示上面的<code>AdditiveAttention</code>类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>, <span class="number">4</span>).repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>,</span><br><span class="line">                              dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span><br><span class="line"></span><br><span class="line">        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>注意力权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/65/output1.svg" alt="output1"></p><p>缩放点积注意力：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>, <span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure><p>演示上述的<code>DotProductAttention</code>类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],</span><br><span class="line"></span><br><span class="line">        [[10.0000, 11.0000, 12.0000, 13.0000]]])</span><br></pre></td></tr></table></figure><p>均匀的注意力权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/65/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 65 注意力分数。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 64 注意力机制</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/64/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/64/</id>
    <published>2023-12-11T07:00:00.000Z</published>
    <updated>2023-12-11T07:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="64-注意力机制"><a href="#64-注意力机制" class="headerlink" title="64 注意力机制"></a>64 注意力机制</h1><h2 id="心理学"><a href="#心理学" class="headerlink" title="心理学"></a>心理学</h2><ul><li><p>动物需要在复杂环境下有效关注值得注意的点</p></li><li><p>心理学框架：人类根据随意线索和不随意线索选择注意点</p><ul><li><p>红色杯子：不随意线索（不随着自己的意识，仅仅是因为红色显眼）</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/64/image-20231211160731617.png" alt="image-20231211160731617"></p></li><li><p>想读书：随意线索（随着自己的意识，主动的想看书）</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/64/image-20231211160750450.png" alt="image-20231211160750450"></p></li></ul></li></ul><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><ul><li>卷积、全连接、池化层都只考虑不随意线索</li><li>注意力机制则显式的考虑随意线索<ul><li>随意线索被称之为查询（query）</li><li>每个输入是一个值（value）和不随意线索（key）的对</li><li>通过注意力池化层来有偏向性的选择某些输入</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/64/image-20231211160807502.png" alt="image-20231211160807502"></p><h2 id="非参注意力池化层"><a href="#非参注意力池化层" class="headerlink" title="非参注意力池化层"></a>非参注意力池化层</h2><ul><li><p>给定数据$(x_i, y_i), i = 1, 2, \cdots, n$</p></li><li><p>平均池化是最简单的方案：$f(x) = \frac{1}{n}\sum_i y_i$</p></li><li><p>更好的方案是60年代提出来的Nadaraya-Watson核回归</p><script type="math/tex; mode=display">  f(x) = \sum_{i = 1}^n\frac{K(x - x_j)}{\sum_{j = 1}^nK(x - x_j)}y_i</script><ul><li>其中：<ul><li>$x$是query</li><li>$x_j$是key</li><li>$y_i$是value</li></ul></li></ul></li></ul><h2 id="Nadaraya-Watson核回归"><a href="#Nadaraya-Watson核回归" class="headerlink" title="Nadaraya-Watson核回归"></a>Nadaraya-Watson核回归</h2><ul><li><p>使用高斯核$K(u) = \frac{1}{\sqrt{2\pi}}\exp(-\frac{u^2}{2})$</p></li><li><p>那么</p><script type="math/tex; mode=display">  \begin{aligned}  f(x)  &= \sum_{i = 1}^n\frac{\exp(-\frac{1}{2}(x - x_i)^2)}{\sum_{j = 1}^n\exp(-\frac{1}{2}(x - x_j)^2)}y_i \\  &= \sum_{i = 1}^n\text{softmax}\left(-\frac{1}{2}(x - x_i)^2\right)y_i  \end{aligned}</script></li></ul><h2 id="参数化的注意力机制"><a href="#参数化的注意力机制" class="headerlink" title="参数化的注意力机制"></a>参数化的注意力机制</h2><ul><li>在之前的基础上引入可以学习的$w$<script type="math/tex; mode=display">  f(x) = \sum_{i = 1}^n\text{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right)y_i</script></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>心理学认为人通过随意线索和不随意线索选择注意点</li><li>注意力机制中，通过query（随意线索）和key（不随意线索）来有偏向性的选择输入<ul><li>可以一般的写作$f(x) = \sum_i\alpha(x, x_i)y_i$，这里$\alpha(x, x_i)$是注意力权重</li><li>早在60年代就有非参数的注意力机制</li><li>接下来我们会介绍多个不同的权重设计</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>生成数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">n_train = <span class="number">50</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x ** <span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))</span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">y_truth = f(x_test)</span><br><span class="line">n_test = <span class="built_in">len</span>(x_test)</span><br><span class="line">n_test</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">50</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>);</span><br><span class="line"></span><br><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/64/avg-pooling.svg" alt="avg-pooling"></p><p>非参数注意力汇聚：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line">attention_weights = nn.functional.softmax(-(X_repeat - x_train) ** <span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/64/attn-pooling.svg" alt="attn-pooling"></p><p>注意力权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/64/heatmap1.svg" alt="heatmap1"></p><p>带参数注意力汇聚，假定两个张量的形状分别是$(n, a, b)$和$(n, b, c)$，它们的批量矩阵乘法输出的形状为$(n, a, c)$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 1, 6])</span><br></pre></td></tr></table></figure><p>使用小批量矩阵乘法来计算小批量数据中的加权平均值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 4.5000]],</span><br><span class="line"></span><br><span class="line">        [[14.5000]]])</span><br></pre></td></tr></table></figure><p>带参数的注意力汇聚：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NWKernelRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values</span>):</span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape((-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        self.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * self.w) ** <span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>将训练数据集转换为键和值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape((n_train, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>训练带参数的注意力汇聚模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/64/output.svg" alt="output"></p><p>预测结果绘制：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keys = x_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">plot_kernel_reg(y_hat)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/64/nadaraya-watson-gaussian-para-pooling.svg" alt="nadaraya-watson-gaussian-para-pooling"></p><p>曲线在注意力权重较大的区域变得更不平滑：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/64/heatmap2.svg" alt="heatmap2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 64 注意力机制。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 63 束搜索</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/63/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/63/</id>
    <published>2023-12-11T06:00:00.000Z</published>
    <updated>2023-12-11T06:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="63-束搜索"><a href="#63-束搜索" class="headerlink" title="63 束搜索"></a>63 束搜索</h1><h2 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h2><ul><li><p>在seq2seq中我们使用了贪心搜索来预测序列</p><ul><li>将当前时刻预测概率最大的词输出</li></ul></li><li><p>但贪心很可能不是最优的：</p><ul><li><p>贪心：$0.5 \times 0.4 \times 0.4 \times 0.6 = 0.048$</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/63/image-20231211143442233.png" alt="image-20231211143442233"></p></li><li><p>很好的选项：$0.5 \times 0.3 \times 0.6 \times 0.6 = 0.054$</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/63/image-20231211143522750.png" alt="image-20231211143522750"></p></li></ul></li></ul><h2 id="穷举搜索"><a href="#穷举搜索" class="headerlink" title="穷举搜索"></a>穷举搜索</h2><ul><li>最优算法：对所有可能的序列，计算它的概率，然后选取最好的那个</li><li>如果输出字典大小为$n$，序列最长为$T$，那么我们需要考察$n^T$个序列<ul><li>$n = 10000, T = 10 \to n^T = 10^{40}$</li><li>计算上不可行</li></ul></li></ul><h2 id="束搜索"><a href="#束搜索" class="headerlink" title="束搜索"></a>束搜索</h2><ul><li><p>保存最好的$k$个候选</p></li><li><p>在每个时刻，对每个候选新加一项（$n$种可能），在$kn$个选项中选出最好的$k$个（在下图中，$k = 2$，即每列选2个）</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/63/image-20231211144552184.png" alt="image-20231211144552184"></p></li><li><p>时间复杂度$O(knT)$</p><ul><li>$k = 5, n = 10000, T = 10 \to knT = 5 \times 10^5$</li></ul></li><li><p>每个候选的最终分数是：</p><script type="math/tex; mode=display">  \frac{1}{L^\alpha}\log p(y_1, \cdots, y_L) = \frac{1}{L^\alpha}\sum_{t^\prime = 1}^L\log p(y_{t^\prime} | y_1, \cdots, y_{t^\prime - 1}, \mathbf{c})</script><ul><li>通常$\alpha = 0.75$</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>束搜索在每次搜索时保存$k$个最好的候选<ul><li>$k = 1$时是贪心搜索</li><li>$k = n$是穷举搜索</li></ul></li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 63 束搜索。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 62 序列到序列学习（seq2seq）</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/62/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/62/</id>
    <published>2023-12-10T14:00:00.000Z</published>
    <updated>2023-12-10T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="62-序列到序列学习（seq2seq）"><a href="#62-序列到序列学习（seq2seq）" class="headerlink" title="62 序列到序列学习（seq2seq）"></a>62 序列到序列学习（seq2seq）</h1><h2 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h2><ul><li>给定一个源语言的句子，自动翻译为目标语言</li><li>这两个句子可以有不同的长度</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/62/image-20231210212427850.png" alt="image-20231210212427850"></p><h2 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/62/image-20231210212645519.png" alt="image-20231210212645519"></p><ul><li>编码器是一个RNN，读取输入句子<ul><li>可以是双向</li></ul></li><li>解码器使用另外一个RNN来输出</li></ul><h2 id="编码器-解码器细节"><a href="#编码器-解码器细节" class="headerlink" title="编码器-解码器细节"></a>编码器-解码器细节</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/62/image-20231210213505869.png" alt="image-20231210213505869"></p><ul><li>编码器是没有输出层到RNN</li><li>编码器最后时间步的隐状态用作解码器的初始隐状态</li></ul><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul><li><p>训练时解码器使用目标句子作为输入</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/62/image-20231210213820977.png" alt="image-20231210213820977"></p></li><li><p>推理</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/62/image-20231210213903089.png" alt="image-20231210213903089"></p></li></ul><h2 id="衡量生成序列好坏的BLEU"><a href="#衡量生成序列好坏的BLEU" class="headerlink" title="衡量生成序列好坏的BLEU"></a>衡量生成序列好坏的BLEU</h2><ul><li><p>$p_n$是预测中所有n-gram的精度</p><ul><li><p>标签序列$A\ B\ C\ D\ E\ F\ G$和预测序列$A\ B\ B\ C\ D$，有$p_1 = \frac{4}{5}$，$p_2 = \frac{3}{4}$，$p_3 = \frac{1}{3}$和$p_4 = 0$</p><ul><li><p>$p_1$的计算过程如下：</p><p>  在预测序列$A\ B\ B\ C\ D$中，<code>unigram</code>（一元语法）的个数为5（分别为$A$，$B$，$B$，$C$和$D$），那么这5个<code>unigram</code>在标签序列中出现了4次，那么可得$p_1 = \frac{4}{5}$</p></li><li><p>$p_2$的计算过程如下：</p><p>  在预测序列$A\ B\ B\ C\ D$中，<code>bigram</code>（二元语法）的个数为4，（分别为$A\ B$，$B\ B$，$B\ C$和$C\ D$），那么这4个<code>bigram</code>在标签序列中出现了3次，那么可得$p_2 = \frac{3}{4}$</p></li><li><p>$p_3$和$p_4$的计算同理</p></li></ul></li></ul></li><li><p>BLEU定义（越大越好）</p><script type="math/tex; mode=display">  \exp\left(\min\left(0, 1 - \frac{\text{len}_\mathsf{label}}{\text{len}_\mathsf{pred}}\right)\right)\prod_{n = 1}^kp_n^{\frac{1}{2^n}}</script><ul><li>其中：<ul><li>$\text{len}_\mathsf{label}$是标签序列的长度</li><li>$\text{len}_\mathsf{pred}$是预测序列的长度</li><li>那么当预测序列的长度过短，小于标签序列时，$\frac{\text{len}_\mathsf{label}}{\text{len}_\mathsf{pred}} &gt; 1$，会导致指数的次数为一个负数，最后结果是一个很小的数，也就实现了对过短的预测进行惩罚</li><li>在最后的$p_n^{\frac{1}{2^n}}$处，因为$p_n \le 1$，越长的匹配会有更大的$n$，导致指数的次数更靠近0，值会越大，从而权重会随着$n$的增大而增大</li></ul></li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>Seq2seq从一个句子生成另一个句子</li><li>编码器和解码器都是RNN</li><li>将编码器最后时间隐状态来初始解码器隐状态来完成信息传递</li><li>常用BLEU来衡量生成序列的好坏</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>实现循环神经网络编码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        X = self.embedding(X)</span><br><span class="line">        X = X.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>上述编码器的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder = Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">output, state = encoder(X)</span><br><span class="line">output.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">7</span>, <span class="number">4</span>, <span class="number">16</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">state.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 16])</span><br></pre></td></tr></table></figure><p>解码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于序列到序列学习的循环神经网络解码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> enc_outputs[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        context = state[-<span class="number">1</span>].repeat(X.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        X_and_context = torch.cat((X, context), <span class="number">2</span>)</span><br><span class="line">        output, state = self.rnn(X_and_context, state)</span><br><span class="line">        output = self.dense(output).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br></pre></td></tr></table></figure><p>实例化解码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">decoder = Seq2SeqDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                         num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">state = decoder.init_state(encoder(X))</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, state.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))</span><br></pre></td></tr></table></figure><p>通过零值化屏蔽不相关的项：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sequence_mask</span>(<span class="params">X, valid_len, value=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在序列中屏蔽不相关的项&quot;&quot;&quot;</span></span><br><span class="line">    maxlen = X.size(<span class="number">1</span>)</span><br><span class="line">    mask = torch.arange((maxlen), dtype=torch.float32,</span><br><span class="line">                        device=X.device)[<span class="literal">None</span>, :] &lt; valid_len[:, <span class="literal">None</span>]</span><br><span class="line">    X[~mask] = value</span><br><span class="line">    <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">sequence_mask(X, torch.tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 0, 0],</span><br><span class="line">        [4, 5, 0]])</span><br></pre></td></tr></table></figure><p>通过扩展softmax交叉熵损失函数来遮蔽不相关的预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MaskedSoftmaxCELoss</span>(nn.CrossEntropyLoss):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带遮蔽的softmax交叉熵损失函数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label, valid_len</span>):</span><br><span class="line">        weights = torch.ones_like(label)</span><br><span class="line">        weights = sequence_mask(weights, valid_len)</span><br><span class="line">        self.reduction = <span class="string">&#x27;none&#x27;</span></span><br><span class="line">        unweighted_loss = <span class="built_in">super</span>(MaskedSoftmaxCELoss, self).forward(</span><br><span class="line">            pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>), label)</span><br><span class="line">        weighted_loss = (unweighted_loss * weights).mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> weighted_loss</span><br></pre></td></tr></table></figure><p>代码健全性检查：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = MaskedSoftmaxCELoss()</span><br><span class="line">loss(torch.ones(<span class="number">3</span>, <span class="number">4</span>, <span class="number">10</span>), torch.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=torch.long),</span><br><span class="line">     torch.tensor([<span class="number">4</span>, <span class="number">2</span>, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([2.3026, 1.1513, 0.0000])</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">net, data_iter, lr, num_epochs, tgt_vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练序列到序列模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">xavier_init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.GRU:</span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> m._flat_weights_names:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&quot;weight&quot;</span> <span class="keyword">in</span> param:</span><br><span class="line">                    nn.init.xavier_uniform_(m._parameters[param])</span><br><span class="line"></span><br><span class="line">    net.apply(xavier_init_weights)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    loss = MaskedSoftmaxCELoss()</span><br><span class="line">    net.train()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                     xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer = d2l.Timer()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> data_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, X_valid_len, Y, Y_valid_len = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> batch]</span><br><span class="line">            bos = torch.tensor([tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]] * Y.shape[<span class="number">0</span>],</span><br><span class="line">                          device=device).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">            dec_input = torch.cat([bos, Y[:, :-<span class="number">1</span>]], <span class="number">1</span>)</span><br><span class="line">            Y_hat, _ = net(X, dec_input, X_valid_len)</span><br><span class="line">            l = loss(Y_hat, Y, Y_valid_len)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            num_tokens = Y_valid_len.<span class="built_in">sum</span>()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l.<span class="built_in">sum</span>(), num_tokens)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (metric[<span class="number">0</span>] / metric[<span class="number">1</span>],))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>, <span class="subst">&#123;metric[<span class="number">1</span>] / timer.stop():<span class="number">.1</span>f&#125;</span> &#x27;</span></span><br><span class="line">        <span class="string">f&#x27;tokens/sec on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>创建和训练一个循环神经网络“编码器-解码器”模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">300</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                         dropout)</span><br><span class="line">decoder = Seq2SeqDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers,</span><br><span class="line">                         dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss 0.020, 11362.9 tokens/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/62/output.svg" alt="output"></p><p>预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_seq2seq</span>(<span class="params">net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span><br><span class="line"><span class="params">                    device, save_attention_weights=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;序列到序列模型的预测&quot;&quot;&quot;</span></span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    src_tokens = src_vocab[src_sentence.lower().split(<span class="string">&#x27; &#x27;</span>)] + [src_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">    enc_valid_len = torch.tensor([<span class="built_in">len</span>(src_tokens)], device=device)</span><br><span class="line">    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br><span class="line">    enc_X = torch.unsqueeze(</span><br><span class="line">        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    enc_outputs = net.encoder(enc_X, enc_valid_len)</span><br><span class="line">    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)</span><br><span class="line">    dec_X = torch.unsqueeze(torch.tensor(</span><br><span class="line">        [tgt_vocab[<span class="string">&#x27;&lt;bos&gt;&#x27;</span>]], dtype=torch.long, device=device), dim=<span class="number">0</span>)</span><br><span class="line">    output_seq, attention_weight_seq = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        Y, dec_state = net.decoder(dec_X, dec_state)</span><br><span class="line">        dec_X = Y.argmax(dim=<span class="number">2</span>)</span><br><span class="line">        pred = dec_X.squeeze(dim=<span class="number">0</span>).<span class="built_in">type</span>(torch.int32).item()</span><br><span class="line">        <span class="keyword">if</span> save_attention_weights:</span><br><span class="line">            attention_weight_seq.append(net.decoder.attention_weights)</span><br><span class="line">        <span class="keyword">if</span> pred == tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        output_seq.append(pred)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27; &#x27;</span>.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span><br></pre></td></tr></table></figure><p>BLEU的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bleu</span>(<span class="params">pred_seq, label_seq, k</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算BLEU&quot;&quot;&quot;</span></span><br><span class="line">    pred_tokens, label_tokens = pred_seq.split(<span class="string">&#x27; &#x27;</span>), label_seq.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">    len_pred, len_label = <span class="built_in">len</span>(pred_tokens), <span class="built_in">len</span>(label_tokens)</span><br><span class="line">    score = math.exp(<span class="built_in">min</span>(<span class="number">0</span>, <span class="number">1</span> - len_label / len_pred))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, k + <span class="number">1</span>):</span><br><span class="line">        num_matches, label_subs = <span class="number">0</span>, collections.defaultdict(<span class="built_in">int</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_label - n + <span class="number">1</span>):</span><br><span class="line">            label_subs[<span class="string">&#x27; &#x27;</span>.join(label_tokens[i: i + n])] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len_pred - n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] &gt; <span class="number">0</span>:</span><br><span class="line">                num_matches += <span class="number">1</span></span><br><span class="line">                label_subs[<span class="string">&#x27; &#x27;</span>.join(pred_tokens[i: i + n])] -= <span class="number">1</span></span><br><span class="line">        score *= math.<span class="built_in">pow</span>(num_matches / (len_pred - n + <span class="number">1</span>), math.<span class="built_in">pow</span>(<span class="number">0.5</span>, n))</span><br><span class="line">    <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure><p>将几个英语句子翻译成法语：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, attention_weight_seq = predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, bleu <span class="subst">&#123;bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">go . =&gt; va !, bleu 1.000</span><br><span class="line">i lost . =&gt; j&#x27;ai perdu ., bleu 1.000</span><br><span class="line">he&#x27;s calm . =&gt; il est malade emporté . ., bleu 0.473</span><br><span class="line">i&#x27;m home . =&gt; je suis chez moi signe de la la la la, bleu 0.481</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 62 序列到序列学习（seq2seq）。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 61 编码器-解码器架构</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/61/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/61/</id>
    <published>2023-12-10T13:30:00.000Z</published>
    <updated>2023-12-10T13:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="61-编码器-解码器架构"><a href="#61-编码器-解码器架构" class="headerlink" title="61 编码器-解码器架构"></a>61 编码器-解码器架构</h1><h2 id="重新考察CNN"><a href="#重新考察CNN" class="headerlink" title="重新考察CNN"></a>重新考察CNN</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/61/image-20231210210014513.png" alt="image-20231210210014513"></p><ul><li>编码器：将输入编程成中间表达形式（特征）</li><li>解码器：将中间表示解码成输出</li></ul><h2 id="重新考察RNN"><a href="#重新考察RNN" class="headerlink" title="重新考察RNN"></a>重新考察RNN</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/61/image-20231210210241594.png" alt="image-20231210210241594"></p><ul><li>编码器：将文本表示为向量</li><li>解码器：向量表示成输出</li></ul><h2 id="编码器-解码器架构"><a href="#编码器-解码器架构" class="headerlink" title="编码器-解码器架构"></a>编码器-解码器架构</h2><ul><li>一个模型被分为两块<ul><li>编码器处理输入</li><li>解码器生成输出</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/61/image-20231210210402687.png" alt="image-20231210210402687"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>使用编码器-解码器架构的模型，编码器负责表示输入，解码器负责输出</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>编码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>解码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>合并编码器和解码器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 61 编码器-解码器架构。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 60 机器翻译数据集</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/60/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/60/</id>
    <published>2023-12-10T13:00:00.000Z</published>
    <updated>2023-12-10T13:20:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="60-机器翻译数据集"><a href="#60-机器翻译数据集" class="headerlink" title="60 机器翻译数据集"></a>60 机器翻译数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>下载和预处理数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;fra-eng&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;fra-eng.zip&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;94646ad1522d915e7b0f9296181140edcf86a4f5&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_nmt</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;载入“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;fra-eng&#x27;</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(data_dir, <span class="string">&#x27;fra.txt&#x27;</span>), <span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">             encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">raw_text = read_data_nmt()</span><br><span class="line"><span class="built_in">print</span>(raw_text[:<span class="number">75</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Go.Va !</span><br><span class="line">Hi.Salut !</span><br><span class="line">Run!Cours !</span><br><span class="line">Run!Courez !</span><br><span class="line">Who?Qui ?</span><br><span class="line">Wow!Ça alors !</span><br></pre></td></tr></table></figure><p>几个预处理步骤：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess_nmt</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预处理“英语－法语”数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">no_space</span>(<span class="params">char, prev_char</span>):</span><br><span class="line">        <span class="keyword">return</span> char <span class="keyword">in</span> <span class="built_in">set</span>(<span class="string">&#x27;,.!?&#x27;</span>) <span class="keyword">and</span> prev_char != <span class="string">&#x27; &#x27;</span></span><br><span class="line"></span><br><span class="line">    text = text.replace(<span class="string">&#x27;\u202f&#x27;</span>, <span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\xa0&#x27;</span>, <span class="string">&#x27; &#x27;</span>).lower()</span><br><span class="line">    out = [<span class="string">&#x27; &#x27;</span> + char <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> no_space(char, text[i - <span class="number">1</span>]) <span class="keyword">else</span> char</span><br><span class="line">           <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(text)]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(out)</span><br><span class="line"></span><br><span class="line">text = preprocess_nmt(raw_text)</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">80</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">go .va !</span><br><span class="line">hi .salut !</span><br><span class="line">run !cours !</span><br><span class="line">run !courez !</span><br><span class="line">who ?qui ?</span><br><span class="line">wow !ça alors !</span><br></pre></td></tr></table></figure><p>词元化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_nmt</span>(<span class="params">text, num_examples=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;词元化“英语－法语”数据数据集&quot;&quot;&quot;</span></span><br><span class="line">    source, target = [], []</span><br><span class="line">    <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(text.split(<span class="string">&#x27;\n&#x27;</span>)):</span><br><span class="line">        <span class="keyword">if</span> num_examples <span class="keyword">and</span> i &gt; num_examples:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts = line.split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) == <span class="number">2</span>:</span><br><span class="line">            source.append(parts[<span class="number">0</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">            target.append(parts[<span class="number">1</span>].split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">    <span class="keyword">return</span> source, target</span><br><span class="line"></span><br><span class="line">source, target = tokenize_nmt(text)</span><br><span class="line">source[:<span class="number">6</span>], target[:<span class="number">6</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">([[&#x27;go&#x27;, &#x27;.&#x27;],</span><br><span class="line">  [&#x27;hi&#x27;, &#x27;.&#x27;],</span><br><span class="line">  [&#x27;run&#x27;, &#x27;!&#x27;],</span><br><span class="line">  [&#x27;run&#x27;, &#x27;!&#x27;],</span><br><span class="line">  [&#x27;who&#x27;, &#x27;?&#x27;],</span><br><span class="line">  [&#x27;wow&#x27;, &#x27;!&#x27;]],</span><br><span class="line"> [[&#x27;va&#x27;, &#x27;!&#x27;],</span><br><span class="line">  [&#x27;salut&#x27;, &#x27;!&#x27;],</span><br><span class="line">  [&#x27;cours&#x27;, &#x27;!&#x27;],</span><br><span class="line">  [&#x27;courez&#x27;, &#x27;!&#x27;],</span><br><span class="line">  [&#x27;qui&#x27;, &#x27;?&#x27;],</span><br><span class="line">  [&#x27;ça&#x27;, &#x27;alors&#x27;, &#x27;!&#x27;]])</span><br></pre></td></tr></table></figure><p>绘制每个文本序列所包含的标记数量的直方图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_list_len_pair_hist</span>(<span class="params">legend, xlabel, ylabel, xlist, ylist</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制列表长度对的直方图&quot;&quot;&quot;</span></span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    _, _, patches = d2l.plt.hist(</span><br><span class="line">        [[<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> xlist],</span><br><span class="line">         [<span class="built_in">len</span>(l) <span class="keyword">for</span> l <span class="keyword">in</span> ylist]])</span><br><span class="line">    d2l.plt.xlabel(xlabel)</span><br><span class="line">    d2l.plt.ylabel(ylabel)</span><br><span class="line">    <span class="keyword">for</span> patch <span class="keyword">in</span> patches[<span class="number">1</span>].patches:</span><br><span class="line">        patch.set_hatch(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">    d2l.plt.legend(legend)</span><br><span class="line"></span><br><span class="line">show_list_len_pair_hist([<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;target&#x27;</span>], <span class="string">&#x27;# tokens per sequence&#x27;</span>,</span><br><span class="line">                        <span class="string">&#x27;count&#x27;</span>, source, target);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/60/list-len-pair-hist.svg" alt="list-len-pair-hist"></p><p>词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                      reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line"><span class="built_in">len</span>(src_vocab)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10012</span><br></pre></td></tr></table></figure><p>序列样本都有一个固定的长度，我们截断或填充文本序列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">truncate_pad</span>(<span class="params">line, num_steps, padding_token</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;截断或填充文本序列&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(line) &gt; num_steps:</span><br><span class="line">        <span class="keyword">return</span> line[:num_steps]</span><br><span class="line">    <span class="keyword">return</span> line + [padding_token] * (num_steps - <span class="built_in">len</span>(line))</span><br><span class="line"></span><br><span class="line">truncate_pad(src_vocab[source[<span class="number">0</span>]], <span class="number">10</span>, src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure><p>转换成小批量数据集用于训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_array_nmt</span>(<span class="params">lines, vocab, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将机器翻译的文本序列转换成小批量&quot;&quot;&quot;</span></span><br><span class="line">    lines = [vocab[l] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    lines = [l + [vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]] <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    array = torch.tensor([truncate_pad(</span><br><span class="line">        l, num_steps, vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]) <span class="keyword">for</span> l <span class="keyword">in</span> lines])</span><br><span class="line">    valid_len = (array != vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="built_in">type</span>(torch.int32).<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> array, valid_len</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_nmt</span>(<span class="params">batch_size, num_steps, num_examples=<span class="number">600</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回翻译数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    text = preprocess_nmt(read_data_nmt())</span><br><span class="line">    source, target = tokenize_nmt(text, num_examples)</span><br><span class="line">    src_vocab = d2l.Vocab(source, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    tgt_vocab = d2l.Vocab(target, min_freq=<span class="number">2</span>,</span><br><span class="line">                          reserved_tokens=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;bos&gt;&#x27;</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>])</span><br><span class="line">    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)</span><br><span class="line">    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)</span><br><span class="line">    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)</span><br><span class="line">    data_iter = d2l.load_array(data_arrays, batch_size)</span><br><span class="line">    <span class="keyword">return</span> data_iter, src_vocab, tgt_vocab</span><br></pre></td></tr></table></figure><p>读出“英语 - 法语”数据集中的第一个小批量数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=<span class="number">2</span>, num_steps=<span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> X, X_valid_len, Y, Y_valid_len <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;valid lengths for X:&#x27;</span>, X_valid_len)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Y:&#x27;</span>, Y.<span class="built_in">type</span>(torch.int32))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;valid lengths for Y:&#x27;</span>, Y_valid_len)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X: tensor([[  7, 155,   4,   3,   1,   1,   1,   1],</span><br><span class="line">        [  6,  18,  35,   4,   3,   1,   1,   1]], dtype=torch.int32)</span><br><span class="line">valid lengths for X: tensor([4, 5])</span><br><span class="line">Y: tensor([[  6,   7,   0,   4,   3,   1,   1,   1],</span><br><span class="line">        [  6,   7, 163,   4,   3,   1,   1,   1]], dtype=torch.int32)</span><br><span class="line">valid lengths for Y: tensor([5, 5])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 60 机器翻译数据集。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 59 双向循环神经网络</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/59/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/59/</id>
    <published>2023-12-10T12:00:00.000Z</published>
    <updated>2023-12-10T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="59-双向循环神经网络"><a href="#59-双向循环神经网络" class="headerlink" title="59 双向循环神经网络"></a>59 双向循环神经网络</h1><h2 id="未来很重要"><a href="#未来很重要" class="headerlink" title="未来很重要"></a>未来很重要</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I am _____</span><br><span class="line">I am _____ very hungry,</span><br><span class="line">I am _____ very hungry, I could eat half a pig.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I am happy.</span><br><span class="line">I am not   very hungry,</span><br><span class="line">I am very  very hungry, I could eat half a pig.</span><br></pre></td></tr></table></figure><ul><li>取决于过去和未来的上下文，可以填很不一样的词</li><li>目前为止RNN只看过去</li><li>在填空的时候，我们也可以看未来</li></ul><h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/59/image-20231210195450072.png" alt="image-20231210195450072"></p><ul><li>一个前向RNN隐层</li><li>一个反向RNN隐层</li><li>合并两个隐状态得到输出</li></ul><script type="math/tex; mode=display">\begin{aligned}\overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t\mathbf{W}_{xh}^{(f)} + \overrightarrow{\mathbf{H}}_{t - 1}\mathbf{W}_{hh}^{(f)} + \mathbf{b}_h^{(f)}) \\\overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t\mathbf{W}_{xh}^{(b)} + \overleftarrow{\mathbf{H}}_{t + 1}\mathbf{W}_{hh}^{(b)} + \mathbf{b}_h^{(b)}) \\\mathbf{H}_t &= [\overrightarrow{\mathbf{H}}_t, \overleftarrow{\mathbf{H}}_t] \\\mathbf{O}_t &= \mathbf{H}_t\mathbf{W}_{hq} + \mathbf{b}_q\end{aligned}</script><h2 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h2><ul><li><p>训练：</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/59/image-20231210200400864.png" alt="image-20231210200400864"></p></li><li><p>推理：</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/59/image-20231210200427039.png" alt="image-20231210200427039"></p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>双向循环神经网络通过反向更新的隐藏层来利用方向时间信息</li><li>通常用来对序列数据抽取特征、填空，而不是预测未来</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>双向循环神经网络的错误应用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps, device = <span class="number">32</span>, <span class="number">35</span>, d2l.try_gpu()</span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity 1.2, 52115.2 tokens/sec on cuda:0</span><br><span class="line">time travellerererererererererererererererererererererererererer</span><br><span class="line">travellerererererererererererererererererererererererererer</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/59/output.svg" alt="output"></p><p>可以看到训练效果很好，但是预测的全部都是没有意义的字母，所以我们不能用双向循环神经网络来训练语言模型，因为语言模型是用来预测未来的。</p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 59 双向循环神经网络。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 58 深层循环神经网络</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/58/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/58/</id>
    <published>2023-12-10T11:00:00.000Z</published>
    <updated>2023-12-10T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="58-深层循环神经网络"><a href="#58-深层循环神经网络" class="headerlink" title="58 深层循环神经网络"></a>58 深层循环神经网络</h1><h2 id="回顾：循环神经网络"><a href="#回顾：循环神经网络" class="headerlink" title="回顾：循环神经网络"></a>回顾：循环神经网络</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/58/image-20231210191803170.png" alt="image-20231210191803170"></p><ul><li>更新隐藏状态：$\mathbf{h}_t = \phi(\mathbf{W}_{hh}\mathbf{h}_{t - 1} + \mathbf{W}_{hx}\mathbf{x}_{t - 1} + \mathbf{b}_h)$</li><li>输出：$\mathbf{o}_t = \phi(\mathbf{W}_{ho}\mathbf{h}_t + \mathbf{b}_o)$</li></ul><p>那么如何得到更多的非线性呢？</p><h2 id="更深"><a href="#更深" class="headerlink" title="更深"></a>更深</h2><ul><li>浅RNN<ul><li>输入</li><li>隐层</li><li>输出</li></ul></li><li>深RNN<ul><li>输入</li><li>隐层</li><li>隐层</li><li>…</li><li>输出</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/58/image-20231210192355811.png" alt="image-20231210192355811"></p><script type="math/tex; mode=display">\begin{aligned}\mathbf{H}_t^1 &= f_1(\mathbf{H}_{t - 1}^1, \mathbf{X}_t) \\&\cdots \\\mathbf{H}_t^j &= f_j(\mathbf{H}_{t - 1}^j, \mathbf{H}_t^{j - 1}) \\&\cdots \\\mathbf{O}_t &= g(\mathbf{H}_t^L)\end{aligned}</script><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>深度循环神经网络使用多个隐藏层来获得更多的非线性</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>通过<code>num_layers</code>的值来设定隐藏层数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, num_layers = <span class="built_in">len</span>(vocab), <span class="number">256</span>, <span class="number">2</span></span><br><span class="line">num_inputs = vocab_size</span><br><span class="line">device = d2l.try_gpu()</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">2</span></span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity 1.0, 99460.3 tokens/sec on cuda:0</span><br><span class="line">time travelleryou can show black is white by argument said filby</span><br><span class="line">traveller with a slight accession ofcheerfulness really thi</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/58/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 58 深层循环神经网络。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 57 长短期记忆网络（LSTM）</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/57/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/57/</id>
    <published>2023-12-10T10:00:00.000Z</published>
    <updated>2023-12-10T11:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="57-长短期记忆网络（LSTM）"><a href="#57-长短期记忆网络（LSTM）" class="headerlink" title="57 长短期记忆网络（LSTM）"></a>57 长短期记忆网络（LSTM）</h1><h2 id="长短期记忆网络"><a href="#长短期记忆网络" class="headerlink" title="长短期记忆网络"></a>长短期记忆网络</h2><ul><li>忘记门：将值朝0减少</li><li>输入门：决定是不是忽略掉输入数据</li><li>输出门：决定是不是使用隐状态</li></ul><h2 id="门"><a href="#门" class="headerlink" title="门"></a>门</h2><script type="math/tex; mode=display">\begin{aligned}\mathbf{I}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xi} + \mathbf{H}_{t - 1}\mathbf{W}_{hi} + \mathbf{b}_i) \\\mathbf{F}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xf} + \mathbf{H}_{t - 1}\mathbf{W}_{hf} + \mathbf{b}_f) \\\mathbf{O}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xo} + \mathbf{H}_{t - 1}\mathbf{W}_{ho} + \mathbf{b}_o)\end{aligned}</script><p><img src="https://img.karltan.com/notes-out-class/d2l/57/image-20231210183951149.png" alt="image-20231210183951149"></p><ul><li>忘记门：将值朝0减少</li><li>输入门：决定是不是忽略掉输入数据</li><li>输出门：决定是不是使用隐状态</li></ul><h2 id="候选记忆单元"><a href="#候选记忆单元" class="headerlink" title="候选记忆单元"></a>候选记忆单元</h2><script type="math/tex; mode=display">\tilde{\mathbf{C}}_t = \tanh(\mathbf{X}_t\mathbf{W}_{xc} + \mathbf{H}_{t - 1}\mathbf{W}_{hc} + \mathbf{b}_c)</script><p><img src="https://img.karltan.com/notes-out-class/d2l/57/image-20231210184242309.png" alt="image-20231210184242309"></p><h2 id="记忆单元"><a href="#记忆单元" class="headerlink" title="记忆单元"></a>记忆单元</h2><script type="math/tex; mode=display">\mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t - 1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t</script><p><img src="https://img.karltan.com/notes-out-class/d2l/57/image-20231210184538148.png" alt="image-20231210184538148"></p><h2 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h2><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)</script><p><img src="https://img.karltan.com/notes-out-class/d2l/57/image-20231210184828722.png" alt="image-20231210184828722"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><script type="math/tex; mode=display">\begin{aligned}\mathbf{I}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xi} + \mathbf{H}_{t - 1}\mathbf{W}_{hi} + \mathbf{b}_i) \\\mathbf{F}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xf} + \mathbf{H}_{t - 1}\mathbf{W}_{hf} + \mathbf{b}_f) \\\mathbf{O}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xo} + \mathbf{H}_{t - 1}\mathbf{W}_{ho} + \mathbf{b}_o) \\\tilde{\mathbf{C}}_t &= \tanh(\mathbf{X}_t\mathbf{W}_{xc} + \mathbf{H}_{t - 1}\mathbf{W}_{hc} + \mathbf{b}_c) \\\mathbf{C}_t &= \mathbf{F}_t \odot \mathbf{C}_{t - 1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t \\\mathbf{H}_t &= \mathbf{O}_t \odot \tanh(\mathbf{C}_t)\end{aligned}</script><p><img src="https://img.karltan.com/notes-out-class/d2l/57/image-20231210184828722.png" alt="image-20231210184828722"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_lstm_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device)*<span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xi, W_hi, b_i = three()</span><br><span class="line">    W_xf, W_hf, b_f = three()</span><br><span class="line">    W_xo, W_ho, b_o = three()</span><br><span class="line">    W_xc, W_hc, b_c = three()</span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    params = [</span><br><span class="line">        W_xi, W_hi, b_i,</span><br><span class="line">        W_xf, W_hf, b_f,</span><br><span class="line">        W_xo, W_ho, b_o,</span><br><span class="line">        W_xc, W_hc, b_c,</span><br><span class="line">        W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><p>初始化函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_lstm_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),</span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure><p>实际模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lstm</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    [W_xi, W_hi, b_i,</span><br><span class="line">     W_xf, W_hf, b_f,</span><br><span class="line">     W_xo, W_ho, b_o,</span><br><span class="line">     W_xc, W_hc, b_c,</span><br><span class="line">     W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * torch.tanh(C)</span><br><span class="line">        Y = (H @ W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H, C)</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_lstm_params,</span><br><span class="line">                            init_lstm_state, lstm)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity 1.1, 12155.1 tokens/sec on cuda:0</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/57/output1.svg" alt="output1"></p><p>简洁实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">lstm_layer = nn.LSTM(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity 1.0, 149132.6 tokens/sec on cuda:0</span><br><span class="line">time traveller for so it will be convenient to speak of himwas e</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/57/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 57 长短期记忆网络（LSTM）。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 56 门控循环单元（GRU）</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/56/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/56/</id>
    <published>2023-12-10T08:00:00.000Z</published>
    <updated>2023-12-10T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="56-门控循环单元（GRU）"><a href="#56-门控循环单元（GRU）" class="headerlink" title="56 门控循环单元（GRU）"></a>56 门控循环单元（GRU）</h1><h2 id="关注一个序列"><a href="#关注一个序列" class="headerlink" title="关注一个序列"></a>关注一个序列</h2><ul><li>不是每个观察值都是同等重要</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/56/image-20231210163429337.png" alt="image-20231210163429337"></p><ul><li>想只记住相关的观察需要：<ul><li>能关注的机制（更新门）</li><li>能遗忘的机制（重置门）</li></ul></li></ul><h2 id="门"><a href="#门" class="headerlink" title="门"></a>门</h2><script type="math/tex; mode=display">\begin{aligned}\mathbf{R}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xr} + \mathbf{H}_{t - 1}\mathbf{W}_{hr} + \mathbf{b}_r), \\\mathbf{Z}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xz} + \mathbf{H}_{t - 1}\mathbf{W}_{hz} + \mathbf{b}_z)\end{aligned}</script><p><img src="https://img.karltan.com/notes-out-class/d2l/56/image-20231210163950482.png" alt="image-20231210163950482"></p><h2 id="候选隐状态"><a href="#候选隐状态" class="headerlink" title="候选隐状态"></a>候选隐状态</h2><script type="math/tex; mode=display">\tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t\mathbf{W}_{xh} + (\mathbf{R}_t \odot \mathbf{H}_{t - 1})\mathbf{W}_{hh} + \mathbf{b}_h)</script><blockquote><p>注：$\odot$是按元素乘法。</p></blockquote><p><img src="https://img.karltan.com/notes-out-class/d2l/56/image-20231210164504178.png" alt="image-20231210164504178"></p><h2 id="隐状态"><a href="#隐状态" class="headerlink" title="隐状态"></a>隐状态</h2><script type="math/tex; mode=display">\mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t - 1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t</script><p><img src="https://img.karltan.com/notes-out-class/d2l/56/image-20231210165401558.png" alt="image-20231210165401558"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><script type="math/tex; mode=display">\begin{aligned}\mathbf{R}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xr} + \mathbf{H}_{t - 1}\mathbf{W}_{hr} + \mathbf{b}_r), \\\mathbf{Z}_t &= \sigma(\mathbf{X}_t\mathbf{W}_{xz} + \mathbf{H}_{t - 1}\mathbf{W}_{hz} + \mathbf{b}_z) \\\tilde{\mathbf{H}}_t &= \tanh(\mathbf{X}_t\mathbf{W}_{xh} + (\mathbf{R}_t \odot \mathbf{H}_{t - 1})\mathbf{W}_{hh} + \mathbf{b}_h) \\\mathbf{H}_t &= \mathbf{Z}_t \odot \mathbf{H}_{t - 1} + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t\end{aligned}</script><p><img src="https://img.karltan.com/notes-out-class/d2l/56/image-20231210165401558.png" alt="image-20231210165401558"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">three</span>():</span><br><span class="line">        <span class="keyword">return</span> (normal((num_inputs, num_hiddens)),</span><br><span class="line">                normal((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.zeros(num_hiddens, device=device))</span><br><span class="line"></span><br><span class="line">    W_xz, W_hz, b_z = three()</span><br><span class="line">    W_xr, W_hr, b_r = three()</span><br><span class="line">    W_xh, W_hh, b_h = three()</span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><p>定义隐藏状态的初始化函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_gru_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),)</span><br></pre></td></tr></table></figure><p>定义门控循环单元模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gru</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = H @ W_hq + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><blockquote><p>注：<code>R * H</code>就是$\odot$。</p></blockquote><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocab_size, num_hiddens, device = <span class="built_in">len</span>(vocab), <span class="number">256</span>, d2l.try_gpu()</span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">model = d2l.RNNModelScratch(<span class="built_in">len</span>(vocab), num_hiddens, device, get_params,</span><br><span class="line">                            init_gru_state, gru)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity 1.1, 12881.6 tokens/sec on cuda:0</span><br><span class="line">time travelleryou can show black is white by argument said filby</span><br><span class="line">travelleryou can show black is white by argument said filby</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/56/output1.svg" alt="output1"></p><p>简洁实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = vocab_size</span><br><span class="line">gru_layer = nn.GRU(num_inputs, num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, <span class="built_in">len</span>(vocab))</span><br><span class="line">model = model.to(device)</span><br><span class="line">d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity 1.0, 123492.8 tokens/sec on cuda:0</span><br><span class="line">time traveller with a slight accession ofcheerfulness really thi</span><br><span class="line">traveller with a slight accession ofcheerfulness really thi</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/56/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 56 门控循环单元（GRU）。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 55 循环神经网络 RNN 的实现</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/55/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/55/</id>
    <published>2023-12-09T12:00:00.000Z</published>
    <updated>2023-12-09T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="55-循环神经网络-RNN-的实现"><a href="#55-循环神经网络-RNN-的实现" class="headerlink" title="55 循环神经网络 RNN 的实现"></a>55 循环神经网络 RNN 的实现</h1><h2 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>独热编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.one_hot(torch.tensor([<span class="number">0</span>, <span class="number">2</span>]), <span class="built_in">len</span>(vocab))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0],</span><br><span class="line">        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span><br><span class="line">         0, 0, 0, 0]])</span><br></pre></td></tr></table></figure><p>小批量数据形状是（批量大小，时间步数）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">10</span>).reshape((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">F.one_hot(X.T, <span class="number">28</span>).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([5, 2, 28])</span><br></pre></td></tr></table></figure><p>初始化循环神经网络模型的模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><p>一个<code>init_rnn_state</code>函数在初始化时返回隐藏状态：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device),)</span><br></pre></td></tr></table></figure><p>下面的<code>rnn</code>函数定义了如何在一个时间步内计算隐藏状态和输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><p>创建一个类来包装这些函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span><br><span class="line"><span class="params">                 get_params, init_state, forward_fn</span>):</span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> self.forward_fn(X, state, self.params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure><p>检查输出是否具有正确的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">512</span></span><br><span class="line">net = RNNModelScratch(</span><br><span class="line">    <span class="built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(),</span><br><span class="line">    get_params, init_rnn_state, rnn)</span><br><span class="line">state = net.begin_state(X.shape[<span class="number">0</span>], d2l.try_gpu())</span><br><span class="line">Y, new_state = net(X.to(d2l.try_gpu()), state)</span><br><span class="line">Y.shape, <span class="built_in">len</span>(new_state), new_state[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([10, 28]), 1, torch.Size([2, 512]))</span><br></pre></td></tr></table></figure><p>首先定义预测函数来生成<code>prefix</code>之后的新字符：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch8</span>(<span class="params">prefix, num_preds, net, vocab, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在prefix后面生成新字符&quot;&quot;&quot;</span></span><br><span class="line">    state = net.begin_state(batch_size=<span class="number">1</span>, device=device)</span><br><span class="line">    outputs = [vocab[prefix[<span class="number">0</span>]]]</span><br><span class="line">    get_input = <span class="keyword">lambda</span>: torch.tensor([outputs[-<span class="number">1</span>]], device=device).reshape((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> prefix[<span class="number">1</span>:]:</span><br><span class="line">        _, state = net(get_input(), state)</span><br><span class="line">        outputs.append(vocab[y])</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_preds):</span><br><span class="line">        y, state = net(get_input(), state)</span><br><span class="line">        outputs.append(<span class="built_in">int</span>(y.argmax(dim=<span class="number">1</span>).reshape(<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join([vocab.idx_to_token[i] <span class="keyword">for</span> i <span class="keyword">in</span> outputs])</span><br><span class="line"></span><br><span class="line">predict_ch8(<span class="string">&#x27;time traveller &#x27;</span>, <span class="number">10</span>, net, vocab, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;time traveller eyrnl eyrn&#x27;</span><br></pre></td></tr></table></figure><p>梯度裁剪：</p><script type="math/tex; mode=display">\mathbf{g} \gets \min\left(1, \frac{\theta}{\Vert\mathbf{g}\Vert}\right)\mathbf{g}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grad_clipping</span>(<span class="params">net, theta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;裁剪梯度&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        params = [p <span class="keyword">for</span> p <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        params = net.params</span><br><span class="line">    norm = torch.sqrt(<span class="built_in">sum</span>(torch.<span class="built_in">sum</span>((p.grad ** <span class="number">2</span>)) <span class="keyword">for</span> p <span class="keyword">in</span> params))</span><br><span class="line">    <span class="keyword">if</span> norm &gt; theta:</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param.grad[:] *= theta / norm</span><br></pre></td></tr></table></figure><p>定义一个函数在一个迭代周期内训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure><p>循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span><br><span class="line"><span class="params">              use_random_iter=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 词元/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br></pre></td></tr></table></figure><p>现在，我们训练循环神经网络模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.2, 39553.2 词元/秒 cuda:0</span><br><span class="line">time traveller ofth a slall ghtwe dimensional solid and similldi</span><br><span class="line">traveller ofth medinel is wercound for pome trame eisend of</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/55/output1-1.svg" alt="output1-1"></p><p>最后，让我们检查一下使用随机抽样方法的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),</span><br><span class="line">          use_random_iter=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">困惑度 1.4, 40634.1 词元/秒 cuda:0</span><br><span class="line">time traveller held in his hand was a glitteringmetallic framewo</span><br><span class="line">travellerit s against reason said filbywhai gode monne toiv</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/55/output1-2.svg" alt="output1-2"></p><h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><p>定义模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br></pre></td></tr></table></figure><p>使用张量来初始化隐藏状态：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line">state.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 32, 256])</span><br></pre></td></tr></table></figure><p>通过一个隐藏状态和一个输入，我们就可以用更新后的隐藏状态计算输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br><span class="line">Y.shape, state_new.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))</span><br></pre></td></tr></table></figure><p>我们为一个完整的循环神经网络模型定义了一个<code>RNNModel</code>类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="keyword">return</span> torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device),</span><br><span class="line">                    torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure><p>基于一个具有随机权重的模型进行预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = d2l.try_gpu()</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">d2l.predict_ch8(<span class="string">&#x27;time traveller&#x27;</span>, <span class="number">10</span>, net, vocab, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;time travellerzzzzzzzzzz&#x27;</span><br></pre></td></tr></table></figure><p>使用高级API训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">perplexity 1.3, 279841.0 tokens/sec on cuda:0</span><br><span class="line">time traveller come pand why ha l nat foln the oveme thbut you w</span><br><span class="line">traveller with a slight accession ofche was hexp burn tha g</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/55/output2-1.svg" alt="output2-1"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 55 循环神经网络 RNN 的实现。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 54 循环神经网络 RNN</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/54/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/54/</id>
    <published>2023-12-09T11:00:00.000Z</published>
    <updated>2023-12-09T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="54-循环神经网络-RNN"><a href="#54-循环神经网络-RNN" class="headerlink" title="54 循环神经网络 RNN"></a>54 循环神经网络 RNN</h1><h2 id="潜变量自回归模型"><a href="#潜变量自回归模型" class="headerlink" title="潜变量自回归模型"></a>潜变量自回归模型</h2><ul><li>使用潜变量$h_t$总结过去信息</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/54/image-20231209192158233.png" alt="image-20231209192158233"></p><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/54/image-20231209192806292.png" alt="image-20231209192806292"></p><ul><li>更新隐藏状态：$\mathbf{h}_t = \phi(\mathbf{W}_{hh}\mathbf{h}_{t - 1} + \mathbf{W}_{hx}\mathbf{x}_{t - 1} + \mathbf{b}_h)$</li><li>输出：$\mathbf{o}_t = \phi(\mathbf{W}_{ho}\mathbf{h}_t + \mathbf{b}_o)$</li></ul><blockquote><p>去掉$\mathbf{h}_t = \phi(\mathbf{W}_{hh}\mathbf{h}_{t - 1} + \mathbf{W}_{hx}\mathbf{x}_{t - 1} + \mathbf{b}_h)$中的$\mathbf{W}_{hh}\mathbf{h}_{t - 1}$后，$\mathbf{h}_t$会变成MLP。</p></blockquote><h2 id="使用循环神经网络的语言模型"><a href="#使用循环神经网络的语言模型" class="headerlink" title="使用循环神经网络的语言模型"></a>使用循环神经网络的语言模型</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/54/image-20231209193320650.png" alt="image-20231209193320650"></p><p>当前时刻的输出需要预测当前时刻的观察，但是输出发生在观察之前。</p><h2 id="困惑度（perplexity）"><a href="#困惑度（perplexity）" class="headerlink" title="困惑度（perplexity）"></a>困惑度（perplexity）</h2><ul><li><p>衡量一个语言模型的好坏可以用平均交叉熵</p><script type="math/tex; mode=display">  \pi = \frac{1}{n}\sum_{i = 1}^n -\log p(x_t | x_{t - 1}, \cdots)</script><p>  $p$是语言模型的预测概率，$x_t$是真实词</p></li><li><p>由于历史原因，NLP使用困惑度$\exp(\pi)$来衡量，是平均每次可能选项</p><ul><li>1表示完美，无穷大是最差情况</li></ul></li></ul><h2 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h2><ul><li><p>迭代中计算这$\mathrm{T}$个时间步上的梯度，在反向传播过程中产生长度为$O(\mathrm{T})$的矩阵乘法链，导致数值不稳定</p></li><li><p>梯度裁剪能有效预防梯度爆炸</p><ul><li>如果梯度长度超过$\theta$，那么拖影回长度$\theta$<script type="math/tex; mode=display">  \mathbf{g} \gets \min\left(1, \frac{\theta}{\Vert\mathbf{g}\Vert}\right)\mathbf{g}</script></li></ul></li></ul><h2 id="更多的应用RNNs"><a href="#更多的应用RNNs" class="headerlink" title="更多的应用RNNs"></a>更多的应用RNNs</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/54/image-20231209195506961.png" alt="image-20231209195506961"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>循环神经网络的输出取决于当下输入和前一时间的隐变量</li><li>应用到语言模型中时，循环神经网络根据当前词预测下一时刻词</li><li>通常使用困惑度来衡量语言模型的好坏</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 54 循环神经网络 RNN。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 53 语言模型</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/53/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/53/</id>
    <published>2023-12-09T09:00:00.000Z</published>
    <updated>2023-12-09T10:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="53-语言模型"><a href="#53-语言模型" class="headerlink" title="53 语言模型"></a>53 语言模型</h1><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><ul><li>给定文本序列$x_1, \cdots, x_\mathrm{T}$，语言模型的目标是估计联合概率$p(x_1, \cdots, x_\mathrm{T})$</li><li>它的应用包括<ul><li>做预训练模型（e.g. BERT，GPT-3）</li><li>生成文本，给定前面几个词，不断的使用$x_t \sim p(x_t | x_1, \cdots, x_{t - 1})$来生成后续文本</li><li>判断多个序列中哪个更常见，e.g. “to recognize speech” vs “to wreck a nice beach”（这两个句子的发音相近）</li></ul></li></ul><h2 id="使用计数来建模"><a href="#使用计数来建模" class="headerlink" title="使用计数来建模"></a>使用计数来建模</h2><ul><li><p>假设序列长为2，我们预测</p><script type="math/tex; mode=display">  p(x, x^\prime) = p(x)p(x^\prime | x) = \frac{n(x)}{n}\frac{n(x, x^\prime)}{n(x)}</script><p>  这里$n$是总词数，$n(x)$和$n(x, x^\prime)$是单个单词出现的次数和连续单词对出现的次数</p></li><li><p>很容易拓展到长为3的情况</p><script type="math/tex; mode=display">  p(x, x^\prime, x^{\prime\prime}) = p(x)p(x^\prime | x)p(x^{\prime\prime} | x, x^\prime) = \frac{n(x)}{n}\frac{n(x, x^\prime)}{n(x)}\frac{n(x, x^\prime, x^{\prime\prime})}{n(x, x^\prime)}</script></li></ul><h2 id="N元语法"><a href="#N元语法" class="headerlink" title="N元语法"></a>N元语法</h2><ul><li><p>当序列很长时，因为文本量不够大，很可能$n(x_1, \cdots, x_\mathrm{T}) \le 1$</p></li><li><p>使用马尔科夫假设可以缓解这个问题</p><ul><li><p>一元语法（$\tau = 0$）：</p><script type="math/tex; mode=display">  \begin{aligned}  p(x_1, x_2, x_3, x_4)  &= p(x_1)p(x_2)p(x_3)p(x_4) \\  &= \frac{n(x_1)}{n}\frac{n(x_2)}{n}\frac{n(x_3)}{n}\frac{n(x_4)}{n}  \end{aligned}</script></li><li><p>二元语法（$\tau = 1$）：</p><script type="math/tex; mode=display">  \begin{aligned}  p(x_1, x_2, x_3, x_4)  &= p(x_1)p(x_2 | x_1)p(x_3 | x_2)p(x_4 | x_3) \\  &= \frac{n(x_1)}{n}\frac{n(x_1, x_2)}{n(x_1)}\frac{n(x_2, x_3)}{n(x_2)}\frac{n(x_3, x_4)}{n(x_3)}  \end{aligned}</script></li><li><p>三元语法（$\tau = 2$）：</p><script type="math/tex; mode=display">  p(x_1, x_2, x_3, x_4) = p(x_1)p(x_2 | x_1)p(x_3 | x_1, x_2)p(x_4 | x_2, x_3)</script></li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>语言模型估计文本序列的联合概率</li><li>使用统计方法时常采用n元语法</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">tokens = d2l.tokenize(d2l.read_time_machine())</span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">vocab = d2l.Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;the&#x27;, 2261),</span><br><span class="line"> (&#x27;i&#x27;, 1267),</span><br><span class="line"> (&#x27;and&#x27;, 1245),</span><br><span class="line"> (&#x27;of&#x27;, 1155),</span><br><span class="line"> (&#x27;a&#x27;, 816),</span><br><span class="line"> (&#x27;to&#x27;, 695),</span><br><span class="line"> (&#x27;was&#x27;, 552),</span><br><span class="line"> (&#x27;in&#x27;, 541),</span><br><span class="line"> (&#x27;that&#x27;, 443),</span><br><span class="line"> (&#x27;my&#x27;, 440)]</span><br></pre></td></tr></table></figure><p>最流行的词被称为停用词，画出词频图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line">d2l.plot(freqs, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class="line">         xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/53/unigram-word-frequency-chart.svg" alt="unigram-word-frequency-chart"></p><p>其他的词元组合，比如二元语法，三元语法等等，又会如何呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line">bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class="line">bigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[((&#x27;of&#x27;, &#x27;the&#x27;), 309),</span><br><span class="line"> ((&#x27;in&#x27;, &#x27;the&#x27;), 169),</span><br><span class="line"> ((&#x27;i&#x27;, &#x27;had&#x27;), 130),</span><br><span class="line"> ((&#x27;i&#x27;, &#x27;was&#x27;), 112),</span><br><span class="line"> ((&#x27;and&#x27;, &#x27;the&#x27;), 109),</span><br><span class="line"> ((&#x27;the&#x27;, &#x27;time&#x27;), 102),</span><br><span class="line"> ((&#x27;it&#x27;, &#x27;was&#x27;), 99),</span><br><span class="line"> ((&#x27;to&#x27;, &#x27;the&#x27;), 85),</span><br><span class="line"> ((&#x27;as&#x27;, &#x27;i&#x27;), 78),</span><br><span class="line"> ((&#x27;of&#x27;, &#x27;a&#x27;), 73)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">trigram_tokens = [</span><br><span class="line">    triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class="line">trigram_vocab.token_freqs[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;), 59),</span><br><span class="line"> ((&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;), 30),</span><br><span class="line"> ((&#x27;the&#x27;, &#x27;medical&#x27;, &#x27;man&#x27;), 24),</span><br><span class="line"> ((&#x27;it&#x27;, &#x27;seemed&#x27;, &#x27;to&#x27;), 16),</span><br><span class="line"> ((&#x27;it&#x27;, &#x27;was&#x27;, &#x27;a&#x27;), 15),</span><br><span class="line"> ((&#x27;here&#x27;, &#x27;and&#x27;, &#x27;there&#x27;), 15),</span><br><span class="line"> ((&#x27;seemed&#x27;, &#x27;to&#x27;, &#x27;me&#x27;), 14),</span><br><span class="line"> ((&#x27;i&#x27;, &#x27;did&#x27;, &#x27;not&#x27;), 14),</span><br><span class="line"> ((&#x27;i&#x27;, &#x27;saw&#x27;, &#x27;the&#x27;), 13),</span><br><span class="line"> ((&#x27;i&#x27;, &#x27;began&#x27;, &#x27;to&#x27;), 13)]</span><br></pre></td></tr></table></figure><p>直观地对比三种模型中的标记频率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">trigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line">d2l.plot(</span><br><span class="line">    [freqs, bigram_freqs, trigram_freqs],</span><br><span class="line">    xlabel=<span class="string">&#x27;token: x&#x27;</span>,</span><br><span class="line">    ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>,</span><br><span class="line">    xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">    legend=[<span class="string">&#x27;unigram&#x27;</span>, <span class="string">&#x27;bigram&#x27;</span>, <span class="string">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/53/unigram-bigram-trigram-word-frequency-chart.svg" alt="unigram-bigram-trigram-word-frequency-chart"></p><p>随机地生成一个小批量数据的特征和标签以供读取。在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="keyword">return</span> corpus[pos: pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        initial_indices_per_batch = initial_indices[i: i + batch_size]</span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure><p>生成一个从0到34的序列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X: tensor([[ 7,  8,  9, 10, 11],</span><br><span class="line">        [27, 28, 29, 30, 31]]) </span><br><span class="line">Y: tensor([[ 8,  9, 10, 11, 12],</span><br><span class="line">        [28, 29, 30, 31, 32]])</span><br><span class="line">X: tensor([[12, 13, 14, 15, 16],</span><br><span class="line">        [17, 18, 19, 20, 21]]) </span><br><span class="line">Y: tensor([[13, 14, 15, 16, 17],</span><br><span class="line">        [18, 19, 20, 21, 22]])</span><br><span class="line">X: tensor([[22, 23, 24, 25, 26],</span><br><span class="line">        [ 2,  3,  4,  5,  6]]) </span><br><span class="line">Y: tensor([[23, 24, 25, 26, 27],</span><br><span class="line">        [ 3,  4,  5,  6,  7]])</span><br></pre></td></tr></table></figure><p>保证两个相邻的小批量中的子序列在原始序列上也是相邻的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列&quot;&quot;&quot;</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    Xs = torch.tensor(corpus[offset: offset + num_tokens])</span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>: offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        X = Xs[:, i: i + num_steps]</span><br><span class="line">        Y = Ys[:, i: i + num_steps]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure><p>读取每个小批量的子序列的特征<code>X</code>和标签<code>Y</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X:&#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X: tensor([[ 1,  2,  3,  4,  5],</span><br><span class="line">        [17, 18, 19, 20, 21]]) </span><br><span class="line">Y: tensor([[ 2,  3,  4,  5,  6],</span><br><span class="line">        [18, 19, 20, 21, 22]])</span><br><span class="line">X: tensor([[ 6,  7,  8,  9, 10],</span><br><span class="line">        [22, 23, 24, 25, 26]]) </span><br><span class="line">Y: tensor([[ 7,  8,  9, 10, 11],</span><br><span class="line">        [23, 24, 25, 26, 27]])</span><br><span class="line">X: tensor([[11, 12, 13, 14, 15],</span><br><span class="line">        [27, 28, 29, 30, 31]]) </span><br><span class="line">Y: tensor([[12, 13, 14, 15, 16],</span><br><span class="line">        [28, 29, 30, 31, 32]])</span><br></pre></td></tr></table></figure><p>将上面的两个采样函数包装到一个类中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SeqDataLoader</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class="line">        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class="line">        self.batch_size, self.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)</span><br></pre></td></tr></table></figure><p>最后，我们定义了一个函数<code>load_data_time_machine</code>，它同时返回数据迭代器和词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_time_machine</span>(<span class="params">batch_size, num_steps,</span></span><br><span class="line"><span class="params">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词表&quot;&quot;&quot;</span></span><br><span class="line">    data_iter = SeqDataLoader(</span><br><span class="line">        batch_size, num_steps, use_random_iter, max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 53 语言模型。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 52 文本预处理</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/52/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/52/</id>
    <published>2023-12-09T08:00:00.000Z</published>
    <updated>2023-12-09T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="52-文本预处理"><a href="#52-文本预处理" class="headerlink" title="52 文本预处理"></a>52 文本预处理</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>将数据集读取到由多条文本行组成的列表中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;# text lines: <span class="subst">&#123;<span class="built_in">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># text lines: 3221</span><br><span class="line">the time machine by h g wells</span><br><span class="line">twinkled and his usually pale face was flushed and animated the</span><br></pre></td></tr></table></figure><p>每个文本序列又被拆分成一个标记列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知词元类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[&#x27;i&#x27;]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;, &#x27;for&#x27;, &#x27;so&#x27;, &#x27;it&#x27;, &#x27;will&#x27;, &#x27;be&#x27;, &#x27;convenient&#x27;, &#x27;to&#x27;, &#x27;speak&#x27;, &#x27;of&#x27;, &#x27;him&#x27;]</span><br><span class="line">[&#x27;was&#x27;, &#x27;expounding&#x27;, &#x27;a&#x27;, &#x27;recondite&#x27;, &#x27;matter&#x27;, &#x27;to&#x27;, &#x27;us&#x27;, &#x27;his&#x27;, &#x27;grey&#x27;, &#x27;eyes&#x27;, &#x27;shone&#x27;, &#x27;and&#x27;]</span><br><span class="line">[&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;]</span><br></pre></td></tr></table></figure><p>构建一个字典，通常也叫做词汇表（vocabulary），用来将字符串类型的标记映射到从0开始的数字索引中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        self._token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                   reverse=<span class="literal">True</span>)</span><br><span class="line">        self.idx_to_token = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx_to_token)&#125;</span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> self._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unk</span>(<span class="params">self</span>):  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_freqs</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._token_freqs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure><p>构建词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;&lt;unk&gt;&#x27;, 0), (&#x27;the&#x27;, 1), (&#x27;i&#x27;, 2), (&#x27;and&#x27;, 3), (&#x27;of&#x27;, 4), (&#x27;a&#x27;, 5), (&#x27;to&#x27;, 6), (&#x27;was&#x27;, 7), (&#x27;in&#x27;, 8), (&#x27;that&#x27;, 9)]</span><br></pre></td></tr></table></figure><p>将每一条文本行转换成一个数字索引列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;words:&#x27;</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;indices:&#x27;</span>, vocab[tokens[i]])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">words: [&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;]</span><br><span class="line">indices: [1, 19, 50, 40, 2183, 2184, 400]</span><br><span class="line">words: [&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;]</span><br><span class="line">indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]</span><br></pre></td></tr></table></figure><p>将所有功能打包到<code>load_corpus_time_machine</code>函数中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(170580, 28)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 52 文本预处理。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 51 序列模型</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/51/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/51/</id>
    <published>2023-12-09T02:00:00.000Z</published>
    <updated>2023-12-09T03:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="51-序列模型"><a href="#51-序列模型" class="headerlink" title="51 序列模型"></a>51 序列模型</h1><h2 id="序列数据"><a href="#序列数据" class="headerlink" title="序列数据"></a>序列数据</h2><ul><li>实际中很多数据是有时序结构的</li><li>电影的评价随时间变化而变化<ul><li>拿奖后评分上升，直到奖项被忘记</li><li>看了很多好电影之后，人们的期望变高</li><li>季节性：贺岁片、暑期档</li><li>导演、演员的负面报道导致评分变低</li></ul></li></ul><h2 id="序列数据-更多例子"><a href="#序列数据-更多例子" class="headerlink" title="序列数据 - 更多例子"></a>序列数据 - 更多例子</h2><ul><li>音乐、语言、文本和视频都是连续的<ul><li>标题“狗咬人”远没有“人咬狗”那么令人惊讶</li></ul></li><li>大地震发生之后，很可能会有几次较小的余震</li><li>人的互动是连续的，从网上吵架可以看出</li><li>预测明天的股价要比填补昨天遗失的股价更困难</li></ul><h2 id="统计工具"><a href="#统计工具" class="headerlink" title="统计工具"></a>统计工具</h2><ul><li>在时间$t$观察到$x_t$，那么得到$\mathrm{T}$个不独立的随机变量$(x_1, \cdots, x_\mathrm{T}) \sim p(\mathbf{x})$</li><li>使用条件概率展开$p(a, b) = p(a)p(b | a) = p(b)p(a | b)$</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/51/image-20231209113201661.png" alt="image-20231209113201661"></p><h2 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h2><script type="math/tex; mode=display">p(\mathbf{x}) = p(x_1) \cdot p(x_2 | x_1) \cdot p(x_3 | x_1, x_2) \cdots p(x_\mathrm{T} | x_1, \cdots, x_{\mathrm{T} - 1})</script><p><img src="https://img.karltan.com/notes-out-class/d2l/51/image-20231209113525316.png" alt="image-20231209113525316"></p><ul><li>对条件概率建模</li></ul><script type="math/tex; mode=display">p(x_t | x_1, \cdots, x_{t - 1}) = p(x_t | f(x_1, \cdots, x_{t - 1}))</script><p>对见过的数据建模，也称自回归模型。</p><h2 id="方案A-马尔科夫假设"><a href="#方案A-马尔科夫假设" class="headerlink" title="方案A - 马尔科夫假设"></a>方案A - 马尔科夫假设</h2><script type="math/tex; mode=display">p(\mathbf{x}) = p(x_1) \cdot p(x_2 | x_1) \cdot p(x_3 | x_1, x_2) \cdot \dots p(x_\mathrm{T} | x_1, \cdots, x_{\mathrm{T} - 1})</script><p><img src="https://img.karltan.com/notes-out-class/d2l/51/image-20231209115021367.png" alt="image-20231209115021367"></p><ul><li>假设当前数据只跟$\tau$个过去数据点相关</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/51/image-20231209115042665.png" alt="image-20231209115042665"></p><script type="math/tex; mode=display">p(x_t | x_1, \cdots, x_{t - 1}) = p(x_t | x_{t - \tau}, \cdots, x_{t - 1}) = p(x_t | f(x_{t - \tau}, \cdots, x_{t - 1}))</script><p>例如在过去数据上训练一个MLP模型。</p><h2 id="方案B-潜变量模型"><a href="#方案B-潜变量模型" class="headerlink" title="方案B - 潜变量模型"></a>方案B - 潜变量模型</h2><script type="math/tex; mode=display">p(\mathbf{x}) = p(x_1) \cdot p(x_2 | x_1) \cdot p(x_3 | x_1, x_2) \cdot \dots p(x_\mathrm{T} | x_1, \cdots, x_{\mathrm{T} - 1})</script><p><img src="https://img.karltan.com/notes-out-class/d2l/51/image-20231209120453662.png" alt="image-20231209120453662"></p><ul><li>引入潜变量$h_t$来表示过去信息$h_t = f(x_1, \cdots, x_{t - 1})$<ul><li>这样$x_t = p(x_t | h_t)$</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/51/image-20231209120547632.png" alt="image-20231209120547632"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>时序模型中，当前数据跟之前观察到的数据相关</li><li>自回归模型使用自身过去数据来预测未来</li><li>马尔科夫模型假设当前只跟最近少数数据相关，从而简化模型</li><li>潜变量模型使用潜变量来概括历史信息</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>使用正弦函数和一些可加性噪声来生成序列数据，时间步为$1, 2, \cdots, 1000$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">T = <span class="number">1000</span></span><br><span class="line">time = torch.arange(<span class="number">1</span>, T + <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">x = torch.sin(<span class="number">0.01</span> * time) + torch.normal(<span class="number">0</span>, <span class="number">0.2</span>, (T,))</span><br><span class="line">d2l.plot(time, [x], <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">1000</span>], figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/51/data-generation.svg" alt="data-generation"></p><p>将数据映射为数据对$y_t = x_t$和$\mathbf{x}_t = [x_{t - \tau}, \cdots, x_{t - 1}]$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tau = <span class="number">4</span></span><br><span class="line">features = torch.zeros((T - tau, tau))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: T - tau + i]</span><br><span class="line">labels = x[tau:].reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">batch_size, n_train = <span class="number">16</span>, <span class="number">600</span></span><br><span class="line">train_iter = d2l.load_array((features[:n_train], labels[:n_train]),</span><br><span class="line">                            batch_size, is_train=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>使用一个相当简单的结构，只是一个拥有两个全连接层的多层感知机：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, loss, epochs, lr</span>):</span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;loss: <span class="subst">&#123;d2l.evaluate_loss(net, train_iter, loss):f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">net = get_net()</span><br><span class="line">train(net, train_iter, loss, <span class="number">5</span>, <span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss: 0.068829</span><br><span class="line">epoch 2, loss: 0.062592</span><br><span class="line">epoch 3, loss: 0.058648</span><br><span class="line">epoch 4, loss: 0.059867</span><br><span class="line">epoch 5, loss: 0.055602</span><br></pre></td></tr></table></figure><p>模型预测下一个时间步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">onestep_preds = net(features)</span><br><span class="line">d2l.plot(</span><br><span class="line">    [time, time[tau:]],</span><br><span class="line">    [x.detach().numpy(), onestep_preds.detach().numpy()],</span><br><span class="line">    <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">    legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>],</span><br><span class="line">    xlim=[<span class="number">1</span>, <span class="number">1000</span>],</span><br><span class="line">    figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/51/one-step-ahead-prediction.svg" alt="one-step-ahead-prediction"></p><p>进行多步预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">multistep_preds = torch.zeros(T)</span><br><span class="line">multistep_preds[: n_train + tau] = x[: n_train + tau]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_train + tau, T):</span><br><span class="line">    multistep_preds[i] = net(</span><br><span class="line">        multistep_preds[i - tau:i].reshape((<span class="number">1</span>, -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">d2l.plot(</span><br><span class="line">    [time, time[tau:], time[n_train + tau:]],</span><br><span class="line">    [x.detach().numpy(), onestep_preds.detach().numpy(), multistep_preds[n_train + tau:].detach().numpy()],</span><br><span class="line">    <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">    legend=[<span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;1-step preds&#x27;</span>, <span class="string">&#x27;multistep preds&#x27;</span>],</span><br><span class="line">    xlim=[<span class="number">1</span>, <span class="number">1000</span>],</span><br><span class="line">    figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/51/k-step-ahead-prediction.svg" alt="k-step-ahead-prediction"></p><p>更仔细地看一下$k$步预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">max_steps = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">features = torch.zeros((T - tau - max_steps + <span class="number">1</span>, tau + max_steps))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau):</span><br><span class="line">    features[:, i] = x[i: i + T - tau - max_steps + <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(tau, tau + max_steps):</span><br><span class="line">    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">steps = (<span class="number">1</span>, <span class="number">4</span>, <span class="number">16</span>, <span class="number">64</span>)</span><br><span class="line">d2l.plot(</span><br><span class="line">    [time[tau + i - <span class="number">1</span>: T - max_steps + i] <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">    [features[:, (tau + i - <span class="number">1</span>)].detach().numpy() <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">    <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;x&#x27;</span>,</span><br><span class="line">    legend=[<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>-step preds&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> steps],</span><br><span class="line">    xlim=[<span class="number">5</span>, <span class="number">1000</span>],</span><br><span class="line">    figsize=(<span class="number">6</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/51/detailed-k-step-ahead-prediction.svg" alt="detailed-k-step-ahead-prediction"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 51 序列模型。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 50 课程竞赛：牛仔行头检测</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/50/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/50/</id>
    <published>2023-12-09T01:00:00.000Z</published>
    <updated>2023-12-09T02:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="50-课程竞赛：牛仔行头检测"><a href="#50-课程竞赛：牛仔行头检测" class="headerlink" title="50 课程竞赛：牛仔行头检测"></a>50 课程竞赛：牛仔行头检测</h1><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><ul><li>检测牛仔夹克、墨镜、靴子、牛仔帽、腰带</li><li>6937张训练图片，12600标注框</li><li>数据使用MS-COCO格式，评测使用mAP<ul><li>均可直接调用pycocotools</li></ul></li><li>挑战：类别不平衡</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/50/image-20231209110757588.png" alt="image-20231209110757588"></p><h2 id="安排"><a href="#安排" class="headerlink" title="安排"></a>安排</h2><ul><li>Kaggle地址：<a href="https://www.kaggle.com/c/cowboyoutfits/">CowBoy Outfits Detection | Kaggle</a><ul><li>结果提交请用：<a href="https://competitions.codalab.org/competitions/33573">CodaLab - Competition</a></li></ul></li><li>取得前10名的队伍，并在Kaggle提交notebook将获取签名书</li><li>时间为三周：<ul><li>公榜：现在-8月6日晚12点</li><li>私榜：公榜结束-8月7日中午12点</li></ul></li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 50 课程竞赛：牛仔行头检测。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 49 样式迁移</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/49/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/49/</id>
    <published>2023-12-08T14:00:00.000Z</published>
    <updated>2023-12-08T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="49-样式迁移"><a href="#49-样式迁移" class="headerlink" title="49 样式迁移"></a>49 样式迁移</h1><h2 id="样式迁移"><a href="#样式迁移" class="headerlink" title="样式迁移"></a>样式迁移</h2><ul><li>将样式图片中的样式迁移到内容图片上，得到合成图片</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/49/image-20231208215617127.png" alt="image-20231208215617127"></p><h2 id="基于CNN的样式迁移"><a href="#基于CNN的样式迁移" class="headerlink" title="基于CNN的样式迁移"></a>基于CNN的样式迁移</h2><ul><li>奠基性工作：</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/49/image-20231208215809039.png" alt="image-20231208215809039"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>阅读内容和样式图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">content_img = d2l.Image.<span class="built_in">open</span>(<span class="string">&#x27;../img/rainier.jpg&#x27;</span>) <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">d2l.plt.imshow(content_img);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/49/rainier.svg" alt="rainier"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">style_img = d2l.Image.<span class="built_in">open</span>(<span class="string">&#x27;../img/autumn-oak.jpg&#x27;</span>) <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">d2l.plt.imshow(style_img);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/49/autumn-oak.svg" alt="autumn-oak"></p><p>预处理和后处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">rgb_mean = torch.tensor([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">rgb_std = torch.tensor([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">img, image_shape</span>):</span><br><span class="line">    transforms = torchvision.transforms.Compose([</span><br><span class="line">        torchvision.transforms.Resize(image_shape),</span><br><span class="line">        torchvision.transforms.ToTensor(),</span><br><span class="line">        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])</span><br><span class="line">    <span class="keyword">return</span> transforms(img).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">img</span>):</span><br><span class="line">    img = img[<span class="number">0</span>].to(rgb_std.device)</span><br><span class="line">    img = torch.clamp(img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) * rgb_std + rgb_mean, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> torchvision.transforms.ToPILImage()(img.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>抽取图像特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.vgg19(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">style_layers, content_layers = [<span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">19</span>, <span class="number">28</span>], [<span class="number">25</span>]</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    *[pretrained_net.features[i]</span><br><span class="line">      <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(content_layers + style_layers) + <span class="number">1</span>)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_features</span>(<span class="params">X, content_layers, style_layers</span>):</span><br><span class="line">    contents = []</span><br><span class="line">    styles = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(net)):</span><br><span class="line">        X = net[i](X)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> style_layers:</span><br><span class="line">            styles.append(X)</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> content_layers:</span><br><span class="line">            contents.append(X)</span><br><span class="line">    <span class="keyword">return</span> contents, styles</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_contents</span>(<span class="params">image_shape, device</span>):</span><br><span class="line">    content_X = preprocess(content_img, image_shape).to(device)</span><br><span class="line">    contents_Y, _ = extract_features(content_X, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> content_X, contents_Y</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_styles</span>(<span class="params">image_shape, device</span>):</span><br><span class="line">    style_X = preprocess(style_img, image_shape).to(device)</span><br><span class="line">    _, styles_Y = extract_features(style_X, content_layers, style_layers)</span><br><span class="line">    <span class="keyword">return</span> style_X, styles_Y</span><br></pre></td></tr></table></figure><p>定义损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">content_loss</span>(<span class="params">Y_hat, Y</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.square(Y_hat - Y.detach()).mean()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gram</span>(<span class="params">X</span>):</span><br><span class="line">    num_channels, n = X.shape[<span class="number">1</span>], X.numel() // X.shape[<span class="number">1</span>]</span><br><span class="line">    X = X.reshape((num_channels, n))</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, X.T) / (num_channels * n)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">style_loss</span>(<span class="params">Y_hat, gram_Y</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.square(gram(Y_hat) - gram_Y.detach()).mean()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tv_loss</span>(<span class="params">Y_hat</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * (torch.<span class="built_in">abs</span>(Y_hat[:, :, <span class="number">1</span>:, :] - Y_hat[:, :, :-<span class="number">1</span>, :]).mean() +</span><br><span class="line">                  torch.<span class="built_in">abs</span>(Y_hat[:, :, :, <span class="number">1</span>:] - Y_hat[:, :, :, :-<span class="number">1</span>]).mean())</span><br></pre></td></tr></table></figure><p>风格转移的损失函数是内容损失、风格损失和总变化损失的加权和：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">content_weight, style_weight, tv_weight = <span class="number">1</span>, <span class="number">1e3</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram</span>):</span><br><span class="line">    contents_l = [</span><br><span class="line">        content_loss(Y_hat, Y) * content_weight</span><br><span class="line">        <span class="keyword">for</span> Y_hat, Y <span class="keyword">in</span> <span class="built_in">zip</span>(contents_Y_hat, contents_Y)]</span><br><span class="line">    styles_l = [</span><br><span class="line">        style_loss(Y_hat, Y) * style_weight</span><br><span class="line">        <span class="keyword">for</span> Y_hat, Y <span class="keyword">in</span> <span class="built_in">zip</span>(styles_Y_hat, styles_Y_gram)]</span><br><span class="line">    tv_l = tv_loss(X) * tv_weight</span><br><span class="line">    l = <span class="built_in">sum</span>(<span class="number">10</span> * styles_l + contents_l + [tv_l])</span><br><span class="line">    <span class="keyword">return</span> contents_l, styles_l, tv_l, l</span><br></pre></td></tr></table></figure><p>初始化合成图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SynthesizedImage</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_shape, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(SynthesizedImage, self).__init__(**kwargs)</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(*img_shape))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.weight</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_inits</span>(<span class="params">X, device, lr, styles_Y</span>):</span><br><span class="line">    gen_img = SynthesizedImage(X.shape).to(device)</span><br><span class="line">    gen_img.weight.data.copy_(X.data)</span><br><span class="line">    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)</span><br><span class="line">    styles_Y_gram = [gram(Y) <span class="keyword">for</span> Y <span class="keyword">in</span> styles_Y]</span><br><span class="line">    <span class="keyword">return</span> gen_img(), styles_Y_gram, trainer</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch</span>):</span><br><span class="line">    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, <span class="number">0.8</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">10</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;content&#x27;</span>, <span class="string">&#x27;style&#x27;</span>, <span class="string">&#x27;TV&#x27;</span>],</span><br><span class="line">                            ncols=<span class="number">2</span>, figsize=(<span class="number">7</span>, <span class="number">2.5</span>))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        contents_Y_hat, styles_Y_hat = extract_features(</span><br><span class="line">            X, content_layers, style_layers)</span><br><span class="line">        contents_l, styles_l, tv_l, l = compute_loss(</span><br><span class="line">            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            animator.axes[<span class="number">1</span>].imshow(postprocess(X))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [<span class="built_in">float</span>(<span class="built_in">sum</span>(contents_l)),</span><br><span class="line">                                     <span class="built_in">float</span>(<span class="built_in">sum</span>(styles_l)), <span class="built_in">float</span>(tv_l)])</span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device, image_shape = d2l.try_gpu(), (<span class="number">300</span>, <span class="number">450</span>)</span><br><span class="line">net = net.to(device)</span><br><span class="line">content_X, contents_Y = get_contents(image_shape, device)</span><br><span class="line">_, styles_Y = get_styles(image_shape, device)</span><br><span class="line">output = train(content_X, contents_Y, styles_Y, device, <span class="number">0.3</span>, <span class="number">500</span>, <span class="number">50</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/49/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 49 样式迁移。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 48 全连接卷积神经网络 FCN</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/48/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/48/</id>
    <published>2023-12-08T13:00:00.000Z</published>
    <updated>2023-12-08T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="48-全连接卷积神经网络-FCN"><a href="#48-全连接卷积神经网络-FCN" class="headerlink" title="48 全连接卷积神经网络 FCN"></a>48 全连接卷积神经网络 FCN</h1><h2 id="全连接卷积神经网络（FCN）"><a href="#全连接卷积神经网络（FCN）" class="headerlink" title="全连接卷积神经网络（FCN）"></a>全连接卷积神经网络（FCN）</h2><ul><li>FCN是用深度神经网络来做语义分割的奠基性工作</li><li>它用转置卷积层来替换CNN最后的全连接层，从而可以实现每个像素的预测</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/48/image-20231208210157485.png" alt="image-20231208210157485"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(pretrained_net.children())[-<span class="number">3</span>:]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[Sequential(</span><br><span class="line">   (0): BasicBlock(</span><br><span class="line">     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)</span><br><span class="line">     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     (relu): ReLU(inplace=True)</span><br><span class="line">     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     (downsample): Sequential(</span><br><span class="line">       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)</span><br><span class="line">       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     )</span><br><span class="line">   )</span><br><span class="line">   (1): BasicBlock(</span><br><span class="line">     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">     (relu): ReLU(inplace=True)</span><br><span class="line">     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)</span><br><span class="line">     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</span><br><span class="line">   )</span><br><span class="line"> ),</span><br><span class="line"> AdaptiveAvgPool2d(output_size=(1, 1)),</span><br><span class="line"> Linear(in_features=512, out_features=1000, bias=True)]</span><br></pre></td></tr></table></figure><p>创建一个全卷积网络实例<code>net</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(*<span class="built_in">list</span>(pretrained_net.children())[:-<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">320</span>, <span class="number">480</span>))</span><br><span class="line">net(X).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 512, 10, 15])</span><br></pre></td></tr></table></figure><p>使用$1 \times 1$卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类），将<code>fmap</code>的高度和宽度增加32倍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">21</span></span><br><span class="line">net.add_module(<span class="string">&#x27;final_conv&#x27;</span>,</span><br><span class="line">               nn.Conv2d(<span class="number">512</span>, num_classes, kernel_size=<span class="number">1</span>))</span><br><span class="line">net.add_module(<span class="string">&#x27;transpose_conv&#x27;</span>,</span><br><span class="line">               nn.ConvTranspose2d(</span><br><span class="line">                   num_classes, num_classes,</span><br><span class="line">                   kernel_size=<span class="number">64</span>, padding=<span class="number">16</span>,</span><br><span class="line">                   stride=<span class="number">32</span>))</span><br></pre></td></tr></table></figure><p>初始化转置卷积层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bilinear_kernel</span>(<span class="params">in_channels, out_channels, kernel_size</span>):</span><br><span class="line">    factor = (kernel_size + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> kernel_size % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">        center = factor - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        center = factor - <span class="number">0.5</span></span><br><span class="line">    og = (torch.arange(kernel_size).reshape(-<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">          torch.arange(kernel_size).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">    filt = (<span class="number">1</span> - torch.<span class="built_in">abs</span>(og[<span class="number">0</span>] - center) / factor) * \</span><br><span class="line">           (<span class="number">1</span> - torch.<span class="built_in">abs</span>(og[<span class="number">1</span>] - center) / factor)</span><br><span class="line">    weight = torch.zeros((in_channels, out_channels,</span><br><span class="line">                          kernel_size, kernel_size))</span><br><span class="line">    weight[<span class="built_in">range</span>(in_channels), <span class="built_in">range</span>(out_channels), :, :] = filt</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure><p>双线性插值的上采样实验：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">conv_trans = nn.ConvTranspose2d(</span><br><span class="line">    <span class="number">3</span>, <span class="number">3</span>, kernel_size=<span class="number">4</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">conv_trans.weight.data.copy_(bilinear_kernel(<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>));</span><br><span class="line"></span><br><span class="line">img = torchvision.transforms.ToTensor()(</span><br><span class="line">    d2l.Image.<span class="built_in">open</span>(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>)) <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">X = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line">Y = conv_trans(X)</span><br><span class="line">out_img = Y[<span class="number">0</span>].permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).detach()</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input image shape:&#x27;</span>, img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).shape)</span><br><span class="line">d2l.plt.imshow(img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>));</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output image shape:&#x27;</span>, out_img.shape)</span><br><span class="line">d2l.plt.imshow(out_img);</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input image shape: torch.Size([561, 728, 3])</span><br><span class="line">output image shape: torch.Size([1122, 1456, 3])</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/48/bilinear-kernel-output.svg" alt="bilinear-kernel-output"></p><p>用双线性插值的上采样初始化转置卷积层，对于卷积层，我们使用Xavier初始化参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = bilinear_kernel(num_classes, num_classes, <span class="number">64</span>)</span><br><span class="line">net.transpose_conv.weight.data.copy_(W);</span><br></pre></td></tr></table></figure><p>读取数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size, crop_size = <span class="number">32</span>, (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 1114 examples</span><br><span class="line">read 1078 examples</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    <span class="keyword">return</span> F.cross_entropy(inputs, targets, reduction=<span class="string">&#x27;none&#x27;</span>).mean(<span class="number">1</span>).mean(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">num_epochs, lr, wd, devices = <span class="number">5</span>, <span class="number">0.001</span>, <span class="number">1e-3</span>, d2l.try_all_gpus()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)</span><br><span class="line">d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.416, train acc 0.869, test acc 0.855</span><br><span class="line">34.1 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/48/output.svg" alt="output"></p><p>预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">img</span>):</span><br><span class="line">    X = test_iter.dataset.normalize_image(img).unsqueeze(<span class="number">0</span>)</span><br><span class="line">    pred = net(X.to(devices[<span class="number">0</span>])).argmax(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> pred.reshape(pred.shape[<span class="number">1</span>], pred.shape[<span class="number">2</span>])</span><br></pre></td></tr></table></figure><p>可视化预测的类别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">label2image</span>(<span class="params">pred</span>):</span><br><span class="line">    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[<span class="number">0</span>])</span><br><span class="line">    X = pred.long()</span><br><span class="line">    <span class="keyword">return</span> colormap[X, :]</span><br><span class="line"></span><br><span class="line">voc_dir = d2l.download_extract(<span class="string">&#x27;voc2012&#x27;</span>, <span class="string">&#x27;VOCdevkit/VOC2012&#x27;</span>)</span><br><span class="line">test_images, test_labels = d2l.read_voc_images(voc_dir, <span class="literal">False</span>)</span><br><span class="line">n, imgs = <span class="number">4</span>, []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    crop_rect = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)</span><br><span class="line">    pred = label2image(predict(X))</span><br><span class="line">    imgs += [</span><br><span class="line">        X.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>),</span><br><span class="line">        pred.cpu(),</span><br><span class="line">        torchvision.transforms.functional.crop(</span><br><span class="line">            test_labels[i], *crop_rect).permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line">d2l.show_images(imgs[::<span class="number">3</span>] + imgs[<span class="number">1</span>::<span class="number">3</span>] + imgs[<span class="number">2</span>::<span class="number">3</span>], <span class="number">3</span>, n, scale=<span class="number">2</span>);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/48/predict.svg" alt="predict"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 48 全连接卷积神经网络 FCN。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 47.2 转置卷积是一种卷积</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/47_2/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/47_2/</id>
    <published>2023-12-08T12:00:00.000Z</published>
    <updated>2023-12-08T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="47-2-转置卷积是一种卷积"><a href="#47-2-转置卷积是一种卷积" class="headerlink" title="47.2 转置卷积是一种卷积"></a>47.2 转置卷积是一种卷积</h1><h2 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h2><ul><li>转置卷积是一种卷积<ul><li>转置卷积将输入和核进行了重新排列</li><li>和卷积一般是做下采样不同，转置卷积通常用作上采样</li><li>如果卷积将输入的形状从$(h, w)$变为$(h^\prime, w^\prime)$，那么在相同的超参数下，转置卷积会将输入的形状从$(h^\prime, w^\prime)$变为$(h, w)$</li></ul></li></ul><h2 id="重新排列输入和核"><a href="#重新排列输入和核" class="headerlink" title="重新排列输入和核"></a>重新排列输入和核</h2><ul><li>当填充为0，步幅为1时<ul><li>将输入填充$k - 1$层（$k$是核窗口）</li><li>将核矩阵上下、左右翻转</li><li>然后做正常卷积（填充0，步幅1）</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/47_2/image-20231208201038701.png" alt="image-20231208201038701"></p><ul><li>当填充为$p$，步幅为1时<ul><li>将输入填充$k - p - 1$层（$k$是核窗口）</li><li>将核矩阵上下、左右翻转</li><li>然后做正常卷积（填充0、步幅1）</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/47_2/image-20231208201312437.png" alt="image-20231208201312437"></p><ul><li>当填充为$p$，步幅为$s$时<ul><li>在行和列之间插入$s - 1$行或列</li><li>将输入填充$k - p - 1$层（$k$是核窗口）</li><li>将核矩阵上下、左右翻转</li><li>然后做正常卷积（填充0，步幅1）</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/47_2/image-20231208201707317.png" alt="image-20231208201707317"></p><h2 id="形状换算"><a href="#形状换算" class="headerlink" title="形状换算"></a>形状换算</h2><ul><li>输入高（宽）为$n$，核$k$，填充$p$，步幅$s$</li><li>转置卷积：$n^\prime = sn + k - 2p - s$<ul><li>卷积：$n^\prime = \left\lfloor\frac{n - k + 2p + s}{s}\right\rfloor \to n \ge sn^\prime + k - 2p - s$</li></ul></li><li>如果让高宽成倍增加，那么$k = 2p + s$</li></ul><h2 id="和反卷积的关系"><a href="#和反卷积的关系" class="headerlink" title="和反卷积的关系"></a>和反卷积的关系</h2><ul><li>数学上的反卷积（deconvolution）是指卷积的逆运算<ul><li>对于数学上的反卷积，如果$Y = \text{conv}(X, K)$，那么$X = \text{deconv}(Y, K)$</li></ul></li><li>反卷积很少用在深度学习中<ul><li>我们说的反卷积神经网络指用了转置卷积的神经网络</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>转置卷积是一种变化了输入和核的卷积，来得到上采样的目的</li><li>不等同于数学上的反卷积操作</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 47.2 转置卷积是一种卷积。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 47 转置卷积</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/47/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/47/</id>
    <published>2023-12-08T11:00:00.000Z</published>
    <updated>2023-12-08T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="47-转置卷积"><a href="#47-转置卷积" class="headerlink" title="47 转置卷积"></a>47 转置卷积</h1><h2 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h2><ul><li>卷积不会增大输入的高宽，通常要么不变、要么减半</li><li>转置卷积则可以用来增大输入高宽</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/47/image-20231208191116437.png" alt="image-20231208191116437"></p><script type="math/tex; mode=display">Y[i:i + h, j:j + w] += X[i, j] \cdot K</script><h2 id="为什么称之为“转置”"><a href="#为什么称之为“转置”" class="headerlink" title="为什么称之为“转置”"></a>为什么称之为“转置”</h2><ul><li>对于卷积$Y = X \star W$<ul><li>可以对$W$构造一个$V$，使得卷积等价于矩阵乘法$Y^\prime = VX^\prime$</li><li>这里$Y^\prime$，$X^\prime$是$Y$，$X$的向量版本</li></ul></li><li>转置卷积则等价于$Y^\prime = V^{\mathrm{T}}X^\prime$</li><li>如果卷积将输入从$(h, w)$变成了$(h^\prime, w^\prime)$<ul><li>同样超参数的转置卷积则将输入从$(h^\prime, w^\prime)$变成$(h, w)$</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>实现基本的转置卷积运算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trans_conv</span>(<span class="params">X, K</span>):</span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] + h - <span class="number">1</span>, X.shape[<span class="number">1</span>] + w - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i: i + h, j: j + w] += X[i, j] * K</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>验证上述实现输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">trans_conv(X, K)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  0.,  1.],</span><br><span class="line">        [ 0.,  4.,  6.],</span><br><span class="line">        [ 4., 12.,  9.]])</span><br></pre></td></tr></table></figure><p>使用高级API获得相同的结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X, K = X.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>), K.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 0.,  0.,  1.],</span><br><span class="line">          [ 0.,  4.,  6.],</span><br><span class="line">          [ 4., 12.,  9.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure><p>填充、步幅和多通道：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[4.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure><p>这里<code>padding=1</code>相当于最后再删除了一圈，可以对比上一个输出和这一个输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[0., 0., 0., 1.],</span><br><span class="line">          [0., 0., 2., 3.],</span><br><span class="line">          [0., 2., 0., 3.],</span><br><span class="line">          [4., 6., 6., 9.]]]], grad_fn=&lt;ConvolutionBackward0&gt;)</span><br></pre></td></tr></table></figure><p>增大<code>stride</code>会将输出变大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">10</span>, <span class="number">16</span>, <span class="number">16</span>))</span><br><span class="line">conv = nn.Conv2d(</span><br><span class="line">    <span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span>)</span><br><span class="line">tconv = nn.ConvTranspose2d(</span><br><span class="line">    <span class="number">20</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>, stride=<span class="number">3</span>)</span><br><span class="line">tconv(conv(X)).shape == X.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure><p>但是要注意的是，值不同了。</p><p>与矩阵变换的关系：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">9.0</span>).reshape(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">Y = d2l.corr2d(X, K)</span><br><span class="line">Y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[27., 37.],</span><br><span class="line">        [57., 67.]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">kernel2matrix</span>(<span class="params">K</span>):</span><br><span class="line">    k, W = torch.zeros(<span class="number">5</span>), torch.zeros((<span class="number">4</span>, <span class="number">9</span>))</span><br><span class="line">    k[:<span class="number">2</span>], k[<span class="number">3</span>:<span class="number">5</span>] = K[<span class="number">0</span>, :], K[<span class="number">1</span>, :]</span><br><span class="line">    W[<span class="number">0</span>, :<span class="number">5</span>], W[<span class="number">1</span>, <span class="number">1</span>:<span class="number">6</span>], W[<span class="number">2</span>, <span class="number">3</span>:<span class="number">8</span>], W[<span class="number">3</span>, <span class="number">4</span>:] = k, k, k, k</span><br><span class="line">    <span class="keyword">return</span> W</span><br><span class="line"></span><br><span class="line">W = kernel2matrix(K)</span><br><span class="line">W</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2., 0., 3., 4., 0., 0., 0., 0.],</span><br><span class="line">        [0., 1., 2., 0., 3., 4., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 1., 2., 0., 3., 4., 0.],</span><br><span class="line">        [0., 0., 0., 0., 1., 2., 0., 3., 4.]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y == torch.matmul(W, X.reshape(-<span class="number">1</span>)).reshape(<span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True],</span><br><span class="line">        [True, True]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = trans_conv(Y, K)</span><br><span class="line">Z == torch.matmul(W.T, Y.reshape(-<span class="number">1</span>)).reshape(<span class="number">3</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True, True],</span><br><span class="line">        [True, True, True],</span><br><span class="line">        [True, True, True]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 47 转置卷积。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 46 语义分割和数据集</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/46/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/46/</id>
    <published>2023-12-07T13:00:00.000Z</published>
    <updated>2023-12-08T04:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="46-语义分割和数据集"><a href="#46-语义分割和数据集" class="headerlink" title="46 语义分割和数据集"></a>46 语义分割和数据集</h1><h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h2><h3 id="语义分割-1"><a href="#语义分割-1" class="headerlink" title="语义分割"></a>语义分割</h3><ul><li>语义分割将图片中的每个像素分类到对应的类别</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/46/image-20231207213727526.png" alt="image-20231207213727526"></p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><h4 id="背景虚化"><a href="#背景虚化" class="headerlink" title="背景虚化"></a>背景虚化</h4><p><img src="https://img.karltan.com/notes-out-class/d2l/46/image-20231207214126664.png" alt="image-20231207214126664"></p><h4 id="路面分割"><a href="#路面分割" class="headerlink" title="路面分割"></a>路面分割</h4><p><img src="https://img.karltan.com/notes-out-class/d2l/46/semantic-segmentation.gif" alt="semantic-segmentation"></p><h3 id="与实例分割对比"><a href="#与实例分割对比" class="headerlink" title="与实例分割对比"></a>与实例分割对比</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/46/image-20231207215402381.png" alt="image-20231207215402381"></p><h2 id="语义分割数据集"><a href="#语义分割数据集" class="headerlink" title="语义分割数据集"></a>语义分割数据集</h2><p>最重要的语义分割数据集之一是<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) (ox.ac.uk)</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;voc2012&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;VOCtrainval_11-May-2012.tar&#x27;</span>,</span><br><span class="line">                           <span class="string">&#x27;4e443f8a2eca6b1dac8a6c57641b67dd40621a49&#x27;</span>)</span><br><span class="line"></span><br><span class="line">voc_dir = d2l.download_extract(<span class="string">&#x27;voc2012&#x27;</span>, <span class="string">&#x27;VOCdevkit/VOC2012&#x27;</span>)</span><br></pre></td></tr></table></figure><p>将所有输入的图像和标签读入内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_voc_images</span>(<span class="params">voc_dir, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取所有VOC图像并标注&quot;&quot;&quot;</span></span><br><span class="line">    txt_fname = os.path.join(voc_dir, <span class="string">&#x27;ImageSets&#x27;</span>, <span class="string">&#x27;Segmentation&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;train.txt&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;val.txt&#x27;</span>)</span><br><span class="line">    mode = torchvision.io.image.ImageReadMode.RGB</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(txt_fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        images = f.read().split()</span><br><span class="line">    features, labels = [], []</span><br><span class="line">    <span class="keyword">for</span> i, fname <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        features.append(</span><br><span class="line">            torchvision.io.read_image(</span><br><span class="line">                os.path.join(voc_dir, <span class="string">&#x27;JPEGImages&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.jpg&#x27;</span>)))</span><br><span class="line">        labels.append(</span><br><span class="line">            torchvision.io.read_image(</span><br><span class="line">                os.path.join(voc_dir, <span class="string">&#x27;SegmentationClass&#x27;</span> ,<span class="string">f&#x27;<span class="subst">&#123;fname&#125;</span>.png&#x27;</span>), mode))</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line">train_features, train_labels = read_voc_images(voc_dir, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>绘制前5个输入图像及其标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">5</span></span><br><span class="line">imgs = train_features[<span class="number">0</span>:n] + train_labels[<span class="number">0</span>:n]</span><br><span class="line">imgs = [img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">d2l.show_images(imgs, <span class="number">2</span>, n);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/46/dataset-sample.png" alt="dataset-sample"></p><p>列举RGB颜色值和类名：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">VOC_COLORMAP = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">128</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">64</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">192</span>, <span class="number">128</span>, <span class="number">0</span>], [<span class="number">64</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">192</span>, <span class="number">0</span>, <span class="number">128</span>], [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>],</span><br><span class="line">                [<span class="number">192</span>, <span class="number">128</span>, <span class="number">128</span>], [<span class="number">0</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">64</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">192</span>, <span class="number">0</span>], [<span class="number">128</span>, <span class="number">192</span>, <span class="number">0</span>],</span><br><span class="line">                [<span class="number">0</span>, <span class="number">64</span>, <span class="number">128</span>]]</span><br><span class="line"></span><br><span class="line">VOC_CLASSES = [<span class="string">&#x27;background&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>, <span class="string">&#x27;bicycle&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;boat&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;bottle&#x27;</span>, <span class="string">&#x27;bus&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>, <span class="string">&#x27;chair&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;cow&#x27;</span>, <span class="string">&#x27;diningtable&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;motorbike&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;person&#x27;</span>, <span class="string">&#x27;potted plant&#x27;</span>, <span class="string">&#x27;sheep&#x27;</span>, <span class="string">&#x27;sofa&#x27;</span>, <span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;tv/monitor&#x27;</span>]</span><br></pre></td></tr></table></figure><p>查找标签中每个像素的类索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_colormap2label</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建从RGB到VOC类别索引的映射&quot;&quot;&quot;</span></span><br><span class="line">    colormap2label = torch.zeros(<span class="number">256</span> ** <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">    <span class="keyword">for</span> i, colormap <span class="keyword">in</span> <span class="built_in">enumerate</span>(VOC_COLORMAP):</span><br><span class="line">        colormap2label[</span><br><span class="line">            (colormap[<span class="number">0</span>] * <span class="number">256</span> + colormap[<span class="number">1</span>]) * <span class="number">256</span> + colormap[<span class="number">2</span>]] = i</span><br><span class="line">    <span class="keyword">return</span> colormap2label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">voc_label_indices</span>(<span class="params">colormap, colormap2label</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将VOC标签中的RGB值映射到它们的类别索引&quot;&quot;&quot;</span></span><br><span class="line">    colormap = colormap.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy().astype(<span class="string">&#x27;int32&#x27;</span>)</span><br><span class="line">    idx = ((colormap[:, :, <span class="number">0</span>] * <span class="number">256</span> + colormap[:, :, <span class="number">1</span>]) * <span class="number">256</span></span><br><span class="line">           + colormap[:, :, <span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> colormap2label[idx]</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = voc_label_indices(train_labels[<span class="number">0</span>], voc_colormap2label())</span><br><span class="line">y[<span class="number">105</span>:<span class="number">115</span>, <span class="number">130</span>:<span class="number">140</span>], VOC_CLASSES[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],</span><br><span class="line">         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]),</span><br><span class="line"> &#x27;aeroplane&#x27;)</span><br></pre></td></tr></table></figure><p>使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">voc_rand_crop</span>(<span class="params">feature, label, height, width</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;随机裁剪特征和标签图像&quot;&quot;&quot;</span></span><br><span class="line">    rect = torchvision.transforms.RandomCrop.get_params(</span><br><span class="line">        feature, (height, width))</span><br><span class="line">    feature = torchvision.transforms.functional.crop(feature, *rect)</span><br><span class="line">    label = torchvision.transforms.functional.crop(label, *rect)</span><br><span class="line">    <span class="keyword">return</span> feature, label</span><br><span class="line"></span><br><span class="line">imgs = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    imgs += voc_rand_crop(train_features[<span class="number">0</span>], train_labels[<span class="number">0</span>], <span class="number">200</span>, <span class="number">300</span>)</span><br><span class="line"></span><br><span class="line">imgs = [img.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs]</span><br><span class="line">d2l.show_images(imgs[::<span class="number">2</span>] + imgs[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>, n);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/46/dataset-img-crop.png" alt="dataset-img-crop"></p><p>自定义语义分割数据集类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VOCSegDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载VOC数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train, crop_size, voc_dir</span>):</span><br><span class="line">        self.transform = torchvision.transforms.Normalize(</span><br><span class="line">            mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">        self.crop_size = crop_size</span><br><span class="line">        features, labels = read_voc_images(voc_dir, is_train=is_train)</span><br><span class="line">        self.features = [</span><br><span class="line">            self.normalize_image(feature)</span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> self.<span class="built_in">filter</span>(features)]</span><br><span class="line">        self.labels = self.<span class="built_in">filter</span>(labels)</span><br><span class="line">        self.colormap2label = voc_colormap2label()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features)) + <span class="string">&#x27; examples&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normalize_image</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="keyword">return</span> self.transform(img.<span class="built_in">float</span>() / <span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter</span>(<span class="params">self, imgs</span>):</span><br><span class="line">        <span class="keyword">return</span> [img <span class="keyword">for</span> img <span class="keyword">in</span> imgs <span class="keyword">if</span> (</span><br><span class="line">            img.shape[<span class="number">1</span>] &gt;= self.crop_size[<span class="number">0</span>] <span class="keyword">and</span></span><br><span class="line">            img.shape[<span class="number">2</span>] &gt;= self.crop_size[<span class="number">1</span>])]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],</span><br><span class="line">                                       *self.crop_size)</span><br><span class="line">        <span class="keyword">return</span> (feature, voc_label_indices(label, self.colormap2label))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br></pre></td></tr></table></figure><p>读取数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">crop_size = (<span class="number">320</span>, <span class="number">480</span>)</span><br><span class="line">voc_train = VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir)</span><br><span class="line">voc_test = VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 1114 examples</span><br><span class="line">read 1078 examples</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_iter = torch.utils.data.DataLoader(</span><br><span class="line">    voc_train, batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>,</span><br><span class="line">    num_workers=d2l.get_dataloader_workers())</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape)</span><br><span class="line">    <span class="built_in">print</span>(Y.shape)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([64, 3, 320, 480])</span><br><span class="line">torch.Size([64, 320, 480])</span><br></pre></td></tr></table></figure><p>整合所有组件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_voc</span>(<span class="params">batch_size, crop_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载VOC语义分割数据集&quot;&quot;&quot;</span></span><br><span class="line">    voc_dir = d2l.download_extract(<span class="string">&#x27;voc2012&#x27;</span>,</span><br><span class="line">                                   os.path.join(<span class="string">&#x27;VOCdevkit&#x27;</span>, <span class="string">&#x27;VOC2012&#x27;</span>))</span><br><span class="line">    num_workers = d2l.get_dataloader_workers()</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">True</span>, crop_size, voc_dir), batch_size,</span><br><span class="line">        shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(</span><br><span class="line">        VOCSegDataset(<span class="literal">False</span>, crop_size, voc_dir), batch_size,</span><br><span class="line">        drop_last=<span class="literal">True</span>, num_workers=num_workers)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 46 语义分割和数据集。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 45 SSD实现</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/45/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/45/</id>
    <published>2023-12-07T10:00:00.000Z</published>
    <updated>2023-12-07T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="45-SSD实现"><a href="#45-SSD实现" class="headerlink" title="45 SSD实现"></a>45 SSD实现</h1><h2 id="多尺度锚框"><a href="#多尺度锚框" class="headerlink" title="多尺度锚框"></a>多尺度锚框</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">img = d2l.plt.imread(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>) <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line">h, w</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(561, 728)</span><br></pre></td></tr></table></figure><p>在特征图（<code>fmap</code>）上生成锚框（<code>anchors</code>），每个单位（像素）作为锚框的中心：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">display_anchors</span>(<span class="params">fmap_w, fmap_h, s</span>):</span><br><span class="line">    d2l.set_figsize()</span><br><span class="line">    fmap = torch.zeros((<span class="number">1</span>, <span class="number">10</span>, fmap_h, fmap_w))</span><br><span class="line">    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">    bbox_scale = torch.tensor((w, h, w, h))</span><br><span class="line">    d2l.show_bboxes(d2l.plt.imshow(img).axes, anchors[<span class="number">0</span>] * bbox_scale)</span><br></pre></td></tr></table></figure><p>探测小目标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">4</span>, fmap_h=<span class="number">4</span>, s=[<span class="number">0.15</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/45/anchors-on-4x4fmap.svg" alt="anchors-on-4x4fmap"></p><p>将特征图的高度和宽度减小一半，然后使用较大的锚框来检测较大的目标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">2</span>, fmap_h=<span class="number">2</span>, s=[<span class="number">0.4</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/45/anchors-on-2x2fmap.svg" alt="anchors-on-2x2fmap"></p><p>将特征图的高度和宽度减小一般，然后将锚框的尺寸增加到0.8：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">display_anchors(fmap_w=<span class="number">1</span>, fmap_h=<span class="number">1</span>, s=[<span class="number">0.8</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/45/anchors-on-1x1fmap.svg" alt="anchors-on-1x1fmap"></p><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p>类别预测层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cls_predictor</span>(<span class="params">num_inputs, num_anchors, num_classes</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(num_inputs, num_anchors * (num_classes + <span class="number">1</span>),</span><br><span class="line">                     kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>边界框预测层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_predictor</span>(<span class="params">num_inputs, num_anchors</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(num_inputs, num_anchors * <span class="number">4</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>连接多尺度的预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x, block</span>):</span><br><span class="line">    <span class="keyword">return</span> block(x)</span><br><span class="line"></span><br><span class="line">Y1 = forward(torch.zeros((<span class="number">2</span>, <span class="number">8</span>, <span class="number">20</span>, <span class="number">20</span>)), cls_predictor(<span class="number">8</span>, <span class="number">5</span>, <span class="number">10</span>))</span><br><span class="line">Y2 = forward(torch.zeros((<span class="number">2</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>)), cls_predictor(<span class="number">16</span>, <span class="number">3</span>, <span class="number">10</span>))</span><br><span class="line">Y1.shape, Y2.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">flatten_pred</span>(<span class="params">pred</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.flatten(pred.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), start_dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">concat_preds</span>(<span class="params">preds</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.cat([flatten_pred(p) <span class="keyword">for</span> p <span class="keyword">in</span> preds], dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">concat_preds([Y1, Y2]).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 25300])</span><br></pre></td></tr></table></figure><p>高和宽减半块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">down_sample_blk</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">        blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.BatchNorm2d(out_channels))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    blk.append(nn.MaxPool2d(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line">forward(torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>, <span class="number">20</span>)), down_sample_blk(<span class="number">3</span>, <span class="number">10</span>)).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 10, 10, 10])</span><br></pre></td></tr></table></figure><p>基本网络块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">base_net</span>():</span><br><span class="line">    blk = []</span><br><span class="line">    num_filters = [<span class="number">3</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(num_filters) - <span class="number">1</span>):</span><br><span class="line">        blk.append(down_sample_blk(num_filters[i], num_filters[i+<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line">forward(torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)), base_net()).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 64, 32, 32])</span><br></pre></td></tr></table></figure><p>完整的单发多框检测模型由五个模块组成：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_blk</span>(<span class="params">i</span>):</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        blk = base_net()</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">        blk = down_sample_blk(<span class="number">64</span>, <span class="number">128</span>)</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">4</span>:</span><br><span class="line">        blk = nn.AdaptiveMaxPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        blk = down_sample_blk(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><p>为每个块定义前向运算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">blk_forward</span>(<span class="params">X, blk, size, ratio, cls_predictor, bbox_predictor</span>):</span><br><span class="line">    Y = blk(X)</span><br><span class="line">    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)</span><br><span class="line">    cls_preds = cls_predictor(Y)</span><br><span class="line">    bbox_preds = bbox_predictor(Y)</span><br><span class="line">    <span class="keyword">return</span> (Y, anchors, cls_preds, bbox_preds)</span><br></pre></td></tr></table></figure><p>超参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sizes = [[<span class="number">0.2</span>, <span class="number">0.272</span>],</span><br><span class="line">         [<span class="number">0.37</span>, <span class="number">0.447</span>],</span><br><span class="line">         [<span class="number">0.54</span>, <span class="number">0.619</span>],</span><br><span class="line">         [<span class="number">0.71</span>, <span class="number">0.79</span>],</span><br><span class="line">         [<span class="number">0.88</span>, <span class="number">0.961</span>]]</span><br><span class="line">ratios = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>]] * <span class="number">5</span></span><br><span class="line">num_anchors = <span class="built_in">len</span>(sizes[<span class="number">0</span>]) + <span class="built_in">len</span>(ratios[<span class="number">0</span>]) - <span class="number">1</span></span><br></pre></td></tr></table></figure><p>定义完整的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TinySSD</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TinySSD, self).__init__(**kwargs)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        idx_to_in_channels = [<span class="number">64</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="number">128</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            <span class="comment"># 即赋值语句self.blk_i=get_blk(i)</span></span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;blk_<span class="subst">&#123;i&#125;</span>&#x27;</span>, get_blk(i))</span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;cls_<span class="subst">&#123;i&#125;</span>&#x27;</span>, cls_predictor(idx_to_in_channels[i],</span><br><span class="line">                                                    num_anchors, num_classes))</span><br><span class="line">            <span class="built_in">setattr</span>(self, <span class="string">f&#x27;bbox_<span class="subst">&#123;i&#125;</span>&#x27;</span>, bbox_predictor(idx_to_in_channels[i],</span><br><span class="line">                                                      num_anchors))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        anchors, cls_preds, bbox_preds = [<span class="literal">None</span>] * <span class="number">5</span>, [<span class="literal">None</span>] * <span class="number">5</span>, [<span class="literal">None</span>] * <span class="number">5</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">            <span class="comment"># getattr(self,&#x27;blk_%d&#x27;%i)即访问self.blk_i</span></span><br><span class="line">            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(</span><br><span class="line">                X, <span class="built_in">getattr</span>(self, <span class="string">f&#x27;blk_<span class="subst">&#123;i&#125;</span>&#x27;</span>), sizes[i], ratios[i],</span><br><span class="line">                <span class="built_in">getattr</span>(self, <span class="string">f&#x27;cls_<span class="subst">&#123;i&#125;</span>&#x27;</span>), <span class="built_in">getattr</span>(self, <span class="string">f&#x27;bbox_<span class="subst">&#123;i&#125;</span>&#x27;</span>))</span><br><span class="line">        anchors = torch.cat(anchors, dim=<span class="number">1</span>)</span><br><span class="line">        cls_preds = concat_preds(cls_preds)</span><br><span class="line">        cls_preds = cls_preds.reshape(</span><br><span class="line">            cls_preds.shape[<span class="number">0</span>], -<span class="number">1</span>, self.num_classes + <span class="number">1</span>)</span><br><span class="line">        bbox_preds = concat_preds(bbox_preds)</span><br><span class="line">        <span class="keyword">return</span> anchors, cls_preds, bbox_preds</span><br></pre></td></tr></table></figure><p>创建一个模型实例，然后使用它执行前向计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = TinySSD(num_classes=<span class="number">1</span>)</span><br><span class="line">X = torch.zeros((<span class="number">32</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>))</span><br><span class="line">anchors, cls_preds, bbox_preds = net(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output anchors:&#x27;</span>, anchors.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output class preds:&#x27;</span>, cls_preds.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output bbox preds:&#x27;</span>, bbox_preds.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output anchors: torch.Size([1, 5444, 4])</span><br><span class="line">output class preds: torch.Size([32, 5444, 2])</span><br><span class="line">output bbox preds: torch.Size([32, 21776])</span><br></pre></td></tr></table></figure><p>读取香蕉检测数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">train_iter, _ = d2l.load_data_bananas(batch_size)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 1000 training examples</span><br><span class="line">read 100 validation examples</span><br></pre></td></tr></table></figure><p>初始化其参数并定义优化算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device, net = d2l.try_gpu(), TinySSD(num_classes=<span class="number">1</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.2</span>, weight_decay=<span class="number">5e-4</span>)</span><br></pre></td></tr></table></figure><p>定义损失函数和评价函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cls_loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">bbox_loss = nn.L1Loss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calc_loss</span>(<span class="params">cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks</span>):</span><br><span class="line">    batch_size, num_classes = cls_preds.shape[<span class="number">0</span>], cls_preds.shape[<span class="number">2</span>]</span><br><span class="line">    cls = cls_loss(cls_preds.reshape(-<span class="number">1</span>, num_classes),</span><br><span class="line">                   cls_labels.reshape(-<span class="number">1</span>)).reshape(batch_size, -<span class="number">1</span>).mean(dim=<span class="number">1</span>)</span><br><span class="line">    bbox = bbox_loss(bbox_preds * bbox_masks,</span><br><span class="line">                     bbox_labels * bbox_masks).mean(dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> cls + bbox</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cls_eval</span>(<span class="params">cls_preds, cls_labels</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>((cls_preds.argmax(dim=-<span class="number">1</span>).<span class="built_in">type</span>(cls_labels.dtype) == cls_labels).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_eval</span>(<span class="params">bbox_preds, bbox_labels, bbox_masks</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>((torch.<span class="built_in">abs</span>((bbox_labels - bbox_preds) * bbox_masks)).<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, timer = <span class="number">20</span>, d2l.Timer()</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                        legend=[<span class="string">&#x27;class error&#x27;</span>, <span class="string">&#x27;bbox mae&#x27;</span>])</span><br><span class="line">net = net.to(device)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">4</span>)</span><br><span class="line">    net.train()</span><br><span class="line">    <span class="keyword">for</span> features, target <span class="keyword">in</span> train_iter:</span><br><span class="line">        timer.start()</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        X, Y = features.to(device), target.to(device)</span><br><span class="line">        anchors, cls_preds, bbox_preds = net(X)</span><br><span class="line">        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)</span><br><span class="line">        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,</span><br><span class="line">                      bbox_masks)</span><br><span class="line">        l.mean().backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),</span><br><span class="line">                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),</span><br><span class="line">                   bbox_labels.numel())</span><br><span class="line">    cls_err, bbox_mae = <span class="number">1</span> - metric[<span class="number">0</span>] / metric[<span class="number">1</span>], metric[<span class="number">2</span>] / metric[<span class="number">3</span>]</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, (cls_err, bbox_mae))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;class err <span class="subst">&#123;cls_err:<span class="number">.2</span>e&#125;</span>, bbox mae <span class="subst">&#123;bbox_mae:<span class="number">.2</span>e&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">len</span>(train_iter.dataset) / timer.stop():<span class="number">.1</span>f&#125;</span> examples/sec on &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class err 3.24e-03, bbox mae 3.04e-03</span><br><span class="line">3666.4 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/45/output.svg" alt="output"></p><p>预测目标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = torchvision.io.read_image(<span class="string">&#x27;../img/banana.jpg&#x27;</span>).unsqueeze(<span class="number">0</span>).<span class="built_in">float</span>() <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">img = X.squeeze(<span class="number">0</span>).permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).long()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X</span>):</span><br><span class="line">    net.<span class="built_in">eval</span>()</span><br><span class="line">    anchors, cls_preds, bbox_preds = net(X.to(device))</span><br><span class="line">    cls_probs = F.softmax(cls_preds, dim=<span class="number">2</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)</span><br><span class="line">    idx = [i <span class="keyword">for</span> i, row <span class="keyword">in</span> <span class="built_in">enumerate</span>(output[<span class="number">0</span>]) <span class="keyword">if</span> row[<span class="number">0</span>] != -<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> output[<span class="number">0</span>, idx]</span><br><span class="line"></span><br><span class="line">output = predict(X)</span><br></pre></td></tr></table></figure><p>筛选所有置信度不低于0.9的边界框，作为最终输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">display</span>(<span class="params">img, output, threshold</span>):</span><br><span class="line">    d2l.set_figsize((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">    fig = d2l.plt.imshow(img)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> output:</span><br><span class="line">        score = <span class="built_in">float</span>(row[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> score &lt; threshold:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        h, w = img.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        bbox = [row[<span class="number">2</span>:<span class="number">6</span>] * torch.tensor((w, h, w, h), device=row.device)]</span><br><span class="line">        d2l.show_bboxes(fig.axes, bbox, <span class="string">&#x27;%.2f&#x27;</span> % score, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">display(img, output.cpu(), threshold=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/45/predict.svg" alt="predict"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 45 SSD实现。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 44 物体检测算法：R-CNN，SSD，YOLO</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/44/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/44/</id>
    <published>2023-12-07T04:00:00.000Z</published>
    <updated>2023-12-07T05:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="44-物体检测算法：R-CNN，SSD，YOLO"><a href="#44-物体检测算法：R-CNN，SSD，YOLO" class="headerlink" title="44 物体检测算法：R-CNN，SSD，YOLO"></a>44 物体检测算法：R-CNN，SSD，YOLO</h1><h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207123221890.png" alt="image-20231207123221890"></p><ul><li>使用启发式搜索算法来选择锚框</li><li>使用预训练模型来对每个锚框抽取特征</li><li>训练一个SVM来对类别分类</li><li>训练一个线性回归模型来预测边缘框偏移</li></ul><h3 id="兴趣区域（RoI）池化层"><a href="#兴趣区域（RoI）池化层" class="headerlink" title="兴趣区域（RoI）池化层"></a>兴趣区域（RoI）池化层</h3><ul><li>给定一个锚框，均匀分割成$n \times m$块，输出每块里的最大值</li><li>不管锚框多大，总是输出$nm$个值</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207123726368.png" alt="image-20231207123726368"></p><p>这里的2x2 RoI Pooling会将原矩阵$\begin{bmatrix}0 &amp; 1 &amp; 2 \\ 4 &amp; 5 &amp; 6 \\ 8 &amp; 9 &amp; 10\end{bmatrix}$分为$\begin{bmatrix}0 &amp; 1 \\ 4 &amp; 5\end{bmatrix}$，$\begin{bmatrix}2 \\ 6\end{bmatrix}$，$\begin{bmatrix}8 &amp; 9\end{bmatrix}$和$\begin{bmatrix}10\end{bmatrix}$四块，然后在每一块中找出最大值，最后的输出为$\begin{bmatrix}5 &amp; 6 \\ 9 &amp; 10\end{bmatrix}$。</p><h3 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h3><ul><li>使用CNN对图片抽取特征</li><li>使用RoI池化层对每个锚框生成固定长度特征</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207124333767.png" alt="image-20231207124333767"></p><p>基础的R-CNN算法是对每个锚框抽取特征，但是这样就导致了效率低下，而Fast R-CNN先对整张图片抽取特征（也只抽取这一次），然后基于原图上的锚框，在特征图中按照比例找出对应的锚框：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207130821578.png" alt="image-20231207130821578"></p><p>然后再对锚框进行RoI Pooling，假设$n = m = 2$，则每个锚框会变为一个$2 \times 2$的矩阵，再拉平，变成一个长为4的向量：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207131003547.png" alt="image-20231207131003547"></p><p>假设有100个锚框，在经过图中黄色部分的处理后，会变为一个$100 \times 4$的向量，最后进入全连接层。</p><h3 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h3><ul><li>使用一个区域提议网络来替代启发式搜索来获得更好的锚框</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207131322854.png" alt="image-20231207131322854"></p><h3 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h3><ul><li>如果有像素级别的标号，使用FCN来利用这些信息</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207132059772.png" alt="image-20231207132059772"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207132127321.png" alt="image-20231207132127321"></p><h3 id="GluonCV-Model-Zoo"><a href="#GluonCV-Model-Zoo" class="headerlink" title="GluonCV Model Zoo"></a>GluonCV Model Zoo</h3><p><a href="https://cv.gluon.ai/model_zoo/detection.html">Detection — gluoncv 0.11.0 documentation</a></p><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207132705540.png" alt="image-20231207132705540"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>R-CNN是最早、也是最有名的一类基于锚框和CNN的目标检测算法</li><li>Fast/Faster R-CNN持续提高性能</li><li>Faster R-CNN和Mask R-CNN是在追求高精度场景下的常用算法</li></ul><h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207133418388.png" alt="image-20231207133418388"></p><h3 id="生成锚框"><a href="#生成锚框" class="headerlink" title="生成锚框"></a>生成锚框</h3><ul><li><p>对每个像素，生成多个以它为中心的锚框</p></li><li><p>给定$n$个大小$s_1, \cdots, s_n$和$m$个高宽比，那么生成$n + m - 1$个锚框，其大小和高宽比分别为：</p><script type="math/tex; mode=display">  (s_1, r_1), (s_2, r_1), \cdots, (s_n, r_1), (s_1, r_2), \cdots, (s_1, r_m)</script></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207133649738.png" alt="image-20231207133649738"></p><h3 id="SSD模型"><a href="#SSD模型" class="headerlink" title="SSD模型"></a>SSD模型</h3><ul><li>一个基础网络来抽取特征，然后多个卷积层块来减半高宽</li><li>在每段都生成锚框<ul><li>底部段来拟合小物体（看到的范围小），顶部段来拟合大物体（看到的范围大）</li></ul></li><li>对每个锚框预测类别和边缘框</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207133919046.png" alt="image-20231207133919046"></p><h3 id="GluonCV-Model-Zoo-1"><a href="#GluonCV-Model-Zoo-1" class="headerlink" title="GluonCV Model Zoo"></a>GluonCV Model Zoo</h3><p><a href="https://cv.gluon.ai/model_zoo/detection.html">Detection — gluoncv 0.11.0 documentation</a></p><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207134537511.png" alt="image-20231207134537511"></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>SSD通过单神经网络来检测模型</li><li>以每个像素为中心，产生多个锚框</li><li>在多个段的输出上进行多尺度的检测</li></ul><h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207135107808.png" alt="image-20231207135107808"></p><h3 id="YOLO（你只看一次）"><a href="#YOLO（你只看一次）" class="headerlink" title="YOLO（你只看一次）"></a>YOLO（你只看一次）</h3><ul><li>SSD中锚框大量重叠，因此浪费了很多计算</li><li>YOLO将图片均匀分成$S \times S$个锚框</li><li>每个锚框预测$B$个边缘框</li><li>后续版本（V2，V3，V4…）有持续改进</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207135253699.png" alt="image-20231207135253699"></p><h3 id="GluonCV-Model-Zoo-2"><a href="#GluonCV-Model-Zoo-2" class="headerlink" title="GluonCV Model Zoo"></a>GluonCV Model Zoo</h3><p><a href="https://cv.gluon.ai/model_zoo/detection.html">Detection — gluoncv 0.11.0 documentation</a></p><p><img src="https://img.karltan.com/notes-out-class/d2l/44/image-20231207140320206.png" alt="image-20231207140320206"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 44 物体检测算法：R-CNN，SSD，YOLO。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 43 树叶分类竞赛技术总结</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/43/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/43/</id>
    <published>2023-12-06T13:00:00.000Z</published>
    <updated>2023-12-06T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="43-树叶分类竞赛技术总结"><a href="#43-树叶分类竞赛技术总结" class="headerlink" title="43 树叶分类竞赛技术总结"></a>43 树叶分类竞赛技术总结</h1><h2 id="比赛结果"><a href="#比赛结果" class="headerlink" title="比赛结果"></a>比赛结果</h2><ul><li><p>176类，18353训练样本</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/43/image-20231206211910882.png" alt="image-20231206211910882"></p></li><li><p>165支参赛队伍</p><ul><li>41支队伍精度 &gt; 98%（非常好）</li><li>83支队伍精度 &gt; 95%（够用）</li></ul></li></ul><h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><ul><li>16支队伍提供了代码<ul><li><a href="https://www.kaggle.com/c/classify-leaves/code">Classify Leaves | Kaggle</a></li></ul></li><li>私榜排名1-5，7-9，11-14，16-20将获得签名书<ul><li>请将你微信号邮件发给mli@amazon.com</li></ul></li><li>额外加上Neko Kiku<ul><li>因为很多人用了你的代码:)</li></ul></li></ul><h2 id="技术分析"><a href="#技术分析" class="headerlink" title="技术分析"></a>技术分析</h2><ul><li>相比于课程介绍的代码，同学们主要做了下面这些加强<ul><li>数据增强，在测试时多次使用稍弱的增强然后取平均</li><li>使用多个模型预测，最后结果加权平均<ul><li>有使用10种模型的，也有使用单一模型的</li></ul></li><li>训练算法和学习率</li><li>清理数据</li></ul></li></ul><h2 id="数据方面"><a href="#数据方面" class="headerlink" title="数据方面"></a>数据方面</h2><ul><li>有重复图片，可以手动去除</li><li>图片背景较多，而且树叶没有方向性，可以做更多增强<ul><li>随机旋转，更大的剪裁</li></ul></li><li>跨图片增强<ul><li>Mixup：随机叠加两张图片</li><li>CutMix：随机组合来自不同图片的块</li></ul></li></ul><h2 id="模型方面"><a href="#模型方面" class="headerlink" title="模型方面"></a>模型方面</h2><ul><li>模型多为ResNet变种<ul><li>DenseNet，ResNeXt，ResNeSt，…</li><li>EfficientNet</li></ul></li><li>优化算法多为Adam或其变种</li><li>学习率一般是Cosine或者训练不动时往下调</li></ul><h2 id="AutoGluon"><a href="#AutoGluon" class="headerlink" title="AutoGluon"></a>AutoGluon</h2><ul><li>15行代码，安装加训练花时100分钟<ul><li><a href="https://www.kaggle.com/code/zhreshold/autogluon-vision-0-96-with-15-lines/notebook">AutoGluon.vision: 0.96+ with 15 lines | Kaggle</a></li></ul></li><li>精度96%<ul><li>可以通过定制化提升精度</li><li>下一个版本将搜索更多的模型超参数</li><li>AG目前主要仍是关注工业界应用上，非比赛</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>提升精度思路：根据数据挑选增强，使用新模型、新优化算法，多个模型融合，测试时使用增强</li><li>数据相对简单，排名有相对随机性</li><li>在工业界应用中：<ul><li>少使用模型融合和测试时增强，计算代价过高</li><li>通常固定模型超参数，而将精力主要花在提升数据质量</li></ul></li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 43 树叶分类竞赛技术总结。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 42 锚框</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/42/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/42/</id>
    <published>2023-12-06T11:00:00.000Z</published>
    <updated>2023-12-06T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="42-锚框"><a href="#42-锚框" class="headerlink" title="42 锚框"></a>42 锚框</h1><h2 id="锚框"><a href="#锚框" class="headerlink" title="锚框"></a>锚框</h2><ul><li>一类目标检测算法是基于锚框<ul><li>提出多个被称为锚框的区域（边缘框）</li><li>预测每个锚框里是否含有关注的物体</li><li>如果是，预测从这个锚框到真实边缘框的偏移</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/42/image-20231206190735064.png" alt="image-20231206190735064"></p><h2 id="IoU-交并比"><a href="#IoU-交并比" class="headerlink" title="IoU - 交并比"></a>IoU - 交并比</h2><p>IoU - Intersection over Union</p><ul><li><p>IoU用来计算两个框之间的相似度</p><ul><li>0表示无重叠，1表示重合</li></ul></li><li><p>这是Jacquard指数的一个特殊情况</p><ul><li>给定两个集合A和B<script type="math/tex; mode=display">  J(A, B) = \frac{\left|A \cap B\right|}{\left|A \cup B\right|}</script></li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/42/image-20231206191528481.png" alt="image-20231206191528481"></p><h2 id="赋予锚框标号"><a href="#赋予锚框标号" class="headerlink" title="赋予锚框标号"></a>赋予锚框标号</h2><ul><li>每个锚框是一个训练样本</li><li>将每个锚框，要么标注成背景，要么与一个真实边缘框相关联</li><li>我们可能会生成大量的锚框<ul><li>这个导致大量的负类样本</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/42/image-20231206192237687.png" alt="image-20231206192237687"></p><h2 id="使用非极大值抑制（NMS）输出"><a href="#使用非极大值抑制（NMS）输出" class="headerlink" title="使用非极大值抑制（NMS）输出"></a>使用非极大值抑制（NMS）输出</h2><ul><li>每个锚框预测一个边缘框</li><li>NMS可以合并相似的预测<ul><li>选中非背景类的最大预测值</li><li>去掉所有其它和它IoU值大于$\theta$的预测</li><li>重复上述过程直到所有预测要么被选中，要么被去掉</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/42/image-20231206193025796.png" alt="image-20231206193025796"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>一类目标检测算法基于锚框来预测</li><li>首先生成锚框，并赋予标号，每个锚框作为一个样本进行训练</li><li>在预测时，使用NMS来去掉冗余的预测</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">torch.set_printoptions(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>锚框的宽度和高度分别是$ws\sqrt{r}$和$\frac{hs}{\sqrt{r}}$，其中：</p><ul><li>$w$是图像的宽</li><li>$h$是图像的高</li><li>$s$是缩放比，$s \in (0, 1]$</li><li>$r$是宽高比，$r &gt; 0$</li></ul><p>我们只考虑组合：</p><script type="math/tex; mode=display">(s_1, r_1), (s_1, r_2), \cdots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \cdots, (s_n, r_1)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_prior</span>(<span class="params">data, sizes, ratios</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成以每个像素为中心具有不同形状的锚框&quot;&quot;&quot;</span></span><br><span class="line">    in_height, in_width = data.shape[-<span class="number">2</span>:]</span><br><span class="line">    device, num_sizes, num_ratios = data.device, <span class="built_in">len</span>(sizes), <span class="built_in">len</span>(ratios)</span><br><span class="line">    boxes_per_pixel = (num_sizes + num_ratios - <span class="number">1</span>)</span><br><span class="line">    size_tensor = torch.tensor(sizes, device=device)</span><br><span class="line">    ratio_tensor = torch.tensor(ratios, device=device)</span><br><span class="line"></span><br><span class="line">    offset_h, offset_w = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line">    steps_h = <span class="number">1.0</span> / in_height</span><br><span class="line">    steps_w = <span class="number">1.0</span> / in_width</span><br><span class="line"></span><br><span class="line">    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h</span><br><span class="line">    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w</span><br><span class="line">    shift_y, shift_x = torch.meshgrid(center_h, center_w)</span><br><span class="line">    shift_y, shift_x = shift_y.reshape(-<span class="number">1</span>), shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] * torch.sqrt(ratio_tensor[<span class="number">1</span>:])))\</span><br><span class="line">                   * in_height / in_width  <span class="comment"># 处理矩形输入</span></span><br><span class="line">    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] / torch.sqrt(ratio_tensor[<span class="number">1</span>:])))</span><br><span class="line">    anchor_manipulations = torch.stack(</span><br><span class="line">        (-w, -h, w, h)).T.repeat(in_height * in_width, <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],</span><br><span class="line">                           dim=<span class="number">1</span>).repeat_interleave(boxes_per_pixel, dim=<span class="number">0</span>)</span><br><span class="line">    output = out_grid + anchor_manipulations</span><br><span class="line">    <span class="keyword">return</span> output.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>返回的锚框变量<code>Y</code>的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">img = d2l.plt.imread(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>) <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(h, w)</span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">3</span>, h, w))</span><br><span class="line">Y = multibox_prior(X, sizes=[<span class="number">0.75</span>, <span class="number">0.5</span>, <span class="number">0.25</span>], ratios=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0.5</span>])</span><br><span class="line">Y.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">561 728</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 2042040, 4])</span><br></pre></td></tr></table></figure><p>访问以$(250, 250)$为中心的第一个锚框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">boxes = Y.reshape(h, w, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">boxes[<span class="number">250</span>, <span class="number">250</span>, <span class="number">0</span>, :]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.06, 0.07, 0.63, 0.82])</span><br></pre></td></tr></table></figure><p>显示以图像中第一个像素为中心的所有锚框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_bboxes</span>(<span class="params">axes, bboxes, labels=<span class="literal">None</span>, colors=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;显示所有边界框&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_list</span>(<span class="params">obj, default_values=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> obj <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            obj = default_values</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(obj, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            obj = [obj]</span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line"></span><br><span class="line">    labels = _make_list(labels)</span><br><span class="line">    colors = _make_list(colors, [<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> i, bbox <span class="keyword">in</span> <span class="built_in">enumerate</span>(bboxes):</span><br><span class="line">        color = colors[i % <span class="built_in">len</span>(colors)]</span><br><span class="line">        rect = d2l.bbox_to_rect(bbox.detach().numpy(), color)</span><br><span class="line">        axes.add_patch(rect)</span><br><span class="line">        <span class="keyword">if</span> labels <span class="keyword">and</span> <span class="built_in">len</span>(labels) &gt; i:</span><br><span class="line">            text_color = <span class="string">&#x27;k&#x27;</span> <span class="keyword">if</span> color == <span class="string">&#x27;w&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;w&#x27;</span></span><br><span class="line">            axes.text(rect.xy[<span class="number">0</span>], rect.xy[<span class="number">1</span>], labels[i],</span><br><span class="line">                      va=<span class="string">&#x27;center&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, fontsize=<span class="number">9</span>, color=text_color,</span><br><span class="line">                      bbox=<span class="built_in">dict</span>(facecolor=color, lw=<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>以$(250, 250)$为中心的所有锚框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d2l.set_figsize()</span><br><span class="line">bbox_scale = torch.tensor((w, h, w, h))</span><br><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, boxes[<span class="number">250</span>, <span class="number">250</span>, :, :] * bbox_scale, [</span><br><span class="line">    <span class="string">&#x27;s=0.75, r=1&#x27;</span>, <span class="string">&#x27;s=0.5, r=1&#x27;</span>, <span class="string">&#x27;s=0.25, r=1&#x27;</span>, <span class="string">&#x27;s=0.75, r=2&#x27;</span>, <span class="string">&#x27;s=0.75, r=0.5&#x27;</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/42/anchors.svg" alt="anchors"></p><p>交并比（IoU）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">box_iou</span>(<span class="params">boxes1, boxes2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算两个锚框或边界框列表中成对的交并比&quot;&quot;&quot;</span></span><br><span class="line">    box_area = <span class="keyword">lambda</span> boxes: ((boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>]) *</span><br><span class="line">                              (boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]))</span><br><span class="line">    areas1 = box_area(boxes1)</span><br><span class="line">    areas2 = box_area(boxes2)</span><br><span class="line">    inter_upperlefts = torch.<span class="built_in">max</span>(boxes1[:, <span class="literal">None</span>, :<span class="number">2</span>], boxes2[:, :<span class="number">2</span>])</span><br><span class="line">    inter_lowerrights = torch.<span class="built_in">min</span>(boxes1[:, <span class="literal">None</span>, <span class="number">2</span>:], boxes2[:, <span class="number">2</span>:])</span><br><span class="line">    inters = (inter_lowerrights - inter_upperlefts).clamp(<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">    inter_areas = inters[:, :, <span class="number">0</span>] * inters[:, :, <span class="number">1</span>]</span><br><span class="line">    union_areas = areas1[:, <span class="literal">None</span>] + areas2 - inter_areas</span><br><span class="line">    <span class="keyword">return</span> inter_areas / union_areas</span><br></pre></td></tr></table></figure><p>将真实边界框分配给锚框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">assign_anchor_to_bbox</span>(<span class="params">ground_truth, anchors, device, iou_threshold=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将最接近的真实边界框分配给锚框&quot;&quot;&quot;</span></span><br><span class="line">    num_anchors, num_gt_boxes = anchors.shape[<span class="number">0</span>], ground_truth.shape[<span class="number">0</span>]</span><br><span class="line">    jaccard = box_iou(anchors, ground_truth)</span><br><span class="line">    anchors_bbox_map = torch.full((num_anchors,), -<span class="number">1</span>, dtype=torch.long,</span><br><span class="line">                                  device=device)</span><br><span class="line">    max_ious, indices = torch.<span class="built_in">max</span>(jaccard, dim=<span class="number">1</span>)</span><br><span class="line">    anc_i = torch.nonzero(max_ious &gt;= <span class="number">0.5</span>).reshape(-<span class="number">1</span>)</span><br><span class="line">    box_j = indices[max_ious &gt;= <span class="number">0.5</span>]</span><br><span class="line">    anchors_bbox_map[anc_i] = box_j</span><br><span class="line">    col_discard = torch.full((num_anchors,), -<span class="number">1</span>)</span><br><span class="line">    row_discard = torch.full((num_gt_boxes,), -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_gt_boxes):</span><br><span class="line">        max_idx = torch.argmax(jaccard)</span><br><span class="line">        box_idx = (max_idx % num_gt_boxes).long()</span><br><span class="line">        anc_idx = (max_idx / num_gt_boxes).long()</span><br><span class="line">        anchors_bbox_map[anc_idx] = box_idx</span><br><span class="line">        jaccard[:, box_idx] = col_discard</span><br><span class="line">        jaccard[anc_idx, :] = row_discard</span><br><span class="line">    <span class="keyword">return</span> anchors_bbox_map</span><br></pre></td></tr></table></figure><p>标记类和偏移：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">offset_boxes</span>(<span class="params">anchors, assigned_bb, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对锚框偏移量的转换&quot;&quot;&quot;</span></span><br><span class="line">    c_anc = d2l.box_corner_to_center(anchors)</span><br><span class="line">    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)</span><br><span class="line">    offset_xy = <span class="number">10</span> * (c_assigned_bb[:, :<span class="number">2</span>] - c_anc[:, :<span class="number">2</span>]) / c_anc[:, <span class="number">2</span>:]</span><br><span class="line">    offset_wh = <span class="number">5</span> * torch.log(eps + c_assigned_bb[:, <span class="number">2</span>:] / c_anc[:, <span class="number">2</span>:])</span><br><span class="line">    offset = torch.cat([offset_xy, offset_wh], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> offset</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_target</span>(<span class="params">anchors, labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用真实边界框标记锚框&quot;&quot;&quot;</span></span><br><span class="line">    batch_size, anchors = labels.shape[<span class="number">0</span>], anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    batch_offset, batch_mask, batch_class_labels = [], [], []</span><br><span class="line">    device, num_anchors = anchors.device, anchors.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        label = labels[i, :, :]</span><br><span class="line">        anchors_bbox_map = assign_anchor_to_bbox(</span><br><span class="line">            label[:, <span class="number">1</span>:], anchors, device)</span><br><span class="line">        bbox_mask = ((anchors_bbox_map &gt;= <span class="number">0</span>).<span class="built_in">float</span>().unsqueeze(-<span class="number">1</span>)).repeat(</span><br><span class="line">            <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        class_labels = torch.zeros(num_anchors, dtype=torch.long,</span><br><span class="line">                                   device=device)</span><br><span class="line">        assigned_bb = torch.zeros((num_anchors, <span class="number">4</span>), dtype=torch.float32,</span><br><span class="line">                                  device=device)</span><br><span class="line">        indices_true = torch.nonzero(anchors_bbox_map &gt;= <span class="number">0</span>)</span><br><span class="line">        bb_idx = anchors_bbox_map[indices_true]</span><br><span class="line">        class_labels[indices_true] = label[bb_idx, <span class="number">0</span>].long() + <span class="number">1</span></span><br><span class="line">        assigned_bb[indices_true] = label[bb_idx, <span class="number">1</span>:]</span><br><span class="line">        offset = offset_boxes(anchors, assigned_bb) * bbox_mask</span><br><span class="line">        batch_offset.append(offset.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_mask.append(bbox_mask.reshape(-<span class="number">1</span>))</span><br><span class="line">        batch_class_labels.append(class_labels)</span><br><span class="line">    bbox_offset = torch.stack(batch_offset)</span><br><span class="line">    bbox_mask = torch.stack(batch_mask)</span><br><span class="line">    class_labels = torch.stack(batch_class_labels)</span><br><span class="line">    <span class="keyword">return</span> (bbox_offset, bbox_mask, class_labels)</span><br></pre></td></tr></table></figure><p>在图像中绘制这些GT边界框和锚框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ground_truth = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>],</span><br><span class="line">                             [<span class="number">1</span>, <span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">anchors = torch.tensor([[<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.15</span>, <span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.4</span>],</span><br><span class="line">                        [<span class="number">0.63</span>, <span class="number">0.05</span>, <span class="number">0.88</span>, <span class="number">0.98</span>], [<span class="number">0.66</span>, <span class="number">0.45</span>, <span class="number">0.8</span>, <span class="number">0.8</span>],</span><br><span class="line">                        [<span class="number">0.57</span>, <span class="number">0.3</span>, <span class="number">0.92</span>, <span class="number">0.9</span>]])</span><br><span class="line"></span><br><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, ground_truth[:, <span class="number">1</span>:] * bbox_scale, [<span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>], <span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">show_bboxes(fig.axes, anchors * bbox_scale, [<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>]);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/42/anchors-gt.svg" alt="anchors-gt"></p><p>根据狗和猫的真实边缘框，标注这些锚框的分类和偏移量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">labels = multibox_target(anchors.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                         ground_truth.unsqueeze(dim=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">labels[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 1, 2, 0, 2]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.,</span><br><span class="line">         1., 1.]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00,  1.40e+00,  1.00e+01,</span><br><span class="line">          2.59e+00,  7.18e+00, -1.20e+00,  2.69e-01,  1.68e+00, -1.57e+00,</span><br><span class="line">         -0.00e+00, -0.00e+00, -0.00e+00, -0.00e+00, -5.71e-01, -1.00e+00,</span><br><span class="line">          4.17e-06,  6.26e-01]])</span><br></pre></td></tr></table></figure><p>应用逆偏移变换来返回预测的边界框坐标：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">offset_inverse</span>(<span class="params">anchors, offset_preds</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;根据带有预测偏移量的锚框来预测边界框&quot;&quot;&quot;</span></span><br><span class="line">    anc = d2l.box_corner_to_center(anchors)</span><br><span class="line">    pred_bbox_xy = (offset_preds[:, :<span class="number">2</span>] * anc[:, <span class="number">2</span>:] / <span class="number">10</span>) + anc[:, :<span class="number">2</span>]</span><br><span class="line">    pred_bbox_wh = torch.exp(offset_preds[:, <span class="number">2</span>:] / <span class="number">5</span>) * anc[:, <span class="number">2</span>:]</span><br><span class="line">    pred_bbox = torch.cat((pred_bbox_xy, pred_bbox_wh), axis=<span class="number">1</span>)</span><br><span class="line">    predicted_bbox = d2l.box_center_to_corner(pred_bbox)</span><br><span class="line">    <span class="keyword">return</span> predicted_bbox</span><br></pre></td></tr></table></figure><p>以下<code>nms</code>函数按降序对置信度进行排序并返回其索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">boxes, scores, iou_threshold</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;对预测边界框的置信度进行排序&quot;&quot;&quot;</span></span><br><span class="line">    B = torch.argsort(scores, dim=-<span class="number">1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    keep = []</span><br><span class="line">    <span class="keyword">while</span> B.numel() &gt; <span class="number">0</span>:</span><br><span class="line">        i = B[<span class="number">0</span>]</span><br><span class="line">        keep.append(i)</span><br><span class="line">        <span class="keyword">if</span> B.numel() == <span class="number">1</span>: <span class="keyword">break</span></span><br><span class="line">        iou = box_iou(boxes[i, :].reshape(-<span class="number">1</span>, <span class="number">4</span>),</span><br><span class="line">                      boxes[B[<span class="number">1</span>:], :].reshape(-<span class="number">1</span>, <span class="number">4</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line">        inds = torch.nonzero(iou &lt;= iou_threshold).reshape(-<span class="number">1</span>)</span><br><span class="line">        B = B[inds + <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(keep, device=boxes.device)</span><br></pre></td></tr></table></figure><p>将非极大值抑制应用于预测边界框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_detection</span>(<span class="params">cls_probs, offset_preds, anchors, nms_threshold=<span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">                       pos_threshold=<span class="number">0.009999999</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用非极大值抑制来预测边界框&quot;&quot;&quot;</span></span><br><span class="line">    device, batch_size = cls_probs.device, cls_probs.shape[<span class="number">0</span>]</span><br><span class="line">    anchors = anchors.squeeze(<span class="number">0</span>)</span><br><span class="line">    num_classes, num_anchors = cls_probs.shape[<span class="number">1</span>], cls_probs.shape[<span class="number">2</span>]</span><br><span class="line">    out = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        conf, class_id = torch.<span class="built_in">max</span>(cls_prob[<span class="number">1</span>:], <span class="number">0</span>)</span><br><span class="line">        predicted_bb = offset_inverse(anchors, offset_pred)</span><br><span class="line">        keep = nms(predicted_bb, conf, nms_threshold)</span><br><span class="line"></span><br><span class="line">        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)</span><br><span class="line">        combined = torch.cat((keep, all_idx))</span><br><span class="line">        uniques, counts = combined.unique(return_counts=<span class="literal">True</span>)</span><br><span class="line">        non_keep = uniques[counts == <span class="number">1</span>]</span><br><span class="line">        all_id_sorted = torch.cat((keep, non_keep))</span><br><span class="line">        class_id[non_keep] = -<span class="number">1</span></span><br><span class="line">        class_id = class_id[all_id_sorted]</span><br><span class="line">        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]</span><br><span class="line">        below_min_idx = (conf &lt; pos_threshold)</span><br><span class="line">        class_id[below_min_idx] = -<span class="number">1</span></span><br><span class="line">        conf[below_min_idx] = <span class="number">1</span> - conf[below_min_idx]</span><br><span class="line">        pred_info = torch.cat(</span><br><span class="line">            (class_id.unsqueeze(<span class="number">1</span>), conf.unsqueeze(<span class="number">1</span>), predicted_bb), dim=<span class="number">1</span>)</span><br><span class="line">        out.append(pred_info)</span><br><span class="line">    <span class="keyword">return</span> torch.stack(out)</span><br></pre></td></tr></table></figure><p>将上述算法应用到一个带有四个锚框的具体示例中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">anchors = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.08</span>, <span class="number">0.52</span>, <span class="number">0.92</span>], [<span class="number">0.08</span>, <span class="number">0.2</span>, <span class="number">0.56</span>, <span class="number">0.95</span>],</span><br><span class="line">                        [<span class="number">0.15</span>, <span class="number">0.3</span>, <span class="number">0.62</span>, <span class="number">0.91</span>], [<span class="number">0.55</span>, <span class="number">0.2</span>, <span class="number">0.9</span>, <span class="number">0.88</span>]])</span><br><span class="line">offset_preds = torch.tensor([<span class="number">0</span>] * anchors.numel())</span><br><span class="line">cls_probs = torch.tensor([[<span class="number">0</span>] * <span class="number">4</span>,</span><br><span class="line">                          [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],</span><br><span class="line">                          [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.9</span>]])</span><br></pre></td></tr></table></figure><p>在图像上绘制这些预测边界框和置信度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">show_bboxes(fig.axes, anchors * bbox_scale,</span><br><span class="line">            [<span class="string">&#x27;dog=0.9&#x27;</span>, <span class="string">&#x27;dog=0.8&#x27;</span>, <span class="string">&#x27;dog=0.7&#x27;</span>, <span class="string">&#x27;cat=0.9&#x27;</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/42/anchors-predict.svg" alt="anchors-predict"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output = multibox_detection(cls_probs.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            offset_preds.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            anchors.unsqueeze(dim=<span class="number">0</span>),</span><br><span class="line">                            nms_threshold=<span class="number">0.5</span>)</span><br><span class="line">output</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0.00,  0.90,  0.10,  0.08,  0.52,  0.92],</span><br><span class="line">         [ 1.00,  0.90,  0.55,  0.20,  0.90,  0.88],</span><br><span class="line">         [-1.00,  0.80,  0.08,  0.20,  0.56,  0.95],</span><br><span class="line">         [-1.00,  0.70,  0.15,  0.30,  0.62,  0.91]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">0</span>].detach().numpy():</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] == -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    label = (<span class="string">&#x27;dog=&#x27;</span>, <span class="string">&#x27;cat=&#x27;</span>)[<span class="built_in">int</span>(i[<span class="number">0</span>])] + <span class="built_in">str</span>(i[<span class="number">1</span>])</span><br><span class="line">    show_bboxes(fig.axes, [torch.tensor(i[<span class="number">2</span>:]) * bbox_scale], label)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/42/anchors-predict-nms.svg" alt="anchors-predict-nms"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 42 锚框。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 41 物体检测和数据集</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/41/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/41/</id>
    <published>2023-12-05T07:00:00.000Z</published>
    <updated>2023-12-05T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="41-物体检测和数据集"><a href="#41-物体检测和数据集" class="headerlink" title="41 物体检测和数据集"></a>41 物体检测和数据集</h1><h2 id="物体检测"><a href="#物体检测" class="headerlink" title="物体检测"></a>物体检测</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/41/image-20231205152707485.png" alt="image-20231205152707485"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/41/video-object-detection.gif" alt="video-object-detection"></p><h3 id="边缘框"><a href="#边缘框" class="headerlink" title="边缘框"></a>边缘框</h3><ul><li>一个边缘框可以通过4个数字定义：<ul><li>（左上x，左上y，右下x，右下y）</li><li>（左上x，左上y，宽，高）</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/41/image-20231205153420990.png" alt="image-20231205153420990"></p><h3 id="目标检测数据集"><a href="#目标检测数据集" class="headerlink" title="目标检测数据集"></a>目标检测数据集</h3><ul><li>每行表示一个物体<ul><li>图片文件名，物体类别，边缘框</li></ul></li><li>COCO（<a href="https://cocodataset.org/#home">COCO - Common Objects in Context</a>）<ul><li>80物体，330K图片，1.5M物体</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/41/image-20231205154321930.png" alt="image-20231205154321930"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>物体检测识别图片里的多个物体的类别和位置</li><li>位置通常用边缘框表示</li></ul><h2 id="边缘框实现"><a href="#边缘框实现" class="headerlink" title="边缘框实现"></a>边缘框实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">img = d2l.plt.imread(<span class="string">&#x27;../img/catdog.jpg&#x27;</span>) <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">d2l.plt.imshow(img);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/41/catdog.svg" alt="catdog"></p><p>定义在这两种表示之间进行转换的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">box_corner_to_center</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（左上，右下）转换到（中间，宽度，高度）&quot;&quot;&quot;</span></span><br><span class="line">    x1, y1, x2, y2 = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    cx = (x1 + x2) / <span class="number">2</span></span><br><span class="line">    cy = (y1 + y2) / <span class="number">2</span></span><br><span class="line">    w = x2 - x1</span><br><span class="line">    h = y2 - y1</span><br><span class="line">    boxes = torch.stack((cx, cy, w, h), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">box_center_to_corner</span>(<span class="params">boxes</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;从（中间，宽度，高度）转换到（左上，右下）&quot;&quot;&quot;</span></span><br><span class="line">    cx, cy, w, h = boxes[:, <span class="number">0</span>], boxes[:, <span class="number">1</span>], boxes[:, <span class="number">2</span>], boxes[:, <span class="number">3</span>]</span><br><span class="line">    x1 = cx - <span class="number">0.5</span> * w</span><br><span class="line">    y1 = cy - <span class="number">0.5</span> * h</span><br><span class="line">    x2 = cx + <span class="number">0.5</span> * w</span><br><span class="line">    y2 = cy + <span class="number">0.5</span> * h</span><br><span class="line">    boxes = torch.stack((x1, y1, x2, y2), axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> boxes</span><br></pre></td></tr></table></figure><p>定义图像中狗和猫的边界框：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dog_bbox, cat_bbox = [<span class="number">60.0</span>, <span class="number">45.0</span>, <span class="number">378.0</span>, <span class="number">516.0</span>], [<span class="number">400.0</span>, <span class="number">112.0</span>, <span class="number">655.0</span>, <span class="number">493.0</span>]</span><br><span class="line"></span><br><span class="line">boxes = torch.tensor((dog_bbox, cat_bbox))</span><br><span class="line">box_center_to_corner(box_corner_to_center(boxes)) == boxes</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True, True, True],</span><br><span class="line">        [True, True, True, True]])</span><br></pre></td></tr></table></figure><p>将边界框在图中画出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_to_rect</span>(<span class="params">bbox, color</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.plt.Rectangle(</span><br><span class="line">        xy=(bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]),</span><br><span class="line">        width=bbox[<span class="number">2</span>] - bbox[<span class="number">0</span>],</span><br><span class="line">        height=bbox[<span class="number">3</span>] - bbox[<span class="number">1</span>],</span><br><span class="line">        fill=<span class="literal">False</span>, edgecolor=color, linewidth=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">fig = d2l.plt.imshow(img)</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(dog_bbox, <span class="string">&#x27;blue&#x27;</span>))</span><br><span class="line">fig.axes.add_patch(bbox_to_rect(cat_bbox, <span class="string">&#x27;red&#x27;</span>));</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/41/catdog-bbox.svg" alt="catdog-bbox"></p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>我们收集并标记了一个小型数据集，下载数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.DATA_HUB[<span class="string">&#x27;banana-detection&#x27;</span>] = (</span><br><span class="line">    d2l.DATA_URL + <span class="string">&#x27;banana-detection.zip&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;5de26c8fce5ccdea9f91267273464dc968d20d72&#x27;</span>)</span><br></pre></td></tr></table></figure><p>读取香蕉检测数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_data_bananas</span>(<span class="params">is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取香蕉检测数据集中的图像和标签&quot;&quot;&quot;</span></span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;banana-detection&#x27;</span>)</span><br><span class="line">    csv_fname = os.path.join(data_dir,</span><br><span class="line">                             <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;bananas_val&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;label.csv&#x27;</span>)</span><br><span class="line">    csv_data = pd.read_csv(csv_fname)</span><br><span class="line">    csv_data = csv_data.set_index(<span class="string">&#x27;img_name&#x27;</span>)</span><br><span class="line">    images, targets = [], []</span><br><span class="line">    <span class="keyword">for</span> img_name, target <span class="keyword">in</span> csv_data.iterrows():</span><br><span class="line">        images.append(</span><br><span class="line">            torchvision.io.read_image(</span><br><span class="line">                os.path.join(data_dir,</span><br><span class="line">                             <span class="string">&#x27;bananas_train&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">&#x27;bananas_val&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;images&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;img_name&#125;</span>&#x27;</span>)))</span><br><span class="line">        targets.append(<span class="built_in">list</span>(target))</span><br><span class="line">    <span class="keyword">return</span> images, torch.tensor(targets).unsqueeze(<span class="number">1</span>) / <span class="number">256</span></span><br></pre></td></tr></table></figure><p>创建一个自定义<code>Dataset</code>实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BananasDataset</span>(torch.utils.data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;一个用于加载香蕉检测数据集的自定义数据集&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, is_train</span>):</span><br><span class="line">        self.features, self.labels = read_data_bananas(is_train)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;read &#x27;</span> + <span class="built_in">str</span>(<span class="built_in">len</span>(self.features))</span><br><span class="line">              + (<span class="string">f&#x27; training examples&#x27;</span> <span class="keyword">if</span> is_train <span class="keyword">else</span> <span class="string">f&#x27; validation examples&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> (self.features[idx].<span class="built_in">float</span>(), self.labels[idx])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.features)</span><br></pre></td></tr></table></figure><p>为训练集和测试集返回两个数据加载器实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_bananas</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载香蕉检测数据集&quot;&quot;&quot;</span></span><br><span class="line">    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=<span class="literal">True</span>),</span><br><span class="line">                                             batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=<span class="literal">False</span>),</span><br><span class="line">                                           batch_size)</span><br><span class="line">    <span class="keyword">return</span> train_iter, val_iter</span><br></pre></td></tr></table></figure><p>读取一个小批量，并打印其中的图像和标签的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_size, edge_size = <span class="number">32</span>, <span class="number">256</span></span><br><span class="line">train_iter, _ = load_data_bananas(batch_size)</span><br><span class="line">batch = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_iter))</span><br><span class="line">batch[<span class="number">0</span>].shape, batch[<span class="number">1</span>].shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">read 1000 training examples</span><br><span class="line">read 100 validation examples</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([32, 3, 256, 256]), torch.Size([32, 1, 5]))</span><br></pre></td></tr></table></figure><p>示范：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">imgs = (batch[<span class="number">0</span>][<span class="number">0</span>:<span class="number">10</span>].permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)) / <span class="number">255</span></span><br><span class="line">axes = d2l.show_images(imgs, <span class="number">2</span>, <span class="number">5</span>, scale=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> ax, label <span class="keyword">in</span> <span class="built_in">zip</span>(axes, batch[<span class="number">1</span>][<span class="number">0</span>:<span class="number">10</span>]):</span><br><span class="line">    d2l.show_bboxes(ax, [label[<span class="number">0</span>][<span class="number">1</span>:<span class="number">5</span>] * edge_size], colors=[<span class="string">&#x27;w&#x27;</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/41/dataset-sample.png" alt="dataset-sample"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 41 物体检测和数据集。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 40 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/40/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/40/</id>
    <published>2023-12-05T06:00:00.000Z</published>
    <updated>2023-12-05T07:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="40-实战Kaggle比赛：狗的品种识别（ImageNet-Dogs）"><a href="#40-实战Kaggle比赛：狗的品种识别（ImageNet-Dogs）" class="headerlink" title="40 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）"></a>40 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）</h1><p>比赛的网址是：<a href="https://www.kaggle.com/c/dog-breed-identification">Dog Breed Identification | Kaggle</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>我们提供完整数据集的小规模样本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;dog_tiny&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;kaggle_dog_tiny.zip&#x27;</span>,</span><br><span class="line">                            <span class="string">&#x27;0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">demo = <span class="literal">True</span></span><br><span class="line"><span class="keyword">if</span> demo:</span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;dog_tiny&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_dir = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;dog-breed-identification&#x27;</span>)</span><br></pre></td></tr></table></figure><p>整理数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_dog_data</span>(<span class="params">data_dir, valid_ratio</span>):</span><br><span class="line">    labels = d2l.read_csv_labels(os.path.join(data_dir, <span class="string">&#x27;labels.csv&#x27;</span>))</span><br><span class="line">    d2l.reorg_train_valid(data_dir, labels, valid_ratio)</span><br><span class="line">    d2l.reorg_test(data_dir)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span> <span class="keyword">if</span> demo <span class="keyword">else</span> <span class="number">128</span></span><br><span class="line">valid_ratio = <span class="number">0.1</span></span><br><span class="line">reorg_dog_data(data_dir, valid_ratio)</span><br></pre></td></tr></table></figure><p>图像增广：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">transform_train = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">224</span>, scale=(<span class="number">0.08</span>, <span class="number">1.0</span>),</span><br><span class="line">                                             ratio=(<span class="number">3.0</span>/<span class="number">4.0</span>, <span class="number">4.0</span>/<span class="number">3.0</span>)),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ColorJitter(brightness=<span class="number">0.4</span>,</span><br><span class="line">                                       contrast=<span class="number">0.4</span>,</span><br><span class="line">                                       saturation=<span class="number">0.4</span>),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                                     [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">transform_test = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    torchvision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                                     [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br></pre></td></tr></table></figure><p>读取数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_ds, train_valid_ds = [</span><br><span class="line">    torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">        transform=transform_train) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;train_valid&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">valid_ds, test_ds = [</span><br><span class="line">    torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">        transform=transform_test) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;valid&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">train_iter, train_valid_iter = [</span><br><span class="line">    torch.utils.data.DataLoader(</span><br><span class="line">        dataset, batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">for</span> dataset <span class="keyword">in</span> (train_ds, train_valid_ds)]</span><br><span class="line"></span><br><span class="line">valid_iter = torch.utils.data.DataLoader(</span><br><span class="line">    valid_ds, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_iter = torch.utils.data.DataLoader(</span><br><span class="line">    test_ds, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>微调预训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>(<span class="params">devices</span>):</span><br><span class="line">    finetune_net = nn.Sequential()</span><br><span class="line">    finetune_net.features = torchvision.models.resnet34(pretrained=<span class="literal">True</span>)</span><br><span class="line">    finetune_net.output_new = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">1000</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">120</span>))</span><br><span class="line">    finetune_net = finetune_net.to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> finetune_net.features.parameters():</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> finetune_net</span><br></pre></td></tr></table></figure><p>计算损失：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">data_iter, net, devices</span>):</span><br><span class="line">    l_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> features, labels <span class="keyword">in</span> data_iter:</span><br><span class="line">        features, labels = features.to(devices[<span class="number">0</span>]), labels.to(devices[<span class="number">0</span>])</span><br><span class="line">        outputs = net(features)</span><br><span class="line">        l = loss(outputs, labels)</span><br><span class="line">        l_sum += l.<span class="built_in">sum</span>()</span><br><span class="line">        n += labels.numel()</span><br><span class="line">    <span class="keyword">return</span> (l_sum / n).to(<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, valid_iter, num_epochs, lr, wd, devices,</span></span><br><span class="line"><span class="params">          lr_period, lr_decay</span>):</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    trainer = torch.optim.SGD(</span><br><span class="line">        (param <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters() <span class="keyword">if</span> param.requires_grad),</span><br><span class="line">        lr=lr, momentum=<span class="number">0.9</span>, weight_decay=wd)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)</span><br><span class="line">    num_batches, timer = <span class="built_in">len</span>(train_iter), d2l.Timer()</span><br><span class="line">    legend = [<span class="string">&#x27;train loss&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        legend.append(<span class="string">&#x27;valid loss&#x27;</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=legend)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            features, labels = features.to(devices[<span class="number">0</span>]), labels.to(devices[<span class="number">0</span>])</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            output = net(features)</span><br><span class="line">            l = loss(output, labels).<span class="built_in">sum</span>()</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">            metric.add(l, labels.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (metric[<span class="number">0</span>] / metric[<span class="number">1</span>], <span class="literal">None</span>))</span><br><span class="line">        measures = <span class="string">f&#x27;train loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">1</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            valid_loss = evaluate_loss(valid_iter, net, devices)</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, valid_loss.detach()))</span><br><span class="line">        scheduler.step()</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        measures += <span class="string">f&#x27;, valid loss <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span>&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(measures + <span class="string">f&#x27;\n<span class="subst">&#123;metric[<span class="number">1</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span>&#x27;</span></span><br><span class="line">          <span class="string">f&#x27; examples/sec on <span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练和验证模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">devices, num_epochs, lr, wd = d2l.try_all_gpus(), <span class="number">10</span>, <span class="number">1e-4</span>, <span class="number">1e-4</span></span><br><span class="line">lr_period, lr_decay, net = <span class="number">2</span>, <span class="number">0.9</span>, get_net(devices)</span><br><span class="line">train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,</span><br><span class="line">      lr_decay)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train loss 1.087, valid loss 1.542</span><br><span class="line">338.0 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/40/output1.svg" alt="output1"></p><p>对测试集分类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">net = get_net(devices)</span><br><span class="line">train(net, train_valid_iter, <span class="literal">None</span>, num_epochs, lr, wd, devices, lr_period,</span><br><span class="line">      lr_decay)</span><br><span class="line"></span><br><span class="line">preds = []</span><br><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> test_iter:</span><br><span class="line">    output = torch.nn.functional.softmax(net(data.to(devices[<span class="number">0</span>])), dim=<span class="number">0</span>)</span><br><span class="line">    preds.extend(output.cpu().detach().numpy())</span><br><span class="line">ids = <span class="built_in">sorted</span>(</span><br><span class="line">    os.listdir(os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;unknown&#x27;</span>)))</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;submission.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;id,&#x27;</span> + <span class="string">&#x27;,&#x27;</span>.join(train_valid_ds.classes) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i, output <span class="keyword">in</span> <span class="built_in">zip</span>(ids, preds):</span><br><span class="line">        f.write(</span><br><span class="line">            i.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>] + <span class="string">&#x27;,&#x27;</span> + <span class="string">&#x27;,&#x27;</span>.join([<span class="built_in">str</span>(num) <span class="keyword">for</span> num <span class="keyword">in</span> output]) + <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train loss 0.966</span><br><span class="line">462.8 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/40/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 40 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 39 实战Kaggle比赛：图像分类（CIFAR-10）</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/39/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/39/</id>
    <published>2023-12-04T13:00:00.000Z</published>
    <updated>2023-12-04T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="39-实战Kaggle比赛：图像分类（CIFAR-10）"><a href="#39-实战Kaggle比赛：图像分类（CIFAR-10）" class="headerlink" title="39 实战Kaggle比赛：图像分类（CIFAR-10）"></a>39 实战Kaggle比赛：图像分类（CIFAR-10）</h1><p>比赛的网址是：<a href="https://www.kaggle.com/c/cifar-10">CIFAR-10 - Object Recognition in Images | Kaggle</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>我们提供包含前1000个训练图像和5个随机测试图像的数据集的小规模样本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;cifar10_tiny&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;kaggle_cifar10_tiny.zip&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;2068874e4b9a9f0fb07ebe0ad2b29754449ccacd&#x27;</span>)</span><br><span class="line"></span><br><span class="line">demo = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> demo:</span><br><span class="line">    data_dir = d2l.download_extract(<span class="string">&#x27;cifar10_tiny&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_dir = <span class="string">&#x27;../data/cifar-10/&#x27;</span></span><br></pre></td></tr></table></figure><p>整理数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">read_csv_labels</span>(<span class="params">fname</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;读取fname来给标签字典返回一个文件名&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()[<span class="number">1</span>:]</span><br><span class="line">    tokens = [l.rstrip().split(<span class="string">&#x27;,&#x27;</span>) <span class="keyword">for</span> l <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(((name, label) <span class="keyword">for</span> name, label <span class="keyword">in</span> tokens))</span><br><span class="line"></span><br><span class="line">labels = read_csv_labels(os.path.join(data_dir, <span class="string">&#x27;trainLabels.csv&#x27;</span>))</span><br><span class="line">labels</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;1&#x27;: &#x27;frog&#x27;,</span><br><span class="line"> &#x27;2&#x27;: &#x27;truck&#x27;,</span><br><span class="line"> &#x27;3&#x27;: &#x27;truck&#x27;,</span><br><span class="line"> &#x27;4&#x27;: &#x27;deer&#x27;,</span><br><span class="line"> &#x27;5&#x27;: &#x27;automobile&#x27;,</span><br><span class="line"> &#x27;6&#x27;: &#x27;automobile&#x27;,</span><br><span class="line"> &#x27;7&#x27;: &#x27;bird&#x27;,</span><br><span class="line"> &#x27;8&#x27;: &#x27;horse&#x27;,</span><br><span class="line"> &#x27;9&#x27;: &#x27;ship&#x27;,</span><br><span class="line"> &#x27;10&#x27;: &#x27;cat&#x27;,</span><br><span class="line"> &#x27;11&#x27;: &#x27;deer&#x27;,</span><br><span class="line"> &#x27;12&#x27;: &#x27;horse&#x27;,</span><br><span class="line"> &#x27;13&#x27;: &#x27;horse&#x27;,</span><br><span class="line"> &#x27;14&#x27;: &#x27;bird&#x27;,</span><br><span class="line"> &#x27;15&#x27;: &#x27;truck&#x27;,</span><br><span class="line"> &#x27;16&#x27;: &#x27;truck&#x27;,</span><br><span class="line"> &#x27;17&#x27;: &#x27;truck&#x27;,</span><br><span class="line"> &#x27;18&#x27;: &#x27;cat&#x27;,</span><br><span class="line"> &#x27;19&#x27;: &#x27;bird&#x27;,</span><br><span class="line"> &#x27;20&#x27;: &#x27;frog&#x27;,</span><br><span class="line"> &#x27;21&#x27;: &#x27;deer&#x27;,</span><br><span class="line"> &#x27;22&#x27;: &#x27;cat&#x27;,</span><br><span class="line"> &#x27;23&#x27;: &#x27;frog&#x27;,</span><br><span class="line"> &#x27;24&#x27;: &#x27;frog&#x27;,</span><br><span class="line"> &#x27;25&#x27;: &#x27;bird&#x27;,</span><br><span class="line">...</span><br><span class="line"> &#x27;996&#x27;: &#x27;cat&#x27;,</span><br><span class="line"> &#x27;997&#x27;: &#x27;dog&#x27;,</span><br><span class="line"> &#x27;998&#x27;: &#x27;automobile&#x27;,</span><br><span class="line"> &#x27;999&#x27;: &#x27;cat&#x27;,</span><br><span class="line"> &#x27;1000&#x27;: &#x27;dog&#x27;&#125;</span><br></pre></td></tr></table></figure><p>将验证集从原始的训练集中拆分出来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">copyfile</span>(<span class="params">filename, target_dir</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文件复制到目标目录&quot;&quot;&quot;</span></span><br><span class="line">    os.makedirs(target_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    shutil.copy(filename, target_dir)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_train_valid</span>(<span class="params">data_dir, labels, valid_ratio</span>):</span><br><span class="line">    n = collections.Counter(labels.values()).most_common()[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">    n_valid_per_label = <span class="built_in">max</span>(<span class="number">1</span>, math.floor(n * valid_ratio))</span><br><span class="line">    label_count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> train_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>)):</span><br><span class="line">        label = labels[train_file.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]]</span><br><span class="line">        fname = os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>, train_file)</span><br><span class="line">        copyfile(</span><br><span class="line">            fname,</span><br><span class="line">            os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;train_valid&#x27;</span>, label))</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count <span class="keyword">or</span> label_count[label] &lt; n_valid_per_label:</span><br><span class="line">            copyfile(</span><br><span class="line">                fname,</span><br><span class="line">                os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>, label))</span><br><span class="line">            label_count[label] = label_count.get(label, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            copyfile(</span><br><span class="line">                fname,</span><br><span class="line">                os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;train&#x27;</span>, label))</span><br><span class="line">    <span class="keyword">return</span> n_valid_per_label</span><br></pre></td></tr></table></figure><p>在预测期间整理测试集，以方便读取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_test</span>(<span class="params">data_dir</span>):</span><br><span class="line">    <span class="keyword">for</span> test_file <span class="keyword">in</span> os.listdir(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>)):</span><br><span class="line">        copyfile(</span><br><span class="line">            os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>, test_file),</span><br><span class="line">            os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;unknown&#x27;</span>))</span><br></pre></td></tr></table></figure><p>调用前面定义的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reorg_cifar10_data</span>(<span class="params">data_dir, valid_ratio</span>):</span><br><span class="line">    labels = read_csv_labels(os.path.join(data_dir, <span class="string">&#x27;trainLabels.csv&#x27;</span>))</span><br><span class="line">    reorg_train_valid(data_dir, labels, valid_ratio)</span><br><span class="line">    reorg_test(data_dir)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span> <span class="keyword">if</span> demo <span class="keyword">else</span> <span class="number">128</span></span><br><span class="line">valid_ratio = <span class="number">0.1</span></span><br><span class="line">reorg_cifar10_data(data_dir, valid_ratio)</span><br></pre></td></tr></table></figure><p>图像增广：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">transform_train = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize(<span class="number">40</span>),</span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">32</span>, scale=(<span class="number">0.64</span>, <span class="number">1.0</span>),</span><br><span class="line">                                             ratio=(<span class="number">1.0</span>, <span class="number">1.0</span>)),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],</span><br><span class="line">                                     [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>])])</span><br><span class="line"></span><br><span class="line">transform_test = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    torchvision.transforms.Normalize([<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],</span><br><span class="line">                                     [<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>])])</span><br></pre></td></tr></table></figure><p>读取由原始图像组成的数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_ds, train_valid_ds = [</span><br><span class="line">    torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">        transform=transform_train) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;train_valid&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">valid_ds, test_ds = [</span><br><span class="line">    torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train_valid_test&#x27;</span>, folder),</span><br><span class="line">        transform=transform_test) <span class="keyword">for</span> folder <span class="keyword">in</span> [<span class="string">&#x27;valid&#x27;</span>, <span class="string">&#x27;test&#x27;</span>]]</span><br></pre></td></tr></table></figure><p>指定上面定义的所有图像增广操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_iter, train_valid_iter = [</span><br><span class="line">    torch.utils.data.DataLoader(</span><br><span class="line">        dataset, batch_size, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">for</span> dataset <span class="keyword">in</span> (train_ds, train_valid_ds)]</span><br><span class="line"></span><br><span class="line">valid_iter = torch.utils.data.DataLoader(</span><br><span class="line">    valid_ds, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_iter = torch.utils.data.DataLoader(</span><br><span class="line">    test_ds, batch_size, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    num_classes = <span class="number">10</span></span><br><span class="line">    net = d2l.resnet18(num_classes, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br></pre></td></tr></table></figure><p>训练函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_iter, valid_iter, num_epochs, lr, wd, devices,</span></span><br><span class="line"><span class="params">          lr_period, lr_decay</span>):</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=<span class="number">0.9</span>,</span><br><span class="line">                              weight_decay=wd)</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)</span><br><span class="line">    num_batches, timer = <span class="built_in">len</span>(train_iter), d2l.Timer()</span><br><span class="line">    legend = [<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        legend.append(<span class="string">&#x27;valid acc&#x27;</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=legend)</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            l, acc = d2l.train_batch_ch13(net, features, labels,</span><br><span class="line">                                          loss, trainer, devices)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(</span><br><span class="line">                    epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                    (metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>], <span class="literal">None</span>))</span><br><span class="line">        <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, valid_acc))</span><br><span class="line">        scheduler.step()</span><br><span class="line">    measures = (<span class="string">f&#x27;train loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">                <span class="string">f&#x27;train acc <span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> valid_iter <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        measures += <span class="string">f&#x27;, valid acc <span class="subst">&#123;valid_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(measures + <span class="string">f&#x27;\n<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span>&#x27;</span></span><br><span class="line">          <span class="string">f&#x27; examples/sec on <span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练和验证模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">devices, num_epochs, lr, wd = d2l.try_all_gpus(), <span class="number">20</span>, <span class="number">2e-4</span>, <span class="number">5e-4</span></span><br><span class="line">lr_period, lr_decay, net = <span class="number">4</span>, <span class="number">0.9</span>, get_net()</span><br><span class="line">train(net, train_iter, valid_iter, num_epochs, lr, wd, devices,</span><br><span class="line">      lr_period, lr_decay)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train loss 0.655, train acc 0.767, valid acc 0.359</span><br><span class="line">663.8 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/39/output1.svg" alt="output1"></p><p>对测试集进行分类并提交结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net, preds = get_net(), []</span><br><span class="line">train(net, train_valid_iter, <span class="literal">None</span>, num_epochs, lr, wd, devices, lr_period,</span><br><span class="line">      lr_decay)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, _ <span class="keyword">in</span> test_iter:</span><br><span class="line">    y_hat = net(X.to(devices[<span class="number">0</span>]))</span><br><span class="line">    preds.extend(y_hat.argmax(dim=<span class="number">1</span>).<span class="built_in">type</span>(torch.int32).cpu().numpy())</span><br><span class="line">sorted_ids = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_ds) + <span class="number">1</span>))</span><br><span class="line">sorted_ids.sort(key=<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x))</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;id&#x27;</span>: sorted_ids, <span class="string">&#x27;label&#x27;</span>: preds&#125;)</span><br><span class="line">df[<span class="string">&#x27;label&#x27;</span>] = df[<span class="string">&#x27;label&#x27;</span>].apply(<span class="keyword">lambda</span> x: train_valid_ds.classes[x])</span><br><span class="line">df.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train loss 0.675, train acc 0.769</span><br><span class="line">771.2 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/39/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 39 实战Kaggle比赛：图像分类（CIFAR-10）。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 38 第二次竞赛 树叶分类结果</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/38/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/38/</id>
    <published>2023-12-04T12:00:00.000Z</published>
    <updated>2023-12-04T12:10:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="38-第二次竞赛-树叶分类结果"><a href="#38-第二次竞赛-树叶分类结果" class="headerlink" title="38 第二次竞赛 树叶分类结果"></a>38 第二次竞赛 树叶分类结果</h1><ul><li>165支队伍，1877次提交</li><li>前五将获得签名书<ul><li>欢迎分享方法</li></ul></li><li>第6-20名同学，如果能在下周五晚12点前在Kaggle上提交可读的可重复结果的代码，也可以获得签名书</li></ul><p><a href="https://www.kaggle.com/c/classify-leaves/code">Classify Leaves | Kaggle</a></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 38 第二次竞赛 树叶分类结果。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 37 微调</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/37/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/37/</id>
    <published>2023-12-04T11:00:00.000Z</published>
    <updated>2023-12-04T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="37-微调"><a href="#37-微调" class="headerlink" title="37 微调"></a>37 微调</h1><h2 id="标注一个数据集很贵"><a href="#标注一个数据集很贵" class="headerlink" title="标注一个数据集很贵"></a>标注一个数据集很贵</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/37/image-20231204193704054.png" alt="image-20231204193704054"></p><h2 id="网络架构"><a href="#网络架构" class="headerlink" title="网络架构"></a>网络架构</h2><ul><li>一个神经网络一般可以分为两块<ul><li>特征抽取将原始像素变成容易线性分割的特征</li><li>线性分类器来做分类</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/37/image-20231204194019698.png" alt="image-20231204194019698"></p><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/37/image-20231204194206807.png" alt="image-20231204194206807"></p><p>这里的微调实际上指的是迁移学习。</p><h2 id="微调中的权重初始化"><a href="#微调中的权重初始化" class="headerlink" title="微调中的权重初始化"></a>微调中的权重初始化</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/37/image-20231204194358539.png" alt="image-20231204194358539"></p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul><li>是一个目标数据集上的正常训练任务，但使用更强的正则化<ul><li>使用更小的学习率</li><li>使用更少的数据迭代</li></ul></li><li>源数据集远复杂于目标数据，通常微调效果更好</li></ul><h2 id="重用分类器权重"><a href="#重用分类器权重" class="headerlink" title="重用分类器权重"></a>重用分类器权重</h2><ul><li>源数据集可能也有目标数据集中的部分标号</li><li>可以使用预训练好模型分类器中对应标号对应的向量来做初始化</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/37/image-20231204194916562.png" alt="image-20231204194916562"></p><h2 id="固定一些层"><a href="#固定一些层" class="headerlink" title="固定一些层"></a>固定一些层</h2><ul><li>神经网络通常学习有层次的特征表示<ul><li>低层次的特征更加通用</li><li>高层次的特征则更跟数据集祥光</li></ul></li><li>可以固定底部一些层的参数，不参与更新<ul><li>更强的正则</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/37/image-20231204195031750.png" alt="image-20231204195031750"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>微调通过使用在大数据上得到的预训练好的模型来初始化模型权重来提升精度</li><li>预训练模型质量很重要</li><li>微调通常速度更快、精度更高</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>热狗🌭数据集来源于网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;hotdog&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;hotdog.zip&#x27;</span>,</span><br><span class="line">                         <span class="string">&#x27;fba480ffa8aa7e0febbb511d181409f899b9baa5&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data_dir = d2l.download_extract(<span class="string">&#x27;hotdog&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>))</span><br><span class="line">test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>))</span><br></pre></td></tr></table></figure><p>图像的大小和纵横比各有不同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hotdogs = [train_imgs[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">not_hotdogs = [train_imgs[-i - <span class="number">1</span>][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>)]</span><br><span class="line">d2l.show_images(hotdogs + not_hotdogs, <span class="number">2</span>, <span class="number">8</span>, scale=<span class="number">1.4</span>);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/37/image-20231204200424402.png" alt="image-20231204200424402"></p><p>数据增广：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">normalize = torchvision.transforms.Normalize(</span><br><span class="line">    [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line">train_augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br><span class="line"></span><br><span class="line">test_augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    torchvision.transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">    normalize])</span><br></pre></td></tr></table></figure><p>定义和初始化模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pretrained_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">pretrained_net.fc</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Linear(in_features=512, out_features=1000, bias=True)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">finetune_net = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">finetune_net.fc = nn.Linear(finetune_net.fc.in_features, <span class="number">2</span>)</span><br><span class="line">nn.init.xavier_uniform_(finetune_net.fc.weight);</span><br></pre></td></tr></table></figure><p>微调模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_fine_tuning</span>(<span class="params">net, learning_rate, batch_size=<span class="number">128</span>, num_epochs=<span class="number">5</span>,</span></span><br><span class="line"><span class="params">                      param_group=<span class="literal">True</span></span>):</span><br><span class="line">    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;train&#x27;</span>), transform=train_augs),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span><br><span class="line">        os.path.join(data_dir, <span class="string">&#x27;test&#x27;</span>), transform=test_augs),</span><br><span class="line">        batch_size=batch_size)</span><br><span class="line">    devices = d2l.try_all_gpus()</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> param_group:</span><br><span class="line">        params_1x = [param <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()</span><br><span class="line">             <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;fc.weight&quot;</span>, <span class="string">&quot;fc.bias&quot;</span>]]</span><br><span class="line">        trainer = torch.optim.SGD([&#123;<span class="string">&#x27;params&#x27;</span>: params_1x&#125;,</span><br><span class="line">                                   &#123;<span class="string">&#x27;params&#x27;</span>: net.fc.parameters(),</span><br><span class="line">                                    <span class="string">&#x27;lr&#x27;</span>: learning_rate * <span class="number">10</span>&#125;],</span><br><span class="line">                                lr=learning_rate, weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,</span><br><span class="line">                                  weight_decay=<span class="number">0.001</span>)</span><br><span class="line">    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,</span><br><span class="line">                   devices)</span><br></pre></td></tr></table></figure><p>使用较小的学习率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_fine_tuning(finetune_net, <span class="number">5e-5</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.255, train acc 0.914, test acc 0.934</span><br><span class="line">97.0 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/37/output1.svg" alt="output1"></p><p>为了进行比较，所有模型参数初始化为随机值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scratch_net = torchvision.models.resnet18()</span><br><span class="line">scratch_net.fc = nn.Linear(scratch_net.fc.in_features, <span class="number">2</span>)</span><br><span class="line">train_fine_tuning(scratch_net, <span class="number">5e-4</span>, param_group=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.450, train acc 0.822, test acc 0.850</span><br><span class="line">259.5 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/37/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 37 微调。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 36 数据增广</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/36/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/36/</id>
    <published>2023-12-04T05:00:00.000Z</published>
    <updated>2023-12-04T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="36-数据增广"><a href="#36-数据增广" class="headerlink" title="36 数据增广"></a>36 数据增广</h1><p>封面图片展示的方法来自于这个仓库：<a href="https://github.com/aleju/imgaug">aleju/imgaug: Image augmentation for machine learning experiments. (github.com)</a>。</p><p>下面是截取的一些示例：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/channelshuffle.gif" alt="channelshuffle"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/coarsedropout_p_0_2.gif" alt="coarsedropout_p_0_2"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/dropout2d.gif" alt="dropout2d"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/sigmoidcontrast_gain_10.gif" alt="sigmoidcontrast_gain_10"></p><h2 id="CES上的真实故事"><a href="#CES上的真实故事" class="headerlink" title="CES上的真实故事"></a>CES上的真实故事</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204163134323.png" alt="image-20231204163134323"></p><ul><li>有一家做智能售货机的公司，发现他们的演示机器在现场效果很差，因为现场：<ul><li>不同的色温</li><li>桌面的灯光反射不一样</li></ul></li><li>他们连夜现场收集了数据，训练了一个新的模型，同时买了一块新桌布</li></ul><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><ul><li>增加一个已有数据集，使得有更多的多样性<ul><li>在语言里面加入各种不同的背景噪音</li><li>改变图片的颜色和形状</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204163607379.png" alt="image-20231204163607379"></p><h2 id="使用增强数据训练"><a href="#使用增强数据训练" class="headerlink" title="使用增强数据训练"></a>使用增强数据训练</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204163804692.png" alt="image-20231204163804692"></p><h2 id="翻转"><a href="#翻转" class="headerlink" title="翻转"></a>翻转</h2><ul><li><p>左右翻转</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204163940159.png" alt="image-20231204163940159"></p></li><li><p>上下翻转</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204164005546.png" alt="image-20231204164005546"></p></li><li><p>不总是可行</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204164030995.png" alt="image-20231204164030995"></p></li></ul><h2 id="切割"><a href="#切割" class="headerlink" title="切割"></a>切割</h2><ul><li>从图片中切割一块，然后变形到固定形状<ul><li>随机宽高比（e.g. 3/4, 4/3）</li><li>随机大小（e.g. [8%, 100%]）</li><li>随机位置</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204164134578.png" alt="image-20231204164134578"></p><h2 id="颜色"><a href="#颜色" class="headerlink" title="颜色"></a>颜色</h2><ul><li>改变色调，饱和度，明亮度（e.g. [0.5, 1.5]）</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/36/image-20231204164446848.png" alt="image-20231204164446848"></p><h2 id="几十种其他的办法"><a href="#几十种其他的办法" class="headerlink" title="几十种其他的办法"></a>几十种其他的办法</h2><p><a href="https://github.com/aleju/imgaug">aleju/imgaug: Image augmentation for machine learning experiments. (github.com)</a></p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/identity.gif" alt="identity"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/examples_grid.jpg" alt="examples_grid"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>数据增广通过变形数据来获取多样性从而使得模型泛化性能更好</li><li>常见图片增广包括翻转、切割、变色</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.set_figsize()</span><br><span class="line">img = d2l.Image.<span class="built_in">open</span>(<span class="string">&#x27;../img/cat1.jpg&#x27;</span>) <span class="comment"># 请修改为自己的路径</span></span><br><span class="line">d2l.plt.imshow(img)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;matplotlib.image.AxesImage at 0x11c8aec4dc0&gt;</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/36/cat1.png" alt="cat1"></p><p>左右翻转图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.RandomHorizontalFlip())</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output1.svg" alt="RandomHorizontalFlip"></p><p>上下翻转图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.RandomVerticalFlip())</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output2.svg" alt="RandomVerticalFlip"></p><p>随机裁剪：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shape_aug = torchvision.transforms.RandomResizedCrop(</span><br><span class="line">    (<span class="number">200</span>, <span class="number">200</span>), scale=(<span class="number">0.1</span>, <span class="number">1</span>), ratio=(<span class="number">0.5</span>, <span class="number">2</span>))</span><br><span class="line">apply(img, shape_aug)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output3.svg" alt="RandomResizedCrop"></p><p>随机更改图像的亮度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0.5</span>, contrast=<span class="number">0</span>, saturation=<span class="number">0</span>, hue=<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output4.svg" alt="brightness=0.5"></p><p>随机改变图像的色调：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apply(img, torchvision.transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0</span>, contrast=<span class="number">0</span>, saturation=<span class="number">0</span>, hue=<span class="number">0.5</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output5.svg" alt="hue=0.5"></p><p>随机更改图像的亮度（<code>brightness</code>）、对比度（<code>contrast</code>）、饱和度（<code>saturation</code>）和色调（<code>hue</code>）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">color_aug = torchvision.transforms.ColorJitter(</span><br><span class="line">    brightness=<span class="number">0.5</span>, contrast=<span class="number">0.5</span>, saturation=<span class="number">0.5</span>, hue=<span class="number">0.5</span>)</span><br><span class="line">apply(img, color_aug)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output6.svg" alt="brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5"></p><p>结合多种图像增广方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])</span><br><span class="line">apply(img, augs)</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output7.svg" alt="torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug"></p><p>使用图像增广进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_images = torchvision.datasets.CIFAR10(train=<span class="literal">True</span>, root=<span class="string">&quot;./data&quot;</span>,</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">d2l.show_images([all_images[i][<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">32</span>)], <span class="number">4</span>, <span class="number">8</span>, scale=<span class="number">0.8</span>);</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/36/output8.svg" alt="CIFAR10"></p><p>只使用最简单的随机左右翻转：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),</span><br><span class="line">    torchvision.transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">test_augs = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()])</span><br></pre></td></tr></table></figure><p>定义一个辅助函数，以便于读取图像和应用图像增广：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_cifar10</span>(<span class="params">is_train, augs, batch_size</span>):</span><br><span class="line">    dataset = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./data&quot;</span>, train=is_train,</span><br><span class="line">                                           transform=augs, download=<span class="literal">True</span>)</span><br><span class="line">    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,</span><br><span class="line">                    shuffle=is_train, num_workers=d2l.get_dataloader_workers())</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure><p>定义一个函数，使用多GPU对模型进行训练和评估：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_batch_ch13</span>(<span class="params">net, X, y, loss, trainer, devices</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">        X = [x.to(devices[<span class="number">0</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        X = X.to(devices[<span class="number">0</span>])</span><br><span class="line">    y = y.to(devices[<span class="number">0</span>])</span><br><span class="line">    net.train()</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    pred = net(X)</span><br><span class="line">    l = loss(pred, y)</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    train_loss_sum = l.<span class="built_in">sum</span>()</span><br><span class="line">    train_acc_sum = d2l.accuracy(pred, y)</span><br><span class="line">    <span class="keyword">return</span> train_loss_sum, train_acc_sum</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch13</span>(<span class="params">net, train_iter, test_iter, loss, trainer, num_epochs,</span></span><br><span class="line"><span class="params">               devices=d2l.try_all_gpus(<span class="params"></span>)</span>):</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">4</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            l, acc = train_batch_ch13(</span><br><span class="line">                net, features, labels, loss, trainer, devices)</span><br><span class="line">            metric.add(l, acc, labels.shape[<span class="number">0</span>], labels.numel())</span><br><span class="line">            timer.stop()</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">3</span>],</span><br><span class="line">                              <span class="literal">None</span>))</span><br><span class="line">        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;metric[<span class="number">0</span>] / metric[<span class="number">2</span>]:<span class="number">.3</span>f&#125;</span>, train acc &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">1</span>] / metric[<span class="number">3</span>]:<span class="number">.3</span>f&#125;</span>, test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec on &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>定义<code>train_with_data_aug</code>函数，使用图像增广来训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">batch_size, devices, net = <span class="number">256</span>, d2l.try_all_gpus(), d2l.resnet18(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) <span class="keyword">in</span> [nn.Linear, nn.Conv2d]:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_with_data_aug</span>(<span class="params">train_augs, test_augs, net, lr=<span class="number">0.001</span></span>):</span><br><span class="line">    train_iter = load_cifar10(<span class="literal">True</span>, train_augs, batch_size)</span><br><span class="line">    test_iter = load_cifar10(<span class="literal">False</span>, test_augs, batch_size)</span><br><span class="line">    loss = nn.CrossEntropyLoss(reduction=<span class="string">&quot;none&quot;</span>)</span><br><span class="line">    trainer = torch.optim.Adam(net.parameters(), lr=lr)</span><br><span class="line">    train_ch13(net, train_iter, test_iter, loss, trainer, <span class="number">10</span>, devices)</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_with_data_aug(train_augs, test_augs, net)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.184, train acc 0.936, test acc 0.833</span><br><span class="line">1039.1 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/36/train-output1.svg" alt="train-output1"></p><p>使用未增广的数据训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_with_data_aug(test_augs, test_augs, net)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.032, train acc 0.989, test acc 0.834</span><br><span class="line">1007.5 examples/sec on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/36/train-output2.svg" alt="train-output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 36 数据增广。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 35 分布式训练</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/35/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/35/</id>
    <published>2023-12-03T12:00:00.000Z</published>
    <updated>2023-12-03T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="35-分布式训练"><a href="#35-分布式训练" class="headerlink" title="35 分布式训练"></a>35 分布式训练</h1><h2 id="分布式计算"><a href="#分布式计算" class="headerlink" title="分布式计算"></a>分布式计算</h2><p>原本的多GPU训练架构如下：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203203141807.png" alt="image-20231203203141807"></p><p>在分布式计算中，数据会放在分布式文件系统上：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203203521815.png" alt="image-20231203203521815"></p><p>同时，会有多台机器，每台机器中会有多个GPU，下图就是两台机器，每台机器中有两个GPU：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203203817725.png" alt="image-20231203203817725"></p><p>不仅如此，会有多个参数服务器：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203203940749.png" alt="image-20231203203940749"></p><p>然后通过网络进行交互：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204030309.png" alt="image-20231203204030309"></p><h2 id="GPU机器架构"><a href="#GPU机器架构" class="headerlink" title="GPU机器架构"></a>GPU机器架构</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204324442.png" alt="image-20231203204324442"></p><p>尽量本地多通讯，机器间少通讯。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204341164.png" alt="image-20231203204341164"></p><h2 id="计算一个小批量"><a href="#计算一个小批量" class="headerlink" title="计算一个小批量"></a>计算一个小批量</h2><ol><li><p>每个计算服务器读取小批量中的一块：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204445954.png" alt="image-20231203204445954"></p></li><li><p>进一步将数据切分到每个GPU上：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204605841.png" alt="image-20231203204605841"></p></li><li><p>每个worker从参数服务器那里获取模型参数：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204637395.png" alt="image-20231203204637395"></p></li><li><p>复制参数到每个GPU上：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204712294.png" alt="image-20231203204712294"></p></li><li><p>每个GPU计算梯度：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204803725.png" alt="image-20231203204803725"></p></li><li><p>将所有GPU上的梯度求和：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204827762.png" alt="image-20231203204827762"></p></li><li><p>梯度传回服务器：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204857898.png" alt="image-20231203204857898"></p></li><li><p>每个服务器对梯度求和，并更新参数：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203204928359.png" alt="image-20231203204928359"></p></li></ol><h2 id="同步SGD"><a href="#同步SGD" class="headerlink" title="同步SGD"></a>同步SGD</h2><ul><li>这里每一个worker都是同步计算一个批量，称为同步SGD</li><li>假设有$n$个GPU，每个GPU每次处理$b$个样本，那么同步SGD等价于在单GPU运行批量大小为$nb$的SGD</li><li>在理想情况下，$n$个GPU可以得到相对于单GPU的$n$倍加速</li></ul><h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><ul><li>$t_1$ = 在单GPU上计算$b$个样本梯度的时间</li><li>假设有$m$个参数，一个worker每次发送和接收$m$个参数和梯度</li><li>$t_2$ = 发送和接收所用时间</li><li>每个批量的计算时间为$\max(t_1, t_2)$<ul><li>选取足够大的$b$使得$t_1 &gt; t_2$，让GPU持续进行计算</li><li>增加$b$或$n$导致更大的批量大小，导致需要更多计算来得到给定的模型精度</li></ul></li></ul><h2 id="性能的权衡"><a href="#性能的权衡" class="headerlink" title="性能的权衡"></a>性能的权衡</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/35/image-20231203211041719.png" alt="image-20231203211041719"></p><h2 id="实践时的建议"><a href="#实践时的建议" class="headerlink" title="实践时的建议"></a>实践时的建议</h2><ul><li>使用一个大数据集</li><li>需要好的GPU-GPU和机器-机器带宽</li><li>高效的数据读取和预处理</li><li>模型需要有好的计算（FLOP）通讯（model size）比<ul><li>Inception &gt; ResNet &gt; AlexNet</li></ul></li><li>使用足够大的批量大小来得到好的系统性能</li><li>使用高效的优化算法对应大批量大小</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>分布式同步数据并行是多GPU数据并行在多机器上的拓展</li><li>网络通讯通常是瓶颈</li><li>需要注意使用特别大的批量大小时收敛效率</li><li>更复杂的分布式有异步、模型并行</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 35 分布式训练。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 34 多GPU训练实现</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/34/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/34/</id>
    <published>2023-12-03T09:00:00.000Z</published>
    <updated>2023-12-03T11:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="34-多GPU训练实现"><a href="#34-多GPU训练实现" class="headerlink" title="34 多GPU训练实现"></a>34 多GPU训练实现</h1><h2 id="Kaggle教程"><a href="#Kaggle教程" class="headerlink" title="Kaggle教程"></a>Kaggle教程</h2><p>没有多GPU的同学可以到Kaggle上<del>白嫖</del>借用。</p><p>使用过程如下：</p><ol><li><p>登录Kaggle，点击左侧的<code>Code</code>：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/34/image-20231203200126203.png" alt="image-20231203200126203"></p></li><li><p>点击<code>New Notebook</code>：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/34/image-20231203200228073.png" alt="image-20231203200228073"></p></li><li><p>切换加速器为GPU T4 x2：</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/34/image-20231203200345929.png" alt="image-20231203200345929"></p></li><li><p>别忘了安装<code>d2l</code>：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install d2l</span><br></pre></td></tr></table></figure><p> 注意，后面简洁实现中，在<code>resnet18</code>中有一处代码需要修改。</p></li></ol><h2 id="从零开始"><a href="#从零开始" class="headerlink" title="从零开始"></a>从零开始</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>简单网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">scale = <span class="number">0.01</span></span><br><span class="line">W1 = torch.randn(size=(<span class="number">20</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)) * scale</span><br><span class="line">b1 = torch.zeros(<span class="number">20</span>)</span><br><span class="line">W2 = torch.randn(size=(<span class="number">50</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">5</span>)) * scale</span><br><span class="line">b2 = torch.zeros(<span class="number">50</span>)</span><br><span class="line">W3 = torch.randn(size=(<span class="number">800</span>, <span class="number">128</span>)) * scale</span><br><span class="line">b3 = torch.zeros(<span class="number">128</span>)</span><br><span class="line">W4 = torch.randn(size=(<span class="number">128</span>, <span class="number">10</span>)) * scale</span><br><span class="line">b4 = torch.zeros(<span class="number">10</span>)</span><br><span class="line">params = [W1, b1, W2, b2, W3, b3, W4, b4]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lenet</span>(<span class="params">X, params</span>):</span><br><span class="line">    h1_conv = F.conv2d(<span class="built_in">input</span>=X, weight=params[<span class="number">0</span>], bias=params[<span class="number">1</span>])</span><br><span class="line">    h1_activation = F.relu(h1_conv)</span><br><span class="line">    h1 = F.avg_pool2d(<span class="built_in">input</span>=h1_activation, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    h2_conv = F.conv2d(<span class="built_in">input</span>=h1, weight=params[<span class="number">2</span>], bias=params[<span class="number">3</span>])</span><br><span class="line">    h2_activation = F.relu(h2_conv)</span><br><span class="line">    h2 = F.avg_pool2d(<span class="built_in">input</span>=h2_activation, kernel_size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    h2 = h2.reshape(h2.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">    h3_linear = torch.mm(h2, params[<span class="number">4</span>]) + params[<span class="number">5</span>]</span><br><span class="line">    h3 = F.relu(h3_linear)</span><br><span class="line">    y_hat = torch.mm(h3, params[<span class="number">6</span>]) + params[<span class="number">7</span>]</span><br><span class="line">    <span class="keyword">return</span> y_hat</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><p>向多个设备分发参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">params, device</span>):</span><br><span class="line">    new_params = [p.to(device) <span class="keyword">for</span> p <span class="keyword">in</span> params]</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> new_params:</span><br><span class="line">        p.requires_grad_()</span><br><span class="line">    <span class="keyword">return</span> new_params</span><br><span class="line"></span><br><span class="line">new_params = get_params(params, d2l.try_gpu(<span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b1 weight:&#x27;</span>, new_params[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b1 grad:&#x27;</span>, new_params[<span class="number">1</span>].grad)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 weight: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">       device=&#x27;cuda:0&#x27;, requires_grad=True)</span><br><span class="line">b1 grad: None</span><br></pre></td></tr></table></figure><p><code>allreduce</code>函数将所有向量相加，并将结果广播给所有GPU：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">allreduce</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(data)):</span><br><span class="line">        data[<span class="number">0</span>][:] += data[i].to(data[<span class="number">0</span>].device)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(data)):</span><br><span class="line">        data[i][:] = data[<span class="number">0</span>].to(data[i].device)</span><br><span class="line"></span><br><span class="line">data = [torch.ones((<span class="number">1</span>, <span class="number">2</span>), device=d2l.try_gpu(i)) * (i + <span class="number">1</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;before allreduce:\n&#x27;</span>, data[<span class="number">0</span>], <span class="string">&#x27;\n&#x27;</span>, data[<span class="number">1</span>])</span><br><span class="line">allreduce(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;after allreduce:\n&#x27;</span>, data[<span class="number">0</span>], <span class="string">&#x27;\n&#x27;</span>, data[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">before allreduce:</span><br><span class="line"> tensor([[1., 1.]], device=&#x27;cuda:0&#x27;) </span><br><span class="line"> tensor([[2., 2.]], device=&#x27;cuda:1&#x27;)</span><br><span class="line">after allreduce:</span><br><span class="line"> tensor([[3., 3.]], device=&#x27;cuda:0&#x27;) </span><br><span class="line"> tensor([[3., 3.]], device=&#x27;cuda:1&#x27;)</span><br></pre></td></tr></table></figure><p>将一个小批量数据均匀地分布在多个GPU上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = torch.arange(<span class="number">20</span>).reshape(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">devices = [torch.device(<span class="string">&#x27;cuda:0&#x27;</span>), torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)]</span><br><span class="line">split = nn.parallel.scatter(data, devices)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input :&#x27;</span>, data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;load into&#x27;</span>, devices)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;output:&#x27;</span>, split)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input : tensor([[ 0,  1,  2,  3,  4],</span><br><span class="line">        [ 5,  6,  7,  8,  9],</span><br><span class="line">        [10, 11, 12, 13, 14],</span><br><span class="line">        [15, 16, 17, 18, 19]])</span><br><span class="line">load into [device(type=&#x27;cuda&#x27;, index=0), device(type=&#x27;cuda&#x27;, index=1)]</span><br><span class="line">output: (tensor([[0, 1, 2, 3, 4],</span><br><span class="line">        [5, 6, 7, 8, 9]], device=&#x27;cuda:0&#x27;), tensor([[10, 11, 12, 13, 14],</span><br><span class="line">        [15, 16, 17, 18, 19]], device=&#x27;cuda:1&#x27;))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">split_batch</span>(<span class="params">X, y, devices</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;将X和y拆分到多个设备上&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> X.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> (nn.parallel.scatter(X, devices),</span><br><span class="line">            nn.parallel.scatter(y, devices))</span><br></pre></td></tr></table></figure><p>在一个小批量上实现多GPU训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_batch</span>(<span class="params">X, y, device_params, devices, lr</span>):</span><br><span class="line">    X_shards, y_shards = split_batch(X, y, devices)</span><br><span class="line">    ls = [loss(lenet(X_shard, device_W), y_shard).<span class="built_in">sum</span>()</span><br><span class="line">          <span class="keyword">for</span> X_shard, y_shard, device_W <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">              X_shards, y_shards, device_params)]</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> ls:</span><br><span class="line">        l.backward()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(device_params[<span class="number">0</span>])):</span><br><span class="line">            allreduce(</span><br><span class="line">                [device_params[c][i].grad <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(devices))])</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> device_params:</span><br><span class="line">        d2l.sgd(param, lr, X.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>定义训练函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">num_gpus, batch_size, lr</span>):</span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">    devices = [d2l.try_gpu(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_gpus)]</span><br><span class="line">    device_params = [get_params(params, d) <span class="keyword">for</span> d <span class="keyword">in</span> devices]</span><br><span class="line">    num_epochs = <span class="number">10</span></span><br><span class="line">    animator = d2l.Animator(<span class="string">&#x27;epoch&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs])</span><br><span class="line">    timer = d2l.Timer()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        timer.start()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            train_batch(X, y, device_params, devices, lr)</span><br><span class="line">            torch.cuda.synchronize()</span><br><span class="line">        timer.stop()</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_accuracy_gpu(</span><br><span class="line">            <span class="keyword">lambda</span> x: lenet(x, device_params[<span class="number">0</span>]), test_iter, devices[<span class="number">0</span>]),))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;test acc: <span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.2</span>f&#125;</span>, <span class="subst">&#123;timer.avg():<span class="number">.1</span>f&#125;</span> sec/epoch, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在单个GPU上运行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(num_gpus=<span class="number">1</span>, batch_size=<span class="number">256</span>, lr=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test acc: 0.82, 3.8 sec/epoch, on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/34/output1-1.svg" alt="output1-1"></p><p>增加为2个GPU：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(num_gpus=<span class="number">2</span>, batch_size=<span class="number">256</span>, lr=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test acc: 0.83, 4.2 sec/epoch, on [device(type=&#x27;cuda&#x27;, index=0), device(type=&#x27;cuda&#x27;, index=1)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/34/output1-2.svg" alt="output1-2"></p><h2 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>简单网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">resnet18</span>(<span class="params">num_classes, in_channels=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;稍加修改的ResNet-18模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">in_channels, out_channels, num_residuals,</span></span><br><span class="line"><span class="params">                     first_block=<span class="literal">False</span></span>):</span><br><span class="line">        blk = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">                blk.append(d2l.Residual(in_channels, out_channels, strides=<span class="number">2</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                blk.append(d2l.Residual(out_channels, out_channels))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU())</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block1&quot;</span>, resnet_block(</span><br><span class="line">        <span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block2&quot;</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block3&quot;</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block4&quot;</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;global_avg_pool&quot;</span>, nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)))</span><br><span class="line">    net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(nn.Flatten(),</span><br><span class="line">                                       nn.Linear(<span class="number">512</span>, num_classes)))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = resnet18(<span class="number">10</span>)</span><br><span class="line">devices = d2l.try_all_gpus()</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, num_gpus, batch_size, lr</span>):</span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">    devices = [d2l.try_gpu(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_gpus)]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) <span class="keyword">in</span> [nn.Linear, nn.Conv2d]:</span><br><span class="line">            nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    timer, num_epochs = d2l.Timer(), <span class="number">10</span></span><br><span class="line">    animator = d2l.Animator(<span class="string">&#x27;epoch&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        timer.start()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            X, y = X.to(devices[<span class="number">0</span>]), y.to(devices[<span class="number">0</span>])</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        timer.stop()</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_accuracy_gpu(net, test_iter),))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;test acc: <span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.2</span>f&#125;</span>, <span class="subst">&#123;timer.avg():<span class="number">.1</span>f&#125;</span> sec/epoch, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>在单个GPU上训练网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(net, num_gpus=<span class="number">1</span>, batch_size=<span class="number">256</span>, lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test acc: 0.92, 31.8 sec/epoch, on [device(type=&#x27;cuda&#x27;, index=0)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/34/output2-1.svg" alt="output2-1"></p><p>使用2个GPU进行训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(net, num_gpus=<span class="number">2</span>, batch_size=<span class="number">256</span>, lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test acc: 0.92, 19.3 sec/epoch, on [device(type=&#x27;cuda&#x27;, index=0), device(type=&#x27;cuda&#x27;, index=1)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/34/output2-2.svg" alt="output2-2"></p><p>更多的测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(net, num_gpus=<span class="number">2</span>, batch_size=<span class="number">256</span>*<span class="number">2</span>, lr=<span class="number">0.1</span>*<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test acc: 0.73, 17.9 sec/epoch, on [device(type=&#x27;cuda&#x27;, index=0), device(type=&#x27;cuda&#x27;, index=1)]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/34/output2-3.svg" alt="output2-3"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 34 多GPU训练实现。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 33 单机多卡并行</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/33/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/33/</id>
    <published>2023-12-03T07:00:00.000Z</published>
    <updated>2023-12-03T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="33-单机多卡并行"><a href="#33-单机多卡并行" class="headerlink" title="33 单机多卡并行"></a>33 单机多卡并行</h1><h2 id="单机多卡并行"><a href="#单机多卡并行" class="headerlink" title="单机多卡并行"></a>单机多卡并行</h2><ul><li>一台机器可以安装多个GPU（1-16）</li><li>在训练和预测时，我们将一个小批量计算切分到多个GPU上来达到加速目的</li><li>常用切分方案有<ul><li>数据并行</li><li>模型并行</li><li>通道并行（数据+模型并行）</li></ul></li></ul><h2 id="数据并行-vs-模型并行"><a href="#数据并行-vs-模型并行" class="headerlink" title="数据并行 vs 模型并行"></a>数据并行 vs 模型并行</h2><ul><li>数据并行：将小批量分成n块，每个GPU拿到完整参数计算一块数据的梯度<ul><li>通常性能更好</li></ul></li><li>模型并行：将模型分为n块，每个GPU拿到一块模型计算它的前向和反向结果<ul><li>通常用于模型大到单GPU放不下</li></ul></li></ul><h2 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h2><ol><li><p>读一个数据块</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/33/image-20231203155643023.png" alt="image-20231203155643023"></p></li><li><p>拿回参数</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/33/image-20231203155707785.png" alt="image-20231203155707785"></p></li><li><p>计算梯度</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/33/image-20231203155735339.png" alt="image-20231203155735339"></p></li><li><p>发出梯度</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/33/image-20231203155829103.png" alt="image-20231203155829103"></p></li><li><p>更新梯度</p><p> <img src="https://img.karltan.com/notes-out-class/d2l/33/image-20231203155941151.png" alt="image-20231203155941151"></p></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>当一个模型能用单卡计算时，通常使用数据并行拓展到多卡上</li><li>模型并行则用在超大模型上</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 33 单机多卡并行。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 32 深度学习硬件：TPU和其他</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/32/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/32/</id>
    <published>2023-12-03T05:00:00.000Z</published>
    <updated>2023-12-03T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="32-深度学习硬件：TPU和其他"><a href="#32-深度学习硬件：TPU和其他" class="headerlink" title="32 深度学习硬件：TPU和其他"></a>32 深度学习硬件：TPU和其他</h1><h2 id="更多的芯片"><a href="#更多的芯片" class="headerlink" title="更多的芯片"></a>更多的芯片</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203135437575.png" alt="image-20231203135437575"></p><h2 id="DSP：数字信号处理"><a href="#DSP：数字信号处理" class="headerlink" title="DSP：数字信号处理"></a>DSP：数字信号处理</h2><ul><li>为数字信号处理算法设计：点积，卷积，FFT</li><li>低功耗、高性能<ul><li>比移动GPU快5x，功耗更低</li></ul></li><li>VLIW：Very Long Instruction Word（超长指令字）<ul><li>一条指令计算上百次乘累加</li></ul></li><li>编程和调试困难</li><li>编译器质量良莠不齐</li></ul><h2 id="可编程阵列（FPGA）"><a href="#可编程阵列（FPGA）" class="headerlink" title="可编程阵列（FPGA）"></a>可编程阵列（FPGA）</h2><ul><li>有大量可以编程的逻辑单元和可配置的连接</li><li>可以配置成复杂计算函数<ul><li>编程语言：VHDL，Verilog</li></ul></li><li>通常比通用硬件更高效</li><li>工具链质量良莠不齐</li><li>一次“编译”需要数小时</li></ul><h2 id="AI-ASIC"><a href="#AI-ASIC" class="headerlink" title="AI ASIC"></a>AI ASIC</h2><ul><li>深度学习的热门领域<ul><li>大公司都在造自己的芯片（Intel，Qualcomm，Google，Amazon，Facebook，…）</li></ul></li><li>Google TPU是标志性芯片<ul><li>能够媲美Nvidia GPU性能</li><li>在Google大量部署</li><li>核心是systolic array（专门用于大矩阵乘法）</li></ul></li></ul><h2 id="Systolic-Array"><a href="#Systolic-Array" class="headerlink" title="Systolic Array"></a>Systolic Array</h2><ul><li>计算单元（PE）阵列</li><li>特别适合做矩阵乘法</li><li>设计和制造相对简单</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203142016129.png" alt="image-20231203142016129"></p><h2 id="Systolic-Array的矩阵乘法"><a href="#Systolic-Array的矩阵乘法" class="headerlink" title="Systolic Array的矩阵乘法"></a>Systolic Array的矩阵乘法</h2><script type="math/tex; mode=display">\mathbf{Y} = \mathbf{WX}</script><p>其中：</p><ul><li>$\mathbf{Y}$的形状为$3 \times 2$</li><li>$\mathbf{W}$的形状为$3 \times 3$</li><li>$\mathbf{X}$的形状为$3 \times 2$</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203143555856.png" alt="image-20231203143555856"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203143646466.png" alt="image-20231203143646466"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203143835899.png" alt="image-20231203143835899"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203144055097.png" alt="image-20231203144055097"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203144250206.png" alt="image-20231203144250206"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203144301734.png" alt="image-20231203144301734"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203144317677.png" alt="image-20231203144317677"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203144351603.png" alt="image-20231203144351603"></p><ul><li>对于一般的矩阵乘法，通过切开和填充来匹配SA大小</li><li>批量输入来降低延迟</li><li>通常有其他硬件单元来处理别的NN操作子，例如激活层</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/32/image-20231203145003886.png" alt="image-20231203145003886"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 32 深度学习硬件：TPU和其他。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《POPE：6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/"/>
    <id>http://blog.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/</id>
    <published>2023-11-15T06:00:00.000Z</published>
    <updated>2023-11-16T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="POPE-6-DoF-Promptable-Pose-Estimation-of-Any-Object-in-Any-Scene-with-One-Reference"><a href="#POPE-6-DoF-Promptable-Pose-Estimation-of-Any-Object-in-Any-Scene-with-One-Reference" class="headerlink" title="POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference"></a>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/">论文笔记《POPE：6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/134449576">论文笔记《POPE：6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference》-CSDN博客</a></p><p>论文链接：<a href="https://arxiv.org/abs/2305.15727">[2305.15727] POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference (arxiv.org)</a></p><p>代码链接：<a href="https://github.com/paulpanwang/POPE">paulpanwang/POPE: Welcome to the project repository for POPE (Promptable Pose Estimation), a state-of-the-art technique for 6-DoF pose estimation of any object in any scene using a single reference. (github.com)</a></p><p>项目链接：<a href="https://paulpanwang.github.io/POPE/">POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference (paulpanwang.github.io)</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>尽管在六自由度（6DoF）物体位姿估计方面取得了重大进展，但现有方法在涉及实体代理和下游3D视觉任务的现实场景中的适用性有限。这些限制主要来自3D模型、封闭类别检测（只能检测特定的类别）和大量密集注释支持视图的必要性。为了缓解这个问题，我们提出了一个一般的对象位姿估计范式，称为可提示对象位姿估计（Promptable object pose estimation, POPE）。</p><p>我们所提出的POPE方法能够对任何场景中的任何目标对象进行零样本6DoF对象位姿估计，同时仅采用单个参考作为支持视图。为此，POPE利用了预训练2D基础大模型的强大功能，并采用了一个具有分层特征表示和3D几何原理的框架。此外，在新的视图中，它估计目标提示与目标对象之间的相对相机位姿，实现了双视图和多视图6DoF位姿估计任务。</p><p>综合实验结果表明，POPE在零样本中表现出了无与伦比的鲁棒性能，在LINEMOD[1]和OnePose[2]数据集上，平均位姿误差分别显著降低了52.38%和50.47%。</p><p>我们还对随意捕获的图像进行了更具挑战性的测试，见图1：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231115162836525.png" alt="image-20231115162836525"></p><p>这进一步证明了POPE的鲁棒性。项目页面可以在<a href="https://paulpanwang.github.io/POPE/">https://paulpanwang.github.io/POPE/</a>中找到。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>在众多行业中，机器人系统和增强现实/虚拟现实（AR/VR）应用程序已变得无处不在，它可以促进复杂任务的执行或提供身临其境的用户体验。描述物体的状态，特别是它们的六自由度（6DoF）位姿，是实现深入场景理解和微妙交互的关键一步。更重要的是，考虑到现实世界场景的多样性，必须有一种可以对任意对象进行操作的方法。</p><p>然而，使用简单且易于获得的参考来对看不见的物体进行物体6DoF位姿估计具有挑战性。</p><p>传统的实例级[6, 7, 8, 9, 10, 11, 12]或类别级[13, 14, 15]位姿估计器在处理不同对象时表现出局限性，因为它们是专门为特定实例或类别设计的。</p><p>在测试期间，这些设计原则将它们的泛化能力对于未见过的实例或类别进行了限制，因为它们依赖于CAD模型或定义良好的类别级规范空间。</p><p>后来，人们付出了巨大的努力，通过采用运动结构（SfM [16]）技术[2, 3]、减少支持视图的数量[4]或利用深度图和自监督训练的Vision Transformers[5]来解决上述挑战。图2总结了详细的可视化比较：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231115171618909.png" alt="image-20231115171618909"></p><p>在单支持视图下实现6DoF目标位姿估计的一种简单方法是通过在查询图像和参考图像之间进行2D-2D匹配来估计相对位姿[17, 18]。然而，任意对象上的密集匹配非常不稳定，特别是对于宽基线相机视图或混乱背景。除了图像匹配的困难之外，现实场景中的另一个重大问题是目标对象可能被严重遮挡，从而难以被检测到。以前的方法倾向于针对特定实例/类别采用现成的检测器[19]，或者在小规模数据集上设计基于相关的对象检测器[4]。因此，它们在处理不同场景中的新物体时的鲁棒性得不到保证。</p><p>为了解决开放世界中对任意目标对象的检测和鲁棒2D-2D匹配的障碍，一个很有前途的方法是利用在大规模数据集上训练后的基础模型。最近，社区见证了这些基础模型在少样本甚至零样本泛化方面的新兴特性，从语言[20, 21]到视觉[22, 23, 24]。</p><p>这些进展揭示了未被充分研究的零样本目标位姿估计问题——对目标类别不做任何假设，只使用一幅参考图像的诱人可能性。具体来说，新出现的跨不同图像数据域的分割[24]和非参数实例级识别[22]在解决这些挑战方面显示出了潜力。</p><p>在本文中，我们引入了一个名为Promptable Object Pose Estimation的新任务，以解决在给定的对象提示（每个实例的一张图像，用作支持）和任何新捕获的具有复杂背景（目标）的视角之间估计6DoF对象位姿的挑战。我们提出的模型称为POPE，在一个统一的pipeline中包含四个主要特征：</p><ol><li>Segment Objects对于新视角下的任何图像，生成一组有效的分割建议；</li><li>Retrieve Objects在两个视图中构造对象提示和分割对象建议之间的对象级匹配；</li><li>Pose Objects利用成对目标图像之间的匹配关系来估计相对位姿；</li><li>Online Refinement for Arbitrary View-number可以在新的目标视图上，通过高效的2D-2D全局匹配和2D-3D局部匹配，触发粗-精位姿估计过程。</li></ol><p>我们概述了我们的贡献如下：</p><ul><li>我们建立了一个新的和具有挑战性的任务：可提示的目标位姿估计，旨在估计野外场景下目标的位姿，没有对对象类别的假设，只使用一个参考图像。</li><li>为了解决这个问题，我们提出了一种6DoF位姿基础模型，即POPE，该模型无缝地集成了预训练的基础模型和3D几何原理，用于高质量的分割、分层对象检索和鲁棒图像匹配，以实现在不同和不受控制的环境中进行精确的目标位姿估计。</li><li>为了进行评估，我们引入了一个包含不同数据源的大规模测试数据集。POPE优于现有的可泛化位姿估计器，并在可提示位姿估计和下游3D视觉任务中显示出显著的有效性。</li></ul><h2 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2 Related Works"></a>2 Related Works</h2><p><strong>Large-scale Pre-trained 2D Foundation Models.（大规模预训练2D基础模型。）</strong></p><p>在大规模数据集上训练的模型作为基础模型，并以数据参数平衡的方式表现尺度效应。</p><p>最近，我们发现基础模型[21]表现出了很强的泛化能力，并在广泛的任务[21]中作为基础模型。例如，CLIP[25]利用对比学习来构造文本和图像形态的联合嵌入空间。类似地，像DINO[23]和DINOv2[22]这样的自监督模型具有学习鲁棒视觉特征的新特性。Segment-Anything Model（SAM）[24]演示了提示分割能力，支持具有视觉提示（如点和bounding boxes）的交互式分割。</p><p>在本文中，我们利用基础模型的强大功能来实现可提示物体位姿估计的目标。我们构建了一个融合了SAM和DINO精髓的系统，通过密集分割和实例级匹配来帮助POPE处理混乱的真实场景。</p><p><strong>Generalizable Object Pose Estimator.（可泛化的对象位姿估计器。）</strong></p><p>早期估计对象6DoF位姿的方法建立了实例级[26, 27, 28]或类别级[13, 15, 29, 30, 31, 15, 32, 33, 34, 35, 5]框架。</p><p>它们通常需要特定实例的完美的CAD模型或特定类别的成熟规范空间。因此，这些方法只适用于特定的实例和类别。他们不能泛化到训练中未见过的新实例/类别。根据是否使用三维模型，可归纳的目标位姿估计器的最新进展可分为两大类：</p><ul><li>一类方法通过形状嵌入[36, 37, 38]、模板匹配[39, 40, 41, 42]和渲染对比方法[43, 9, 44, 45]来采用高质量的3D对象；</li><li>另一类方法则是为了避免对三维对象的需求，利用了深度图[46]、对象掩模[47, 46, 48]和参考图像[4, 2, 3]。</li></ul><p>具体来说，Gen6D[4]首先检测目标对象并根据密集参考视图初始化位姿估计。然后，Gen6D使用特征量和3D神经网络细化位姿。OnePose[2]和OnePose++[3]从所有支持视角的RGB序列构建稀疏点云，然后通过将目标视图与稀疏点云进行匹配来确定对象位姿。然而，这些工作仍然需要密集的支持视图，即$\ge 32$个视图，其中每个视图都需要用ground-truth位姿标注。</p><p>我们认为密集支持视图的需求对于真实世界的应用程序是不实际的。为此，我们提出了可提示的位姿估计范式，其中我们只使用一个支持视图作为参考。我们将6DoF目标位姿估计任务转化为目标视图和支持视图中检索到的目标之间的相对位姿估计。因此，我们不需要对物体类别进行任何假设，从而实现了可泛化的物体位姿估计。</p><p><strong>Two-view Object Pose Estimation.（两视角目标位姿估计。）</strong></p><p>估计两视图之间相机相对位姿的方法可以分为两类：</p><ul><li>对应法</li><li>直接位姿回归法</li></ul><p>对应法建立交叉视图像素级对应，通过求解基本矩阵[49]即可恢复位姿。这些方法基于手工制作的特征，如SIFT[50]和SURF[51]，或使用学习的特征[52, 53, 54, 55, 56, 57]来建立对应关系。有些方法还结合了鲁棒估计方法[58]或形状重建和位姿估计之间的协同作用[59]。</p><p>直接位姿回归法以端到端方式学习位姿估计线索[18, 60, 61, 62]。例如，RelPose[18]构建了一个基于能量的框架来处理位姿模糊。8-Point Transformer[61]将8-Point算法的感应偏置引入到Transformer设计中。FORGE[62]利用3D特征量来减轻2D特征学习的模糊性。</p><p>在本文中，由于经典的对应法在新的实例/类别上具有较好的泛化能力，因此我们坚持采用经典的方法。与之前的工作建立图像级对应（支持图像与整个目标图像匹配）[52, 53]不同，我们提出了一个由粗到细的范式。我们首先通过将提示对象（如支持图像所示）与目标图像中的分割对象实例进行匹配来构建实例级对应，这将识别提示对象的高度可能区域。然后在支持图像和目标图像中识别的区域之间建立细粒度的密集对应关系，避免了与杂波背景区域的噪声匹配。</p><h2 id="3-Propmtable-Object-Pose-Estimation-Task"><a href="#3-Propmtable-Object-Pose-Estimation-Task" class="headerlink" title="3 Propmtable Object Pose Estimation Task"></a>3 Propmtable Object Pose Estimation Task</h2><p>可推广的6DoF物体位姿估计器在机器人和3D视觉任务中发挥着至关重要的作用，它可以在不需要微调的情况下准确地确定三维空间中新物体的位置和方向。</p><p>然而，目前的方法[4, 2, 3, 5, 47]存在局限性。它们只能处理使用现成检测器从背景中分离封闭类别物体的情况[2, 3, 5]。此外，由于遮挡、物体外观变化和传感器限制[47]，机器人系统抓取物体所需的支持视图数量通常是不确定的。不仅如此，由于支持视图中对位姿标注[4, 2, 3]或深度图[5]的冗长要求，使其难以向不同场景进行扩展和泛化。这些限制阻碍了现有位姿估计器在不同和不受控制的场景中的部署。</p><p>为了解决这些问题，我们提出将6DoF目标位姿估计问题分解为相对目标位姿估计问题。这种方法减少了对绝对位姿注释的依赖，并允许从双视图扩展到多视图场景。此外，我们引入了一种开放世界检测器，它是类别不可知的，对遮挡和位姿变化具有鲁棒性。</p><h3 id="3-1-Task-Definition"><a href="#3-1-Task-Definition" class="headerlink" title="3.1 Task Definition"></a>3.1 Task Definition</h3><p>我们介绍了一项新颖的任务：Propmtable Object Pose Estimation（POPE）。</p><p>该任务的主要目标是根据一系列（单视角）参考图像提示来估计同一场景图像中所有物体的相对位姿。具体来说，我们的POPE模型接收任意场景图像和任意参考图像序列作为输入。作为输出，POPE同时从场景中检测出所有的对象，并根据参考图像标注出它们的位姿。</p><p><strong>Why Promptable?（为什么是可提示的？）</strong></p><p>对象提示符的使用提供了更高的交互性和灵活性，使最终用户能够通过对象图像或甚至抽象草图等提示表示他们对特定对象的兴趣。可提示的设置消除了对对象大小和形状的预定义类别或假设的依赖，从而产生了更通用的方法，该方法可以应用于任何对象，只要它包含在对象提示集中即可。</p><p><strong>Why Single-View Prompt?（为什么是单视图提示？）</strong></p><p>我们认为，在大多数用户案例中，单图像参考是更受偏爱的。一方面，在野外和网络采集中，对于同一对象，从不同角度捕捉到的一致图像几乎不存在。另一方面，用多个视图估计6DoF位姿需要额外的参考视图校准，这导致了一个先有鸡还是先有蛋的问题。高性能的双视角几何结构也使机器人agent从获取CAD模型中解放出来，并有利于用更少的视图来进行3D重建。</p><p>尽管仅通过一个参考视图来估计位姿是一个具有挑战性的设置，但幸运的是，流行的基础模型可以实现检测和匹配的鲁棒特征表示。此外，单参考位姿估计可以作为多视角几何的起点。</p><p>我们的POPE方法可以无缝集成到多视图渐进重建方法中，从一组未经处理的图像开始，可以持续提高位姿估计和重建精度。</p><h3 id="3-2-Preliminary-of-Two-view-Pose-Estimation"><a href="#3-2-Preliminary-of-Two-view-Pose-Estimation" class="headerlink" title="3.2 Preliminary of Two-view Pose Estimation"></a>3.2 Preliminary of Two-view Pose Estimation</h3><p>在没有三维CAD模型的情况下，从两个独立的图像中估计相对相机的位姿的任务被称为双视图目标的位姿估计。经典几何视觉理论认为，相机的位姿和深度图可以仅通过图像匹配点来计算，而不需要任何附加信息[63]。</p><p>给定一组齐次坐标下的图像匹配点$\mathrm{x}_i$和$\mathrm{x}^\prime_i$，以及已知的摄像机内参矩阵$\mathbf{K}$，两视点目标位姿估计的任务是找到摄像机旋转矩阵$\mathbf{R}$、平移向量$\mathbf{t}$和相应的3D齐次点$\mathbf{X}_i$。目标是对于所有的$i$，满足方程$\mathrm{x}_i = \mathbf{K}[\mathbf{I}|\mathbf{0}]\mathbf{X}_i$和$\mathrm{x}^\prime_i = \mathbf{K}[\mathbf{R}|\mathbf{t}]\mathbf{X}_i$。解决该问题的经典方法包括三个步骤：</p><ol><li>由图像匹配点计算基本矩阵$\mathbf{E}$</li><li>从$\mathbf{E}$中提取相机相对位姿$\mathbf{R}$和$\mathbf{t}$</li><li>并对匹配点进行三角化得到$\mathbf{X}_i$</li></ol><p>基本矩阵可以通过至少5个匹配点来求解[64]，$\mathbf{R}$和$\mathbf{t}$可以通过矩阵分解从$\mathbf{E}$中得到。相对相机位姿估计存在尺度模糊，可以利用全局尺度模糊来计算三维点$\mathbf{X}_i$。</p><h3 id="3-3-Modular-Approach-to-Zero-shot-Promptable-Object-Pose-Estimation"><a href="#3-3-Modular-Approach-to-Zero-shot-Promptable-Object-Pose-Estimation" class="headerlink" title="3.3 Modular Approach to Zero-shot Promptable Object Pose Estimation"></a>3.3 Modular Approach to Zero-shot Promptable Object Pose Estimation</h3><p>在提示图像和包含相同对象的复杂目标之间直接应用双视图图像匹配框架容易失败。这是因为一个复杂的场景可能有许多嘈杂的匹配，特别是当限制到只有两个观察。因此，在本文中，我们提出了一种模块化的方法来解决这个问题，将其分解为多个步骤。首先，我们设计了一个开放世界检测器，用于分割和识别目标图像中被查询的对象提示。接下来，我们与新的视图建立对应关系，改进不正确的对象检索，并解决相对位姿估计的任务。</p><p><strong>Open-world Object Detector.（开放世界对象检测器。）</strong></p><p>在本文中，我们提出了一种鲁棒且通用的检测器，该检测器的条件是基于用户提供的对象提示图像$I_P$和目标视图$I_T$中的图像，而不需要对对象类别做任何假设。所提出的检测器旨在通过使用分割模型[24]的自动对象掩模生成方法，在$I_T$内生成所有$K$个有效掩码$\mathcal{M} = \{m^1, m^2, \cdots, m^K\}$来获得目标视图中匹配的对象掩模，并检索具有最佳全局图像属性的掩码对象图像。</p><p>具体来说，我们在图像格上生成密集的均匀点，为可提示分割模型（SAM）[24]提供提示以得到$\mathcal{M}$，$\mathcal{M}$表示目标分割。</p><p>下一个目标是在给定一个对象提示图像$I_P$和$K$个目标中对象分割的条件下，通过建立对象提示图像$I_P$与掩码对象图像集$\mathcal{I}_T^K = \{I_T^1, I_T^2, \cdots, I_T^K\}$之间的关系来检索目标视图$I_T$中的掩码对象图像。</p><p>然而，我们不能保证图像对具有足够的纹理[65]或足够的图像内容重叠来进行开放世界对象的局部特征匹配。</p><p>受自监督预训练Vision Transformer（ViT）模型[23]最新进展的启发，我们在DINO-v2模型[22]中采用检索增强数据引擎来执行稳健的全局检索。</p><p>在这里，我们利用嵌入的[CLS]标记来捕获全局图像属性，并通过[CLS]标记的内积构造形状为$1 \times K$的余弦相似度矩阵：$\mathcal{S}(P, T, k) = \left\langle CLS_P, CLS_T(k) \right\rangle$，它揭示了提示图像$I_P$和集合$I_T^K$中的第$k$个掩模图像之间的对象关系。通过找到矩阵中的最高分数，我们可以在两个视图中检索同一对象的匹配图像。此外，通过将相似度矩阵放大到$M \times K$，可以很容易地实现从单个提示图像扩展到多个（例如，$M$个提示图像）。</p><p><strong>Hierarchical Retrieval Refinement with Local Descriptors.（使用局部描述符的分层检索细化。）</strong></p><p>然而，尽管在大规模数据集上进行了训练，DINO-v2可能会为具有相似外观的对象生成较高的相似度分数，从而导致错误的全局对象级检索（图3的最后一列）。</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116124724542.png" alt="image-20231116124724542"></p><p>反过来，这会对位姿估计阶段的准确性产生负面影响。为了解决这个问题，我们提出了一种细粒度的方法，该方法结合了本地描述符来增强检索过程并提供更可靠的对象匹配。具体来说，我们利用局部描述符来总结局部视觉模式的相似性，包括边缘、角点和纹理。这些描述符补充了仅从全局表示中获得的潜在错误检索。</p><p>为了实现这种方法，我们考虑DINO-v2生成的Top-K提案，按降序排列相似性分数。然后，当使用自然RGB图像作为提示时，我们使用基于转换器的局部特征估计框架[53]建立图像对应关系。预测置信矩阵$\mathcal{P}_c$表示所有对应关系的匹配概率。</p><p>为了确定匹配的置信度，我们引入基于阈值$\sigma$的置信度标准。在所有的$n$个匹配中，我们选择置信度高于阈值的匹配，并记录这种匹配的数量。此判据定义为：$\text{Criteria} = \frac{1}{n}\sum_{i = 1}^n\mathbb{1}(c_i \ge \sigma)$，其中$c_i$表示第$i$个匹配的置信度得分，$\mathbb{1}$是指示函数，如果参数为真则返回1，否则返回0。在Top-K提议中，标准分数最大的提议被选为最匹配的一对，提供了更可靠的对象位姿估计。</p><p><strong>Pose Estimation.（位姿估计。）</strong></p><p>通过在最佳匹配视图之间建立密集的对应关系，我们继续估计相机的相对位姿。</p><p>这种位姿估计涉及通过匹配描述符、计算基本矩阵以及应用RANSAC来处理异常值并确保结果可靠来确定旋转$\mathbf{R} \in \text{SO}(3)$和平移向量$\mathbf{t} \in \mathbb{R}^3$[64]。</p><p>值得注意的是，我们的方法能够准确地恢复相对旋转。然而，预测的平移是按比例的，类似于其他相对位姿估计器[4, 5]。这种限制是由于当仅考虑两个视图时恢复绝对平移（或对象比例）是一个不适定问题，因为它容易受到比例平移模糊性的影响。</p><p>为了解决这个问题，我们采用PnP算法，并利用未裁剪的支撑视图中提示对象的边界框来恢复平移的比例。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><p>我们首先展示了使用双视图场景在四个不同数据集上实现零样本6DoF对象位姿估计的方法。随后，我们通过评估其分割和检索准确性来验证所提出的开放世界检测器。最后，为了使POPE适应多视图位姿估计并评估多视图位姿的准确性，我们使用额外的输入目标帧来可视化性能，并评估新视图合成任务的位姿。</p><h3 id="4-1-Evaluation-Setup"><a href="#4-1-Evaluation-Setup" class="headerlink" title="4.1 Evaluation Setup"></a>4.1 Evaluation Setup</h3><p><strong>Datasets.（数据集。）</strong></p><p>我们在四个广泛使用的6DoF物体位姿估计数据集上评估我们的方法，以测试POPE的零样本可传递性，而无需任何微调。</p><p><strong>The LINEMOD Dataset[1]</strong>是使用ground-truth CAD模型进行6DoF物体位姿估计的标准基准数据集。LINEMOD数据集由杂乱场景和不同光照条件下的13个低纹理物体的图像组成。</p><p><strong>The YCB-Video Dataset[66]</strong>由21个YCB对象的92段RGBD视频组成，具有适中混乱度的背景，并使用ground-truth CAD模型进行评价。</p><p><strong>The OnePose Dataset[2]</strong>包含约450个真实世界的视频序列，包含150个物体，具有丰富的纹理和简单的背景。每一帧都用相机位姿和3D bounding box标注。</p><p><strong>The OnePose++ Dataset[3]</strong>用40个家用低纹理对象补充了原始OnePose数据集。</p><p>由于不同的数据集会有不同的位姿分布，我们组织并管理了一个位姿在$0^\circ \sim 30^\circ$上均匀分布的测试集。</p><p>总体而言，测试集包含LIMEMOD上的5796对、YCB-Video上的2843对、OnePose上的2751对以及OnePose++上的3166对。</p><p><strong>Model Selection and Baselines.（模型选择和基线。）</strong></p><p>我们将我们提出的POPE方法与其他两种方法进行了比较：</p><ul><li>LoFTR[53]，一种基于图像匹配的方法，直接执行对应匹配以进行位姿估计；</li><li>Gen6D[4]，利用基于相关的网络来寻找对象框、找到位姿初始化并细化相对对象位姿。</li></ul><p>我们排除了OnePose和OnePose++的对比，因为它们无法从单个支持视图生成点云。</p><p>在POPE中，我们利用预训练模型来处理不同的任务：</p><ul><li>具有ViT-H结构的Segment Anything model[24]来生成对象掩码；</li><li>先使用在ViT-S/14的基础上进行预训练，然后再应用DINO-v2算法进行自监督学习得到模型，以此模型来生成对象建议；</li><li>使用在室内场景图像上预训练后的LoFTR模型来实现基于自然图像的图像匹配。</li></ul><p>实验中$\sigma$设为0.9，$K$设为3。</p><p>值得注意的是，对于真实环境中的任何对象，可评估可提示的对象位姿估计并不依赖于有标签的样例（支持图像中的位姿和对象掩码）进行微调。</p><p><strong>Evaluation.（评估。）</strong></p><p>我们遵循相对物体位姿估计的标准做法，报告每对样本的中值误差，以及$15^\circ$和$30^\circ$的精度[5]。准确性指标表示错误低于这些阈值的预测的百分比。在主设计中，我们的评估主要集中在双视图设置上，同时我们提供了下游应用程序的附加结果（多视图位姿估计、新颖的视图合成）。</p><h3 id="4-2-Comparisons"><a href="#4-2-Comparisons" class="headerlink" title="4.2 Comparisons"></a>4.2 Comparisons</h3><p><strong>Results on LINEMOD and YCB-video datasets.（LINEMOD和YCB-video数据集的结果。）</strong></p><p>我们在表1中列出了不同阈值下的总体平均中值误差和位姿精度：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116132951667.png" alt="image-20231116132951667"></p><p>由于篇幅限制，我们在第4.2节中包含了完整的表格，并演示了本节中五个实例的中值误差。从结果中可以明显看出，所提出的POPE在所有指标上始终优于其他方法，在每个实例上都表现出显著的优势。定性结果如图4所示，突出显示了重要的观察结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116133147908.png" alt="image-20231116133147908"></p><p>Gen6D[4]严重依赖于准确的初始化来进行位姿细化，并在单参考场景中陷入困境。LoFTR[53]在处理具有对象遮挡的集群场景时无法提供准确的匹配，从而导致框预测不准确。值得注意的是，对象框的可视化结合了ground-truth转换来解决尺度模糊性。</p><p><strong>Results on OnePose and OnePose++ datasets.（OnePose和OnePose++数据集的结果。）</strong></p><p>除了杂乱场景中包含多个对象的数据集之外，我们还在最近引入的一次性对象位姿估计数据集上评估了所提出的框架。与之前依赖位姿或框注释的方法不同，我们在没有此类注释的情况下进行零样本双视图位姿估计。</p><p>表1中的结果表明，POPE在这两个数据集的相对物体位姿估计任务中实现了较小的中值误差：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116132951667.png" alt="image-20231116132951667"></p><p>随着位姿差距的增加，LoFTR可以通过利用整个图像进行匹配来提高其准确性，结合背景中的更多纹理细节，同时仍然与我们的方法保持一致。</p><p><strong>Scaling from 2-view to Multi-view Promptable Pose Estimation(POPE).（从双视图到多视图的可提示位姿估计（POPE））</strong></p><p>为了满足现实场景中稀疏视图数据集的需求，我们将我们的方法从双视图提示位姿估计（POPE）扩展到适应多视图场景。最初，我们利用从双视图POPE获得的图像匹配结果。我们利用LOFTR[53]的半密集对应关系，使用COLMAP[16]重建半密集点云。</p><p>为了引入新的目标视点，我们随机选择一幅图像，并以可提示的方式进行对象分割。这使我们能够检索对象的身份，并排除混乱背景造成的任何负面影响。然后，将提示图像与新添加的目标图像进行图像匹配，并对其进行配准，提取新图像与半密集点云的对应关系。通过求解PnP算法估计新目标图像的位姿。最后，通过最小化重投影误差更新稀疏点云，并进行反投影得到优化、精确的目标点云和更新的目标位姿。</p><p>为了演示我们方法的可伸缩性，我们通过随机增加视图的数量来可视化性能曲线。图6显示，随着更多的视觉信息的加入，整体精度显著提高：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116134315524.png" alt="image-20231116134315524"></p><p><strong>Novel View Synthesis, an Application of POPE.（新颖的视图合成，POPE的一个应用。）</strong></p><p>我们的下一个目标是验证我们预测的位姿估计的准确性，并证明其在下游应用中的实际适用性。为此，我们采用了从我们的POPE模型中获得的估计的多视图位姿，并结合了一个预训练的可推广的神经辐射场（GNT）[67]。</p><p>具体来说，我们将GNT配置为源视图数为10，并利用ground truth位姿进行源视图翘曲。随后，我们利用POPE模型中估计的位姿，根据获得的POPE位姿生成新的视点。值得注意的是，我们的渲染结果与ground-truth彩色图像非常相似，如图7所示，验证了我们估计位姿的精度：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116140229223.png" alt="image-20231116140229223"></p><p>这些发现为支持我们的位姿估计方法的准确性和可靠性提供了令人信服的证据，为其在各种下游应用中的有效实施铺平了道路。</p><p><strong>Promptable Object Pose Estimation in Arbitrary Scene.（任意场景下的快速物体位姿估计。）</strong></p><p>我们在图8、图9和图10中提供了补充视觉示例，以进一步说明我们的提示6DoF对象位姿估计方法的有效性：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116140411689.png" alt="image-20231116140411689"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116140436959.png" alt="image-20231116140436959"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116140458903.png" alt="image-20231116140458903"></p><p>这种方法利用了包含感兴趣对象的提示图像，我们的算法POPE展示了通过分割和检索过程识别各种类别对象的能力，最终实现相对对象位姿的准确估计。</p><p><strong>Necessity of Open-world Object Detector.（开放世界物体检测器的必要性。）</strong></p><p>具有挑战性的场景，例如杂乱或复杂的背景、遮挡或照明条件的变化，可能对传统的双视图对象检测和位姿估计构成重大挑战，参见表1：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116132951667.png" alt="image-20231116132951667"></p><p>而我们提出的方法利用开放世界对象检测器，不限于特定的类组，通过检索和匹配策略改进了零样本上的泛化性。当使用全局特征表示进行检索时，可能会错误地对不相关的对象进行较大的激活（图11），从而导致后期的6DoF估计不准确：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116140924819.png" alt="image-20231116140924819"></p><p>我们所提出的跨视点对象检索的分层表示（表2）既提高了分割和检索的准确性，也有利于后续的位姿估计：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116141102389.png" alt="image-20231116141102389"></p><p><strong>Quantitative Results on Each Instance.（每个实例的定量结果。）</strong></p><p>我们对不同阈值的平均中值误差和位姿精度进行全面分析。具体来说，我们提出了用于双视图6DoF对象位姿估计的每个实例指标，重点关注具有混乱背景的数据集，即LINEMOD[1]和YCB-Video[66]。表3和表4总结了结果，表明每个实例的准确度和整体准确度都有显着提高：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116141230402.png" alt="image-20231116141230402"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116141242698.png" alt="image-20231116141242698"></p><p>这一观察结果凸显了我们的快速方法在减轻背景杂波的负面影响和大幅提高估计准确性方面的有效性。</p><p>此外，我们在每一个包含复杂纹理[2]和简单纹理[3]单物体场景的数据集中为双视图6DoF物体位姿估计提供了每实例度量。如表5和表6所示，在前景对象分割和检索的帮助下，我们的方法在位姿准确性方面优于其他基于双视图的方法：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116141559057.png" alt="image-20231116141559057"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231116141614422.png" alt="image-20231116141614422"></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>在本文中，我们提出了可提示对象位姿估计（Promptable object pose estimation, POPE），这是一种零样本解决方案，用于仅使用一个支持图像来估计任何场景中的6DoF物体位姿。我们的解决方案强调了利用2D预训练基础模型来提升典型物体位姿估计方法以推广到更实用的范例中的潜力。它采用模块化设计，将可提示的物体位姿估计分解为几个步骤。我们展示了我们提出的解决方案的可扩展性，可以在极端集群场景下使用单个支持图像作为提示，扩展到多个视点，以及对新颖视图合成的验证。未来工作存在几个潜在的方向，包括将大型基础模型提炼为较小的模型以实现实时推理，以及合并来自单目深度估计器的单视图深度信息以提高零样本精度。我们设想，我们的解决方案将使用户能够仅使用少量图像（甚至是稀疏的两张图像）为增强或虚拟现实应用程序生成逼真的3D资产。</p>]]></content>
    
    
    <summary type="html">在任何场景下，对任何对象的6D可提示位姿估计器。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2023" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2023/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="arXiv" scheme="http://blog.karltan.com/tags/arXiv/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 31 深度学习硬件：CPU 和 GPU</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/31/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/31/</id>
    <published>2023-11-06T11:00:00.000Z</published>
    <updated>2023-11-06T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="31-深度学习硬件：CPU-和-GPU"><a href="#31-深度学习硬件：CPU-和-GPU" class="headerlink" title="31 深度学习硬件：CPU 和 GPU"></a>31 深度学习硬件：CPU 和 GPU</h1><h2 id="你的GPU电脑"><a href="#你的GPU电脑" class="headerlink" title="你的GPU电脑"></a>你的GPU电脑</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/31/image-20231106172507638.png" alt="image-20231106172507638"></p><h2 id="Intel-i7-6700K"><a href="#Intel-i7-6700K" class="headerlink" title="Intel i7-6700K"></a>Intel i7-6700K</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/31/image-20231106172826300.png" alt="image-20231106172826300"></p><h2 id="提升CPU利用率-I"><a href="#提升CPU利用率-I" class="headerlink" title="提升CPU利用率 I"></a>提升CPU利用率 I</h2><ul><li>在计算$a + b$之前，需要准备数据<ul><li>主内存$\to$L3$\to$L2$\to$L1$\to$寄存器<ul><li>L1访问延时：0.5ns</li><li>L2访问延时：7ns（14 x L1）</li><li>主内存访问延时：100ns（200 x L1）</li></ul></li></ul></li><li>提升空间和时间的内存本地性（局部性）<ul><li>时间：重用数据使得它们在缓存里</li><li>空间：按序读写数据使得可以预读取</li></ul></li></ul><h2 id="样例分析"><a href="#样例分析" class="headerlink" title="样例分析"></a>样例分析</h2><ul><li>如果一个矩阵是按行存储，访问一行会比访问一列要快<ul><li>CPU一次读取64字节（缓存线）</li><li>CPU会“聪明的”提前读取下一个（缓存线）</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/31/image-20231106173812684.png" alt="image-20231106173812684"></p><h2 id="提升CPU利用率-II"><a href="#提升CPU利用率-II" class="headerlink" title="提升CPU利用率 II"></a>提升CPU利用率 II</h2><ul><li>高端CPU有几十个核<ul><li>EC2 P3.16xlarge: 2 Intel Xeon CPUs, 32物理核</li></ul></li><li>并行来利用所有核<ul><li>超线程不一定提升性能，因为它们共享寄存器</li></ul></li></ul><h2 id="样例分析-1"><a href="#样例分析-1" class="headerlink" title="样例分析"></a>样例分析</h2><ul><li><p><code>for</code>循环代码比加法代码慢：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a)):</span><br><span class="line">    c[i] = a[i] + b[i]</span><br></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c = a + b</span><br></pre></td></tr></table></figure><ul><li><p><code>for</code>循环代码调用n次函数，每次调用有开销</p></li><li><p>加法代码很容易被并行（例如下面C++实现）</p>  <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> omp for</span></span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; a.<span class="built_in">size</span>(); i ++) &#123;</span><br><span class="line">    c[i] = a[i] + b[i]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  可以不同的核算不同的下标范围，然后合并，实现加法的并行</p></li></ul></li></ul><h2 id="Nvidia-Titan-X-Pascal"><a href="#Nvidia-Titan-X-Pascal" class="headerlink" title="Nvidia Titan X (Pascal)"></a>Nvidia Titan X (Pascal)</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/31/image-20231106175256342.png" alt="image-20231106175256342"></p><h2 id="CPU-vs-GPU"><a href="#CPU-vs-GPU" class="headerlink" title="CPU vs GPU"></a>CPU vs GPU</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/31/image-20231106194625926.png" alt="image-20231106194625926"></p><div class="table-container"><table><thead><tr><th style="text-align:center">一般 / 高端</th><th style="text-align:center">CPU</th><th style="text-align:center">GPU</th></tr></thead><tbody><tr><td style="text-align:center"><strong># 核</strong></td><td style="text-align:center">6 / 64</td><td style="text-align:center">2K / 4K</td></tr><tr><td style="text-align:center"><strong>TFLOPS</strong></td><td style="text-align:center">0.2 / 1</td><td style="text-align:center">10 / 100</td></tr><tr><td style="text-align:center"><strong>内存大小</strong></td><td style="text-align:center">32GB / 1TB</td><td style="text-align:center">16GB / 32GB</td></tr><tr><td style="text-align:center"><strong>内存带宽</strong></td><td style="text-align:center">30GB/s / 100GB/s</td><td style="text-align:center">400GB/s / 1TB/s</td></tr><tr><td style="text-align:center"><strong>控制流</strong></td><td style="text-align:center">强</td><td style="text-align:center">弱</td></tr></tbody></table></div><h2 id="提升GPU利用率"><a href="#提升GPU利用率" class="headerlink" title="提升GPU利用率"></a>提升GPU利用率</h2><ul><li>并行<ul><li>使用数千个线程</li></ul></li><li>内存本地性（局部性）<ul><li>缓存更小，架构更加简单</li></ul></li><li>少用控制语句<ul><li>支持有限</li><li>同步开销很大</li></ul></li></ul><h2 id="CPU-GPU带宽"><a href="#CPU-GPU带宽" class="headerlink" title="CPU/GPU带宽"></a>CPU/GPU带宽</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/31/image-20231106200122917.png" alt="image-20231106200122917"></p><p>不要频繁在CPU和GPU之间传数据：带宽限制，同步开销</p><h2 id="更多的CPUs和GPUs"><a href="#更多的CPUs和GPUs" class="headerlink" title="更多的CPUs和GPUs"></a>更多的CPUs和GPUs</h2><ul><li>CPU：AMD，ARM</li><li>GPU：AMD，Intel，ARM，Qualcomm…</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/31/image-20231106200520240.png" alt="image-20231106200520240"></p><h2 id="CPU-GPU高性能计算编程"><a href="#CPU-GPU高性能计算编程" class="headerlink" title="CPU/GPU高性能计算编程"></a>CPU/GPU高性能计算编程</h2><ul><li>CPU：C++或者任何高性能语言<ul><li>编译器成熟</li></ul></li><li>GPU<ul><li>Nvidia上用CUDA<ul><li>编译器和驱动成熟</li></ul></li><li>其他用OpenCL<ul><li>质量取决于硬件厂商</li></ul></li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>CPU：可以处理通用计算。性能优化考虑数据读写效率和多线程</li><li>GPU：使用更多的小核和更好的内存带宽，适合能大规模并行的计算任务</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 31 深度学习硬件：CPU 和 GPU。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 30 第二部分完结竞赛：图片分类</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/30/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/30/</id>
    <published>2023-11-06T09:00:00.000Z</published>
    <updated>2023-11-06T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="30-第二部分完结竞赛：图片分类"><a href="#30-第二部分完结竞赛：图片分类" class="headerlink" title="30 第二部分完结竞赛：图片分类"></a>30 第二部分完结竞赛：图片分类</h1><h2 id="规则"><a href="#规则" class="headerlink" title="规则"></a>规则</h2><ul><li>竞赛地址：<a href="https://www.kaggle.com/c/classify-leaves">Classify Leaves | Kaggle</a></li><li>时间：6月26日直播前1小时</li><li>规则：（跟上次一样）<ul><li>前5名将获得签名书</li><li>自由组队，自由使用算法</li><li>（你可能需要之后几次直播介绍的方法）</li></ul></li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 30 第二部分完结竞赛：图片分类。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 29.2 ResNet为什么能训练出1000层的模型</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/29_2/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/29_2/</id>
    <published>2023-11-03T05:00:00.000Z</published>
    <updated>2023-11-06T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="29-2-ResNet为什么能训练出1000层的模型"><a href="#29-2-ResNet为什么能训练出1000层的模型" class="headerlink" title="29.2 ResNet为什么能训练出1000层的模型"></a>29.2 ResNet为什么能训练出1000层的模型</h1><h2 id="ResNet的梯度计算"><a href="#ResNet的梯度计算" class="headerlink" title="ResNet的梯度计算"></a>ResNet的梯度计算</h2><h3 id="没有残差连接时的梯度计算"><a href="#没有残差连接时的梯度计算" class="headerlink" title="没有残差连接时的梯度计算"></a>没有残差连接时的梯度计算</h3><p>假设神经网络某一层的作用是$f$，那么该层的输出为$y = f(x)$（$x$是上层的输出，该层的输入），与GT相对比，可以得到一个$\ell$，那么梯度为$\frac{\partial\ell}{\partial w}$，参数可以更新为$w = w - \eta\frac{\partial\ell}{\partial w}$。</p><p>再看一层，$y^\prime = g\left(f(x)\right)$，同样可以得到一个$\ell^\prime$，梯度为$\frac{\partial\ell^\prime}{\partial w} = \frac{\partial\ell^\prime}{\partial f(x)}\frac{\partial f(x)}{\partial w}$，那么随着训练的进行，当$y^\prime = g\left(f(x)\right)$层训练较好时，梯度$\frac{\partial\ell^\prime}{\partial f(x)}$会变得很小，这就会导致更新的步长变小，且相较于远离数据一端，靠近数据一端的权重收敛会更快，那么就会加剧$\frac{\partial\ell^\prime}{\partial w}$的减小，从而导致训练的困难。</p><h3 id="有残差连接时的梯度计算"><a href="#有残差连接时的梯度计算" class="headerlink" title="有残差连接时的梯度计算"></a>有残差连接时的梯度计算</h3><p>残差的最简单形式就是直接求和，即$y^{\prime\prime} = g\left(f(x)\right) + f(x) = y^\prime + y$，那么同样进行求导得到$\frac{\partial\ell^{\prime\prime}}{\partial w} = \frac{\partial\ell^\prime}{\partial w} + \frac{\partial\ell}{\partial w}$，这样就可以保证在远离数据一端的参数收敛时，靠近数据一端的梯度不会太小。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/29_2/image-20231106165802264.png" alt="image-20231106165802264"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 29.2 ResNet为什么能训练出1000层的模型。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 29 残差网络 ResNet</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/29/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/29/</id>
    <published>2023-11-02T09:00:00.000Z</published>
    <updated>2023-11-02T11:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="29-残差网络-ResNet"><a href="#29-残差网络-ResNet" class="headerlink" title="29 残差网络 ResNet"></a>29 残差网络 ResNet</h1><h2 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h2><h3 id="加更多的层总是改进精度吗？"><a href="#加更多的层总是改进精度吗？" class="headerlink" title="加更多的层总是改进精度吗？"></a>加更多的层总是改进精度吗？</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/29/image-20231102185334647.png" alt="image-20231102185334647"></p><p>首先，对于上面的两个图形，我们说区域的大小代表的是模型的复杂度，上方的蓝色五角星是最优解，$\mathcal{F}_1 \sim \mathcal{F}_6$是不同的模型，那么：</p><ul><li>对于左侧的模型：<ul><li>由于当$i &lt; j$时，并不满足$\mathcal{F}_i \subset \mathcal{F}_j$的条件，所以我们说更复杂的模型并没有完全包含稍简单的模型，那么我们可以看到的是，当模型越来越复杂时，模型得到的最优解反而离最优解更远了。</li></ul></li><li>对于右侧的模型：<ul><li>由于当$i &lt; j$时，严格满足$\mathcal{F}_i \subset \mathcal{F}_j$的条件，所以复杂模型得到的最优解至少不会比简单模型更差。</li></ul></li></ul><h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><ul><li>串联一个层改变函数类，我们希望能扩大函数类</li><li>残差块加入快速通道（右边）来得到$f(x) = x + g(x)$的结构</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/29/image-20231102190419585.png" alt="image-20231102190419585"></p><h3 id="ResNet块细节"><a href="#ResNet块细节" class="headerlink" title="ResNet块细节"></a>ResNet块细节</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/29/image-20231102190857477.png" alt="image-20231102190857477"></p><h3 id="不同的残差块"><a href="#不同的残差块" class="headerlink" title="不同的残差块"></a>不同的残差块</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/29/image-20231102191215198.png" alt="image-20231102191215198"></p><h3 id="ResNet块"><a href="#ResNet块" class="headerlink" title="ResNet块"></a>ResNet块</h3><ul><li>高宽减半ResNet块（步幅2）</li><li>后接多个高宽不变ResNet块</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/29/image-20231102191338674.png" alt="image-20231102191338674"></p><h3 id="ResNet架构"><a href="#ResNet架构" class="headerlink" title="ResNet架构"></a>ResNet架构</h3><ul><li>类似VGG和GoogLeNet的总体架构</li><li>但替换成了ResNet块</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/29/image-20231102191556865.png" alt="image-20231102191556865"></p><h3 id="GluonCV-Model-Zoo"><a href="#GluonCV-Model-Zoo" class="headerlink" title="GluonCV Model Zoo"></a>GluonCV Model Zoo</h3><p><a href="https://cv.gluon.ai/model_zoo/classification.html">Classification — gluoncv 0.11.0 documentation</a></p><p><img src="https://img.karltan.com/notes-out-class/d2l/29/image-20231102191803706.png" alt="image-20231102191803706"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>残差块使得很深的网络更加容易训练<ul><li>甚至可以训练一千层的网络</li></ul></li><li>残差网络对随后的深层神经网络设计产生了深远影响，无论是卷积类网络还是全连接类网络</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>残差块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels, use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure><p>输入和输出形状一致：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 3, 6, 6])</span><br></pre></td></tr></table></figure><p>增加输出通道数的同时，减半输出的高和宽：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>)</span><br><span class="line">blk(X).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 6, 3, 3])</span><br></pre></td></tr></table></figure><p>ResNet模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(</span><br><span class="line">                Residual(input_channels, num_channels, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>)</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(</span><br><span class="line">                Residual(num_channels, num_channels)</span><br><span class="line">            )</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    b1, b2, b3, b4, b5,</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>观察一下ResNet中不同模块的输入形状是如何变化的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;Output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Sequential Output shape: torch.Size([1, 64, 56, 56])</span><br><span class="line">Sequential Output shape: torch.Size([1, 64, 56, 56])</span><br><span class="line">Sequential Output shape: torch.Size([1, 128, 28, 28])</span><br><span class="line">Sequential Output shape: torch.Size([1, 256, 14, 14])</span><br><span class="line">Sequential Output shape: torch.Size([1, 512, 7, 7])</span><br><span class="line">AdaptiveAvgPool2d Output shape: torch.Size([1, 512, 1, 1])</span><br><span class="line">Flatten Output shape: torch.Size([1, 512])</span><br><span class="line">Linear Output shape: torch.Size([1, 10])</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.014, train acc 0.996, test acc 0.888</span><br><span class="line">1530.1 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/29/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 29 残差网络 ResNet。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 28 批量归一化</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/28/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/28/</id>
    <published>2023-11-02T06:00:00.000Z</published>
    <updated>2023-11-02T07:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="28-批量归一化"><a href="#28-批量归一化" class="headerlink" title="28 批量归一化"></a>28 批量归一化</h1><h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><h3 id="批量归一化-1"><a href="#批量归一化-1" class="headerlink" title="批量归一化"></a>批量归一化</h3><ul><li>损失出现在最后，后面的层训练较快</li><li>数据在最底部<ul><li>底部的层训练较慢（梯度在从顶部回传到底部的过程中会逐渐变小）</li><li>底部层（提取的是简单特征）一变化，所有都得跟着变</li><li>最后的那些层（顶部层，提取的是复杂特征）需要重新学习多次</li><li>导致收敛变慢</li></ul></li><li>我们可以在学习底部层的时候避免变化顶部层吗？</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/28/image-20231102145050373.png" alt="image-20231102145050373"></p><ul><li><p>固定小批量里面的均值和方差（固定批量的分布）：</p><script type="math/tex; mode=display">  \mu_B = \frac{1}{\left|B\right|}\sum_{i \in B}x_i \text{ and } \sigma_B^2 = \frac{1}{\left|B\right|}\sum_{i \in B}(x_i - \mu_B)^2 + \epsilon</script><p>  然后再做额外的调整（可学习的参数）：</p><script type="math/tex; mode=display">  x_{i + 1} = \gamma\frac{x_i - \mu_B}{\sigma_B} + \beta</script><p>  其中，$\gamma$为标准差，$\beta$为均值，且都是可学习的参数。</p><p>  这里是先直接将该批量变成均值为0，方差为1的正态分布，但是实际的分布可能不符合这个分布，所以还有$\gamma$和$\beta$可以进行调整。</p></li><li><p>作用在：</p><ul><li>全连接层和卷积层输出上，激活函数前</li><li>全连接层和卷积层输入上</li></ul></li><li><p>对全连接层，作用在特征维</p><ul><li>输入是一个形状为(batch_size, N)的二维矩阵，每一行是一个样本，每一列是一个特征，那么会对每一个特征（每一列）计算均值和方差，然后根据这个均值和方差，将这个特征的分布变为均值为0，方差为1的正态分布，在此之后，再使用$\gamma$和$\beta$进行调整。</li></ul></li><li><p>对于卷积层，作用在通道维</p><ul><li>对于1x1卷积，输入的形状为(batch_size, H, W, channel)，那么对于某个像素，该位置上所有通道的像素可以构成一个长为channel的向量（想象一根针在某个像素的位置上扎透了所有的通道），我们可以将这个长为channel的向量看作是这个像素的特征，那么这个像素就变成了一个样本，那么对于这个输入来说，样本数为batch_size×H×W，特征数是channel，然后再做和全连接层同样的处理。</li></ul></li></ul><h3 id="批量归一化在做什么？"><a href="#批量归一化在做什么？" class="headerlink" title="批量归一化在做什么？"></a>批量归一化在做什么？</h3><ul><li><p>最初论文是想用它来减少内部协变量转移</p></li><li><p>后续有论文指出它可能就是通过在每个小批量里加入噪音来控制模型复杂度</p><script type="math/tex; mode=display">  x_{i + 1} = \gamma\frac{x_i - \hat{\mu}_B}{\hat{\sigma}_B} + \beta</script><p>  其中$\hat{\mu}_B$是随机偏移，$\hat{\sigma}_B$是随机缩放</p></li><li><p>因此没有必要跟丢弃法混合使用</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放</li><li>可以加速收敛速度，但一般不改变模型精度</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean)**<span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean)**<span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta</span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure><p>创建一个正确的<code>BatchNorm</code>图层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        self.moving_mean = torch.zeros(shape)</span><br><span class="line">        self.moving_var = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.device != X.device:</span><br><span class="line">            self.moving_mean = self.moving_mean.to(X.device)</span><br><span class="line">            self.moving_var = self.moving_var.to(X.device)</span><br><span class="line">        Y, self.moving_mean, self.moving_var = batch_norm(</span><br><span class="line">            X, self.gamma, self.beta, self.moving_mean, self.moving_var,</span><br><span class="line">            eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>应用<code>BatchNorm</code>于LeNet模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(), nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">    BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">    nn.Sigmoid(), nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>在Fashion-MNIST数据集上训练网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.239, train acc 0.912, test acc 0.866</span><br><span class="line">23600.2 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/28/output1.svg" alt="output1"></p><p>拉伸参数<code>gamma</code>和偏移参数<code>beta</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">1</span>].gamma.reshape(-<span class="number">1</span>,), net[<span class="number">1</span>].beta.reshape(-<span class="number">1</span>,)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1.9654, 2.2283, 1.1977, 1.4582, 1.9259, 4.3955], device=&#x27;cuda:0&#x27;,</span><br><span class="line">        grad_fn=&lt;ReshapeAliasBackward0&gt;),</span><br><span class="line"> tensor([-0.5355,  0.2551, -0.7871, -1.2807, -1.0402,  3.8816], device=&#x27;cuda:0&#x27;,</span><br><span class="line">        grad_fn=&lt;ReshapeAliasBackward0&gt;))</span><br></pre></td></tr></table></figure><h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(), nn.Linear(<span class="number">256</span>, <span class="number">120</span>),</span><br><span class="line">    nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">    nn.Sigmoid(), nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">    nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">    nn.Sigmoid(), nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>使用相同超参数来训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.236, train acc 0.913, test acc 0.879</span><br><span class="line">42835.0 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/28/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 28 批量归一化。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 27 含并行连结的网络 GoogLeNet / Inception V3</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/27/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/27/</id>
    <published>2023-11-02T02:00:00.000Z</published>
    <updated>2023-11-02T05:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="27-含并行连结的网络-GoogLeNet-Inception-V3"><a href="#27-含并行连结的网络-GoogLeNet-Inception-V3" class="headerlink" title="27 含并行连结的网络 GoogLeNet / Inception V3"></a>27 含并行连结的网络 GoogLeNet / Inception V3</h1><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><h3 id="最好的卷积层超参数？"><a href="#最好的卷积层超参数？" class="headerlink" title="最好的卷积层超参数？"></a>最好的卷积层超参数？</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102112758365.png" alt="image-20231102112758365"></p><h3 id="Inception块：小学生才做选择题，我全要了"><a href="#Inception块：小学生才做选择题，我全要了" class="headerlink" title="Inception块：小学生才做选择题，我全要了"></a>Inception块：小学生才做选择题，我全要了</h3><p>4个路径从不同层面抽取信息，然后在输出通道维合并。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102113123504.png" alt="image-20231102113123504"></p><h3 id="Inception块"><a href="#Inception块" class="headerlink" title="Inception块"></a>Inception块</h3><p>第一个Inception块，图示通道数：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102114218414.png" alt="image-20231102114218414"></p><p>跟单3x3或5x5卷积相比，Inception块有更少的参数个数和计算复杂度：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">#parameters</th><th style="text-align:center">FLOPS</th></tr></thead><tbody><tr><td style="text-align:center">Inception</td><td style="text-align:center">0.16M</td><td style="text-align:center">128M</td></tr><tr><td style="text-align:center">3x3 Conv</td><td style="text-align:center">0.44M</td><td style="text-align:center">346M</td></tr><tr><td style="text-align:center">5x5 Conv</td><td style="text-align:center">1.22M</td><td style="text-align:center">963M</td></tr></tbody></table></div><h3 id="GoogLeNet-1"><a href="#GoogLeNet-1" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p>5段，9个Inception块：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102114843478.png" alt="image-20231102114843478"></p><h3 id="Stage-1-amp-2"><a href="#Stage-1-amp-2" class="headerlink" title="Stage 1&amp;2"></a>Stage 1&amp;2</h3><p>更小的宽口，更多的通道：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102115314305.png" alt="image-20231102115314305"></p><h3 id="Stage-3"><a href="#Stage-3" class="headerlink" title="Stage 3"></a>Stage 3</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102115722939.png" alt="image-20231102115722939"></p><h3 id="Stage-4-amp-5"><a href="#Stage-4-amp-5" class="headerlink" title="Stage 4&amp;5"></a>Stage 4&amp;5</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102120039701.png" alt="image-20231102120039701"></p><h3 id="Inception有各种后续变种"><a href="#Inception有各种后续变种" class="headerlink" title="Inception有各种后续变种"></a>Inception有各种后续变种</h3><ul><li>Inception-BN(v2) - 使用batch normalization（后面介绍）</li><li>Inception-V3 - 修改了Inception块<ul><li>替换5x5卷积为多个3x3卷积层</li><li>替换5x5卷积为1x7和7x1卷积层</li><li>替换3x3为1x3和3x1卷积层</li><li>更深</li></ul></li><li>Inception-V4 - 使用残差连接（后面介绍）</li></ul><h3 id="Inception-V3块，Stage-3"><a href="#Inception-V3块，Stage-3" class="headerlink" title="Inception V3块，Stage 3"></a>Inception V3块，Stage 3</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102120646964.png" alt="image-20231102120646964"></p><h3 id="Inception-V3块，Stage-4"><a href="#Inception-V3块，Stage-4" class="headerlink" title="Inception V3块，Stage 4"></a>Inception V3块，Stage 4</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102120735771.png" alt="image-20231102120735771"></p><h3 id="Inception-V3块，Stage-5"><a href="#Inception-V3块，Stage-5" class="headerlink" title="Inception V3块，Stage 5"></a>Inception V3块，Stage 5</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102120820790.png" alt="image-20231102120820790"></p><h3 id="GluonCV-Model-Zoo"><a href="#GluonCV-Model-Zoo" class="headerlink" title="GluonCV Model Zoo"></a>GluonCV Model Zoo</h3><p><a href="https://cv.gluon.ai/model_zoo/classification.html">Classification — gluoncv 0.11.0 documentation</a></p><p><img src="https://img.karltan.com/notes-out-class/d2l/27/image-20231102120923092.png" alt="image-20231102120923092"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Inception块用4条有不同超参数的卷积层和池化层的路来抽取不同的信息<ul><li>它的一个主要优点是模型参数小，计算复杂度低</li></ul></li><li>GoogLeNet使用了9个Inception块，是第一个达到上百层的网络<ul><li>后续有一系列改进</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>Inception块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>为了使Fashion-MNIST上的训练短小精悍，我们将输入的高和宽从224降到96：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;Output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Sequential Output shape: torch.Size([1, 64, 24, 24])</span><br><span class="line">Sequential Output shape: torch.Size([1, 192, 12, 12])</span><br><span class="line">Sequential Output shape: torch.Size([1, 480, 6, 6])</span><br><span class="line">Sequential Output shape: torch.Size([1, 832, 3, 3])</span><br><span class="line">Sequential Output shape: torch.Size([1, 1024])</span><br><span class="line">Linear Output shape: torch.Size([1, 10])</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.239, train acc 0.908, test acc 0.891</span><br><span class="line">1140.2 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/27/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 27 含并行连结的网络 GoogLeNet / Inception V3。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>详解位姿估计中的旋转表示</title>
    <link href="http://blog.karltan.com/notes-out-class/detailed-explanation-of-rotation-representation-in-pose-estimation/"/>
    <id>http://blog.karltan.com/notes-out-class/detailed-explanation-of-rotation-representation-in-pose-estimation/</id>
    <published>2023-10-29T12:00:00.000Z</published>
    <updated>2023-10-30T04:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="详解位姿估计中的旋转表示"><a href="#详解位姿估计中的旋转表示" class="headerlink" title="详解位姿估计中的旋转表示"></a>详解位姿估计中的旋转表示</h1><p><a href="https://www.bilibili.com/video/BV1rT4y1r7iW">六维物体位姿估计与追踪：从实例级到类别级 哔哩哔哩 bilibili</a></p><h2 id="旋转矩阵"><a href="#旋转矩阵" class="headerlink" title="旋转矩阵"></a>旋转矩阵</h2><h3 id="绕坐标轴旋转"><a href="#绕坐标轴旋转" class="headerlink" title="绕坐标轴旋转"></a>绕坐标轴旋转</h3><p>我们先从矩阵开始，并且从最简单的二维开始，这是一个简单的单位矩阵，可以表示最基本的平面直角坐标系：</p><script type="math/tex; mode=display">\mathbf{E}= \begin{bmatrix}1 & 0 \\0 & 1\end{bmatrix}</script><p>我们都知道，矩阵代表的是一个变换，假设有如下一个变换$\mathbf{T}$，用矩阵表述如下：</p><script type="math/tex; mode=display">\mathbf{T}= \begin{bmatrix}0 & -1 \\1 & 0\end{bmatrix}</script><p>我们将这个变换作用到$\mathbf{E}$上，即：</p><script type="math/tex; mode=display">\begin{bmatrix}0 & -1 \\1 & 0\end{bmatrix}\begin{bmatrix}1 & 0 \\0 & 1\end{bmatrix}=\begin{bmatrix}0 & -1 \\1 & 0\end{bmatrix}</script><p>那么这个变换的作用就是将整个坐标系逆时针旋转了$90^\circ$。</p><p>那么根据这个旋转角度，我们可以将变换$\mathbf{T}$对应的矩阵重写为（坐标基都是单位向量，可以用逆时针旋转$90^\circ$的例子自行推导这个表述）：</p><script type="math/tex; mode=display">\mathbf{T}= \begin{bmatrix}\cos(90^\circ) & -\sin(90^\circ) \\\sin(90^\circ) & \cos(90^\circ)\end{bmatrix}</script><p>如果是旋转$\theta$度呢？公式如下：</p><script type="math/tex; mode=display">\mathbf{T}= \begin{bmatrix}\cos\theta & -\sin\theta \\\sin\theta & \cos\theta\end{bmatrix}</script><p>那么推广到3D，当我们在xy面内旋转时，z轴是不变的，那么我们也可以将在xy面内旋转$\theta$度称为绕z轴旋转$\theta$度，且该变换对应的矩阵变为：</p><script type="math/tex; mode=display">\begin{bmatrix}\cos\theta & -\sin\theta & 0 \\\sin\theta & \cos\theta & 0 \\0 & 0 & 1\end{bmatrix}</script><p>那么绕y轴旋转对应的矩阵为（注意，在推导公式时假设y轴指向我们，并且绕y轴做逆时针旋转）：</p><script type="math/tex; mode=display">\begin{bmatrix}\cos\theta & 0 & \sin\theta \\0 & 1 & 0 \\-\sin\theta & 0 & \cos\theta\end{bmatrix}</script><p>同理，绕x轴旋转对应的矩阵为：</p><script type="math/tex; mode=display">\begin{bmatrix}1 & 0 & 0 \\0 & \cos\theta & -\sin\theta \\0 & \sin\theta & \cos\theta\end{bmatrix}</script><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>只有三个自由度<ul><li>第一列2个自由度，被长度为1所约束</li><li>第二列1个自由度，被长度为1和垂直于第一列所约束</li><li>第三列没有自由度，直接由前两列叉乘而来</li></ul></li></ul><h2 id="欧拉角"><a href="#欧拉角" class="headerlink" title="欧拉角"></a>欧拉角</h2><p><a href="https://zhuanlan.zhihu.com/p/433131431">3D旋转变换——欧拉角 - 知乎 (zhihu.com)</a></p><h3 id="任意旋转"><a href="#任意旋转" class="headerlink" title="任意旋转"></a>任意旋转</h3><p>需要注意的是，位姿估计中的3D旋转是任意的，不一定需要绕轴，那么该如何表示呢？从而引入欧拉角的概念。</p><p>欧拉角是一种基于三种较简单旋转运动（称为<strong>俯仰Pitch</strong>、<strong>滚动Roll</strong>和<strong>偏航Yaw</strong>）创建一般旋转的机制，如下图：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-rotation-representation-in-pose-estimation/yaw-pitch-roll.gif" alt="yaw-pitch-roll"></p><p>我们假设这个飞机在向x轴正方向飞行：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-rotation-representation-in-pose-estimation/plane.jpg" alt="plane"></p><p>其中绕x轴旋转的角度为$\alpha$，绕y轴旋转的角度为$\beta$，绕z轴旋转的角度为$\gamma$。</p><p>那么将绕x轴旋转对应的矩阵重写为：</p><script type="math/tex; mode=display">R_x(\alpha)= \begin{bmatrix}1 & 0 & 0 \\0 & \cos\alpha & -\sin\alpha \\0 & \sin\alpha & \cos\alpha\end{bmatrix}</script><p>将绕y轴旋转对应的矩阵重写为：</p><script type="math/tex; mode=display">R_y(\beta)= \begin{bmatrix}\cos\beta & 0 & \sin\beta \\0 & 1 & 0 \\-\sin\beta & 0 & \cos\beta\end{bmatrix}</script><p>将绕z轴旋转对应的矩阵重写为：</p><script type="math/tex; mode=display">R_z(\gamma)= \begin{bmatrix}\cos\gamma & -\sin\gamma & 0 \\\sin\gamma & \cos\gamma & 0 \\0 & 0 & 1\end{bmatrix}</script><p>那么最终的旋转可以表示为：</p><script type="math/tex; mode=display">\begin{aligned}R&= R_z(\gamma)R_y(\beta)R_x(\alpha) \\&= \begin{bmatrix}\cos\gamma & -\sin\gamma & 0 \\\sin\gamma & \cos\gamma & 0 \\0 & 0 & 1\end{bmatrix}\begin{bmatrix}\cos\beta & 0 & \sin\beta \\0 & 1 & 0 \\-\sin\beta & 0 & \cos\beta\end{bmatrix}\begin{bmatrix}1 & 0 & 0 \\0 & \cos\alpha & -\sin\alpha \\0 & \sin\alpha & \cos\alpha\end{bmatrix}\end{aligned}</script><h3 id="特点-1"><a href="#特点-1" class="headerlink" title="特点"></a>特点</h3><ul><li>只有3个自由度</li><li>只需要$\alpha$，$\beta$和$\gamma$三个数表示</li></ul><h2 id="轴角"><a href="#轴角" class="headerlink" title="轴角"></a>轴角</h2><p><a href="https://zhuanlan.zhihu.com/p/94584704">做控制要知道的刚体旋转知识（一）轴角法 - 知乎 (zhihu.com)</a></p><p><a href="https://blog.csdn.net/hitgavin/article/details/106713290">12. 机器人正运动学—-姿态描述之轴角（旋转向量）-CSDN博客</a></p><h3 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h3><p>对于空间中两坐标系$xyz$和$x^\prime y^\prime z^\prime$，一定可以找到一个转轴和一个角度，使$xyz$绕这个转轴旋转这个角度后变为$x^\prime y^\prime z^\prime$，这是轴角的基础。</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-rotation-representation-in-pose-estimation/axis-angle.png" alt="axis-angle"></p><p>轴角用四个元素表达旋转，其中的三个元素用来描述旋转轴，另外一个元素描述旋转的角度，如下所示：</p><script type="math/tex; mode=display">(\text{axis}, \text{angle})= \left(\begin{bmatrix}e_x \\e_y \\e_z\end{bmatrix},\theta\right)</script><p>其中单位向量$\mathbf{e} = \begin{bmatrix}e_x \\ e_y \\ e_z\end{bmatrix}$对应的是旋转轴，$\theta$对应的是旋转角度。</p><h3 id="罗德里格斯公式"><a href="#罗德里格斯公式" class="headerlink" title="罗德里格斯公式"></a>罗德里格斯公式</h3><p><del>好好好证明过程我是一点看不懂。</del></p><p>根据罗德里格斯公式，我们可以将旋转矩阵和轴角进行相互转换：</p><ul><li><p>首先得到矩阵$\mathbf{K}$：</p><script type="math/tex; mode=display">  \mathbf{K}  = \begin{bmatrix}  \mathbf{e}  \end{bmatrix}_\times  = \begin{bmatrix}  0 & -e_z & e_y \\  e_z & 0 & -e_x \\  -e_y & e_x & 0  \end{bmatrix}</script></li><li><p>然后直接计算$\mathbf{R}$：</p><script type="math/tex; mode=display">  \mathbf{R} = \mathbf{E} + (\sin\theta)\mathbf{K} + (1 - \cos\theta)\mathbf{K}^2</script></li></ul><h2 id="四元数"><a href="#四元数" class="headerlink" title="四元数"></a>四元数</h2><p><a href="https://www.zhihu.com/tardis/zm/art/97186723">四元数(Quaternions) (zhihu.com)</a></p><p><a href="https://blog.csdn.net/hitgavin/article/details/105656242">11. 机器人正运动学—-姿态描述之四元数_机器人姿态四元数-CSDN博客</a></p><p><strong>我们不需要知道四元数在四维空间中是如何旋转的。</strong></p><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>一个四元数$\mathbf{q}$可以被定义为如下形式，相互等价：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{q}&= (\mathbf{q}_w, \mathbf{q}_v) \\&= q_w + \mathbf{i}q_x + \mathbf{j}q_y + \mathbf{k}q_z \\&= \mathbf{q}_w + \mathbf{q}_v\end{aligned}</script><p>其中：</p><ul><li>$\mathbf{q}_v = \mathbf{i}q_x + \mathbf{j}q_y + \mathbf{k}q_z = (q_x, q_y, q_z)$</li><li>$\mathbf{i}^2 = \mathbf{j}^2 = \mathbf{k}^2 = \mathbf{i}\mathbf{j}\mathbf{k} = -1$</li><li>$\mathbf{i}\mathbf{j} = -\mathbf{j}\mathbf{i} = \mathbf{k}$</li><li>$\mathbf{j}\mathbf{k} = -\mathbf{k}\mathbf{j} = \mathbf{i}$</li><li>$\mathbf{k}\mathbf{i} = -\mathbf{i}\mathbf{k} = \mathbf{j}$</li></ul><p>我们称$q_w$为四元数$\mathbf{q}$的实部，$\mathbf{q}_v$为虚部，$\mathbf{i}$，$\mathbf{j}$和$\mathbf{k}$是虚数单位。</p><p><strong>注意，虚数单位之间的乘法不遵守交换律。</strong></p><p>对于虚部$\mathbf{q}_v$，我们可以施加所有普通向量运算操作，例如，加法，缩放，点乘，叉积等。</p><h3 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h3><h4 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h4><p>根据四元数的定义，可以推导出两个四元数$\mathbf{q}$和$\mathbf{r}$之间的乘法运算：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{q}\mathbf{r}&= (\mathbf{q}_w, \mathbf{q}_v)(\mathbf{r}_w, \mathbf{r}_v) \\&= \left(q_w + \mathbf{i}q_x + \mathbf{j}q_y + \mathbf{k}q_z\right)\left(r_w + \mathbf{i}r_x + \mathbf{j}r_y + \mathbf{k}r_z\right) \\&= q_wr_w - q_xr_x - q_yr_y - q_zr_z \\&\quad + \mathbf{i}\left(q_wr_x + q_xr_w + q_yr_z - q_zr_y\right) \\&\quad + \mathbf{j}\left(q_wr_y - q_xr_z + q_yr_w + q_zr_x\right) \\&\quad + \mathbf{k}\left(q_wr_z + q_xr_y - q_yr_x + q_zr_w\right) \\&= (q_wr_w - \mathbf{q}_v \cdot \mathbf{r}_v) + (\mathbf{q}_v \times \mathbf{r}_v + r_w\mathbf{q}_v + q_w\mathbf{r}_v) \\&= \left(q_wr_w - \mathbf{q}_v \cdot \mathbf{r}_v, \mathbf{q}_v \times \mathbf{r}_v + r_w\mathbf{q}_v + q_w\mathbf{r}_v\right)\end{aligned}</script><h4 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h4><script type="math/tex; mode=display">\begin{aligned}\mathbf{q} + \mathbf{r}&= (\mathbf{q}_w, \mathbf{q}_v) + (\mathbf{r}_w, \mathbf{r}_v) \\&= (\mathbf{q}_w + \mathbf{r}_w, \mathbf{q}_v + \mathbf{r}_v)\end{aligned}</script><h4 id="共轭"><a href="#共轭" class="headerlink" title="共轭"></a>共轭</h4><script type="math/tex; mode=display">\mathbf{q}^\ast = (\mathbf{q}_w, \mathbf{q}_v)^\ast = (\mathbf{q}_w, -\mathbf{q}_v)</script><h4 id="模"><a href="#模" class="headerlink" title="模"></a>模</h4><script type="math/tex; mode=display">n(\mathbf{q}) = \sqrt{\mathbf{q}\mathbf{q}^\ast} = \sqrt{q_w^2 + q_x^2 + q_y^2 + q_z^2}</script><h4 id="单位四元数"><a href="#单位四元数" class="headerlink" title="单位四元数"></a>单位四元数</h4><script type="math/tex; mode=display">\text{identity} = (\mathbf{0}, \mathbf{1})</script><h4 id="逆"><a href="#逆" class="headerlink" title="逆"></a>逆</h4><p>逆必须满足：$\mathbf{q}\mathbf{q}^{-1} = \mathbf{q}^{-1}\mathbf{q} = 1$，从上面模的定义可以推导出：</p><script type="math/tex; mode=display">n(\mathbf{q})^2 = \mathbf{q}\mathbf{q}^\ast \Leftrightarrow \frac{\mathbf{q}\mathbf{q}^\ast}{n(\mathbf{q})^2} = 1</script><p>所以，四元数的逆为：</p><script type="math/tex; mode=display">\mathbf{q}^{-1} = \frac{1}{n(\mathbf{q})^2}\mathbf{q}^\ast</script><h3 id="单位四元数-1"><a href="#单位四元数-1" class="headerlink" title="单位四元数"></a>单位四元数</h3><p>模为1的四元数为单位四元数，即单位四元数$\mathbf{q} = (\mathbf{q}_w, \mathbf{q}_v)$，$n(\mathbf{q}) = 1$，那么$\mathbf{q}$可以写作：</p><script type="math/tex; mode=display">\mathbf{q} = (\cos\theta, \sin\theta\mathbf{n}) = \cos\theta + \sin\theta\mathbf{n}</script><p>其中$\mathbf{n}$是一个三维向量，且$\Vert\mathbf{n}\Vert = 1$，因为当且仅当$\Vert\mathbf{n}\Vert = 1$时，有：</p><script type="math/tex; mode=display">n(\mathbf{q}) = \sqrt{\cos^2\theta + \sin^2\theta\Vert\mathbf{n}\Vert} = \sqrt{\cos^2\theta + \sin^2\theta} = 1</script><h3 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h3><p>对于一个单位四元数，其对空间中的点进行变换有固定的表达形式，假设这个点为$p(p_x, p_y, p_z)$，设单位四元数$\mathbf{q} = (\cos\frac{\theta}{2}, \sin\frac{\theta}{2}\mathbf{n}) = \left[w, (x, y, z)\right]$，那么该四元数可以通过如下的式子对这个点进行旋转：</p><script type="math/tex; mode=display">p^\prime = \mathbf{q}p\mathbf{q}^{-1}</script><p>那么这个操作就是将$p$点绕$\mathbf{n}$轴旋转了$\theta$度得到$p^\prime$。</p><p>根据求逆的公式，可知单位四元数的逆四元数就是其共轭四元数，即：</p><script type="math/tex; mode=display">\mathbf{q}^{-1} = \mathbf{q}^\ast</script><p>在旋转之前，先把$p$点变为纯四元数的形式$p = \left[0, (p_x, p_y, p_z)\right]$，那么有：</p><script type="math/tex; mode=display">\begin{aligned}p^\prime&= \mathbf{q}p\mathbf{q}^{-1} \\&= \mathbf{q}p\mathbf{q}^\ast \\&= (w + x\mathbf{i} + y\mathbf{j} + z\mathbf{k})(p_x\mathbf{i}, p_y\mathbf{j}, p_z\mathbf{k})(w - x\mathbf{i} - y\mathbf{j} - z\mathbf{k}) \\&= \begin{bmatrix}1 - 2y^2 - 2z^2 & 2(xy - wz) & 2(xz + wy) \\2(xy + wz) & 1 - 2x^2 - 2z^2 & 2(yz - wx) \\2(xz - wy) & 2(yz + wx) & 1 - 2x^2 - 2y^2\end{bmatrix}\begin{bmatrix}p_x \\p_y \\p_z\end{bmatrix}\end{aligned}</script><p>那么该四元数对应的旋转矩阵为：</p><script type="math/tex; mode=display">\mathbf{R}= \begin{bmatrix}1 - 2y^2 - 2z^2 & 2(xy - wz) & 2(xz + wy) \\2(xy + wz) & 1 - 2x^2 - 2z^2 & 2(yz - wx) \\2(xz - wy) & 2(yz + wx) & 1 - 2x^2 - 2y^2\end{bmatrix}</script><h3 id="已知旋转矩阵求四元数"><a href="#已知旋转矩阵求四元数" class="headerlink" title="已知旋转矩阵求四元数"></a>已知旋转矩阵求四元数</h3><h4 id="先求-w"><a href="#先求-w" class="headerlink" title="先求$w$"></a>先求$w$</h4><p>将对角元素相加：</p><script type="math/tex; mode=display">r_{11} + r_{22} + r_{33} = 3 - 4x^2 - 4y^2 - 4z^2</script><p>又$w^2 + x^2 + y^2 + z^2 = 1$，所以：</p><script type="math/tex; mode=display">r_{11} + r_{22} + r_{33} = 3 + (4w^2 - 4) = 4w^2 - 1</script><p>那么：</p><script type="math/tex; mode=display">w = \frac{\sqrt{1 + r_{11} + r_{22} + r_{33}}}{2}</script><p>求出$w$后，可求出$x$，$y$和$z$：</p><script type="math/tex; mode=display">\begin{aligned}x &= \frac{r_{32} - r_{23}}{4w} \\y &= \frac{r_{13} - r_{31}}{4w} \\z &= \frac{r_{21} - r_{12}}{4w}\end{aligned}</script><p>如果我们求出的$w$过小，为了避免解的不稳定，需要采用其他的求解方法，我们可以从对角线元素中先求$x$，$y$或者$z$。具体先求哪一个呢？我们当然希望先求大的那一个，因为你会看到，通过对角线元素求出的那个值始终是作为另外三个等式的分母，这个值越大越不容易受到舍入误差的影响。</p><p>具体的比较方法就是比较三个对角线元素哪个大，如果$r_{11}$最大，说明$x$相对来说绝对值最大：</p><script type="math/tex; mode=display">\begin{aligned}r_{11} &= 1 - 2y^2 - 2z^2 = 2 - 2y^2 - 2z^2 - 1 = 2(1 - y^2 - z^2) - 1 = 2(w^2 + x^2) - 1 \\r_{22} &= 2(w^2 + y^2) - 1 \\r_{33} &= 2(w^2 + z^2) - 1\end{aligned}</script><p>这样很明显可以看出当$r_{11}$最大时$x$的绝对值最大，当$r_{22}$最大时$y$的绝对值最大，当$r_{33}$最大时$z$的绝对值最大。</p><h4 id="先求-x"><a href="#先求-x" class="headerlink" title="先求$x$"></a>先求$x$</h4><p>当$r_{11}$最大时，我们就先求$x$，根据归一化四元数元素平方和为1，利用对角线元素我们就可以求解$x$，如下：</p><script type="math/tex; mode=display">x = \frac{\sqrt{1 + r_{11} - r_{22} - r_{33}}}{2}</script><p>然后：</p><script type="math/tex; mode=display">\begin{aligned}w &= \frac{r_{32} - r_{23}}{4x} \\y &= \frac{r_{12} + r_{21}}{4x} \\z &= \frac{r_{13} + r_{31}}{4x}\end{aligned}</script><h4 id="先求-y"><a href="#先求-y" class="headerlink" title="先求$y$"></a>先求$y$</h4><p>当$r_{22}$最大时，我们就先求$y$：</p><script type="math/tex; mode=display">\begin{aligned}y &= \frac{\sqrt{1 - r_{11} + r_{22} - r_{33}}}{2} \\w &= \frac{r_{13} - r_{31}}{4y} \\x &= \frac{r_{12} + r_{21}}{4y} \\z &= \frac{r_{23} + r_{32}}{4y}\end{aligned}</script><h4 id="先求-z"><a href="#先求-z" class="headerlink" title="先求$z$"></a>先求$z$</h4><p>当$r_{33}$最大时，我们就先求$z$：</p><script type="math/tex; mode=display">\begin{aligned}z &= \frac{\sqrt{1 - r_{11} - r_{22} + r_{33}}}{2} \\w &= \frac{r_{21} - r_{12}}{4z} \\x &= \frac{r_{13} + r_{31}}{4z} \\y &= \frac{r_{23} + r_{32}}{4z}\end{aligned}</script>]]></content>
    
    
    <summary type="html">详解位姿估计中的旋转表示。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="位姿估计" scheme="http://blog.karltan.com/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/"/>
    
    <category term="旋转表示" scheme="http://blog.karltan.com/tags/%E6%97%8B%E8%BD%AC%E8%A1%A8%E7%A4%BA/"/>
    
  </entry>
  
  <entry>
    <title>论文总结</title>
    <link href="http://blog.karltan.com/dissertation-notes/dissertation-summary/"/>
    <id>http://blog.karltan.com/dissertation-notes/dissertation-summary/</id>
    <published>2023-10-28T06:00:00.000Z</published>
    <updated>2024-04-11T03:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/dissertation-summary/">论文总结 | Karl的博客 (karltan.com)</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/134484399">【精选】论文总结-CSDN博客</a></p><h2 id="2023"><a href="#2023" class="headerlink" title="2023"></a>2023</h2><h3 id="PoseCNN-A-Convolutional-Neural-Network-for-6D-Object-Pose-Estimation-in-Cluttered-Scenes"><a href="#PoseCNN-A-Convolutional-Neural-Network-for-6D-Object-Pose-Estimation-in-Cluttered-Scenes" class="headerlink" title="PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes"></a>PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/134442631">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019163316117.png" alt="image-20231019163316117"></p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019182446364.png" alt="image-20231019182446364"></p><p><strong>主要贡献</strong>：</p><ol><li>提出了PoseCNN；</li><li>PoseCNN通过定位目标中心和预测中心距离来估计3D平移；</li><li>引入了两个损失函数，<code>PoseLoss</code>和<code>ShapeMatch-Loss</code>，其中<code>ShapeMatch-Loss</code>设计用于处理对称对象，从而让PoseCNN具有处理对称对象的能力；</li><li>提出了一个YCB数据集。</li></ol><p><strong>方法</strong>：</p><ol><li><p>为输入图像中的每个像素预测一个对象标签，做语义分割；</p></li><li><p>然后对于图像中的某一个对象，网络为图像中的每一个像素$\mathbf{p} = (x, y)^\mathrm{T}$回归到三个变量（这里顺便回归出了未被遮挡像素的深度，因为物体中心可能被遮挡，所以不能直接回归物体中心的深度）：</p><script type="math/tex; mode=display"> (x, y) \to \left(n_x = \frac{c_x - x}{\Vert\mathbf{c} - \mathbf{p}\Vert}, n_y = \frac{c_y - y}{\Vert\mathbf{c} - \mathbf{p}\Vert}, T_z\right)</script><p> 然后结合语义分割的结果，对于某个对象，它首先计算图像中每个位置的投票分数。投票分数表明该位置是该对象中心的可能性有多大（如果该对象在图像中多次出现，那么使用非极大值预测，然后选择分数大于某一阈值的位置），在这一步能够得到物体的中心；</p></li><li><p>中心得到后，中心的深度可计算为该类像素的平均深度（这是为了解决中心被遮挡的问题），得到深度后，根据公式可得到3D平移；</p></li><li><p>使用<code>PoseLoss</code>和<code>ShapeMatch-Loss</code>估计3D旋转，其中：</p><ul><li><p><code>PoseLoss</code>表达式为：</p><script type="math/tex; mode=display">  \text{PLOSS}(\tilde{\mathbf{q}}, \mathbf{q}) = \frac{1}{2m}\sum_{\mathbf{x} \in \mathcal{M}}\Vert R(\tilde{\mathbf{q}})\mathbf{x} - R(\mathbf{q})\mathbf{x}\Vert^2</script><p>  可以看到，这是直接计算对应点之间的平均平方距离；</p></li><li><p><code>ShapeMatch-Loss</code>表达式为：</p><script type="math/tex; mode=display">  \text{SLOSS}(\tilde{\mathbf{q}}, \mathbf{q}) = \frac{1}{2m}\sum_{\mathbf{x}_1 \in \mathcal{M}}\min_{\mathbf{x}_2 \in \mathcal{M}}\Vert R(\tilde{\mathbf{q}})\mathbf{x}_1 - R(\mathbf{q})\mathbf{x}_2\Vert^2</script><p>  可以看到，这里计算的不是对应点，而是为某个点选取了一个最近点进行计算，能够一定程度上解决对称物体的旋转问题；</p></li></ul></li><li><p>至此，3D平移和3D旋转估计结束。</p></li></ol><p><strong>备注</strong>：</p><ul><li>此文章中的方法是实例级的；</li><li>只能估计那些已知的对象，即你需要知道待估计物体的CAD模型，不然你不知道物体的正面是哪里，也不知道大小；</li><li>在网络的训练过程中不需要使用深度信息，但是在最后的ICP阶段，如果能够使用深度信息，在精度上会有大幅提升。</li></ul><h3 id="Normalized-Object-Coordinate-Space-for-Category-Level-6D-Object-Pose-and-Size-Estimation"><a href="#Normalized-Object-Coordinate-Space-for-Category-Level-6D-Object-Pose-and-Size-Estimation" class="headerlink" title="Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation"></a>Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/">论文笔记《Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/134443252">论文笔记《Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026163707517.png" alt="image-20231026163707517"></p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026190304830.png" alt="image-20231026190304830"></p><p><strong>主要贡献</strong>：</p><ol><li>可为未见过的对象实例做类别级的6D姿态和尺寸估计；</li><li>提出了Normalized Object Coordinate Space（NOCS），将各个对象归一化到一个标准的空间中；</li><li>提出了一个CNN来得到NOCS。</li></ol><p><strong>方法</strong>：</p><ol><li><p>该CNN建立在基于区域的Mask R-CNN框架上，而Mask R-CNN框架又建立在Faster R-CNN框架上：</p><blockquote><p>Mask R-CNN框架由两个模块组成：</p><ul><li>一个模块提出可能包含对象的区域；</li><li>一个检测和分类区域内的对象；</li><li>还能做语义分割。</li></ul></blockquote><p> 那么这一步可以得到类标签和语义分割；</p></li><li><p>在Mask R-CNN的基础上，还加入了3个头部结构，用于预测NOCS的$x, y, z$分量，从而得到物体的NOCS坐标；</p></li><li><p>使用语义分割的结果和RGB-D相机的深度信息，得到被检测对象的点云信息$P_m$（这个是真实拍摄出来的）；</p></li><li><p>使用NOCS映射，也得到一个点云信息$P_n$，这个点云应该是在原点处的；</p></li><li><p>最后，根据上面的两个点云信息$P_m$和$P_n$，我们可以计算出如何从$P_n$变换到$P_m$，从而得到缩放，旋转和平移。</p></li></ol><p><strong>备注</strong>：</p><ul><li>此文章中的方法是类别级的；</li><li>可以估计那些未知的对象；</li><li>全过程不需要CAD模型。</li></ul><h3 id="POPE-6-DoF-Promptable-Pose-Estimation-of-Any-Object-in-Any-Scene-with-One-Reference"><a href="#POPE-6-DoF-Promptable-Pose-Estimation-of-Any-Object-in-Any-Scene-with-One-Reference" class="headerlink" title="POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference"></a>POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/">论文笔记《POPE：6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/134449576">论文笔记《POPE：6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231115162836525.png" alt="image-20231115162836525"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Fan_et_al-2023-POPE/image-20231115171618909.png" alt="image-20231115171618909"></p><p><strong>主要贡献</strong>：</p><ol><li>提出了可提示的目标位姿估计（Propmtable Object Pose Estimation, POPE），旨在估计野外场景下目标的<strong>相对位姿</strong>，没有对对象类别的假设，只使用一个参考图像；</li><li>提出了一种6DoF位姿基础模型，即POPE，该模型无缝地集成了预训练的基础模型和3D几何原理，用于高质量的分割、分层对象检索和鲁棒图像匹配，以实现在不同和不受控制的环境中进行精确的目标位姿估计。</li></ol><p><strong>方法</strong>：</p><ol><li><p>先使用SAM（Segment-Anything Model）在支持图（我要找的）和目标图（在这里找）上做语义分割；</p></li><li><p>语义分割结束后，在支持图上会生成一个掩码，在目标图上会生成$\mathcal{M}$，$\mathcal{M}$中会有$K$个掩码：$\mathcal{M} = \{m^1, m^2, \cdots, m^K\}$。将支持图记为$I_P$，目标图集合记为$I_T^K$；</p></li><li><p><del>利用掩码的CLS token计算$I_P$和$I_T^K$之间的余弦相似度矩阵，它揭示了提示图像$I_P$和集合$I_T^K$中的第$k$个掩模图像之间的对象关系，通过找到矩阵中的最高分数，我们可以在两个视图中检索同一对象的匹配图像</del>（为了准确度，这里没有使用这种方法计算掩码之间的匹配）；</p></li><li><p>利用DINO-v2生成的Top-K提案，降序排列相似性分数，然后选出最匹配的一对，实现掩码匹配；</p></li><li><p>利用LoFTR在匹配的掩码上进行2D特征点匹配；</p></li><li><p>得到特征点后，然后用双目相机模型估计深度，但是只能估计相对深度：</p><p> <a href="https://blog.csdn.net/m0_46384757/article/details/125036675">【精选】双目视觉原理（万字总结，包含Halcon代码）_双目视觉测量 halcon_happylife_mini的博客-CSDN博客</a></p><p> 深度$Z$的计算公式为：</p><script type="math/tex; mode=display"> Z = \frac{fb}{x_L - x_R}</script><p> 其中：</p><ul><li>$f$为相机焦距（这里假设双目相机两个目的焦距是相同的）；</li><li>$b$是两个相机投影中心连线的距离，也称基线；</li><li>$x_L$是左相机成像点到左成像面的距离；</li><li><p>$x_R$是右相机成像点到左成像面的距离；</p><p>由于$b$是未知的，所以估计出的深度为相对深度；</p></li></ul></li><li><p>得到深度后，也就得到了2D-3D点的匹配，然后利用PnP算法，就可以计算相对位移和绝对旋转（PnP计算旋转时，$b$会消掉）。</p></li></ol><p><strong>备注</strong>：</p><ul><li>只能估计相对位姿；</li><li>没有使用深度信息；</li><li>模型较大；</li><li>首次实现了Any Scene, Any Objects。</li></ul><h2 id="2024"><a href="#2024" class="headerlink" title="2024"></a>2024</h2><h3 id="Triton-An-Intermediate-Language-and-Compiler-for-Tiled-Neural-Network-Computations"><a href="#Triton-An-Intermediate-Language-and-Compiler-for-Tiled-Neural-Network-Computations" class="headerlink" title="Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations"></a>Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2019/tillet_et_al-2019-triton/">论文笔记《Triton：An Intermediate Language and Compiler for Tiled Neural Network Computations》 | Karl的博客 (karltan.com)</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/135374382">论文笔记《Triton：An Intermediate Language and Compiler for Tiled Neural Network Computations》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><strong>主要贡献</strong>：</p><p>我们提出了Triton，一种以图块（tile）概念为中心的语言和编译器，即静态形状的多维子数组。我们的方法围绕：</p><ol><li>基于C语言和基于LLVM的中间表示（IR），用于根据参数图块变量的操作来表达张量程序；</li><li>以及一组新颖的图块级优化过程，用于将这些程序编译成高效的GPU代码。</li></ol><p>我们将演示如何使用Triton来构建矩阵乘法和卷积核的可移植实现，与手工调优的供应商库（cuBLAS/cuDNN）相当，或者高效地实现最近的研究想法，如移位卷积。</p><p><strong>方法</strong>：</p><ul><li><strong>Triton-C</strong>（第3节）：一种类似C的语言，用于根据参数图块变量表达张量程序。该语言的目的是为现有的DNN转编译器（例如PlaidML、Tensor Compressive）和熟悉CUDA的程序员提供稳定的接口。</li><li><strong>Triton-IR</strong>（第4节）：基于LLVM的中间表示（IR），提供适合切片级程序分析、转换和优化的环境。Listing 5显示了修正线性单元（ReLU）函数的Triton-IR代码。这里，Triton-IR程序是在解析过程中直接从Triton-C构建的，但未来也可以探索从嵌入式DSL或更高级别的DNN编译器（例如TVM）自动生成。</li><li><strong>Triton-JIT</strong>（第5节）：即时（Just-In-Time, JIT）编译器和代码生成后端，用于将Triton-IR程序编译为高效的LLVM位代码。这包括：<ol><li>一组图块级、独立于机器的通道，旨在独立于任何编译目标简化输入计算内核；</li><li>一组图块级机器相关通道，用于生成高效的GPU就绪LLVM-IR；</li><li>一个自动调节器，用于优化与上述过程相关的任何元参数。</li></ol></li><li><strong>Numerical Experiments</strong>（第6节）：对Triton的数值评估证明了它的能力：<ol><li>在循环神经网络和Transformer上生成与cuBLAS相当的矩阵乘法实现，并且比替代的DSL快3倍；</li><li>重新实现了cuDNN中用于密集卷积的<code>IMPLICIT_GEMM</code>算法，而不损失性能；</li><li>创建新颖的研究思想的有效实现，例如shift-conv[47]模块。</li></ol></li></ul><p>本文将以对现有相关文献的简要分析作为序言（第2节），并以总结和未来工作方向作为总结（第7节）。</p><p><strong>备注</strong>：</p><p>暂无备注。</p><h3 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2017/vaswani_et_al-2017-attention_is_all_you_need/">论文笔记《Attention Is All You Need》 | Karl的博客 (karltan.com)</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/136089101">论文笔记《Attention Is All You Need》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><img src="https://img.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/image-20240209112650024.png" alt="image-20240209112650024"></p><p><img src="https://img.karltan.com/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/image-20240209115434068.png" alt="image-20240209115434068"></p><p><strong>主要贡献</strong>：</p><ol><li>提出了Transformer，完全摈弃了RNN和CNN；</li><li>完全基于注意力机制来绘制输入和输出之间的全局依赖关系；</li><li>可以并行进行训练，使得训练速度大大加快。</li></ol><p><strong>方法</strong>：</p><ul><li><strong>Embedding</strong>（第3.4节）：<ul><li>进行单词Embedding，这是NLP的基本操作。</li></ul></li><li><strong>Positional Encoding</strong>（第3.5节）：<ul><li>对输入进行位置Encoding，因为Transformer本身不采用RNN的结构，而是使用全局信息，所以不能利用单词的顺序信息，而这部分信息对NLP来说很重要，所以Transformer中使用位置Encoding保存单词在序列中的相对或绝对信息。</li></ul></li><li><strong>Encoder</strong>（第3.1节）：<ul><li>Encoder Block的结构大致为：“多头注意力-残差连接-层归一化-全连接-ReLU-全连接-残差连接-层归一化”，而Encoder是由6个这样的结构堆叠而成的。</li></ul></li><li><strong>Decoder</strong>（第3.1节）：<ul><li>Decoder Block的输入来源于两部分，一部分是输入文本，还有一部分是Encoder的输出。对于输入文本来说，经过“Masked多头注意力-残差连接-层归一化”结构后，会作为$Q$(<code>Query</code>)，输入到一个类似于Encoder的结构中。而对于Encoder的输出来说，会作为$K$(<code>Key</code>)和$V$(<code>Value</code>)，直接输入到这个类似于Encoder的结构中。</li></ul></li><li><strong>Attention</strong>（第3.2节）：<ul><li><strong>Scaled Dot-Product Attention</strong>（第3.2.1节）：<ul><li>在注意力机制中，$Q$(<code>Query</code>)会先和$K$(<code>Key</code>)进行矩阵乘法，然后经过缩放、Mask（可选）和SoftMax操作后，再和$V$(<code>Value</code>)进行矩阵乘法。</li></ul></li><li><strong>Multi-Head Attention</strong>（第3.2.2节）：<ul><li>多头注意力是多个注意力机制组合而成的，多头注意力会对同一个输入执行多次注意力机制的计算，会得到多个结果，然后会将这些结果进行拼接，最后传给一个线性层，得到多头注意力的结果。</li></ul></li></ul></li><li><strong>Position-wise Feed-Forward Networks</strong>（第3.3节）：<ul><li>就是简单的“全连接-ReLU-全连接”的结构。</li></ul></li></ul><p><strong>备注</strong>：</p><ol><li>Transformer本身不能利用单词的顺序信息，因此需要加入Embedding层，否则Transformer会变成一个词袋模型；</li><li>Transformer的重点是Self-Attention结构，其中用到的$Q, K, V$矩阵是由输入$X$分别和$W^Q$，$W^K$和$W^V$进行矩阵乘法得到的。</li></ol><h3 id="NOPE-Novel-Object-Pose-Estimation-from-a-Single-Image"><a href="#NOPE-Novel-Object-Pose-Estimation-from-a-Single-Image" class="headerlink" title="NOPE: Novel Object Pose Estimation from a Single Image"></a>NOPE: Novel Object Pose Estimation from a Single Image</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/">论文笔记《NOPE：Novel Object Pose Estimation from a Single Image》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/137119067">论文笔记《NOPE: Novel Object Pose Estimation from a Single Image》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/image-20240328160810385.png" alt="image-20240328160810385"></p><p><strong>主要贡献</strong>：</p><p>提出了NOPE，该方法在已知对象一张图片的情况下，将对象的单个新图像作为输入，并在不事先了解对象的3D模型的情况下预测该对象在新图像中的相对位姿，并且不需要对新对象和新类别进行训练。</p><p><strong>方法</strong>：</p><p>在每次训练迭代时，构建一个由$N$对图像组成的批，每一对图像由<strong>参考图像</strong>和<strong>已知相对位姿的另一图像</strong>组成。</p><ol><li>计算两张图像的嵌入；</li><li>使用MLP计算另一图像位姿的嵌入；</li><li>将参考图像的嵌入和相对位姿的嵌入输入到U-Net中，U-Net会用这两个输入来预测另一图像的嵌入；</li><li>在U-Net和MLP训练结束后，可以开始预测新图像的嵌入，给定一个参考图像$\mathbf{I}_\mathrm{r}$和$N$个相对位姿$\mathcal{P} = (\Delta R_1, \Delta R_2, \cdots, \Delta R_N)$，可以得到对应的$N$个图像的嵌入$(\mathbf{e}_1, \mathbf{e}_2, \cdots, \mathbf{e}_N)$；</li><li>最后，输入查询图像，在生成的候选结果中执行最近邻搜索，输出最好的结果。</li></ol><p><strong>备注</strong>：</p><p>暂无备注。</p><h3 id="ConvNeXt-V2-Co-designing-and-Scaling-ConvNets-with-Masked-Autoencoders"><a href="#ConvNeXt-V2-Co-designing-and-Scaling-ConvNets-with-Masked-Autoencoders" class="headerlink" title="ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders"></a>ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/">论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/137381266">论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404145621488.png" alt="image-20240404145621488"></p><p><img src="https://img.karltan.com/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/image-20240404155853899.png" alt="image-20240404155853899"></p><p><strong>主要贡献</strong>：</p><p>提出了一个全卷积掩码自编码器框架和一个新的全局响应归一化（GRN）层，该层可以添加到ConvNeXt架构中以增强信道间特征竞争。这种自监督学习技术和架构改进的共同设计产生了一个名为ConvNeXt V2的新模型家族，它显著提高了纯ConvNets在各种识别基准上的性能，包括ImageNet分类、COCO检测和ADE20K分割。我们还提供了各种大小的预训练ConvNeXt V2模型，从在ImageNet上具有76.7% top-1精度的高效3.7 M参数Atto模型，到仅使用公共训练数据即可实现最先进的88.9%精度的650M Huge模型。</p><p><strong>方法</strong>：</p><ul><li><p><strong>Fully Convolutional Masked Autoencoder</strong>（第3节）：</p><p>  该框架在输入时会随机掩蔽输入中的一部分，且框架的任务是预测出被掩蔽的部分；</p><p>  将被掩蔽的图像看作二维稀疏的像素数组，从而使用稀疏卷积促进自编码器的预训练；</p><p>  使用普通的ConvNeXt作为解码器，因为实验表明这种做法效果更好。</p></li><li><p><strong>Global Response Normalization</strong>（第4节）：</p><p>  作者首先在特征空间中对使用FCMAE预训练的ConvNeXt-Base模型进行了分析，发现有很多特征图要么过于饱和，要么是死的，即对特征提取没有帮助。</p><p>  为了解决这个问题，作者提出了GRN，包括如下三个步骤：</p><ol><li>全局特征聚合；</li><li>特征归一化；</li><li><p>特征校准。</p><p>实际的GRN块非常容易实现，最后将GRN合并到原来的ConvNeXt中。</p></li></ol></li></ul><p><strong>备注</strong>：</p><p>暂无备注。</p><h3 id="AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE"><a href="#AN-IMAGE-IS-WORTH-16X16-WORDS-TRANSFORMERS-FOR-IMAGE-RECOGNITION-AT-SCALE" class="headerlink" title="AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE"></a>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h3><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/">论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》 | Karl的博客</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/138014340">论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》-CSDN博客</a></p><p><strong>网络结构图</strong>：</p><p><img src="https://img.karltan.com/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/image-20240416090340705.png" alt="image-20240416090340705"></p><p><strong>主要贡献</strong>：</p><p>提出了ViT，使Transformer能够应用于图像。</p><p><strong>方法</strong>：</p><ol><li><p>将维度为$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$的图像重塑为一系列平坦的2D patch $\mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$，其中$(H, W)$为原始图像的分辨率，$C$为通道数，$(P, P)$为每个图像patch的分辨率，$N = \frac{HW}{P^2}$为得到的patch数，它也作为Transformer的有效输入序列长度；</p></li><li><p>使用下面的计算方式将patch展平并映射到$D$维：</p><script type="math/tex; mode=display"> \mathbf{z}_0 = \left[\mathbf{x}_\text{class};\ \mathbf{x}_p^1\mathbf{E};\ \mathbf{x}_p^2\mathbf{E};\ \cdots;\ \mathbf{x}_p^N\mathbf{E}\right] + \mathbf{E}_{pos}, \quad \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D},\ \mathbf{E}_{pos} \in \mathbb{R}^{(N + 1) \times D}</script><p> 我们将该投影的输出称为patch embedding；</p></li><li><p>在嵌入的patch序列（$\mathbf{z}_0^0 = \mathbf{x}_\text{class}$）上添加一个可学习的嵌入，其在Transformer编码器（$\mathbf{z}_L^0$）输出处的状态作为图像表示$\mathbf{y}$：</p><script type="math/tex; mode=display"> \mathbf{y} = \text{LN}(\mathbf{z}_L^0)</script><p> 在预训练和微调过程中，$\mathbf{z}_L^0$都附加了一个分类头。分类头在预训练时由一个隐藏层的MLP实现，在微调时由一个线性层实现；</p></li><li><p>然后应用Transformer编码器：</p><script type="math/tex; mode=display"> \begin{aligned} \mathbf{z}^\prime_\ell &= \text{MSA}(\text{LN}(\mathbf{z}_{\ell - 1})) + \mathbf{z}_{\ell - 1} & & \ell = 1 \dots L \\ \mathbf{z}_\ell &= \text{MSA}(\text{LN}(\mathbf{z}^\prime_\ell)) + \mathbf{z}^\prime_\ell & & \ell = 1 \dots L \end{aligned}</script></li><li><p>最后的计算流程为：</p><script type="math/tex; mode=display"> \begin{aligned} \mathbf{z}_0 &= \left[\mathbf{x}_\text{class};\ \mathbf{x}_p^1\mathbf{E};\ \mathbf{x}_p^2\mathbf{E};\cdots\ ;\ \mathbf{x}_p^N\mathbf{E}\right] + \mathbf{E}_{pos}, & & \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}, \mathbf{E}_{pos} \in \mathbb{R}^{(N + 1) \times D} \\ \mathbf{z}^\prime_\ell &= \text{MSA}(\text{LN}(\mathbf{z}_{\ell - 1})) + \mathbf{z}_{\ell - 1}, & & \ell = 1 \dots L \\ \mathbf{z}_\ell &= \text{MLP}(\text{LN}(\mathbf{z}^\prime_\ell)) + \mathbf{z}^\prime_\ell, & & \ell = 1 \dots L \\ \mathbf{y} &= \text{LN}(\mathbf{z}^0_L) \end{aligned}</script><p> $\mathbf{y}$即为最后的分类结果。</p></li></ol><p><strong>备注</strong>：</p><p>暂无备注。</p>]]></content>
    
    
    <summary type="html">总结读过的论文。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>详解目标检测中的mAP</title>
    <link href="http://blog.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/"/>
    <id>http://blog.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/</id>
    <published>2023-10-27T07:00:00.000Z</published>
    <updated>2023-10-27T07:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="详解目标检测中的mAP"><a href="#详解目标检测中的mAP" class="headerlink" title="详解目标检测中的mAP"></a>详解目标检测中的mAP</h1><p><a href="https://www.zhihu.com/question/53405779">目标检测中的mAP是什么含义？ - 知乎 (zhihu.com)</a></p><h2 id="AP和mAP"><a href="#AP和mAP" class="headerlink" title="AP和mAP"></a>AP和mAP</h2><p>AP：PR曲线下面积</p><p>mAP：mean Average Precision，即各类别AP的平均值</p><h2 id="TP、FP、FN和TN"><a href="#TP、FP、FN和TN" class="headerlink" title="TP、FP、FN和TN"></a>TP、FP、FN和TN</h2><p>TP - True Positive：$\text{IoU} &gt; \text{IoU}_{\text{threshold}}$的检测框数量（同一GT只计算一次）</p><p>FP - False Positive：$\text{IoU} \le \text{IoU}_{\text{threshold}}$的检测框数量，或者是检测到同一个GT的多余检测框的数量</p><p>FN - False Negative：没有检测到的GT的数量</p><p>TN - True Negative：在mAP评价指标中不会使用到</p><h2 id="查准率和查全率"><a href="#查准率和查全率" class="headerlink" title="查准率和查全率"></a>查准率和查全率</h2><p>查准率 - Precision：$\frac{\text{TP}}{\text{TP} + \text{FP}}$，所有预测为正例的样本中，真的为正样本的比例</p><p>查全率 - Recall：$\frac{\text{TP}}{\text{TP} + \text{FN}}$，正样本被挑出来的比例</p><p>二者绘制的曲线称为P-R曲线：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/image-20231027145922162.png" alt="image-20231027145922162"></p><h2 id="交并比-Intersection-Over-Union（IOU）"><a href="#交并比-Intersection-Over-Union（IOU）" class="headerlink" title="交并比 - Intersection Over Union（IOU）"></a>交并比 - Intersection Over Union（IOU）</h2><p>交并比（IOU）是度量两个检测框（对于目标检测来说）的交叠程度，公式如下：</p><script type="math/tex; mode=display">\text{IoU} = \frac{\text{area}(B_p \cap B_{gt})}{\text{area}(B_p \cup B_{gt})}</script><p>$B_{gt}$代表的是目标实际的边框（Ground Truth，GT），$B_p$代表的是预测的边框，通过计算这两者的IoU，可以判断预测的检测框是否符合条件，IoU用图片展示如下：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/IoU.png" alt="IoU"></p><h2 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h2><p>下面用一个例子说明AP和mAP的计算：</p><p>先规定两个公式，一个是Precision，一个是Recall，这两个公式同上面的一样，我们把它们扩展开来，用另外一种形式进行展示，其中<code>all detctions</code>代表所有预测框的数量，<code>all ground truths</code>代表所有GT的数量：</p><script type="math/tex; mode=display">\begin{array}{ll}\text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{\text{TP}}{\text{all detctions}} \\\text{Recall} &= \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{\text{TP}}{\text{all ground truths}}\end{array}</script><p>AP是计算某一类P-R曲线下的面积，mAP则是计算所有类别P-R曲线下面积的平均值。</p><p>假设我们有7张图片（Image1 - Image7），这些图片有15个目标（绿色的框，GT的数量，上文提及的<code>all ground truths</code>）以及24个预测边框（红色的框，A - Y编号表示，并且有一个置信度值）：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/images.png" alt="images"></p><p>根据上图以及说明，我们可以列出以下表格，其中Images代表图片的编号，Detections代表预测边框的编号，Confidences代表预测边框的置信度，TP or FP代表预测的边框是标记为TP还是FP（认为预测边框与GT的IoU值大于等于0.3就标记为TP；若一个GT有多个预测边框，则认为IoU最大且大于等于0.3的预测框标记为TP，其他的标记为FP，即一个GT只能有一个预测框标记为TP），<strong>这里的0.3是随机取的一个值</strong>。</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/table1.png" alt="table1"></p><p>通过上表，我们可以绘制出P-R曲线（因为AP就是P-R曲线下面的面积），但是在此之前我们需要计算出P-R曲线上各个点的坐标，根据置信度从大到小排序所有的预测框，然后就可以计算Precision和Recall的值，见下表（需要记住一个叫<strong>累加的概念，就是下图的ACC TP和ACC FP</strong>）：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/table2.png" alt="table2"></p><p>其中：</p><ul><li><p>标号为1的Precision和Recall的计算方式：</p><script type="math/tex; mode=display">  \begin{array}{ll}  \text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{1}{1 + 0} = 1 \\  \text{Recall} &= \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{\text{TP}}{\text{all ground truths}} = \frac{1}{15} = 0.0666  \end{array}</script></li><li><p>标号2：</p><script type="math/tex; mode=display">  \begin{array}{ll}  \text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{1}{1 + 1} = 0.5 \\  \text{Recall} &= \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{\text{TP}}{\text{all ground truths}} = \frac{1}{15} = 0.0666  \end{array}</script></li><li><p>标号3：</p><script type="math/tex; mode=display">  \begin{array}{ll}  \text{Precision} &= \frac{\text{TP}}{\text{TP} + \text{FP}} = \frac{2}{2 + 1} = 0.6666 \\  \text{Recall} &= \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{\text{TP}}{\text{all ground truths}} = \frac{2}{15} = 0.1333  \end{array}</script></li><li><p>依此类推</p></li></ul><p>然后就可以绘制出P-R曲线：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/P-R1.png" alt="P-R1"></p><p>得到P-R曲线就可以计算AP（P-R曲线下的面积），要计算P-R下方的面积，一般使用的是插值的方法，取11个点<strong>[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]</strong>的插值所得：</p><p><img src="https://img.karltan.com/notes-out-class/detailed-explanation-of-mAP-in-object-detection/P-R2.png" alt="P-R2"></p><p>得到一个类别的AP结果如下：</p><script type="math/tex; mode=display">\begin{aligned}AP&= \frac{1}{11}\sum_{\text{recall}}\text{precision} \\&= \frac{1}{11}(1 + 0.6666 + 0.4285 + 0.4285 + 0.4285 + 0 + 0 + 0 + 0 + 0 + 0) \\&= 26.84\%\end{aligned}</script><p>要计算mAP，就把所有类别的AP计算出来，然后求取平均即可。</p>]]></content>
    
    
    <summary type="html">详解目标检测中的mAP。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="目标检测" scheme="http://blog.karltan.com/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
    <category term="mAP" scheme="http://blog.karltan.com/tags/mAP/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/"/>
    <id>http://blog.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/</id>
    <published>2023-10-25T15:00:00.000Z</published>
    <updated>2023-10-28T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Normalized-Object-Coordinate-Space-for-Category-Level-6D-Object-Pose-and-Size-Estimation"><a href="#Normalized-Object-Coordinate-Space-for-Category-Level-6D-Object-Pose-and-Size-Estimation" class="headerlink" title="Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation"></a>Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/">论文笔记《Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation》 | Karl的博客 (karltan.com)</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/134443252">论文笔记《Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation》-CSDN博客</a></p><p>论文链接：<a href="https://arxiv.org/abs/1901.02970">[1901.02970] Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation (arxiv.org)</a></p><p>代码链接：<a href="https://github.com/hughw19/NOCS_CVPR2019">hughw19/NOCS_CVPR2019: [CVPR2019 Oral] Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation on Python3, Tensorflow, and Keras (github.com)</a></p><p>项目链接：<a href="https://geometry.stanford.edu/projects/NOCS_CVPR2019/">Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation, CVPR 2019 (Oral) (stanford.edu)</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文的目标是估计RGB-D图像中未见过物体的6D位姿和尺寸。与“实例级”6D姿态估计任务相反，我们的问题假设在训练或测试期间没有精确的对象CAD模型可用。</p><p>为了处理给定类别中不同且未见过的对象实例，我们引入了<strong>标准化对象坐标空间（NOCS）</strong>——一个类别中所有可能对象实例的共享规范表示。然后，我们基于区域的神经网络被训练用来直接推断从观察到的像素到这个共享对象表示（NOCS）的对应关系，以及其他对象信息，如类标签和实例掩码。这些预测可以与深度图结合，共同估计在混乱场景中多个物体的度量6D姿态和大小。</p><p>为了训练我们的网络，我们提出了一种新的上下文感知技术来生成大量有标签的混合现实数据。为了进一步改进我们的模型并评估其在真实数据上的性能，我们还提供了一个具有多个环境和实例变化的有标签真实数据集。</p><p>大量的实验表明，该方法能够稳健地估计真实环境中未见过对象的姿态和大小，同时在标准6D姿态估计基准上实现最先进的性能。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>在虚拟和增强现实（AR）、机器人技术和3D场景理解中，目标检测和3D位置、方向和大小的估计是一个重要的需求。这些应用程序需要在新的环境中进行操作，这些环境可能包含以前未见过的对象实例。</p><p>过去的工作已经探索了实例级6D位姿估计问题[37, 46, 27, 51, 6, 28]，其中精确的CAD模型和它们的尺寸是预先可得的。</p><p>不幸的是，这些技术不能在一般情况下使用，因为大多数对象以前从未见过，也没有已知的CAD模型。</p><p>另一方面，类别级3D对象检测方法[43, 36, 9, 34, 49, 12]可以估计出对象类别标签和3D bounding boxes，而不需要精确的CAD模型。然而，估计的3D bounding boxes依赖于视角，方法中并不会编码对象的精确方向。</p><p>因此，这两类方法都不能满足需要未见过对象6D姿态和3个非均匀尺度参数（编码维度）应用程序的需要。</p><p>在本文中，我们的目标是通过介绍第一个多对象的<strong>类别级6D位姿和大小估计</strong>的方法来弥合这两类方法之间的差距，据我们所知，这是一个对新对象实例具有挑战性的问题。</p><p>由于我们不能对未见过的物体使用CAD模型，<strong>第一个挑战</strong>是找到一种表示方式，允许在特定类别中定义不同物体的6D位姿和大小。<strong>第二个挑战</strong>是无法获得用于训练和测试的大规模数据集。像SUN RGB-D[41]或NYU v2[40]这样的数据集缺乏精确的6D姿态和大小的标签，或者不包含桌面尺度对象类别——确切地说是在桌面或桌面操作任务中出现的对象类型，对于这种类型的对象来说，知道6D姿态和大小将是有用的。</p><p><strong>为了解决表示的挑战</strong>，我们将这个问题表述为在共享对象描述空间中寻找对象像素与归一化坐标之间的对应关系（参见第3节）。我们定义了一个称为<strong>归一化对象坐标空间（NOCS）</strong>的共享空间，其中所有对象都包含在一个公共归一化空间中，一个类别中的所有实例方向都是一致的。这使6D姿态和大小估计成为可能，即使是未见过的对象实例。</p><p>我们的方法的核心是一个卷积神经网络（CNN），它从单个RGB图像中联合估计对象类、实例掩码和多个对象的NOCS映射。直观地，NOCS映射通过预测目标像素和NOCS之间的密集对应来捕获目标可见部分的归一化形状。我们的CNN通过<strong>像素回归</strong>或<strong>分类问题</strong>来估计NOCS映射。</p><p>然后，NOCS映射与深度图一起使用，以使用位姿拟合方法来估计对象的全度量6D位姿和大小。</p><p><strong>为了解决数据挑战</strong>，我们引入了一种空间上下文感知的混合现实方法来自动生成大量的数据（275K训练，25K测试），这些数据由ShapeNetCore[8]中逼真的合成对象与真实的桌面场景组合而成。这种方法允许自动生成具有混乱对象的真实数据，并为类标签、实例掩码、NOCS映射、6D位姿和大小提供完整的GT标签。</p><p>我们还提供了<strong>一个用于训练和测试的真实世界数据集</strong>，该数据集中有6个对象类别，有18个不同场景，共计42个独特的实例，并且为每个实例提供了6D位姿和大小的GT标签。</p><p>据我们所知，我们的数据集是最大和最全面的，用于6D位姿和尺寸，以及3D对象检测任务的训练和测试数据集。</p><p>我们的方法使用来自商用RGB-D传感器的输入，设计用于处理对称和非对称对象，使其适用于许多应用程序。</p><p>图1显示了我们的方法在一个桌面场景上操作的例子，在训练期间有多个对象未见过：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026141109178.png" alt="image-20231026141109178"></p><p>综上所述，本工作的主要贡献有：</p><ul><li>标准化对象坐标空间（NOCS），一个统一的共享空间，允许不同但相关的对象有一个共同的参考框架，以便对未见过的对象进行6D位姿和大小估计。</li><li>一种联合预测RGB图像中多个未见过对象的类标签、实例掩码和NOCS映射的CNN。在位姿拟合算法中，我们使用NOCS映射和深度图来估计对象的全度量6D位姿和尺寸。</li><li>数据集：一种空间上下文感知的混合现实技术，在真实图像中去合成合成对象，允许我们生成一个大规模的有标签数据集来训练我们的CNN。我们还为训练和测试提供了完整标签的真实世界数据集。</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>在本节中，我们重点回顾了基于RGB-D图像的类别级3D目标检测、实例级6D位姿估计、类别级4 DoF位姿估计以及不同数据生成策略的相关工作。</p><h3 id="Category-Level-3D-Object-Detection"><a href="#Category-Level-3D-Object-Detection" class="headerlink" title="Category-Level 3D Object Detection"></a>Category-Level 3D Object Detection</h3><p>预测物体的6D姿态和大小的挑战之一是在场景中定位物体并找到它们的物理尺寸，这可以表述为一个3D检测问题[54, 22, 21, 31, 14]。值得注意的尝试包括[43, 55]，他们将3D体积数据作为输入，直接在3D中检测物体。另一种思路[36, 20, 10, 29]提出先在2D图像中生成2D对象推荐，然后将该推荐投影到3D空间中，进一步细化最终的3D bounding box位置。上面描述的技术达到了令人印象深刻的3D检测率，但不幸的是，仅仅关注于寻找物体的边界体积，而不能预测物体的6D姿态。</p><h3 id="Instance-Level-6-DoF-Pose-Estimation"><a href="#Instance-Level-6-DoF-Pose-Estimation" class="headerlink" title="Instance-Level 6 DoF Pose Estimation"></a>Instance-Level 6 DoF Pose Estimation</h3><p>考虑到它的实际重要性，有大量的工作集中在实例级6D位姿估计。</p><p>在这里，任务是推断对象的3D位置和3D旋转（无比例），假设在训练期间可以获得这些对象的确切3D CAD模型和大小。目前的SOTA可以大致分为<strong>模板匹配或对象坐标回归技术</strong>。</p><p><strong>模板匹配技术</strong>通过迭代最近点[4, 53]等算法将GT 3D CAD模型与预测到的3D点云进行对齐，或者使用手工制作的局部描述符进一步指导对齐过程[26, 11]。这类技术经常遭受对象间和对象内的遮挡，这是我们只有部分扫描对象时的典型情况。</p><p>第二类是基于<strong>对象坐标回归</strong>的方法，目的是回归每个物体像素对应的物体表面位置。这些技术已成功应用于身体姿态估计[45, 18]，相机重定位[39, 48]和6D目标姿态估计[5]。</p><p>上述两种方法都需要在训练和测试过程中建立<strong>精确</strong>的3D模型。除了在测试时存储所有3D CAD模型或学习过的物体坐标回归器的实际限制外，捕获大量物体的高保真和完整的3D模型是一项具有挑战性的任务。</p><p>虽然我们的方法受到了对象坐标回归技术的启发，但它也与上面的方法有很大的不同，因为我们在测试时不再需要完整和高保真的对象3D CAD模型。</p><h3 id="Category-Level-4-DoF-Pose-Estimation"><a href="#Category-Level-4-DoF-Pose-Estimation" class="headerlink" title="Category-Level 4 DoF Pose Estimation"></a>Category-Level 4 DoF Pose Estimation</h3><p>有一些关于类别级姿态估计的研究[20, 42, 19, 35, 7]，但它们都做出了简化的假设。</p><p><strong>首先</strong>，这些算法将旋转预测限制为仅沿重力方向（只有4个自由度）。</p><p><strong>其次</strong>，他们关注一些大房间尺度的物体类别（例如，椅子、沙发、床或汽车），而没有考虑到物体的对称性[20, 42, 19]。相反，我们估计了各种手尺度物体的姿态，由于姿态变化较大，手尺度物体的姿态往往比室内尺度物体更具挑战性。我们的方法也可以在不假设物体重力方向的情况下预测完整的6D位姿和大小。</p><p>最后，我们的方法以交互帧率运行（每帧0.5秒），这比其他方法（[20]为每帧约70秒，[42]为每帧25分钟）快得多。</p><h3 id="Training-Data-Generation"><a href="#Training-Data-Generation" class="headerlink" title="Training Data Generation"></a>Training Data Generation</h3><p>训练CNNs的一个主要挑战是缺乏足够的类别个数、实例个数、姿态种类、混乱程度和光照变化的训练数据。</p><p>为了构建包含对象标签的真实数据集（例如，[40, 41, 50]），人们已经做出了一些努力。不幸的是，这些数据集往往相对较小，这主要是由于与GT标签相关的高成本（时间和金钱）。这个限制是其他工作（例如，[35, 44, 51]）的动力，这些工作生成的数据完全是合成的，这允许以较小的成本生成大量具有完美标签的训练数据。</p><p>为了简单起见，所有这些数据集都忽略了一些因素的组合（材料、传感器噪声和照明），这些因素会在合成数据和真实数据分布之间造成事实上的领域差距。</p><p>为了缩小这种差距，[13]生成了混合真实数据和合成数据的数据集，方法是在真实背景上呈现虚拟对象。虽然背景是真实的，但渲染的对象是飞行在半空中的，并且是脱离上下文的[13]，这阻止算法利用重要的上下文线索。</p><p>我们引入了一种新的混合真实的方法，以上下文感知的方式自动生成大量的由合成物体和真实背景组成的数据，使其更加真实。</p><p>这得到了实验的支持，表明我们的上下文感知训练数据能够使模型更好地泛化到真实世界测试数据。</p><p>我们还提供了一个真实世界的数据集，以进一步改善学习和评估。</p><h2 id="3-Background-and-Overview"><a href="#3-Background-and-Overview" class="headerlink" title="3. Background and Overview"></a>3. Background and Overview</h2><h3 id="Category-Level-6D-Object-Pose-and-Size-Estimation"><a href="#Category-Level-6D-Object-Pose-and-Size-Estimation" class="headerlink" title="Category-Level 6D Object Pose and Size Estimation"></a>Category-Level 6D Object Pose and Size Estimation</h3><p>我们关注对象实例的3个旋转、3个平移和3个缩放参数（维度）的估计问题。</p><p>此问题的解决方案可以可视化为围绕对象的一个紧密定向的bounding box，参见图1：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026141109178.png" alt="image-20231026141109178"></p><p>虽然图像中的这些对象之前没有见过，但是这些对象来自于训练过程中的已知对象类别（如相机），并且在训练过程中，模型会见到这些类别中的若干个样本。</p><p>这项任务特别具有挑战性，因为我们不能在测试时使用CAD模型，而且6D位姿对未见过的物体没有明确定义。为了克服这个问题，我们提出了一种新的表示方式，它定义了一个共享的对象空间，可以定义未见过的对象的6D位姿和大小。</p><h3 id="Normalized-Object-Coordinate-Space-NOCS"><a href="#Normalized-Object-Coordinate-Space-NOCS" class="headerlink" title="Normalized Object Coordinate Space (NOCS)"></a>Normalized Object Coordinate Space (NOCS)</h3><p>NOCS是一个在空间中的单位立方体，即：$\{x, y, z\} \in [0, 1]$。</p><p>给定每个类别的已知对象CAD模型的形状集合，通过统一缩放对象，我们将其大小归一化，使其bounding box的对角线长度收缩为1（？），并在NOCS空间中居中，参见图2：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026155511223.png" alt="image-20231026155511223"></p><p>此外，我们让同一类别对象的中心和方向一致。</p><p>我们使用ShapeNetCore[8]中的模型，这些模型已经对比例、位置和方向进行了规范化。图2显示了<strong>相机</strong>类别中规范化形状的示例。我们的表示允许形状的每个顶点在NOCS（图2中的颜色编码）中表示为一个元组$(x, y, z)$。</p><p><strong>我们的CNN预测彩色编码的NOCS坐标的2D透视投影，即NOCS映射（图2左下角）。</strong></p><p><strong>注意，进行2D透视投影之后，像素的坐标仍然是三维的：</strong></p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/project.png" alt="project"></p><p><strong>图中右侧的竖线是图像平面，$(x, y, z)$是点在空间中的位置，$(x^\prime, y^\prime, z^\prime)$是点在图像平面中的位置。</strong></p><p><strong>那么预测的坐标也是一个三维坐标。</strong></p><p>有多种方式来解释NOCS映射：</p><ol><li>作为在NOCS中观测到的物体部分的<strong>形状重建</strong></li><li>作为密集的像素-NOCS对应</li></ol><p>我们的CNN学习对未见过的物体进行形状预测，或者在大的形状集合上训练，学习去预测物体的像素-NOCS对应。这种表示比其他方法（例如bounding boxes）更健壮，因为即使对象只有部分可见，我们也可以操作。</p><h3 id="Method-Overview"><a href="#Method-Overview" class="headerlink" title="Method Overview"></a>Method Overview</h3><p>图3说明了我们使用RGB图像和深度图作为输入的方法：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026163707517.png" alt="image-20231026163707517"></p><p>我们的CNN仅从RGB图像估计图像中对象的类标签、实例掩码（就是语义分割）和NOCS映射。</p><p>除此之外，我们没有在CNN中使用深度图，因为我们想利用现有不包含深度的RGB数据集，如COCO数据集来提高性能。</p><p>NOCS映射可以在一个标准化的空间中编码对象的形状和大小。因此，我们可以在稍后阶段使用深度图来提升这一归一化空间，并使用鲁棒的离群值去除和对齐技术来预测全度量6D对象的姿态和大小。</p><p>我们的CNN是建立在Mask R-CNN框架[23]上的，除了类标签和实例掩码外，在改进后还能够联合预测NOCS映射。第5节详细介绍了我们的改进和可以处理对称对象的新的loss函数。</p><p>在训练过程中，我们使用一种新的Context-Aware MixEd ReAlity（CAMERA）方法（参见第4节）来渲染GT图像。</p><p>这个大数据集允许我们在测试时从新的类别泛化到新的实例。并且为了进一步缩小这个领域的差距，我们还使用了一个更小的真实数据集。</p><h2 id="4-Datasets"><a href="#4-Datasets" class="headerlink" title="4. Datasets"></a>4. Datasets</h2><p>在类别级3D检测和6D位姿与尺寸估计中，一个主要的挑战是无法获得GT数据。虽然已经有了像NYU v2[40]和SUNRGB-D[41]这样的尝试，但它们都有重要的局限性。</p><p>首先，它们不提供对象的6D位姿，只关注3D bounding boxes。</p><p>其次，增强现实和机器人等应用得益于桌面设置中的手尺度对象，而当前的数据集关注的是更大的对象，如椅子和桌子。</p><p>最后，这些数据集不包含我们需要的GT类型的标签（即NOCS映射），并且包含的示例数量有限。</p><h3 id="4-1-Context-Aware-Mixed-Reality-Approach"><a href="#4-1-Context-Aware-Mixed-Reality-Approach" class="headerlink" title="4.1. Context-Aware Mixed Reality Approach"></a>4.1. Context-Aware Mixed Reality Approach</h3><p>为了促进大量手尺度对象GT训练数据的生成，我们提出了一种新的<strong>Context-Aware MixEd ReAlity（CAMERA）</strong>方法，它解决了以前方法的局限性，使数据生成需要的时间更少，并降低了成本。</p><p>它以一种<strong>上下文感知</strong>的方式将真实的背景图像与综合渲染的前景对象结合起来，即，将合成的对象渲染并组合成具有合理的物理位置、照明和比例的真实场景，见图4（左边蓝色框内）：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026170636706.png" alt="image-20231026170636706"></p><p>这种<strong>混合现实</strong>的方法允许我们生成比以前多得多的训练数据。</p><h4 id="Real-Scenes"><a href="#Real-Scenes" class="headerlink" title="Real Scenes"></a>Real Scenes</h4><p>我们使用31幅变化广泛的室内场景的真实RGB-D图像作为背景（图4中间粉色框内）。</p><p>我们的重点是桌面场景，因为大多数室内以人为中心的空间由桌面表面和手尺度的对象组成。我们总共为31个场景收集了553张图像，其中4个场景留作验证。</p><h4 id="Synthetic-Objects"><a href="#Synthetic-Objects" class="headerlink" title="Synthetic Objects"></a>Synthetic Objects</h4><p>为了在上面的真实场景中渲染逼真的对象，我们从ShapeNetCore[8]中选择手动缩放的对象，手动删除那些看起来不真实或有拓扑问题的对象。</p><p>总的来说，我们挑选了6种物品类别——<strong>瓶子、碗、相机、罐子、笔记本电脑和马克杯</strong>。我们还创建了一个<strong>干扰</strong>类别，其中包含了上面没有列出的类别中的对象实例，比如<strong>显示器、电话和吉他</strong>。这提高了预测我们的主要类别时的鲁棒性，即使场景中出现了其他对象。</p><p>我们策划的ShapeNetCore版本包含1085个单独的对象实例，我们留出184个实例用于验证。</p><h4 id="Context-Aware-Compositing"><a href="#Context-Aware-Compositing" class="headerlink" title="Context-Aware Compositing"></a>Context-Aware Compositing</h4><p>为了提高真实感，我们以一种上下文感知的方式组合虚拟对象，即，我们将其放置在它们自然出现的地方（例如，在支持表面上），并使用合理的照明。</p><p>我们使用了一种平面检测算法[15]对真实图像进行像素级平面分割。</p><p>随后，我们在可放置合成物体的分割平面上取样随机位置和方向。</p><p>然后我们放置几个虚拟光源来模拟真实的室内照明条件。</p><p>最后，我们将渲染的图像和真实的图像结合起来，以生成一个具有NOCS映射、掩码和类标签的合成图片，并且这个合成图片有完美的GT标签。</p><p>我们总共渲染了300K的合成图像，其中25K用于验证。据我们所知，这是分类级6D位姿和尺寸估计的最大数据集。</p><p>我们的混合现实合成技术是使用Unity游戏引擎[2]实现的，并且我们为引擎增加了用于平面检测和点采样的自定义插件（所有这些都将公开发布）。</p><p>与使用非上下文感知数据相比，使用我们的方法生成的图像看起来似乎合理和真实，从而提高了泛化。</p><h3 id="4-2-Real-World-Data"><a href="#4-2-Real-World-Data" class="headerlink" title="4.2. Real-World Data"></a>4.2. Real-World Data</h3><p>为了进一步改进和验证算法在具有挑战性的混乱和光照条件下的真实世界性能，我们捕获了两个真实世界的数据集：</p><ol><li>我们前面生成的混合现实数据的真实训练数据集的补充</li><li>一个真实世界的测试数据集来评估6D姿态和尺寸估计的性能</li></ol><p>我们开发了一种半自动的方法来标注物体的GT位姿和大小。图4显示了真实世界数据的示例（右边绿色框内）：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026170636706.png" alt="image-20231026170636706"></p><p>我们使用Structure Sensor[1]在18个不同的真实场景（7个场景用于训练，5个场景用于验证，6个场景用于测试）中，捕获了8K个RGB-D帧（4300个帧用于训练，950个帧用于验证，2750个帧用于测试）。</p><blockquote><p>训练集用于训练，会反复遍历；验证集用于评估模型的好坏；测试集只用一次。</p></blockquote><p>对于每个训练和测试子集，我们使用6个类别，每个类别选取3个不同的实例。对于验证集，我们使用6个类别，每个类别选取1个不同的实例。</p><p>我们在每个场景中放置超过5个对象实例来模拟真实世界的混乱。对于每个实例，我们使用我们为此目的开发的RGB-D重建算法来获得一个干净和准确的3D网格。</p><p>总的来说，我们的组合数据集包含18个不同的真实场景，42个独特的对象实例，跨越6个类别，使其成为分类级6D位姿和大小估计最全面的数据集。</p><h2 id="5-Method"><a href="#5-Method" class="headerlink" title="5. Method"></a>5. Method</h2><p>图3显示了我们从RGB-D图像中估计多个之前未见过物体的6D姿态和大小的方法：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026163707517.png" alt="image-20231026163707517"></p><p>该CNN会预测对象的类标签、掩码和NOCS映射。然后我们使用NOCS映射和深度图来估计对象的度量6D位姿和大小。</p><h3 id="5-1-NOCS-Map-Prediction-CNN"><a href="#5-1-NOCS-Map-Prediction-CNN" class="headerlink" title="5.1. NOCS Map Prediction CNN"></a>5.1. NOCS Map Prediction CNN</h3><p>我们CNN的目标是纯粹基于RGB图像估计对象的类标签、实例掩码和NOCS映射。</p><p>该CNN建立在基于区域的Mask R-CNN框架[23]上，因为它在2D目标检测和实例分割任务上展示了最先进的性能，是模块化的，灵活的，快速的，并可以很容易地被增强从而预测NOCS映射，如下所述。</p><h4 id="5-1-1-NOCS-Map-Head"><a href="#5-1-1-NOCS-Map-Head" class="headerlink" title="5.1.1 NOCS Map Head"></a>5.1.1 NOCS Map Head</h4><p>Mask R-CNN构建在Faster R-CNN架构[38]之上，由两个模块组成——一个模块提出可能包含对象的区域，一个检测器检测和分类区域内的对象。此外，它还预测区域内对象的实例掩码。</p><p>我们的主要贡献是在Mask R-CNN中添加了3个头部结构，用于预测NOCS映射的$x, y, z$分量，见图5：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231026190304830.png" alt="image-20231026190304830"></p><p>对于每个提议的感兴趣区域（ROI），一个头部的输出大小为$28 \times 28 \times N$，其中$N$是类别的数量，每个类别包含该类别中所有检测到的对象的$x$（或$y, z$）坐标。</p><p>与掩码头类似，我们在测试时使用对象类别，然后查找相应的预测通道。</p><p>在训练过程中，损失函数中只使用了来自GT对象类别的NOCS映射组件。</p><p>我们使用ResNet50[25]骨干网和特征金字塔网络（FPN）。</p><h5 id="Regression-vs-Classification"><a href="#Regression-vs-Classification" class="headerlink" title="Regression vs. Classification"></a>Regression vs. Classification</h5><p>为了预测NOCS映射，我们要么回归每个像素值，要么将像素值离散化，将其作为一个分类问题（图5中(B)）。</p><p>直接回归可能是一个更困难的任务，可能会在训练中引入不稳定性。类似地，具有大量类的像素分类（例如，$B = 128$或者$B = 256$，其中，$B$是像素的取值数）可能引入更多的参数，使训练比直接回归更具有挑战性。</p><p>实验结果表明，$B = 32$的像素分类优于直接回归。</p><h5 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h5><p>我们网络中的类、框和掩码头使用与[23]中描述的相同的损失函数。</p><p>对于NOCS映射头，我们使用两个损失函数：</p><ol><li><p>一个标准的softmax损失函数用于分类</p></li><li><p>另一个soft $L^1$损失函数用于回归，使学习更鲁棒，表示如下：</p><script type="math/tex; mode=display"> L(\mathbf{y}, \mathbf{y}^\ast) = \frac{1}{n}\begin{cases} \begin{array}{ll} 5(\mathbf{y} - \mathbf{y}^\ast)^2, & \left|\mathbf{y} - \mathbf{y}^\ast\right| \le 0.1 \\ \left|\mathbf{y} - \mathbf{y}^\ast\right| - 0.05, & \left|\mathbf{y} - \mathbf{y}^\ast\right| > 0.1 \end{array} \end{cases}</script><p> 其中，$\forall\mathbf{y} \in N, \mathbf{y}^\ast \in N_p$。</p></li></ol><p>在该损失函数中：</p><ul><li>$\mathbf{y} \in \mathcal{R}^3$，是NOCS映射像素的GT值</li><li>$\mathbf{y}^\ast$是NOCS映射像素的预测值</li><li>$n$是ROI中掩码像素的个数</li><li>$I$是NOCS的GT值</li><li>$I_p$是NOCS的预测值</li></ul><h5 id="Object-Symmetry"><a href="#Object-Symmetry" class="headerlink" title="Object Symmetry"></a>Object Symmetry</h5><p>许多常见的家用物品（如瓶子）都展现出了绕轴对称的特性。我们的NOCS表示没有考虑到对称性，这导致了一些对象类的巨大错误。</p><p>为了缓解这个问题，我们引入了一个考虑对称性的损失函数的变体。对于训练数据中的每个类别，我们定义了一个对称轴。在NOCS映射中，对象发生预定义的绕轴旋转时，会产生相同的损失函数值。</p><p>例如，顶部为方形的长方体有一个垂直的对称轴。在这个轴上旋转$\theta = \{0^\circ, 90^\circ, 180^\circ, 270^\circ\}$时，会导致相同的映射，因此有相同的损失。</p><p>对于非对称对象，$\theta = 0^\circ$是唯一的。</p><p>我们发现$\left|\theta\right| \le 6$（$\left|\theta\right|$为$\theta$取值的数量）足以处理大多数对称类。</p><p>我们生成GT NOCS映射，$\{\tilde{\mathbf{y}}_1, \tilde{\mathbf{y}}_2, \cdots, \tilde{\mathbf{y}}_{\left|\theta\right|}\}$，下标表示沿着对称轴旋转的次数。</p><p>然后我们定义我们的对称损失函数$L_s$，$L_s = \min_{i = 1, \cdots, \left|\theta\right|}L(\tilde{\mathbf{y}}_i, \mathbf{y}^\ast)$，其中$\mathbf{y}^\ast$表示的是预测的NOCS映射像素$(x, y, z)$。</p><p>这里相当于是将GT的NOCS旋转$\left|\theta\right|$次，分别计算损失函数值，然后选取一个最小的损失函数值来作为最后的损失函数值。</p><h5 id="Training-Protocol"><a href="#Training-Protocol" class="headerlink" title="Training Protocol"></a>Training Protocol</h5><p>我们在COCO数据集[33]上，用在二维实例分割任务上训练的权值来初始化RESNET50主干、RPN和FPN。</p><p>对于所有头，我们使用[24]中提出的初始化技术。</p><p>我们使用的批量大小为$2$，初始学习率为$0.001$，SGD优化器的动量为$0.9$，权重衰减为$1 \times 10^{-4}$。</p><p><strong>在训练的第一阶段</strong>，我们冻结ResNet50权重，只训练头部、RPN和FPN中的层，进行10K次迭代。</p><p><strong>在第二阶段</strong>，我们将ResNet50层冻结在4级以下，并训练进行3K迭代。</p><p><strong>在最后阶段</strong>，我们将ResNet50层冻结在第3级以下，持续70K次迭代。当切换到每个阶段时，我们将学习速度降低了10倍。</p><h3 id="5-2-6D-Pose-and-Size-Estimation"><a href="#5-2-6D-Pose-and-Size-Estimation" class="headerlink" title="5.2. 6D Pose and Size Estimation"></a>5.2. 6D Pose and Size Estimation</h3><p>我们的目标是通过使用NOCS映射和输入深度图来估计被检测物体的全度量6D位姿和尺寸。</p><p>为此，我们使用RGB-D相机的内部和外部特性来对齐深度图像和彩色图像。然后应用预测的对象掩码来得到被检测对象的三维点云$P_m$（如果语义分割正确，那么这个点云是真实点云）。我们还使用NOCS映射来获得$P_n$的3D表示（这个是在原点处的点云）。然后我们估计从$P_n$转换到$P_m$的缩放，旋转和平移。</p><p>我们使用了Umeyama算法[47]来解决这个7维刚性变换估计问题，并使用RANSAC算法[16]来去除离群点。定性结果见补充资料。</p><p>从NOCS得到的点云应该是在原点位置的，然后经过缩放，旋转和平移到深度相机处，从而得到6D姿态估计。</p><h2 id="6-Experiments-and-Results"><a href="#6-Experiments-and-Results" class="headerlink" title="6. Experiments and Results"></a>6. Experiments and Results</h2><h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><p>我们报告了3D物体检测和6D姿态估计指标的结果。</p><p>为了评估三维检测和目标尺寸估计，我们使用了阈值为50%[17]的交并比（IoU）度量。</p><p>对于6D位姿估计，我们报告的对象实例的平均精度，其中平移的误差小于$m$ cm，旋转的误差小于$n^\circ$，类似于[39, 30]。</p><p>我们将目标检测与6D姿态评估分离，因为它可以更清晰地显示性能。</p><p>我们在预测和GT之间设置了10%的bounding box重叠检测阈值，以确保大多数目标都包含在评估中。</p><p>对于对称对象类别（瓶、碗和罐），我们允许预测的3D bounding box围绕对象的垂直轴自由旋转，而不会受到任何惩罚。</p><p>我们对马克杯类杯子进行了特殊处理，在手柄不可见的情况下使其对称，因为在这种情况下很难判断它的位姿，即使是人类。我们使用[52]来检测CAMERA数据的处理可视性，并手动标注真实数据。</p><h3 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h3><p>由于我们不知道有其他方法来估计类别级6D的位姿和大小，我们建立了自己的基线来帮助比较性能。</p><p>它由Mask R-CNN网络组成，在相同的数据上训练，但没有NOCS映射头。我们使用预测的实例掩码从深度图中获得对象的三维点云。我们（使用ICP[4]）将掩码点云与相应类别中随机选择的模型进行对齐。</p><p>对于6D位姿估计，我们给出的结果可以很容易地与[51]进行比较。</p><h3 id="Evaluation-Data"><a href="#Evaluation-Data" class="headerlink" title="Evaluation Data"></a>Evaluation Data</h3><p>我们所有的实验都使用这些评估数据集中的一个或两个：</p><ol><li>CAMERA验证数据集（CAMERA25）</li><li>一个2.75K的真实数据集（REAL275），带有GT标签</li></ol><p>由于真实数据是有限的，这允许我们在不涉及姿态估计和域泛化的情况下研究性能。</p><h3 id="6-1-Category-Level-6D-Pose-and-Size-Estimation"><a href="#6-1-Category-Level-6D-Pose-and-Size-Estimation" class="headerlink" title="6.1. Category-Level 6D Pose and Size Estimation"></a>6.1. Category-Level 6D Pose and Size Estimation</h3><h4 id="Test-on-CAMERA25"><a href="#Test-on-CAMERA25" class="headerlink" title="Test on CAMERA25"></a>Test on CAMERA25</h4><p>我们报告了我们的方法的类别级结果，CNN仅在275K CAMERA训练集（CAMERA$^\ast$）上训练。我们在CAMERA25上测试性能，它由训练中完全未见过的对象和背景组成。</p><p>我们在3D IoU为50%时实现了83.9%的mAP，在$(5^\circ, 5 \text{ cm})$的测量中实现了40.9%的mAP。<strong>$(5^\circ, 5 \text{ cm})$是用于估计6D姿态的严格度量，即使对于已知实例[51, 6, 37]。</strong>更多细节请参见图6：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027134714854.png" alt="image-20231027134714854"></p><p>因为这里的测试集中都是在训练中没有见过的背景和对象，Baselines是无法处理的，所以在这个数据集上是无法将NOCS方法和Baselines方法进行比较的。</p><h4 id="Test-on-REAL275"><a href="#Test-on-REAL275" class="headerlink" title="Test on REAL275"></a>Test on REAL275</h4><p>然后，我们在结合CAMERA$^\ast$、真实数据集（REAL$^\ast$）和来自COCO[33]的弱监督下训练我们的网络，并在真实测试集上对其进行评估。因为COCO没有GT的NOCS映射，所以我们在训练中不使用NOCS损失。</p><p>我们使用了20K张包含我们类实例的COCO图片。</p><p>为了平衡这些数据集，对于每个小批量，我们从三个数据源中选择图像，CAMERA$^\ast$的概率为60%，COCO的概率为20%，REAL$^\ast$的概率为20%。</p><p>这个网络是我们产生的所有可视化结果中表现最好的，见图8：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027135558831.png" alt="image-20231027135558831"></p><p>在真实的测试集中，我们在3D IoU为50%时实现了76.4%的mAP，在$(5^\circ, 5 \text{ cm})$的测量中实现了10.2%的mAP，在$(10^\circ, 5 \text{ cm})$的测量中实现了23.1%的mAP。</p><p>相比之下，基线算法（Mask R-CNN + ICP alignment）在3D IoU为50%时实现了43.8%的mAP，在$(5^\circ, 5 \text{ cm})$和$(10^\circ, 5 \text{ cm})$的测量中实现了0.8%的mAP，这显著低于我们算法的性能。</p><p>图7显示了更详细的分析和比较：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027140036076.png" alt="image-20231027140036076"></p><p>该实验表明，通过学习去预测密集NOCS映射，我们的算法能够提供有关对象的形状，部件和可见性的额外详细信息，这些信息对于正确估计对象的6D姿态和大小都至关重要。</p><h3 id="6-2-Ablation-Studies"><a href="#6-2-Ablation-Studies" class="headerlink" title="6.2. Ablation Studies"></a>6.2. Ablation Studies</h3><h4 id="CAMERA-Approach"><a href="#CAMERA-Approach" class="headerlink" title="CAMERA Approach"></a>CAMERA Approach</h4><p>为了评估我们的CAMERA数据生成方法，我们对在不同训练数据组合上训练的网络进行了实验。对于这个实验，我们设置网络架构来回归NOCS映射。</p><p>表1显示了我们的网络在REAL275测试集上的性能：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027140302082.png" alt="image-20231027140302082"></p><p>我们还创建了CAMERA$^\ast$的变体，其中图像以非上下文感知的方式合成（在表1中由B表示）。</p><p>如表中所示，由于域间隙，仅使用CAMERA$^\ast$会导致性能较差。我们看到了在添加COCO和REAL$^\ast$后的逐步改进。</p><p>仅在REAL$^\ast$或在REAL$^\ast$和COCO上进行训练往往会由于数据集大小较小而过拟合到训练数据。使用COCO和REAL$^\ast$进行CAMERA$^\ast$训练可获得最佳效果。</p><p>此外，我们看到，非上下文感知数据的结果比上下文感知数据的性能更差，这表明我们的CAMERA方法是有用的。</p><h4 id="Classification-vs-Regression"><a href="#Classification-vs-Regression" class="headerlink" title="Classification vs. Regression"></a>Classification vs. Regression</h4><p>在CAMERA25和REAL275上，像素分类始终优于回归。</p><p>使用32bits最适合姿态估计，而128bits在检测上更好，参见表2：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027140845274.png" alt="image-20231027140845274"></p><h4 id="Symmetry-Loss"><a href="#Symmetry-Loss" class="headerlink" title="Symmetry Loss"></a>Symmetry Loss</h4><p>这种损失对于许多日常对称对象类别来说是至关重要的。</p><p>为了研究对称性损失的影响，我们在CAMERA25和REAL275集上对回归网络进行了消融实验。表2示出了如果不使用对称性损失（<code>Reg. w/o Sym.</code>表示不使用对称性损失），姿态精度显著降低，特别是对于6D姿态。</p><h3 id="6-3-Instance-level-6D-Pose-Estimation"><a href="#6-3-Instance-level-6D-Pose-Estimation" class="headerlink" title="6.3. Instance-level 6D Pose Estimation"></a>6.3. Instance-level 6D Pose Estimation</h3><p>我们还评估了我们在OccludedLINEMOD[26]上的实例级6D姿态估计任务的方法，并与PoseCNN[51]进行了比较。</p><p>OccludedLINEMOD数据集有9个对象实例，并为每个实例提供一个CAD模型。它有1214张带有标签的GT 6D姿态的图像。我们遵循[46, 27]中的协议，随机选择15%的数据集作为训练图像。然后，我们使用第4节中描述的技术生成15000个合成图像。</p><p>使用32bits分类网络，我们实现了94.7%的检测率，在3D IoU为50%时实现了88.4%的mAP，在$(5^\circ, 5 \text{ cm})$的测量中实现了13.9%的mAP，在$(10^\circ, 5 \text{ cm})$的测量中实现了33.5%的mAP。这远远高于PoseCNN[51]，后者在没有迭代姿态细化的情况下仅实现了1.7%的mAP（在[30]中报告）。图9提供了更详细的分析：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027141645905.png" alt="image-20231027141645905"></p><p>这个实验表明，虽然我们的方法是为类别级姿态估计而设计的，但它也可以在标准6D姿态估计基准上实现最先进的性能。</p><p>使用2D投影度量，其测量GT值和估计的对象姿态之间的平均像素距离，我们在5像素的2D投影上实现了30.2%mAP。我们的方法显著优于PoseCNN[51]，后者在[30]中报告了5像素的2D投影上实现的17.2%mAP。详细对比见补充文件。</p><h4 id="Limitations-and-FutureWork"><a href="#Limitations-and-FutureWork" class="headerlink" title="Limitations and FutureWork"></a>Limitations and FutureWork</h4><p>据我们所知，我们是第一个解决类别级6D姿态和大小估计问题的方法。仍有许多悬而未决的问题需要解决。<strong>首先</strong>，在我们的方法中，位姿估计是以区域推荐和类别预测为条件的，这可能是不正确的，并对结果产生负面影响。<strong>其次</strong>，我们的方法依赖于深度图像来提升NOCS预测到真实世界的坐标。未来的工作应该研究直接从RGB图像估计6D姿态和大小。</p><h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>我们提出了一种方法，为未见过的的对象实例做类别级的6D姿态和尺寸估计。</p><p>我们提出了一个新的规范化对象坐标空间（NOCS），允许我们定义一个共享的空间与一致的对象缩放和方向。</p><p>我们提出了一个CNN来预测NOCS映射，该映射可以与深度图一起使用，使用一种位姿拟合方法来估计未见过物体的全度量6D位姿和大小。</p><p>我们的方法在增强现实，机器人和3D场景理解等领域有重要的应用。</p><h2 id="A-Implementation-and-Computation-Times"><a href="#A-Implementation-and-Computation-Times" class="headerlink" title="A. Implementation and Computation Times"></a>A. Implementation and Computation Times</h2><p>我们的网络在Python 3，Keras和Tensorflow上实现。代码基于MatterPort的Mask RCNN实现[3]。该网络使用特征金字塔网络（FPN）[32]和ResNet50骨干网络[25]。</p><p>我们的网络将分辨率为640×360的图像作为输入。我们在Intel Xeon Gold 5122 CPU@3.60GHz台式机上使用NVIDIA TITAN Xp实现了约4fps的交互速率。我们的实现使用Umeyama算法，神经网络推理的平均时间为210ms，姿态对齐的平均时间为34ms。</p><h2 id="B-Scanned-Real-Instances"><a href="#B-Scanned-Real-Instances" class="headerlink" title="B. Scanned Real Instances"></a>B. Scanned Real Instances</h2><p>我们的真实数据集包含6个对象类别和42个真实扫描的唯一实例。对于每个类别，我们收集了7个实例，其中4个用于训练和验证，其余3个用于测试。图10显示了我们实例的一个子集：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027144043766.png" alt="image-20231027144043766"></p><p>在这里可以看到数据集中有很大的类内形状差异。</p><p>第一行是在训练中使用的实例。第二行和第三行是为测试而保留的实例。</p><h2 id="C-Result-Visualization"><a href="#C-Result-Visualization" class="headerlink" title="C. Result Visualization"></a>C. Result Visualization</h2><p>这里我们提供了6D位姿和尺寸估计的更多视觉结果。</p><p>由于有足够的训练数据，我们的方法在CAMERA25验证集上取得了非常好的性能，如图11所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027144353576.png" alt="image-20231027144353576"></p><p>在REAL275测试集上，尽管实际训练数据量很小，但我们仍然观察到良好的性能，如图12所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027144824279.png" alt="image-20231027144824279"></p><p>我们观察到真实数据的几种失效模式，包括缺失检测、错误分类和预测坐标图的不一致。</p><h2 id="D-Comparisons-on-the-OccludedLINEMOD-Dataset"><a href="#D-Comparisons-on-the-OccludedLINEMOD-Dataset" class="headerlink" title="D. Comparisons on the OccludedLINEMOD Dataset"></a>D. Comparisons on the OccludedLINEMOD Dataset</h2><p>我们的方法与现有的在OccludedLINEMOD数据集[6]上使用二维投影度量的方法的比较如图13所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/image-20231027144659132.png" alt="image-20231027144659132"></p>]]></content>
    
    
    <summary type="html">用于类别级6D对象姿态和尺寸估计的归一化对象坐标空间。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2019" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2019/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="CVPR" scheme="http://blog.karltan.com/tags/CVPR/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 26 网络中的网络 NiN</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/26/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/26/</id>
    <published>2023-10-25T13:00:00.000Z</published>
    <updated>2023-10-25T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="26-网络中的网络-NiN"><a href="#26-网络中的网络-NiN" class="headerlink" title="26 网络中的网络 NiN"></a>26 网络中的网络 NiN</h1><h2 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h2><h3 id="全连接层的问题"><a href="#全连接层的问题" class="headerlink" title="全连接层的问题"></a>全连接层的问题</h3><ul><li><p>卷积层需要较少的参数</p><script type="math/tex; mode=display">  c_i \times c_o \times k^2</script></li><li><p>但卷积层后的第一个全连接层的参数</p><ul><li>LeNet：16x5x5x120 = 48k</li><li>AlexNet：256x5x5x4096 = 26M</li><li>VGG：512x7x7x4096 = 102M</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/26/image-20231025203136259.png" alt="image-20231025203136259"></p><h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><ul><li>一个卷积层后跟两个全连接层<ul><li>步幅1，无填充，输出形状跟卷积层输出一样</li><li>起到全连接层的作用</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/26/image-20231025203740321.png" alt="image-20231025203740321"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/26/image-20231025203829371.png" alt="image-20231025203829371"></p><h3 id="NiN架构"><a href="#NiN架构" class="headerlink" title="NiN架构"></a>NiN架构</h3><ul><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层<ul><li>逐步减小高宽和增大通道数</li></ul></li><li>最后使用全局平均池化层得到输出<ul><li>其输入通道数是类别数</li></ul></li></ul><h3 id="NiN-Networks"><a href="#NiN-Networks" class="headerlink" title="NiN Networks"></a>NiN Networks</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/26/image-20231025204420268.png" alt="image-20231025204420268"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>NiN块使用卷积层加两个1x1卷积层<ul><li>后者对每个像素增加了非线性性</li></ul></li><li>NiN使用全局平均池化层来代替VGG和AlexNet中的全连接层<ul><li>不容易过拟合，更少的参数个数</li></ul></li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>NiN块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>NiN模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看每个块的输出形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;Output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Sequential Output shape: torch.Size([1, 96, 54, 54])</span><br><span class="line">MaxPool2d Output shape: torch.Size([1, 96, 26, 26])</span><br><span class="line">Sequential Output shape: torch.Size([1, 256, 26, 26])</span><br><span class="line">MaxPool2d Output shape: torch.Size([1, 256, 12, 12])</span><br><span class="line">Sequential Output shape: torch.Size([1, 384, 12, 12])</span><br><span class="line">MaxPool2d Output shape: torch.Size([1, 384, 5, 5])</span><br><span class="line">Dropout Output shape: torch.Size([1, 384, 5, 5])</span><br><span class="line">Sequential Output shape: torch.Size([1, 10, 5, 5])</span><br><span class="line">AdaptiveAvgPool2d Output shape: torch.Size([1, 10, 1, 1])</span><br><span class="line">Flatten Output shape: torch.Size([1, 10])</span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.605, train acc 0.769, test acc 0.770</span><br><span class="line">883.7 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/26/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 26 网络中的网络 NiN。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 25 使用块的网络 VGG</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/25/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/25/</id>
    <published>2023-10-25T11:00:00.000Z</published>
    <updated>2023-10-25T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="25-使用块的网络-VGG"><a href="#25-使用块的网络-VGG" class="headerlink" title="25 使用块的网络 VGG"></a>25 使用块的网络 VGG</h1><h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><h3 id="VGG-1"><a href="#VGG-1" class="headerlink" title="VGG"></a>VGG</h3><ul><li>AlexNet比LeNet更深更大来得到更好的精度</li><li>能不能更深和更大？</li><li>选项<ul><li>更多的全连接层（太贵）</li><li>更多的卷积层</li><li>将卷积层组合成块</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/25/image-20231025192701403.png" alt="image-20231025192701403"></p><h3 id="VGG块"><a href="#VGG块" class="headerlink" title="VGG块"></a>VGG块</h3><ul><li>深 vs. 宽？<ul><li>5x5卷积</li><li>3x3卷积</li><li>深但窄效果更好</li></ul></li><li>VGG块<ul><li>3x3卷积（填充1）（n层，m通道）</li><li>2x2最大池化（步幅2）</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/25/image-20231025193035531.png" alt="image-20231025193035531"></p><h3 id="VGG架构"><a href="#VGG架构" class="headerlink" title="VGG架构"></a>VGG架构</h3><ul><li>多个VGG块后接全连接层</li><li>不同次数的重复块得到不同的架构VGG-16，VGG-19，…</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/25/image-20231025193344997.png" alt="image-20231025193344997"></p><h3 id="进度"><a href="#进度" class="headerlink" title="进度"></a>进度</h3><ul><li>LeNet（1995）<ul><li>2卷积+池化层</li><li>2全连接层</li></ul></li><li>AlexNet<ul><li>更大更深</li><li>ReLu，Dropout，数据增强</li></ul></li><li>VGG<ul><li>更大更深的AlexNet（重复的VGG块）</li></ul></li></ul><h3 id="GluonCV-Model-Zoo"><a href="#GluonCV-Model-Zoo" class="headerlink" title="GluonCV Model Zoo"></a>GluonCV Model Zoo</h3><p><a href="https://cv.gluon.ai/model_zoo/classification.html">Classification — gluoncv 0.11.0 documentation</a></p><p><img src="https://img.karltan.com/notes-out-class/d2l/25/image-20231025193830307.png" alt="image-20231025193830307"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>VGG使用可重复使用的卷积块来构建深度卷积神经网络</li><li>不同的卷积块个数和超参数可以得到不同复杂度的变种</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>VGG块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><p>VGG网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(</span><br><span class="line">            vgg_block(num_convs, in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>), nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">        nn.Dropout(<span class="number">0.5</span>), nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure><p>观察每个层输出的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__, <span class="string">&quot;Output shape:\t&quot;</span>, X.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Sequential Output shape: torch.Size([1, 64, 112, 112])</span><br><span class="line">Sequential Output shape: torch.Size([1, 128, 56, 56])</span><br><span class="line">Sequential Output shape: torch.Size([1, 256, 28, 28])</span><br><span class="line">Sequential Output shape: torch.Size([1, 512, 14, 14])</span><br><span class="line">Sequential Output shape: torch.Size([1, 512, 7, 7])</span><br><span class="line">Flatten Output shape: torch.Size([1, 25088])</span><br><span class="line">Linear Output shape: torch.Size([1, 4096])</span><br><span class="line">ReLU Output shape: torch.Size([1, 4096])</span><br><span class="line">Dropout Output shape: torch.Size([1, 4096])</span><br><span class="line">Linear Output shape: torch.Size([1, 4096])</span><br><span class="line">ReLU Output shape: torch.Size([1, 4096])</span><br><span class="line">Dropout Output shape: torch.Size([1, 4096])</span><br><span class="line">Linear Output shape: torch.Size([1, 10])</span><br></pre></td></tr></table></figure><p>由于VGG-11比AlexNet计算量更大，因此我们构建了一个通道数较少的网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br></pre></td></tr></table></figure><p>模型训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.184, train acc 0.931, test acc 0.920</span><br><span class="line">636.2 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/25/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 25 使用块的网络 VGG。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 24 深度卷积神经网络 AlexNet</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/24/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/24/</id>
    <published>2023-10-23T11:00:00.000Z</published>
    <updated>2023-10-23T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="24-深度卷积神经网络-AlexNet"><a href="#24-深度卷积神经网络-AlexNet" class="headerlink" title="24 深度卷积神经网络 AlexNet"></a>24 深度卷积神经网络 AlexNet</h1><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><h3 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023202435516.png" alt="image-20231023202435516"></p><ul><li>特征提取</li><li>选择核函数来计算相似性</li><li>凸优化问题</li><li>漂亮的定理</li></ul><h3 id="几何学"><a href="#几何学" class="headerlink" title="几何学"></a>几何学</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023202933229.png" alt="image-20231023202933229"></p><ul><li>抽取特征</li><li>描述几何（例如多相机）</li><li>（非）凸优化</li><li>漂亮定理</li><li>如果假设满足了，效果很好</li></ul><h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023203313338.png" alt="image-20231023203313338"></p><ul><li>特征工程是关键</li><li>特征描述子：SIFT，SURF</li><li>视觉词袋（聚类）</li><li>最后用SVM</li></ul><h3 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023203549802.png" alt="image-20231023203549802"></p><h3 id="ImageNet-2010"><a href="#ImageNet-2010" class="headerlink" title="ImageNet(2010)"></a>ImageNet(2010)</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023204213732.png" alt="image-20231023204213732"></p><div class="table-container"><table><thead><tr><th style="text-align:center">图片</th><th style="text-align:center">自然物体的彩色图片</th><th style="text-align:center">手写数字的黑白图片</th></tr></thead><tbody><tr><td style="text-align:center">大小</td><td style="text-align:center">469x387</td><td style="text-align:center">28x28</td></tr><tr><td style="text-align:center">样本数</td><td style="text-align:center">1.2M</td><td style="text-align:center">60K</td></tr><tr><td style="text-align:center">类数</td><td style="text-align:center">1000</td><td style="text-align:center">10</td></tr></tbody></table></div><h3 id="AlexNet-1"><a href="#AlexNet-1" class="headerlink" title="AlexNet"></a>AlexNet</h3><ul><li>AlexNet赢了2012年ImageNet竞赛</li><li>更深更大的LeNet</li><li>主要改进：<ul><li>丢弃法</li><li>ReLU</li><li>MaxPooling</li></ul></li><li>计算机视觉方法论的改变</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023204656310.png" alt="image-20231023204656310"></p><h3 id="AlexNet架构"><a href="#AlexNet架构" class="headerlink" title="AlexNet架构"></a>AlexNet架构</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023210236979.png" alt="image-20231023210236979"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023210006606.png" alt="image-20231023210006606"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023205309739.png" alt="image-20231023205309739"></p><h3 id="更多细节"><a href="#更多细节" class="headerlink" title="更多细节"></a>更多细节</h3><ul><li>激活函数从sigmoid变到了ReLU（减缓梯度消失）</li><li>隐藏全连接层后加入了丢弃层</li><li>数据增强</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023210601131.png" alt="image-20231023210601131"></p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/24/image-20231023210923069.png" alt="image-20231023210923069"></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">AlexNet的参数个数</th><th style="text-align:center">LeNet参数个数</th><th style="text-align:center">AlexNet的FLOP</th><th style="text-align:center">LeNet的FLOP</th></tr></thead><tbody><tr><td style="text-align:center">Conv1</td><td style="text-align:center">35K</td><td style="text-align:center">150</td><td style="text-align:center">101M</td><td style="text-align:center">1.2M</td></tr><tr><td style="text-align:center">Conv2</td><td style="text-align:center">614K</td><td style="text-align:center">2.4K</td><td style="text-align:center">415M</td><td style="text-align:center">2.4M</td></tr><tr><td style="text-align:center">Conv3-5</td><td style="text-align:center">3M</td><td style="text-align:center"></td><td style="text-align:center">445M</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Dense1</td><td style="text-align:center">26M</td><td style="text-align:center">0.48M</td><td style="text-align:center">26M</td><td style="text-align:center">0.48M</td></tr><tr><td style="text-align:center">Dense2</td><td style="text-align:center">16M</td><td style="text-align:center">0.1M</td><td style="text-align:center">16M</td><td style="text-align:center">0.1M</td></tr><tr><td style="text-align:center">Total</td><td style="text-align:center">46M</td><td style="text-align:center">0.6M</td><td style="text-align:center">1G</td><td style="text-align:center">4M</td></tr><tr><td style="text-align:center">Increase</td><td style="text-align:center">11x</td><td style="text-align:center">1x</td><td style="text-align:center">250x</td><td style="text-align:center">1x</td></tr></tbody></table></div><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>AlexNet是更大更深的LeNet，10x参数个数，260x计算复杂度</li><li>新进入了丢弃法，ReLU，最大池化层和数据增强</li><li>AlexNet赢下了2012ImageNet竞赛后，标志着新的一轮神经网络热潮的开始</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们构造一个单通道数据，来观察每一层输出的形状：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;Output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Conv2d Output shape: torch.Size([1, 96, 54, 54])</span><br><span class="line">ReLU Output shape: torch.Size([1, 96, 54, 54])</span><br><span class="line">MaxPool2d Output shape: torch.Size([1, 96, 26, 26])</span><br><span class="line">Conv2d Output shape: torch.Size([1, 256, 26, 26])</span><br><span class="line">ReLU Output shape: torch.Size([1, 256, 26, 26])</span><br><span class="line">MaxPool2d Output shape: torch.Size([1, 256, 12, 12])</span><br><span class="line">Conv2d Output shape: torch.Size([1, 384, 12, 12])</span><br><span class="line">ReLU Output shape: torch.Size([1, 384, 12, 12])</span><br><span class="line">Conv2d Output shape: torch.Size([1, 384, 12, 12])</span><br><span class="line">ReLU Output shape: torch.Size([1, 384, 12, 12])</span><br><span class="line">Conv2d Output shape: torch.Size([1, 256, 12, 12])</span><br><span class="line">ReLU Output shape: torch.Size([1, 256, 12, 12])</span><br><span class="line">MaxPool2d Output shape: torch.Size([1, 256, 5, 5])</span><br><span class="line">Flatten Output shape: torch.Size([1, 6400])</span><br><span class="line">Linear Output shape: torch.Size([1, 4096])</span><br><span class="line">ReLU Output shape: torch.Size([1, 4096])</span><br><span class="line">Dropout Output shape: torch.Size([1, 4096])</span><br><span class="line">Linear Output shape: torch.Size([1, 4096])</span><br><span class="line">ReLU Output shape: torch.Size([1, 4096])</span><br><span class="line">Dropout Output shape: torch.Size([1, 4096])</span><br><span class="line">Linear Output shape: torch.Size([1, 10])</span><br></pre></td></tr></table></figure><p>Fashion-MNIST图像的分辨率低于ImageNet图像，我们将它们增加到224x224：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size, resize=<span class="number">224</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.01</span>, <span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.327, train acc 0.880, test acc 0.871</span><br><span class="line">1208.3 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/24/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 24 深度卷积神经网络 AlexNet。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 23 经典卷积神经网络 LeNet</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/23/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/23/</id>
    <published>2023-10-23T07:00:00.000Z</published>
    <updated>2023-10-23T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="23-经典卷积神经网络-LeNet"><a href="#23-经典卷积神经网络-LeNet" class="headerlink" title="23 经典卷积神经网络 LeNet"></a>23 经典卷积神经网络 LeNet</h1><h2 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h2><h3 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/23/image-20231023183144039.png" alt="image-20231023183144039"></p><h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h3><ul><li>50000个训练数据</li><li>10000个测试数据</li><li>图像大小28x28</li><li>10类</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/23/image-20231023183533171.png" alt="image-20231023183533171"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/23/LeNet.gif" alt="LeNet"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>LeNet是早期成功的神经网络</li><li>先使用卷积层来学习图片空间信息</li><li>然后使用全连接层来转换到类别空间</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>LeNet（LeNet-5）由两个部分组成：卷积编码块和全连接层密集块。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(</span><br><span class="line">    Reshape(),</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>检查模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;Output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Reshape Output shape: torch.Size([1, 1, 28, 28])</span><br><span class="line">Conv2d Output shape: torch.Size([1, 6, 28, 28])</span><br><span class="line">Sigmoid Output shape: torch.Size([1, 6, 28, 28])</span><br><span class="line">AvgPool2d Output shape: torch.Size([1, 6, 14, 14])</span><br><span class="line">Conv2d Output shape: torch.Size([1, 16, 10, 10])</span><br><span class="line">Sigmoid Output shape: torch.Size([1, 16, 10, 10])</span><br><span class="line">AvgPool2d Output shape: torch.Size([1, 16, 5, 5])</span><br><span class="line">Flatten Output shape: torch.Size([1, 400])</span><br><span class="line">Linear Output shape: torch.Size([1, 120])</span><br><span class="line">Sigmoid Output shape: torch.Size([1, 120])</span><br><span class="line">Linear Output shape: torch.Size([1, 84])</span><br><span class="line">Sigmoid Output shape: torch.Size([1, 84])</span><br><span class="line">Linear Output shape: torch.Size([1, 10])</span><br></pre></td></tr></table></figure><p>LeNet在Fashion-MNIST数据集上的表现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>对<code>evaluate_accuracy</code>函数进行轻微的修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">            X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>为了使用GPU，我们还需要一点小改动：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在GPU上训练模型（在第6章中定义）。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;text acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练和评估LeNet-5模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss 0.457, train acc 0.828, text acc 0.777</span><br><span class="line">49712.0 examples/sec on cuda:0</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/23/output.svg" alt="output"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 23 经典卷积神经网络 LeNet。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 22 池化层</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/22/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/22/</id>
    <published>2023-10-23T05:00:00.000Z</published>
    <updated>2023-10-23T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="22-池化层"><a href="#22-池化层" class="headerlink" title="22 池化层"></a>22 池化层</h1><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><h3 id="池化层-1"><a href="#池化层-1" class="headerlink" title="池化层"></a>池化层</h3><ul><li><p>积对位置敏感</p><ul><li>检测垂直边缘</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/22/image-20231023132754119.png" alt="image-20231023132754119"></p><p>那么1像素移位，会导致0输出</p><ul><li><p>需要一定的平移不变性</p><ul><li>照明，物体位置，比例，外观等因图像而异</li></ul></li></ul><h3 id="二维最大池化"><a href="#二维最大池化" class="headerlink" title="二维最大池化"></a>二维最大池化</h3><ul><li>返回滑动窗口中的最大值</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/22/image-20231023133157343.png" alt="image-20231023133157343"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/22/pooling1.gif" alt="pooling1"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/22/image-20231023133900076.png" alt="image-20231023133900076"></p><p>此时可容1像素移位</p><h3 id="填充，步幅和多个通道"><a href="#填充，步幅和多个通道" class="headerlink" title="填充，步幅和多个通道"></a>填充，步幅和多个通道</h3><ul><li>池化层与卷积层类似，都具有填充和步幅</li><li>没有可学习的参数</li><li>在每个输入通道应用池化层以获得相应的输出通道</li><li>输出通道数 = 输出通道数</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/22/pooling2.gif" alt="pooling2"></p><h3 id="平均池化层"><a href="#平均池化层" class="headerlink" title="平均池化层"></a>平均池化层</h3><ul><li>最大池化层：每个窗口中最强的模式信号</li><li>平均池化层：将最大池化层中的“最大”操作替换为“平均”</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/22/image-20231023134733652.png" alt="image-20231023134733652"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>池化层返回窗口中最大或平均值</li><li>缓解卷积层对位置的敏感性</li><li>窗口大小、填充和步幅作为超参数</li></ul><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="实现池化层的正向传播"><a href="#实现池化层的正向传播" class="headerlink" title="实现池化层的正向传播"></a>实现池化层的正向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros(X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i + p_h, j:j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><h3 id="验证二维最大池化的输出"><a href="#验证二维最大池化的输出" class="headerlink" title="验证二维最大池化的输出"></a>验证二维最大池化的输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([</span><br><span class="line">    [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">    [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>],</span><br><span class="line">    [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]</span><br><span class="line">])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[4., 5.],</span><br><span class="line">        [7., 8.]])</span><br></pre></td></tr></table></figure><h3 id="验证平均池化层"><a href="#验证平均池化层" class="headerlink" title="验证平均池化层"></a>验证平均池化层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[2., 3.],</span><br><span class="line">        [5., 6.]])</span><br></pre></td></tr></table></figure><h3 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 0.,  1.,  2.,  3.],</span><br><span class="line">          [ 4.,  5.,  6.,  7.],</span><br><span class="line">          [ 8.,  9., 10., 11.],</span><br><span class="line">          [12., 13., 14., 15.]]]])</span><br></pre></td></tr></table></figure><h3 id="深度学习框架中的步幅与池化窗口的大小相同"><a href="#深度学习框架中的步幅与池化窗口的大小相同" class="headerlink" title="深度学习框架中的步幅与池化窗口的大小相同"></a>深度学习框架中的步幅与池化窗口的大小相同</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[10.]]]])</span><br></pre></td></tr></table></figure><h3 id="填充和步幅可以手动设定"><a href="#填充和步幅可以手动设定" class="headerlink" title="填充和步幅可以手动设定"></a>填充和步幅可以手动设定</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 5.,  7.],</span><br><span class="line">          [13., 15.]]]])</span><br></pre></td></tr></table></figure><h3 id="设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度"><a href="#设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度" class="headerlink" title="设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度"></a>设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 1.,  3.],</span><br><span class="line">          [ 9., 11.],</span><br><span class="line">          [13., 15.]]]])</span><br></pre></td></tr></table></figure><h3 id="池化层在每个输入通道上单独运算"><a href="#池化层在每个输入通道上单独运算" class="headerlink" title="池化层在每个输入通道上单独运算"></a>池化层在每个输入通道上单独运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 0.,  1.,  2.,  3.],</span><br><span class="line">          [ 4.,  5.,  6.,  7.],</span><br><span class="line">          [ 8.,  9., 10., 11.],</span><br><span class="line">          [12., 13., 14., 15.]],</span><br><span class="line"></span><br><span class="line">         [[ 1.,  2.,  3.,  4.],</span><br><span class="line">          [ 5.,  6.,  7.,  8.],</span><br><span class="line">          [ 9., 10., 11., 12.],</span><br><span class="line">          [13., 14., 15., 16.]]]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 5.,  7.],</span><br><span class="line">          [13., 15.]],</span><br><span class="line"></span><br><span class="line">         [[ 6.,  8.],</span><br><span class="line">          [14., 16.]]]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 22 池化层。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 21 卷积层里的多输入多输出通道</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/21/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/21/</id>
    <published>2023-10-22T12:00:00.000Z</published>
    <updated>2023-10-22T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="21-卷积层里的多输入多输出通道"><a href="#21-卷积层里的多输入多输出通道" class="headerlink" title="21 卷积层里的多输入多输出通道"></a>21 卷积层里的多输入多输出通道</h1><h2 id="多输入输出通道"><a href="#多输入输出通道" class="headerlink" title="多输入输出通道"></a>多输入输出通道</h2><h3 id="多个输入通道"><a href="#多个输入通道" class="headerlink" title="多个输入通道"></a>多个输入通道</h3><ul><li>彩色图像可能会有RGB三个通道</li><li>转换为灰度会丢失信息</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/21/image-20231022201517928.png" alt="image-20231022201517928"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/21/image-20231022201647347.png" alt="image-20231022201647347"></p><ul><li>每个通道都有一个卷积核，结果是所有通道卷积结果的和</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/21/image-20231022201757027.png" alt="image-20231022201757027"></p><blockquote><p>上图中只有一个卷积核。</p></blockquote><ul><li>输入$\mathbf{X}$：$c_i \times n_h \times n_w$</li><li>核$\mathbf{W}$：$c_i \times k_h \times k_w$</li><li>输出$\mathbf{Y}$：$m_h \times m_w$</li></ul><script type="math/tex; mode=display">\mathbf{Y} = \sum_{i = 0}^{c_i}\mathbf{X}_{i, :, :} \star \mathbf{W}_{i, :, :}</script><h3 id="多个输出通道"><a href="#多个输出通道" class="headerlink" title="多个输出通道"></a>多个输出通道</h3><ul><li>无论有多少输入通道，到目前为止我们只用到单输出通道</li><li>我们可以有多个三维卷积核，每个核生成一个输出通道</li><li>输入$\mathbf{X}$：$c_i \times n_h \times n_w$</li><li>核$\mathbf{W}$：$c_o \times c_i \times k_h \times k_w$</li><li>输出$\mathbf{Y}$：$c_o \times m_h \times m_w$</li></ul><script type="math/tex; mode=display">\mathbf{Y}_{i, :, :} = \mathbf{X} \star \mathbf{W}_{i, :, :} \quad \text{for } i = 1, \cdots, c_o</script><h3 id="多个输入和输出通道"><a href="#多个输入和输出通道" class="headerlink" title="多个输入和输出通道"></a>多个输入和输出通道</h3><ul><li>每个输出通道可以识别特定模式</li><li>输入通道核识别并组合输入中的模式</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/21/image-20231022202941361.png" alt="image-20231022202941361"></p><h3 id="1-times-1-卷积层"><a href="#1-times-1-卷积层" class="headerlink" title="$1 \times 1$卷积层"></a>$1 \times 1$卷积层</h3><p>$k_h = k_w = 1$是一个受欢迎的选择。它不识别空间模式，只是融合通道。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/21/image-20231022210432461.png" alt="image-20231022210432461"></p><blockquote><p>解释一下上面这张图：</p><p>上图中有两个卷积核，浅蓝色一个，深蓝色一个，浅蓝色卷积核生成的输出是前面的这个矩阵，深蓝色卷积核生成的输出是后面的这个矩阵。</p></blockquote><p>相当于输入形状为$n_hn_w \times c_i$，权重为$c_o \times c_i$的全连接层</p><h3 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><ul><li><p>输入$\mathbf{X}$：$c_i \times n_h \times n_w$</p></li><li><p>核$\mathbf{W}$：$c_o \times c_i \times k_h \times k_w$</p></li><li><p>偏差$\mathbf{B}$：$c_o \times c_i$</p></li><li><p>输出$\mathbf{Y}$：$c_o \times m_h \times m_w$</p><script type="math/tex; mode=display">  \mathbf{Y} = \mathbf{X} \star \mathbf{W} + \mathbf{B}</script></li><li><p>计算复杂度（浮点数计算数FLOP）$O(c_i c_o k_h k_w m_h m_w)$</p><script type="math/tex; mode=display">  \left.\begin{matrix}  \begin{aligned}  &c_i = c_o = 100 \\  &k_h = k_w = 5 \\  &m_h = m_w = 64  \end{aligned}  \end{matrix}\right\} \Rightarrow \text{1GFLOP}</script></li><li><p>10层，1M样本，10PFlops</p><p>  （CPU：0.15TF = 18h；GPU：12TF = 14min）</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>输出通道数是卷积层的超参数</li><li>每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果</li><li>每个输出通道有独立的三维卷积核</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="实现一下多输入通道互相关运算"><a href="#实现一下多输入通道互相关运算" class="headerlink" title="实现一下多输入通道互相关运算"></a>实现一下多输入通道互相关运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br></pre></td></tr></table></figure><h3 id="验证互相关运算的输出"><a href="#验证互相关运算的输出" class="headerlink" title="验证互相关运算的输出"></a>验证互相关运算的输出</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">        [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>],</span><br><span class="line">        [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>],</span><br><span class="line">        [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>],</span><br><span class="line">        [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]</span><br><span class="line">    ]</span><br><span class="line">])</span><br><span class="line">K = torch.tensor([</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">0.0</span>, <span class="number">1.0</span>],</span><br><span class="line">        [<span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [<span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">        [<span class="number">3.0</span>, <span class="number">4.0</span>]</span><br><span class="line">    ]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">X.shape, K.shape, corr2d_multi_in(X, K)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([2, 3, 3]),</span><br><span class="line"> torch.Size([2, 2, 2]),</span><br><span class="line"> tensor([[ 56.,  72.],</span><br><span class="line">         [104., 120.]]))</span><br></pre></td></tr></table></figure><p>这里相当于输入是一个2通道的$3 \times 3$矩阵，卷积核只有1个，大小是$2 \times 2$，那么输出的通道应该只有1，并且是对应的做卷积，即：</p><script type="math/tex; mode=display">\begin{bmatrix}0.0^\ast & 1.0^\ast & 2.0 \\3.0^\ast & 4.0^\ast & 5.0 \\6.0 & 7.0 & 8.0\end{bmatrix}\star\begin{bmatrix}0.0 & 1.0 \\2.0 & 3.0\end{bmatrix}+\begin{bmatrix}1.0^\ast & 2.0^\ast & 3.0 \\4.0^\ast & 5.0^\ast & 6.0 \\7.0 & 8.0 & 9.0\end{bmatrix}\star\begin{bmatrix}1.0 & 2.0 \\3.0 & 4.0\end{bmatrix}=\begin{bmatrix}56.^\ast & 72. \\104. & 120.\end{bmatrix}</script><p>（$^\ast$表示当前计算的位置）</p><h3 id="计算多个通道的输出的互相关函数"><a href="#计算多个通道的输出的互相关函数" class="headerlink" title="计算多个通道的输出的互相关函数"></a>计算多个通道的输出的互相关函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line">K.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([3, 2, 2, 2])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corr2d_multi_in_out(X, K)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 56.,  72.],</span><br><span class="line">         [104., 120.]],</span><br><span class="line"></span><br><span class="line">        [[ 76., 100.],</span><br><span class="line">         [148., 172.]],</span><br><span class="line"></span><br><span class="line">        [[ 96., 128.],</span><br><span class="line">         [192., 224.]]])</span><br></pre></td></tr></table></figure><h3 id="1-times-1-卷积"><a href="#1-times-1-卷积" class="headerlink" title="$1 \times 1$卷积"></a>$1 \times 1$卷积</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    c_i, h, w = X.shape</span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]</span><br><span class="line">    X = X.reshape(c_i, h * w)</span><br><span class="line">    K = K.reshape(c_o, c_i)</span><br><span class="line">    Y = torch.matmul(K, X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(c_o, h, w)</span><br><span class="line"></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)</span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)</span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br><span class="line">Y1.shape, Y2.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([2, 3, 3]), torch.Size([2, 3, 3]))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 21 卷积层里的多输入多输出通道。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 20 卷积层里的填充和步幅</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/20/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/20/</id>
    <published>2023-10-21T12:00:00.000Z</published>
    <updated>2023-10-21T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="20-卷积层里的填充和步幅"><a href="#20-卷积层里的填充和步幅" class="headerlink" title="20 卷积层里的填充和步幅"></a>20 卷积层里的填充和步幅</h1><h2 id="填充和步幅"><a href="#填充和步幅" class="headerlink" title="填充和步幅"></a>填充和步幅</h2><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><ul><li>给定$32 \times 32$的输入图像</li><li>应用$5 \times 5$的卷积核<ul><li>第1层得到输出大小$28 \times 28$</li><li>第2层$24 \times 24$</li><li>第3层$20 \times 20$</li><li>第4层$16 \times 16$</li><li>第5层$12 \times 12$</li><li>第6层$8 \times 8$</li><li>第7层得到输出大小$4 \times 4$</li></ul></li><li>更大的卷积核可以更快地减小输出大小<ul><li>形状从$n_h \times n_w$减少到$(n_h - k_h + 1) \times (n_w - k_w + 1)$</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/20/image-20231021201621492.png" alt="image-20231021201621492"></p><p>在输入周围添加额外的行/列：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/20/image-20231021203757305.png" alt="image-20231021203757305"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/20/convolution1.gif" alt="convolution1"></p><ul><li><p>填充$p_h$行和$p_w$列，输出形状为：</p><script type="math/tex; mode=display">  (n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)</script></li><li><p>通常取$p_h = k_h - 1$，$p_w = k_w - 1$，这样卷积核不会让输出变小</p><ul><li>当$k_h$为奇数：在上下两侧填充$\frac{p_h}{2}$</li><li>当$k_h$为偶数：在上侧填充$\left\lceil\frac{p_h}{2}\right\rceil$，在下侧填充$\left\lfloor\frac{p_h}{2}\right\rfloor$</li></ul></li></ul><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><ul><li>填充减小的输出大小与层数线性相关<ul><li>给定输入大小$224 \times 224$，在使用$5 \times 5$卷积核的情况下，需要55层将输出降低到$4 \times 4$</li><li>需要大量计算才能得到较小输出</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/20/image-20231021204905896.png" alt="image-20231021204905896"></p><ul><li>步幅是指行/列的滑动步长<ul><li>例：高度3宽度2的步幅</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/20/image-20231021205240257.png" alt="image-20231021205240257"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/20/convolution2.gif" alt="convolution2"></p><ul><li><p>给定高度$s_h$和宽度$s_w$的步幅，输出形状是：</p><script type="math/tex; mode=display">  \left\lfloor\frac{n_h - k_h + p_h + s_h}{s_h}\right\rfloor \times \left\lfloor\frac{n_w - k_w + p_w + s_w}{s_w}\right\rfloor</script></li><li><p>如果$p_h = k_h - 1$，$p_w = k_w - 1$，输出形状是：</p><script type="math/tex; mode=display">  \left\lfloor\frac{n_h + s_h - 1}{s_h}\right\rfloor \times \left\lfloor\frac{n_w + s_w - 1}{s_w}\right\rfloor</script></li><li><p>如果输入高度和宽度可以被步幅整除：</p><script type="math/tex; mode=display">  \frac{n_h}{s_h} \times \frac{n_w}{s_w}</script></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>填充和步幅是卷积层的超参数</li><li>填充在输入周围添加额外的行/列，来控制输出形状的减少量</li><li>步幅是每次滑动核窗口的行/列的步长，可以成倍的减少输出形状</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="在所有侧边填充1个像素"><a href="#在所有侧边填充1个像素" class="headerlink" title="在所有侧边填充1个像素"></a>在所有侧边填充1个像素</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([8, 8])</span><br></pre></td></tr></table></figure><p>公式为：$(n_h - k_h + 1) \times (n_w - k_w + 1)$。</p><p>这里feature map的大小由<code>X = torch.rand(size=(8, 8))</code>指定，是一个$8 \times 8$的矩阵。</p><p>然后卷积核被声明为大小为$3 \times 3$，然后padding为1，那么会在<code>X</code>的周围加上一圈0，形状变为$10 \times 10$，最后卷积结果的形状仍然为$8 \times 8$。</p><h3 id="填充不同的高度和宽度"><a href="#填充不同的高度和宽度" class="headerlink" title="填充不同的高度和宽度"></a>填充不同的高度和宽度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([8, 8])</span><br></pre></td></tr></table></figure><p>公式为：$(n_h - k_h + p_h + 1) \times (n_w - k_w + p_w + 1)$。</p><p>这里同理，卷积核大小为$5 \times 3$，然后padding为$(2, 1)$，那么<code>X</code>的形状变为$12 \times 10$，最后卷积结果的形状为$(8 - 5 + 4 + 1) \times (8 - 3 + 2 + 1) = 8 \times 8$。</p><h3 id="将高度和宽度的步幅设置为2"><a href="#将高度和宽度的步幅设置为2" class="headerlink" title="将高度和宽度的步幅设置为2"></a>将高度和宽度的步幅设置为2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4, 4])</span><br></pre></td></tr></table></figure><p>公式为：$\left\lfloor\frac{n_h - k_h + p_h + s_h}{s_h}\right\rfloor \times \left\lfloor\frac{n_w - k_w + p_w + s_w}{s_w}\right\rfloor$。</p><p>代入可得：</p><script type="math/tex; mode=display">\begin{aligned}\left\lfloor\frac{n_h - k_h + p_h + s_h}{s_h}\right\rfloor \times \left\lfloor\frac{n_w - k_w + p_w + s_w}{s_w}\right\rfloor&= \left\lfloor\frac{8 - 3 + 1 + 2}{2}\right\rfloor \times \left\lfloor\frac{8 - 3 + 1 + 2}{2}\right\rfloor \\&= 4 \times 4\end{aligned}</script><h3 id="一个稍微复杂的例子"><a href="#一个稍微复杂的例子" class="headerlink" title="一个稍微复杂的例子"></a>一个稍微复杂的例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 2])</span><br></pre></td></tr></table></figure><p>公式为：$\left\lfloor\frac{n_h - k_h + p_h + s_h}{s_h}\right\rfloor \times \left\lfloor\frac{n_w - k_w + p_w + s_w}{s_w}\right\rfloor$。</p><p>代入可得：</p><script type="math/tex; mode=display">\begin{aligned}\left\lfloor\frac{n_h - k_h + p_h + s_h}{s_h}\right\rfloor \times \left\lfloor\frac{n_w - k_w + p_w + s_w}{s_w}\right\rfloor&= \left\lfloor\frac{8 - 3 + 0 + 3}{3}\right\rfloor \times \left\lfloor\frac{8 - 5 + 1 + 4}{4}\right\rfloor \\&= \left\lfloor\frac{8}{3}\right\rfloor \times \left\lfloor2\right\rfloor \\&= 2 \times 2\end{aligned}</script>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 20 卷积层里的填充和步幅。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》</title>
    <link href="http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/"/>
    <id>http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/</id>
    <published>2023-10-19T06:00:00.000Z</published>
    <updated>2023-10-21T11:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PoseCNN-A-Convolutional-Neural-Network-for-6D-Object-Pose-Estimation-in-Cluttered-Scenes"><a href="#PoseCNN-A-Convolutional-Neural-Network-for-6D-Object-Pose-Estimation-in-Cluttered-Scenes" class="headerlink" title="PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes"></a>PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》 | Karl的博客 (karltan.com)</a></p><p>CSDN链接：<a href="https://blog.csdn.net/karltan0328/article/details/134442631">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》-CSDN博客</a></p><p>论文链接：<a href="https://arxiv.org/abs/1711.00199">[1711.00199] PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes (arxiv.org)</a></p><p>代码链接：<a href="https://github.com/yuxng/PoseCNN">yuxng/PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes (github.com)</a></p><p>项目链接：<a href="https://rse-lab.cs.washington.edu/projects/posecnn/">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes – UW Robotics and State Estimation Lab (washington.edu)</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>对一个已知物体进行6D姿态估计对于机器人与现实世界的交互是非常重要的。</p><p>但是由于对象的多样性、对象的杂乱以及对象之间的遮挡引起的复杂性，这个问题是具有挑战性的。</p><p>所以这篇文章提出了PoseCNN，一种新的卷积神经网络。PoseCNN通过定位物体在图像中的中心并预测其到相机的距离来估计物体的3D平移。通过回归到四元数表示来估计物体的3D旋转。</p><p>然后这里提出了一种现代的损失函数，能够让PoseCNN有处理对称对象的能力。</p><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>应用场景：</p><ol><li>机器人操作需要识别物体的3D位置和方向</li><li>机器人可以从演示中学习</li></ol><p>挑战：</p><ol><li>对象具有多样性</li><li>对象有不同的3D形状</li><li>对象在图像中的外观受光照、物体之间的杂乱和物体之间的相互遮挡所影响</li></ol><p>在传统方法中，6D姿态估计问题是通过3D模型与图像之间的特征点匹配来解决的[20, 25, 8]。然而，这要求物体具有丰富的纹理。这就导致了传统方法无法处理无纹理对象。</p><p>随着深度相机的出现，人们提出了几种使用RGB-D数据来识别无纹理物体的方法[13, 3, 2, 26, 15]。</p><p>但是对于基于模板的方法[13, 12]，遮挡显著降低了识别性能。</p><p>对于通过学习将图像像素回归到3D对象坐标以建立用于6D姿态估计的2D-3D对应关系的方法[3, 4]不能处理对称对象。</p><p>PoseCNN背后的一个关键思想是将姿态估计任务解耦为不同的组件，这使得网络能够显式地建模它们之间的依赖和独立。</p><p>PoseCNN执行三个相关任务，如图1所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019163316117.png" alt="image-20231019163316117"></p><ol><li><p>首先，它为输入图像中的每个像素预测一个对象标签，做语义分割。</p></li><li><p>其次，通过预测从每个像素到中心的单位向量来估计目标中心的2D像素坐标。</p><p> 使用语义分割标签，与对象相关联的图像像素会在图像中<strong>投票</strong>选出对象中心位置。</p><p> 此外，该网络还可以估算物体中心的距离。</p><p> 假设已知相机特性，估计2D对象中心和它的距离能够让我们还原它的3D平移$\mathbf{T}$。</p></li><li><p>最后，通过将bounding box内提取的卷积特征回归到$\mathbf{R}$的四元数表示来估计3D旋转$\mathbf{R}$。</p></li></ol><p>正如我们将展示的，先进行2D中心投票，然后进行旋转回归来估计$\mathbf{R}$和$\mathbf{T}$，可以应用于纹理/无纹理的物体。</p><p>并且因为网络被训练去给物体的中心投票，所以<strong>对遮挡是鲁棒的</strong>。</p><p>处理<strong>对称对象</strong>是姿态估计的另一个挑战，因为不同的对象方向可能产生相同的观察结果。例如，不可能唯一地估计红色碗（第三行第三个）或木块（第三行第四个）的方向，如图5所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019170722332.png" alt="image-20231019170722332"></p><p>虽然像OccludedLINEMOD数据集[17]这样的基准数据集考虑对这些对象进行特殊的对称评估，但在网络训练期间，对称性通常会被忽略。</p><p>然而，这可能会导致糟糕的训练性能，因为网络接收到不一致的损失信号，例如在目标方向上的高损失，即使从网络中估计的对象的对称性是正确的。</p><p>受此启发，我们引入了ShapeMatch-Loss，这是一个<strong>新的损失函数</strong>，专注于匹配对象的3D形状。</p><p>我们将证明这个损失函数对具有形状对称性的物体产生了更好的估计。</p><p>我们在OccludedLINEMOD数据集[17]上评估了我们的方法，这是一个用于6D姿态估计的基准数据集。在这个具有挑战性的数据集上，PoseCNN实现了纯颜色和RGB-D姿态估计的最新结果（我们在迭代最近点（ICP）算法中使用深度图像进行姿态优化）。</p><p>综上所述，我们的工作有以下主要贡献：</p><ul><li>我们提出了一种用于6D目标位姿估计的卷积神经网络，命名为PoseCNN。我们的网络实现端到端的6D姿态估计，并且对物体之间的遮挡非常鲁棒。</li><li>提出了一种用于对称目标位姿估计的训练损失函数ShapeMatch-Loss。</li><li>我们为6D对象的姿态估计提供了一个大尺度RGB-D视频数据集，其中我们为21个YCB对象提供了6D姿态标注。</li></ul><h2 id="II-RELATED-WORK"><a href="#II-RELATED-WORK" class="headerlink" title="II. RELATED WORK"></a>II. RELATED WORK</h2><p>文献中的6D目标位姿估计方法大致可以分为<strong>基于模板的方法</strong>和<strong>基于特征的方法</strong>。</p><p><strong>在基于模板的方法中</strong>，构造一个刚体模板，用于扫描输入图像中的不同位置。在每个位置，计算一个相似度得分，通过比较这些相似度得分得到最佳匹配[12, 13, 6]。在6D位姿估计中，通常通过渲染相应的3D模型来获得模板。最近，2D<strong>目标检测</strong>方法被用于模板匹配和6D姿态估计，特别是基于深度学习的目标检测器[28, 23, 16, 29]。基于模板的方法在检测无纹理对象方面很有用。但是，<strong>它们不能很好地处理对象之间的遮挡</strong>，因为如果对象被遮挡，模板的相似度分数会很低。</p><p><strong>在基于特征的方法中</strong>，从感兴趣的点或图像中的每个像素点提取局部特征，并与3D模型上的特征进行匹配，建立2D-3D对应关系，从而恢复6D姿态[20, 25, 30, 22]。基于特征的方法能够处理物体之间的遮挡。然而，为了计算局部特征，它们需要在对象上<strong>有足够的纹理</strong>。为了处理无纹理对象，提出了几种使用机器学习技术学习特征描述符的方法[32, 10]。已经有若干种对于每个像素直接回归到3D对象坐标来建立2D-3D对应关系的方法被提出了[3, 17, 4]。但是<strong>3D坐标回归在处理对称对象时遇到了歧义</strong>。</p><p>在这项工作中，我们在深度学习框架中结合了基于模板的方法和基于特征的方法的优点，其中网络结合了自底向上像素标记和自顶向下对象姿态回归。最近，由于亚马逊拾取挑战（APC）的竞争，6D物体的姿态估计问题得到了更多的关注。APC中针对特定的设置引入了几种数据集和方法[24, 35]。只要提供适当的训练数据，我们的网络有潜力应用于APC设置。</p><h2 id="III-POSECNN"><a href="#III-POSECNN" class="headerlink" title="III. POSECNN"></a>III. POSECNN</h2><p>给定一幅输入图像，6D目标位姿估计的任务是估计从目标坐标系$O$到摄像机坐标系$C$的刚性变换。我们假设对象的3D模型是可用的，并且对象的坐标系统定义在模型的3D空间中。这里的刚性变换由一个包含3D旋转R和3D平移T的SE(3)变换组成，其中$\mathbf{R}$表示围绕对象坐标系$O$的X轴、Y轴和Z轴的旋转角度，$\mathbf{T}$是摄像机坐标系$C$中$O$的原点的坐标。</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019190812549.png" alt="image-20231019190812549"></p><p>在成像过程中，$\mathbf{T}$决定了物体在图像中的位置和比例，而$\mathbf{R}$根据物体的3D形状和纹理影响物体的图像外观。由于这两个参数有不同的视觉特性，我们提出了一个卷积神经网络结构，内部解耦$\mathbf{R}$和$\mathbf{T}$的估计。</p><h3 id="A-Overview-of-the-Network"><a href="#A-Overview-of-the-Network" class="headerlink" title="A. Overview of the Network"></a>A. Overview of the Network</h3><p>图2说明了我们用于6D目标姿态估计的网络结构：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019182446364.png" alt="image-20231019182446364"></p><p>网络包含<strong>两个阶段</strong>。</p><p><strong>第一阶段</strong>由13个卷积层和4个maxpooling层组成，从输入图像中提取不同分辨率的特征图。这个阶段是网络的主干，因为所提取的特征在网络执行的所有任务中共享。</p><p><strong>第二阶段</strong>包括一个嵌入步骤，将第一阶段生成的高维特征映射嵌入到低维、特定于任务的特征中。然后，网络执行6D姿态估计的三个不同任务，即语义标记、3D平移估计和3D旋转回归，如下所述。</p><h3 id="B-Semantic-Labeling"><a href="#B-Semantic-Labeling" class="headerlink" title="B. Semantic Labeling"></a>B. Semantic Labeling</h3><p>为了检测图像中的对象，我们采用语义标记，网络将每个图像像素分类成一类（这个像素如果属于罐头，那么这个像素就被分类为罐头类）。</p><p>与最近借助于bounding boxes目标检测的6D位姿估计方法[23, 16, 29]相比，语义分割标记能够提供更丰富的对象信息，并且能够更好的处理遮挡。</p><p>语义标注分支的嵌入步骤如图2所示，以特征提取阶段生成的两个通道尺寸为512的特征映射作为输入。两种特征图的分辨率分别为原始图像尺寸的$\frac{1}{8}$和$\frac{1}{16}$。该网络首先使用两个卷积层将两个特征映射的信道维数降至64。然后用反卷积层将$\frac{1}{16}$特征图的分辨率提高一倍。然后将两个feature map求和，再用另一个反卷积层将分辨率提高8倍，得到与原始图像大小一致的feature map。最后，卷积层对特征图进行操作，生成像素的语义标记分数。过程如下图：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019190153685.png" alt="image-20231019190153685"></p><p>这一层的输出有$n$个通道和$n$个语义类的数量。在训练中，采用softmax交叉熵损失训练语义标注分支。在测试过程中，使用softmax函数计算像素的类概率。语义标注分支的设计灵感来自于[19]中用于语义标注的全卷积网络。</p><h3 id="C-3D-Translation-Estimation"><a href="#C-3D-Translation-Estimation" class="headerlink" title="C. 3D Translation Estimation"></a>C. 3D Translation Estimation</h3><p>3D平移$\mathbf{T} = (T_x, T_y, T_z)^\mathrm{T}$是物体原点$O$在摄像机坐标系$C$中的坐标，如图3所示：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019190812549.png" alt="image-20231019190812549"></p><p>估计$\mathbf{T}$的一种简单方法是直接将图像特征回归到$\mathbf{T}$。但是，由于对象可以出现在图像中的任何位置，所以这种方法是不可泛化的（不能将图像的位置和物体的类型进行关联，这是不对的）。此外，它不能处理同一类别中的多个对象实例。</p><p>因此，我们提出通过定位图像中的2D目标中心，估计目标到摄像机的距离来估计3D平移量。</p><p>假设$\mathbf{T}$在图像上的投影为$\mathbf{c} = (c_x, c_y)^\mathrm{T}$。对于针孔摄像机来说，如果网络能够在图像中定位$\mathbf{c}$并估计出深度$T_z$，那么我们可以根据下面的投影方程恢复$T_x$和$T_y$：</p><script type="math/tex; mode=display">\begin{bmatrix}c_x \\c_y\end{bmatrix}=\begin{bmatrix}f_x\frac{T_x}{T_z} + p_x \\f_y\frac{T_y}{T_z} + p_y\end{bmatrix}</script><p>其中$f_x$和$f_y$表示相机的焦距，$(p_x, p_y)^\mathrm{T}$是主点（相机原点）。</p><blockquote><p>针孔相机模型：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231021153356851.png" alt="image-20231021153356851"></p><p>下图将成像平面平移到了小孔处：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231021142854059.png" alt="针孔相机模型"></p><p>那么显然，下图中的$\bigtriangleup P^\prime OB$与$\bigtriangleup POA$是相似的：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231021191609785.png" alt="image-20231021191609785"></p><p>假设图中$P$点的坐标为$(T_x, T_y, T_z)$，$P^\prime$点的坐标为$(c_x, c_y)$，那么有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{c_x}{f} &= \frac{T_x}{T_z} \\\frac{c_y}{f} &= \frac{T_y}{T_z}\end{aligned}</script><p>那么可得：</p><script type="math/tex; mode=display">\begin{aligned}c_x &= f\frac{T_x}{T_z} \\c_y &= f\frac{T_y}{T_z}\end{aligned}</script><p>又小孔成像模型的原点在图像的中心位置，数字图像处理的原点在图像的左上角，所以需要一个偏移量$(p_x, p_y)$：</p><script type="math/tex; mode=display">\begin{aligned}c_x &= f\frac{T_x}{T_z} + p_x \\c_y &= f\frac{T_y}{T_z} + p_y\end{aligned}</script><p>即：</p><script type="math/tex; mode=display">\begin{bmatrix}c_x \\c_y\end{bmatrix}=\begin{bmatrix}f_x\frac{T_x}{T_z} + p_x \\f_y\frac{T_y}{T_z} + p_y\end{bmatrix}</script></blockquote><p>如果物体原点$O$是物体的质心，我们称$\mathbf{c}$为物体的2D中心（即物体在图像上的位置）。</p><p>一种直接定位2D目标<strong>中心</strong>（注意这里说的是中心，需要与后面的内点做区分）的方法是像现有的关键点检测方法一样直接检测中心点[22, 7]。然而，如果对象中心被遮挡，这些方法将不起作用。</p><p>在隐式形状模型（Implicit Shape Model, ISM）中，为了进行检测，图像块会对物体中心进行投票，受该模型的启发，我们设计了我们的网络以回归预测出图像中每个像素的中心<em>方向</em>。</p><p>具体来说，对于图像上的像素$\mathbf{p} = (x, y)^\mathrm{T}$，它回归到三个变量（这里顺便回归出了内点的深度，因为遮挡，不能直接回归物体中心的深度）：</p><script type="math/tex; mode=display">(x, y) \to \left(n_x = \frac{c_x - x}{\Vert\mathbf{c} - \mathbf{p}\Vert}, n_y = \frac{c_y - y}{\Vert\mathbf{c} - \mathbf{p}\Vert}, T_z\right)</script><p>注意，我们没有直接回归到位移矢量$\mathbf{c} - \mathbf{p}$，而是将网络设计成回归到单位长度矢量$\mathbf{n} = (n_x, n_y)^\mathrm{T} = \frac{\mathbf{c} - \mathbf{p}}{\Vert\mathbf{c} - \mathbf{p}\Vert}$，即2D中心方向，它是尺度不变的，因此更容易训练（正如我们通过实验验证的那样）。</p><p>我们网络的中心回归分支（图2）与语义标记分支采用相同的架构，只是卷积层和反卷积层的通道维数不同。我们将高维特征嵌入到128维空间而不是64维空间，因为这个分支需要为每个对象类回归到三个变量。该分支的最后一个卷积层的通道维数为$3 \times n$，对象类数为$n$。在训练中，使用平滑的L1损失函数进行回归，如[11]。</p><p>为了找到一个对象的2D对象中心$\mathbf{c}$，设计了一个Hough投票层并集成到网络中。Hough投票层采用像素级语义标记结果和中心回归结果作为输入。对于每个对象，它首先计算图像中每个位置的投票分数。投票分数表明该位置是对象中心的可能性有多大。具体地说，对象中的每一个像素都会在网络预测出的方向上投票，见图4：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019195938437.png" alt="image-20231019195938437"></p><p>在处理对象中的所有像素后，我们获得所有图像位置的投票分数。然后得分最高的位置就作为目标中心。</p><p>对于同一对象可能多次出现在图像中的情况，我们对投票分数应用非极大值抑制，然后选择分数大于某一阈值的位置。</p><p>在生成一组对象中心后，我们认为投票给对象中心的像素是中心的<strong>内层</strong>。然后将中心$T_z$的预测深度简单地计算为内层预测深度的平均值。最后，利用公式$\begin{bmatrix}c_x \\ c_y\end{bmatrix} = \begin{bmatrix} f_x\frac{T_x}{T_z} + p_x \\ f_y\frac{T_y}{T_z} + p_y\end{bmatrix}$，我们可以估计出3D平移$\mathbf{T}$。</p><p>此外，网络生成对象的bounding box作为约束所有<strong>内层</strong>的2D矩形，该bounding box用于3D旋转回归。</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019182446364.png" alt="image-20231019182446364"></p><h3 id="D-3D-Rotation-Regression"><a href="#D-3D-Rotation-Regression" class="headerlink" title="D. 3D Rotation Regression"></a>D. 3D Rotation Regression</h3><p>图2的最下方为3D旋转回归分支。</p><p>利用Hough投票层预测的对象的bounding boxes，我们可以用两个RoI池化层[11]对网络第一阶段生成的视觉特征进行“裁剪和池化”，用于3D旋转回归。合并的特征映射被加在一起，并馈送到三个全连接（FC）层中。前两个FC层的维数为4096，最后一个FC层的维数为$4 \times n$，对象类数为$n$。对于每个类，最后一个FC层输出一个四元数表示的3D旋转。</p><p>为了训练四元数回归，我们提出了两个损失函数，其中一个是专门设计用于处理对称对象。</p><p>第一个损失函数称为PoseLoss（PLOSS），它在3D模型空间中运行，并使用正确模型位姿和估计模型位姿两个位姿之间的对应点来测算平均平方距离。</p><p>PLOSS定义为：</p><script type="math/tex; mode=display">\text{PLOSS}(\tilde{\mathbf{q}}, \mathbf{q}) = \frac{1}{2m}\sum_{\mathbf{x} \in \mathcal{M}}\Vert R(\tilde{\mathbf{q}})\mathbf{x} - R(\mathbf{q})\mathbf{x}\Vert^2</script><p>$\mathcal{M}$为3D模型点集，$m$为点个数。$R(\tilde{\mathbf{q}})$和$R(\mathbf{q})$分别表示由估计的四元数和GT四元数计算得到的旋转矩阵。当估计方向与GT方向相同时，该损失有其唯一的最小值。</p><p>不幸的是，PLOSS不能适当地处理对称对象，因为一个对称对象可以有多个正确的3D旋转。在对称物体上使用这样的损失函数会对网络进行不必要的惩罚，因为它会回归到一个可选的3D旋转中，因此可能会给出不一致的训练信号。</p><p>虽然可以通过手动指定对象对称性，然后考虑所有正确的方向作为GT选项来修改PLOSS，进而处理对称对象，但我们在这里仍然要引入ShapeMatch-Loss（SLOSS），这是一个不需要指定对称性的损失函数。</p><p>SLOSS定义为：</p><script type="math/tex; mode=display">\text{SLOSS}(\tilde{\mathbf{q}}, \mathbf{q}) = \frac{1}{2m}\sum_{\mathbf{x}_1 \in \mathcal{M}}\min_{\mathbf{x}_2 \in \mathcal{M}}\Vert R(\tilde{\mathbf{q}})\mathbf{x}_1 - R(\mathbf{q})\mathbf{x}_2\Vert^2</script><p>我们可以看到，就像ICP一样，这个损失测量的是估计模型方向上的每个点与GT模型上最近的点之间的偏移。当两个3D模型匹配时，SLOSS最小。</p><p>通过这种方式，SLOSS将不会惩罚相对于对象的3D形状对称等效的旋转。</p><h2 id="IV-THE-YCB-VIDEO-DATASET"><a href="#IV-THE-YCB-VIDEO-DATASET" class="headerlink" title="IV. THE YCB-VIDEO DATASET"></a>IV. THE YCB-VIDEO DATASET</h2><p>因为标签是手动添加的，所以以对象为中心的为对象位姿和/或分割提供GT的数据集不会太大。例如，流行的LINEMOD数据集[13]为数据集中的15个对象中的每个对象提供了大约1000张图像的手动注释。虽然这样的数据集对基于模型的姿态估计技术的评估是有用的，但它比训练最先进的深度神经网络的典型数据集要小几个数量级。这个问题的一个解决方案是用合成图像来扩充数据。但是，必须注意确保在真实场景和渲染场景之间的性能是通用的。</p><h3 id="A-6D-Pose-Annotation"><a href="#A-6D-Pose-Annotation" class="headerlink" title="A. 6D Pose Annotation"></a>A. 6D Pose Annotation</h3><p>为了避免手动注释所有的视频帧，我们只在每个视频的第一帧手动指定对象的姿势。我们在第一个深度帧中细化每个对象的姿态，使用每个对象的符号距离函数（Signed Distance Function, SDF）表示。接下来，通过固定物体相对于其他物体的姿态并通过深度视频跟踪物体的配置来初始化摄像机的轨迹。最后，对摄像机轨迹和相对目标位姿进行全局优化。</p><h3 id="B-Dataset-Characteristics"><a href="#B-Dataset-Characteristics" class="headerlink" title="B. Dataset Characteristics"></a>B. Dataset Characteristics</h3><p>我们使用的对象是如图5所示的21个YCB对象[5]的一个子集：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019170722332.png" alt="image-20231019170722332"></p><p>选择这些对象是因为高质量的3D模型和良好的深度可见性。视频采集使用华硕Xtion Pro Live RGB-D相机，采用快速裁剪模式，通过在本地设备上捕获1280x960图像，并通过USB只传输中心区域，以30帧/秒的速度提供640x480分辨率的RGB图像。这以较低的视场为代价获得了更高的RGB图像的有效分辨率，但考虑到深度传感器的最小范围，这是一个可以接受的折中。</p><p>完整的数据集包含133,827幅图像，比LINEMOD数据集大两个数量级。关于数据集的更多统计信息请参见表1：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019210710064.png" alt="image-20231019210710064"></p><p>图6为我们数据集中的一个标注示例：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019210728586.png" alt="image-20231019210728586"></p><p>我们根据标注的GT位姿来渲染3D模型。</p><p>请注意，我们的标注精度受到几个误差源的影响，包括RGB传感器的滚动快门、对象模型中的不准确性、RGB和深度传感器之间的轻微异步以及相机内外参数的不确定性。</p><h2 id="V-EXPERIMENTS"><a href="#V-EXPERIMENTS" class="headerlink" title="V. EXPERIMENTS"></a>V. EXPERIMENTS</h2><h3 id="A-Datasets"><a href="#A-Datasets" class="headerlink" title="A. Datasets"></a>A. Datasets</h3><p>在我们的YCB-Video数据集中，我们使用80个视频进行训练，并从剩下的12个测试视频中提取2949个关键帧进行测试。我们还在OccludedLINEMOD数据集[17]上评估了我们的方法。[17]的作者从原始的LINEMOD数据集[13]中选择了一个带有1214帧的视频，并在该视频中标注了8个物体的GT姿势：Ape、Can、Cat、Driller、Duck、Eggbox、Glue和Holepuncher。在这个视频序列中，物体之间有明显的遮挡，这使得这个数据集具有挑战性。为了进行训练，我们使用与这八个对象对应的原始LINEMOD数据集中的八个序列。此外，我们通过在场景中随机放置物体，在两个数据集上生成80,000张合成图像进行训练。</p><h3 id="B-Evaluation-Metrics"><a href="#B-Evaluation-Metrics" class="headerlink" title="B. Evaluation Metrics"></a>B. Evaluation Metrics</h3><p>我们采用[13]中提出的平均距离（ADD）度量进行评估。给定GT旋转$\mathbf{R}$和平移$\mathbf{T}$以及估计的旋转$\tilde{\mathbf{R}}$和平移$\tilde{\mathbf{T}}$，平均距离计算根据GT姿态和估计姿态变换的3D模型点之间成对距离的平均值：</p><script type="math/tex; mode=display">\text{ADD} = \frac{1}{m}\sum_{\mathbf{x} \in \mathcal{M}}\Vert(\mathbf{Rx} + \mathbf{T}) - (\tilde{\mathbf{R}}\mathbf{x} + \tilde{\mathbf{T}})\Vert</script><p>式中，$\mathcal{M}$为3D模型点集，$m$为点个数。如果平均距离小于预定义的阈值，则认为6D姿势是正确的。</p><p>在OccludedLINEMOD数据集中，阈值设置为3D模型直径的10%。</p><p>对于像Eggbox和Glue这样的对称对象，对于某些视图，点之间的匹配是不明确的。因此，利用最近点距离计算平均距离：</p><script type="math/tex; mode=display">\text{ADD-S} = \frac{1}{m}\sum_{\mathbf{x}_1 \in \mathcal{M}}\min_{\mathbf{x}_2 \in \mathcal{M}}\Vert(\mathbf{R}\mathbf{x}_1 + \mathbf{T}) - (\tilde{\mathbf{R}}\mathbf{x}_2 + \tilde{\mathbf{T}})\Vert</script><p>我们为旋转回归设计的损失函数受这两个评价指标所启发。并且在计算位姿精度时使用固定的阈值不能揭示方法在这些不正确的位姿上的表现。因此，我们在评价时改变距离阈值。在这种情况下，我们可以绘制一条精度阈值曲线，并计算曲线下的面积来进行姿态评估。</p><p>我们可以将变换后的点投影到图像上，然后在图像空间中计算成对的距离，而不是在3D空间中计算距离。这个度量被称为重投影损失，广泛用于仅使用彩色图像时的6D姿态估计。</p><h3 id="C-Implementation-Details"><a href="#C-Implementation-Details" class="headerlink" title="C. Implementation Details"></a>C. Implementation Details</h3><p>PoseCNN是使用TensorFlow库[1]实现的。和[31]一样，Hough投票层是在GPU上实现的。在训练中，使用ImageNet[9]上训练的VGG16网络[27]对特征提取阶段的前13个卷积层和3D旋转回归分支的前两个FC层的参数进行初始化。没有梯度通过Hough投票层反向传播。采用带动量的随机梯度下降法（SGD）进行训练。</p><h3 id="D-Baselines"><a href="#D-Baselines" class="headerlink" title="D. Baselines"></a>D. Baselines</h3><p><strong>3D对象坐标回归网络。</strong>由于目前最先进的6D姿态估计方法大多依赖于将图像像素回归到3D物体坐标[3, 4, 21]，因此我们实现了一种用于3D物体坐标回归的网络变体以进行比较。在这个网络中，我们不是像图2那样回归到中心的方向和深度，而是回归到每个像素在对象坐标系中的3D坐标。我们可以使用相同的架构，因为对于每个类，每个像素仍然回归到三个变量。然后我们删除3D旋转回归分支。利用语义标记结果和3D物体坐标回归结果，利用pre-emptive RANSAC恢复6D姿态，如[4]所示。</p><p>意思是现有的方法是直接将2D坐标回归到3D坐标，从而得到对象的3D坐标。但是论文中的方法是先通过语义分割估计物体在图像中的中心坐标，然后利用对内点的深度取平均来得到中心的深度，从而根据公式算出$\mathbf{T}$。那么为了将直接回归的方法和论文中的分步方法进行比较，这里以论文网络架构为基础，又编写了一套直接回归的算法以方便比较。</p><p><strong>姿势细化。</strong>当深度可用时，我们可以通过网络估算出6D姿态。我们使用迭代最近点（Iterative Closest Point, ICP）算法来细化6D位姿。具体地说，我们采用具有投影数据关联和点平面残差项的ICP。每个像素的残差是3D中观测点到由3D中渲染点及其法线定义的平面的最小距离。残差大于指定阈值的点被拒绝，剩余残差使用梯度下降最小化。利用网络中的语义标签对深度图像中的观察点进行裁剪。由于ICP对局部最小值不具有鲁棒性，我们通过对网络中估计的位姿进行扰动来细化多个位姿，然后利用[33]中提出的对准度量来选择最优的细化位姿。</p><h3 id="E-Analysis-on-the-Rotation-Regress-Losses"><a href="#E-Analysis-on-the-Rotation-Regress-Losses" class="headerlink" title="E. Analysis on the Rotation Regress Losses"></a>E. Analysis on the Rotation Regress Losses</h3><p>我们首先通过实验分析了在对称物体上两种损失函数对旋转回归的影响。图7为YCB-Video数据集（木块和大夹子）中两个对称对象使用这两个损失函数进行训练时的旋转损失直方图（PLoss使用正确模型位姿和估计模型位姿两个位姿之间的对应点来测算平均平方距离，SLoss测量的是估计模型方向上的每个点与GT模型上最近的点之间的偏移）：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020214447284.png" alt="image-20231020214447284"></p><p>木块和大夹子的PLOSS的旋转损失跨越了0度到180度。这两个直方图表明网络被对称对象混淆了。显然，SLOSS的损失直方图集中于木块的180度以及大夹子的0度和180度，因为它们围绕其坐标轴旋转180度是对称的。</p><h3 id="F-Results-on-the-YCB-Video-Dataset"><a href="#F-Results-on-the-YCB-Video-Dataset" class="headerlink" title="F. Results on the YCB-Video Dataset"></a>F. Results on the YCB-Video Dataset</h3><p>表II和图8(a)对YCB-Video数据集中的所有21个对象进行了详细的评估：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020215204360.png" alt="image-20231020215204360"></p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020215241179.png" alt="image-20231020215241179"></p><p>我们使用ADD度量和ADD-S度量来显示精度阈值曲线下的面积，在这里，我们改变平均距离的阈值，然后计算姿态精度。最大阈值设置为10cm。</p><p>我们可以看到：</p><ol><li>在仅使用彩色图像的情况下，我们的网络在6D位姿估计方面明显优于结合pre-emptive RANSAC算法的3D坐标回归网络。当3D坐标回归结果存在误差时，估计的6D位姿会偏离GT位姿很远。而在我们的网络中，即使物体被遮挡，中心定位也有助于约束3D平移估计。</li><li>用ICP来改进姿势，可以显著提高表现。与3D坐标回归网络相比，基于ICP的PoseCNN在使用深度图像时具有更好的性能。ICP的初始位姿是其收敛的关键。PoseCNN为ICP改进提供了更好的初始6D位姿。</li><li>我们可以看到，有些物体更难处理，比如金枪鱼罐头，它很小，而且质地不那么好。这个网络也被大夹子和超大夹子弄混了，因为它们有相同的外观。3D坐标回归网络不能很好地处理对称对象，如香蕉和碗。</li></ol><p>图9显示了YCB-Video数据集上的一些6D姿态估计结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020215756663.png" alt="image-20231020215756663"></p><p>我们可以看到，即使中心被另一个物体遮挡，中心的预测也是相当准确的。我们的网络只有颜色，已经能够提供良好的6D姿态估计。随着ICP的细化，6D位姿的精度进一步提高。</p><h3 id="G-Results-on-the-OccludedLINEMOD-Dataset"><a href="#G-Results-on-the-OccludedLINEMOD-Dataset" class="headerlink" title="G. Results on the OccludedLINEMOD Dataset"></a>G. Results on the OccludedLINEMOD Dataset</h3><p>OccludedLINEMOD数据集具有挑战性，因为对象之间存在显著的遮挡。我们首先只使用彩色图像进行实验。图8(b)显示了数据集中7个对象的重投影损失的精度阈值曲线，其中我们使用彩色图像作为输入，在该数据集上比较了PoseCNN和[29]，该[29]达到了当前最先进的结果：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020220102175.png" alt="image-20231020220102175"></p><p>我们的方法明显优于[29]，特别是在重投影损失阈值较小的情况下。这些结果表明，即使在严重的遮挡下，PoseCNN也能够正确地定位目标对象。</p><p>通过在ICP中使用深度图像来优化姿势，我们的方法也优于使用RGB-D数据作为输入的最新方法。表III总结了OccludedLINEMOD数据集上的姿态估计精度：</p><p><img src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020220226331.png" alt="image-20231020220226331"></p><p>最大的改进来自两个对称的对象“Eggbox”和“Glue”。通过使用我们的ShapeMatch-Loss进行训练，PoseCNN能够正确估计出两个物体的6D位姿的对称性。我们还在表三中展示了仅使用颜色的PoseCNN的结果。由于这里的阈值通常小于2cm，所以精度要低得多。当物体之间存在遮挡时，基于颜色的方法很难在这么小的阈值内获得6D位姿。图9显示了OccludedLINEMOD数据集上的两个6D姿态估计结果示例。</p><h2 id="VI-CONCLUSIONS"><a href="#VI-CONCLUSIONS" class="headerlink" title="VI. CONCLUSIONS"></a>VI. CONCLUSIONS</h2><p>在本文中，我们引入了一种用于6D目标位姿估计的卷积神经网络PoseCNN。PoseCNN解耦了3D旋转和3D平移的估计。它通过定位目标中心和预测中心距离来估计3D平移量。通过将每个像素向目标中心回归到一个单位向量，可以独立于尺度稳健地估计目标中心。更重要的是，像素投票的对象中心，即使它被其他对象遮挡。3D旋转可通过回归到一个四元数表示来预测。引入了两个新的损失函数用于旋转估计，其中ShapeMatch-Loss设计用于对称对象。因此，PoseCNN能够处理混乱场景中的遮挡和对称对象。我们还介绍了一个用于6D目标姿态估计的大规模视频数据集。我们的结果非常令人鼓舞，因为他们表明，在混乱的场景中，仅使用视觉数据准确估计物体的6D姿态是可行的。这为使用分辨率和视场远超目前使用的深度相机系统的相机打开了道路。我们注意到，SLOSS有时会在位姿空间中产生类似于ICP的局部最小值。在未来的6D姿态估计中，探索更有效地处理对称对象的方法将是一件有趣的事情。</p>]]></content>
    
    
    <summary type="html">PoseCNN：基于卷积神经网络的6D目标姿态估计。</summary>
    
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="2018" scheme="http://blog.karltan.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2018/"/>
    
    
    <category term="论文笔记" scheme="http://blog.karltan.com/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    <category term="RSS" scheme="http://blog.karltan.com/tags/RSS/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 19 卷积层</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/19/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/19/</id>
    <published>2023-10-16T09:00:00.000Z</published>
    <updated>2023-10-16T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="19-卷积层"><a href="#19-卷积层" class="headerlink" title="19 卷积层"></a>19 卷积层</h1><h2 id="从全连接到卷积"><a href="#从全连接到卷积" class="headerlink" title="从全连接到卷积"></a>从全连接到卷积</h2><h3 id="分类猫和狗的图片"><a href="#分类猫和狗的图片" class="headerlink" title="分类猫和狗的图片"></a>分类猫和狗的图片</h3><ul><li>使用一个还不错的相机采集图片（12M像素）</li><li>RGB图片有36M元素</li><li>使用100大小的单隐藏层MLP，模型有3.6B元素<ul><li>远多于世界上所有猫和狗总数（900M狗，600M猫）</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016162306703.png" alt="image-20231016162306703"></p><h3 id="回顾：单隐藏层MLP"><a href="#回顾：单隐藏层MLP" class="headerlink" title="回顾：单隐藏层MLP"></a>回顾：单隐藏层MLP</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016162521801.png" alt="image-20231016162521801"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016162732818.png" alt="image-20231016162732818"></p><p>有两个原则：</p><ul><li><p>平移不变性</p><p>  在图片中的任何一处地方对同一个物体进行识别时，识别器应该是不变的。</p></li><li><p>局部性</p><p>  识别某物体不需要全局的信息，只需要关注局部即可。</p></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016162853961.png" alt="image-20231016162853961"></p><h3 id="重新考察全连接层"><a href="#重新考察全连接层" class="headerlink" title="重新考察全连接层"></a>重新考察全连接层</h3><p>我们将全连接层的输入和输出变形为矩阵（宽度，高度），然后将权重由$(\mathrm{h}, \mathrm{w})$变到4-D张量$(\mathrm{h}^\prime, \mathrm{w}^\prime)$</p><script type="math/tex; mode=display">\begin{aligned}h_{i, j}&= \sum_{k, l}w_{i, j, k, l}x_{k, l} \\&= \sum_{a, b}v_{i, j, a, b}x_{i + a, j + b}\end{aligned}</script><p>其中：V是W的重新索引$v_{i, j, a, b} = w_{i, j, i + a, j + b}$</p><p>注意，这里不能用矩阵&amp;向量的思维去尝试理解上面的式子，因为人是无法想象4D空间的，最好是通过下标对应的方式去理解。</p><ul><li><p>首先解释$h_{i, j} = \sum_{k, l}w_{i, j, k, l}x_{k, l} = \sum_{a, b}v_{i, j, a, b}x_{i + a, j + b}$：</p><p>  首先，$h_{i, j}$是输出矩阵中的一个元素，不是整个输出矩阵。</p><p>  在4D权重$w_{i, j, k, l}$中，$i, j$代表输出的元素在输出矩阵中的位置，$k, l$代表输入的元素在输入矩阵中的位置。</p><p>  那么在$\sum_{k, l}w_{i, j, k, l}x_{k, l}$中，相当于是遍历了输入矩阵的每一个元素，然后计算这一整个输入矩阵在对应4D权重影响下的输出。</p><p>  回忆实际的卷积（假设你已经了解过卷积相关的概念），如果对于4D权重$w_{i, j, k, l}$的每一个$i, j$组合，其权值矩阵的值都相同，那么就可以对这个权重进行降维（因为下标$i, j$就没有用了），从而得到我们实际用的卷积。</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/19/convolution1.gif" alt="convolution1"></p></li><li><p>然后解释一下后面的换下标，即$v_{i, j, a, b} = w_{i, j, i + a, j + b}$：</p><p>  当对$w$的下标进行修改时，计算会发生如下的变化：</p><script type="math/tex; mode=display">  \begin{aligned}  h_{i, j}  &= \sum_{k, l}w_{i, j, k, l}x_{k, l} \\  &= \sum_{k, l}w_{i, j, i + a, j + b}x_{i + a, j + b}  \end{aligned}</script><p>  然后再令$v_{i, j, a, b} = w_{i, j, i + a, j + b}$，就能够得到$h_{i, j} = \sum_{a, b}v_{i, j, a, b}x_{i + a, j + b}$。</p></li></ul><h3 id="原则1-平移不变性"><a href="#原则1-平移不变性" class="headerlink" title="原则1 - 平移不变性"></a>原则1 - 平移不变性</h3><ul><li><p>x的平移导致h的平移$h_{i, j} = \sum_{a, b}v_{i, j, a, b}x_{i + a, j + b}$</p></li><li><p>v不应该依赖于$(\mathrm{i}, \mathrm{j})$</p></li><li><p>解决方案：$v_{i, j, a, b} = v_{a, b}$（卷积核的权值共享）（压缩掉前两维）</p><script type="math/tex; mode=display">  h_{i, j} = \sum_{a, b}v_{a, b}x_{i + a, j + b}</script><p>  这就是2维<del>卷积</del>（严格来说叫交叉相关）</p></li></ul><h3 id="原则2-局部性"><a href="#原则2-局部性" class="headerlink" title="原则2 - 局部性"></a>原则2 - 局部性</h3><script type="math/tex; mode=display">h_{i, j} = \sum_{a, b}v_{a, b}x_{i + a, j + b}</script><ul><li><p>当评估$h_{i, j}$时，我们不应该用远离$x_{i, j}$的参数</p></li><li><p>解决方案：当$\left|a\right|, \left|b\right| &gt; \Delta$时，使得$v_{a, b} = 0$</p><script type="math/tex; mode=display">  h_{i, j} = \sum_{a = -\Delta}^\Delta\sum_{b = -\Delta}^\Delta v_{a, b}x_{i + a, j + b}</script><p>  这就是卷积核的大小</p></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>对全连接层使用平移不变性和局部性得到卷积层</p><script type="math/tex; mode=display">\begin{aligned}h_{i, j} &= \sum_{a, b}v_{i, j, a, b}x_{i + a, j + b} \\&\Downarrow \\h_{i, j} &= \sum_{a = -\Delta}^\Delta\sum_{b = -\Delta}^\Delta v_{a, b}x_{i + a, j + b}\end{aligned}</script><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016194621113.png" alt="image-20231016194621113"></p><h3 id="二维交叉相关"><a href="#二维交叉相关" class="headerlink" title="二维交叉相关"></a>二维交叉相关</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/19/convolution2.gif" alt="convolution2"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016194809574.png" alt="image-20231016194809574"></p><ul><li>平移不变性：只有一个卷积核</li><li>局部性：卷积核只覆盖了图片的一部分</li></ul><h3 id="二维卷积层"><a href="#二维卷积层" class="headerlink" title="二维卷积层"></a>二维卷积层</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016195210355.png" alt="image-20231016195210355"></p><ul><li><p>输入$\mathbf{X}$：$n_h \times n_w$</p></li><li><p>核$\mathbf{W}$：$k_h \times k_w$</p></li><li><p>偏差$b \in \mathbb{R}$</p></li><li><p>输出$\mathbf{Y}$：$(n_h - k_h + 1) \times (n_w - k_w + 1)$</p><script type="math/tex; mode=display">  \mathbf{Y} = \mathbf{X} \star \mathbf{W} + b</script></li><li><p>$\mathbf{W}$和$b$是可学习的参数</p></li></ul><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>原图：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016195911673.png" alt="image-20231016195911673"></p><ul><li><p>边缘检测：</p><ul><li><p>卷积核为：</p><script type="math/tex; mode=display">  \begin{bmatrix}  -1 & -1 & -1 \\  -1 & 8 & -1 \\  -1 & -1 & -1  \end{bmatrix}</script></li><li><p>结果为：</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016200106084.png" alt="image-20231016200106084"></p></li></ul></li><li><p>锐化：</p><ul><li><p>卷积核为：</p><script type="math/tex; mode=display">  \begin{bmatrix}  0 & -1 & 0 \\  -1 & 5 & -1 \\  0 & -1 & 0  \end{bmatrix}</script></li><li><p>结果为：</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016200313051.png" alt="image-20231016200313051"></p></li></ul></li><li><p>高斯模糊：</p><ul><li><p>卷积核为：</p><script type="math/tex; mode=display">  \frac{1}{16}\begin{bmatrix}  1 & 2 & 1 \\  2 & 4 & 2 \\  1 & 2 & 1  \end{bmatrix}</script></li><li><p>结果为：</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/19/image-20231016200412885.png" alt="image-20231016200412885"></p></li></ul></li></ul><h3 id="交叉相关-vs-卷积"><a href="#交叉相关-vs-卷积" class="headerlink" title="交叉相关 vs 卷积"></a>交叉相关 vs 卷积</h3><p>二维交叉相关：</p><script type="math/tex; mode=display">y_{i, j} = \sum_{a = 1}^h\sum_{b = 1}^w w_{a, b}x_{i + a, j + b}</script><p>二维卷积：</p><script type="math/tex; mode=display">y_{i, j} = \sum_{a = 1}^h\sum_{b = 1}^w w_{-a, -b}x_{i + a, j + b}</script><p>由于对称性，在实际使用中没有区别。</p><h3 id="一维和三维交叉相关"><a href="#一维和三维交叉相关" class="headerlink" title="一维和三维交叉相关"></a>一维和三维交叉相关</h3><p>一维：</p><script type="math/tex; mode=display">y_i = \sum_{a = 1}^hw_ax_{i + a}</script><ul><li>文本</li><li>语言</li><li>时序序列</li></ul><p>三维：</p><script type="math/tex; mode=display">y_{i, j, k} = \sum_{a = 1}^h\sum_{b = 1}^w\sum_{c = 1}^dw_{a, b, c}x_{i + a, j + b, k + c}</script><ul><li>视频</li><li>医学图像</li><li>气象地图</li></ul><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出</li><li>核矩阵和偏移是可学习的参数</li><li>核矩阵的大小是超参数</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="互相关运算"><a href="#互相关运算" class="headerlink" title="互相关运算"></a>互相关运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算。&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros(X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>验证上述二维互相关运算的输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor(</span><br><span class="line">    [[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>],</span><br><span class="line">    [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>],</span><br><span class="line">    [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]]</span><br><span class="line">)</span><br><span class="line">K = torch.tensor(</span><br><span class="line">    [[<span class="number">0.0</span>, <span class="number">1.0</span>],</span><br><span class="line">    [<span class="number">2.0</span>, <span class="number">3.0</span>]]</span><br><span class="line">)</span><br><span class="line">corr2d(X, K)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[19., 25.],</span><br><span class="line">        [37., 43.]])</span><br></pre></td></tr></table></figure><h3 id="实现二维卷积层"><a href="#实现二维卷积层" class="headerlink" title="实现二维卷积层"></a>实现二维卷积层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure><p>卷积层的一个简单应用，检测图像中不同颜色的边缘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">6</span>, <span class="number">8</span>)</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.],</span><br><span class="line">        [1., 1., 0., 0., 0., 0., 1., 1.]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">K = torch.tensor(</span><br><span class="line">    [[<span class="number">1.0</span>, -<span class="number">1.0</span>]]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line">Y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],</span><br><span class="line">        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</span><br></pre></td></tr></table></figure><p>卷积核<code>K</code>只可以检测垂直边缘：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corr2d(X.T, K)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure><p>学习由<code>X</code>生成<code>Y</code>的卷积核：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">X = X.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>) <span class="comment"># (批量大小, 通道数, 高度, 宽度)</span></span><br><span class="line">Y = Y.reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y) ** <span class="number">2</span></span><br><span class="line">    conv2d.zero_grad()</span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    conv2d.weight.data[:] -= <span class="number">3e-2</span> * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;batch <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch 2, loss 20.931</span><br><span class="line">batch 4, loss 7.314</span><br><span class="line">batch 6, loss 2.784</span><br><span class="line">batch 8, loss 1.105</span><br><span class="line">batch 10, loss 0.447</span><br></pre></td></tr></table></figure><p>所学的卷积核的权重张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv2d.weight.data.reshape(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1.0568, -0.9194]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 19 卷积层。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 18 预测房价竞赛总结</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/18/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/18/</id>
    <published>2023-10-16T07:00:00.000Z</published>
    <updated>2023-10-16T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="18-预测房价竞赛总结"><a href="#18-预测房价竞赛总结" class="headerlink" title="18 预测房价竞赛总结"></a>18 预测房价竞赛总结</h1><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><ul><li>172支队伍，2288次提交</li><li>前5位同学将获得奖品</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/18/image-20231016151147190.png" alt="image-20231016151147190"></p><h2 id="方法总结"><a href="#方法总结" class="headerlink" title="方法总结"></a>方法总结</h2><ul><li><p>第二和第七：autogluon</p><p>  <a href="https://www.bilibili.com/video/BV1rh411m7Hb/">10行代码战胜90%数据科学家？</a></p></li><li><p>第三：h2o</p><p>  <a href="https://www.kaggle.com/code/wuwawa/automl-using-h2o">AutoML(Using h2o) | Kaggle</a></p></li><li><p>第四：随机森林</p><p>  <a href="https://www.kaggle.com/code/jackzh/the-4th-place-approach-random-forest">The 4th place approach (Random Forest) | Kaggle</a></p></li></ul><h2 id="一些分析"><a href="#一些分析" class="headerlink" title="一些分析"></a>一些分析</h2><ul><li>已知的排名靠前的4个成绩均使用了集成学习</li><li>目前不知道是否有使用书中的mlp取得好成绩<ul><li>特征预处理和超参数是取得好成绩的基础</li></ul></li><li>这个数据的一些难点<ul><li>数值较大</li><li>有文本特征（地址，介绍）</li><li>训练数据是前6个月，公榜是后3个月，私榜是再后3个月</li></ul></li></ul><h2 id="关于automl"><a href="#关于automl" class="headerlink" title="关于automl"></a>关于automl</h2><ul><li>数据科学家80%的时间在处理数据，20%调模型</li><li>Automl现在能处理一些基础的情况<ul><li>目前节省10%时间，未来节省20%时间</li></ul></li><li>为什么还要学习模型<ul><li>（我初中时说：会加减乘除就可以买菜了，为什么还要学三角函数）</li><li>当人人都会玩游戏时，你需要成为玩得最好的，或者甚至去制作游戏</li></ul></li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 18 预测房价竞赛总结。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 17 使用和购买GPU</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/17/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/17/</id>
    <published>2023-10-15T10:00:00.000Z</published>
    <updated>2023-10-16T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="17-使用和购买GPU"><a href="#17-使用和购买GPU" class="headerlink" title="17 使用和购买GPU"></a>17 使用和购买GPU</h1><h2 id="使用GPU"><a href="#使用GPU" class="headerlink" title="使用GPU"></a>使用GPU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">Mon Oct 16 13:13:29 2023</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 517.00       Driver Version: 517.00       CUDA Version: 11.7     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |</span><br><span class="line">| N/A   55C    P5    25W /  N/A |    939MiB /  6144MiB |      8%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line"></span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|    0   N/A  N/A      3348    C+G   ..._dt26b99r8h8gj\RtkUWP.exe    N/A      |</span><br><span class="line">|    0   N/A  N/A      3588    C+G   ...perience\NVIDIA Share.exe    N/A      |</span><br><span class="line">|    0   N/A  N/A      4148    C+G   ...\MicrosoftSecurityApp.exe    N/A      |</span><br><span class="line">|    0   N/A  N/A      4976    C+G   ...cw5n1h2txyewy\LockApp.exe    N/A      |</span><br><span class="line">|    0   N/A  N/A      7004    C+G   C:\Windows\explorer.exe         N/A      |</span><br><span class="line">|    0   N/A  N/A      7952    C+G   ...d\runtime\WeChatAppEx.exe    N/A      |</span><br><span class="line">|    0   N/A  N/A      8676    C+G   ...artMenuExperienceHost.exe    N/A      |</span><br><span class="line">...</span><br><span class="line">|    0   N/A  N/A     24300    C+G   ...ting\WeMeet\wemeetapp.exe    N/A      |</span><br><span class="line">|    0   N/A  N/A     25372    C+G   ...me\Application\chrome.exe    N/A      |</span><br><span class="line">|    0   N/A  N/A     28804    C+G   D:\Typora\Typora.exe            N/A      |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure><h3 id="计算设备"><a href="#计算设备" class="headerlink" title="计算设备"></a>计算设备</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.cuda.device(<span class="string">&#x27;cuda&#x27;</span>), torch.cuda.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(device(type=&#x27;cpu&#x27;),</span><br><span class="line"> &lt;torch.cuda.device at 0x1f5e2c545e0&gt;,</span><br><span class="line"> &lt;torch.cuda.device at 0x1f5fff1f490&gt;)</span><br></pre></td></tr></table></figure><h3 id="查询可用gpu的数量"><a href="#查询可用gpu的数量" class="headerlink" title="查询可用gpu的数量"></a>查询可用gpu的数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><p>这两个函数允许我们在请求的GPU不存在的情况下运行代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_all_gpus</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。&quot;&quot;&quot;</span></span><br><span class="line">    devices = [</span><br><span class="line">        torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(device(type=&#x27;cuda&#x27;, index=0),</span><br><span class="line"> device(type=&#x27;cpu&#x27;),</span><br><span class="line"> [device(type=&#x27;cuda&#x27;, index=0)])</span><br></pre></td></tr></table></figure><h3 id="查询张量所在的设备"><a href="#查询张量所在的设备" class="headerlink" title="查询张量所在的设备"></a>查询张量所在的设备</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(type=&#x27;cpu&#x27;)</span><br></pre></td></tr></table></figure><h3 id="存储在GPU上"><a href="#存储在GPU上" class="headerlink" title="存储在GPU上"></a>存储在GPU上</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><p>在第二个GPU上创建一个随机张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line">Y, Y.device</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.9206, 0.9959, 0.3249],</span><br><span class="line">         [0.6247, 0.6413, 0.6559]]),</span><br><span class="line"> device(type=&#x27;cpu&#x27;))</span><br></pre></td></tr></table></figure><p>这里我只有一个GPU，所以这个张量被存放在了CPU内。</p><p>要计算<code>X + Y</code>，我们需要决定在哪里执行这个操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Z = Y.cuda(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(Y, Y.device)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.9206, 0.9959, 0.3249],</span><br><span class="line">        [0.6247, 0.6413, 0.6559]]) cpu</span><br><span class="line">tensor([[0.9206, 0.9959, 0.3249],</span><br><span class="line">        [0.6247, 0.6413, 0.6559]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><p>要计算<code>X + Y</code>，需要保证<code>X</code>和<code>Y</code>在同一个GPU上，这里我们将在CPU上的<code>Y</code>拷贝一份到GPU上的<code>Z</code>。</p><p>现在数据在同一块GPU上（<code>Z</code>和<code>X</code>都在），我们可以将他们相加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X + Z</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1.9206, 1.9959, 1.3249],</span><br><span class="line">        [1.6247, 1.6413, 1.6559]], device=&#x27;cuda:0&#x27;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Z.cuda(<span class="number">0</span>) <span class="keyword">is</span> Z</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure><p>如果这里执行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="built_in">print</span>(X + Y)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</span><br></pre></td></tr></table></figure><h3 id="神经网络与GPU"><a href="#神经网络与GPU" class="headerlink" title="神经网络与GPU"></a>神经网络与GPU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br><span class="line"></span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.0506],</span><br><span class="line">        [-0.0506]], device=&#x27;cuda:0&#x27;, grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>确认模型参数存储在同一个GPU上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.device</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device(type=&#x27;cuda&#x27;, index=0)</span><br></pre></td></tr></table></figure><h2 id="购买GPU"><a href="#购买GPU" class="headerlink" title="购买GPU"></a>购买GPU</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/17/flopsvsprice.svg" alt="浮点计算能力和价格比较"></p><p>详见<a href="https://zh-v2.d2l.ai/chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">16.4. 选择服务器和GPU — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>。</p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 17 使用和购买GPU。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 16 PyTorch 神经网络基础</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/16/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/16/</id>
    <published>2023-10-14T13:00:00.000Z</published>
    <updated>2023-10-15T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="16-PyTorch-神经网络基础"><a href="#16-PyTorch-神经网络基础" class="headerlink" title="16 PyTorch 神经网络基础"></a>16 PyTorch 神经网络基础</h1><h2 id="模型构造"><a href="#模型构造" class="headerlink" title="模型构造"></a>模型构造</h2><h3 id="层和块"><a href="#层和块" class="headerlink" title="层和块"></a>层和块</h3><p>首先，我们回顾一下多层感知机：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">256</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.0138,  0.0851, -0.1017,  0.0842,  0.0113,  0.2133, -0.0535,  0.0005,</span><br><span class="line">         -0.0345,  0.0528],</span><br><span class="line">        [-0.0519,  0.0524,  0.0995,  0.1972,  0.0709,  0.2025, -0.1841,  0.0155,</span><br><span class="line">         -0.1136,  0.0266]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p><code>nn.Sequential</code>定义了一种特殊的<code>Module</code>。</p><h3 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure><p>实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.0995, -0.1797,  0.0238,  0.1680,  0.0258,  0.2447, -0.2060,  0.2799,</span><br><span class="line">          0.1381, -0.1721],</span><br><span class="line">        [ 0.0687, -0.0833, -0.0725,  0.0344, -0.1054,  0.1039, -0.1636,  0.2026,</span><br><span class="line">          0.2355, -0.0640]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> args:</span><br><span class="line">            self._modules[block] = block</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">net = MySequential(</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">256</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.1581, -0.0553,  0.0518, -0.2939,  0.3417, -0.0626, -0.1697,  0.3204,</span><br><span class="line">          0.2213,  0.0478],</span><br><span class="line">        [-0.0798, -0.0695,  0.0440, -0.1886,  0.2312, -0.1228, -0.0684,  0.1975,</span><br><span class="line">          0.3014,  0.2149]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="在正向传播中执行代码"><a href="#在正向传播中执行代码" class="headerlink" title="在正向传播中执行代码"></a>在正向传播中执行代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        X = F.relu(torch.mm(X, self.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        X = self.linear(X)</span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">net = FixedHiddenMLP()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(-0.1086, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="混合搭配各种组合块的方法"><a href="#混合搭配各种组合块的方法" class="headerlink" title="混合搭配各种组合块的方法"></a>混合搭配各种组合块的方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">20</span>, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">32</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(</span><br><span class="line">    NestMLP(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>, <span class="number">20</span>),</span><br><span class="line">    FixedHiddenMLP()</span><br><span class="line">)</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(-0.1233, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure><h2 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h2><p>我们首先关注具有单隐藏层的多层感知机：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">8</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5949],</span><br><span class="line">        [-0.4218]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(<span class="string">&#x27;weight&#x27;</span>, tensor([[ <span class="number">0.0061</span>,  <span class="number">0.3225</span>, -<span class="number">0.2198</span>, -<span class="number">0.1175</span>, -<span class="number">0.0139</span>, -<span class="number">0.3222</span>,  <span class="number">0.2318</span>, -<span class="number">0.3322</span>]])), (<span class="string">&#x27;bias&#x27;</span>, tensor([-<span class="number">0.2630</span>]))])</span><br></pre></td></tr></table></figure><h3 id="目标参数"><a href="#目标参数" class="headerlink" title="目标参数"></a>目标参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([-0.2630], requires_grad=True)</span><br><span class="line">tensor([-0.2630])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">2</span>].weight.grad == <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure><h3 id="一次性访问所有参数"><a href="#一次性访问所有参数" class="headerlink" title="一次性访问所有参数"></a>一次性访问所有参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span><br><span class="line">(&#x27;0.weight&#x27;, torch.Size([8, 4])) (&#x27;0.bias&#x27;, torch.Size([8])) (&#x27;2.weight&#x27;, torch.Size([1, 8])) (&#x27;2.bias&#x27;, torch.Size([1]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.state_dict()[<span class="string">&#x27;2.bias&#x27;</span>].data</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-0.2630])</span><br></pre></td></tr></table></figure><h3 id="从嵌套块收集参数"><a href="#从嵌套块收集参数" class="headerlink" title="从嵌套块收集参数"></a>从嵌套块收集参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">4</span>, <span class="number">8</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">8</span>, <span class="number">4</span>),</span><br><span class="line">        nn.ReLU()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(</span><br><span class="line">    block2(),</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.5123],</span><br><span class="line">        [-0.5123]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>我们已经设计了网络，让我们看看它是如何组织的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(rgnet)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Sequential(</span><br><span class="line">    (block 0): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 1): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 2): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block 3): Sequential(</span><br><span class="line">      (0): Linear(in_features=4, out_features=8, bias=True)</span><br><span class="line">      (1): ReLU()</span><br><span class="line">      (2): Linear(in_features=8, out_features=4, bias=True)</span><br><span class="line">      (3): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (1): Linear(in_features=4, out_features=1, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="内置初始化"><a href="#内置初始化" class="headerlink" title="内置初始化"></a>内置初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 0.0049,  0.0002,  0.0002, -0.0152]), tensor(0.))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([1., 1., 1., 1.]), tensor(0.))</span><br></pre></td></tr></table></figure><h3 id="对某型块应用不同的初始化方法"><a href="#对某型块应用不同的初始化方法" class="headerlink" title="对某型块应用不同的初始化方法"></a>对某型块应用不同的初始化方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">net[<span class="number">0</span>].apply(xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([-0.0483,  0.3094,  0.4954, -0.0904])</span><br><span class="line">tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])</span><br></pre></td></tr></table></figure><h3 id="自定义初始化"><a href="#自定义初始化" class="headerlink" title="自定义初始化"></a>自定义初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Init&quot;</span>,</span><br><span class="line">            *[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>]</span><br><span class="line">        )</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Init weight torch.Size([8, 4])</span><br><span class="line">Init weight torch.Size([1, 8])</span><br><span class="line">tensor([[ 0.0000,  0.0000, -8.5484, -0.0000],</span><br><span class="line">        [ 0.0000,  5.0178,  0.0000, -6.0337]], grad_fn=&lt;SliceBackward0&gt;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">42</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([42.0000,  1.0000, -7.5484,  1.0000])</span><br></pre></td></tr></table></figure><h3 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">8</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    shared,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    shared,</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">net(X)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True, True, True, True, True])</span><br><span class="line">tensor([True, True, True, True, True, True, True, True])</span><br></pre></td></tr></table></figure><h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2><h3 id="构造一个没有任何参数的自定义层"><a href="#构造一个没有任何参数的自定义层" class="headerlink" title="构造一个没有任何参数的自定义层"></a>构造一个没有任何参数的自定义层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br><span class="line"></span><br><span class="line">layer = CenteredLayer()</span><br><span class="line">layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-2., -1.,  0.,  1.,  2.])</span><br></pre></td></tr></table></figure><h3 id="将层作为组件合并到构建更复杂的模型中"><a href="#将层作为组件合并到构建更复杂的模型中" class="headerlink" title="将层作为组件合并到构建更复杂的模型中"></a>将层作为组件合并到构建更复杂的模型中</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">8</span>, <span class="number">128</span>),</span><br><span class="line">    CenteredLayer()</span><br><span class="line">)</span><br><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">Y.mean()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(-5.1223e-09, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure><p>由于浮点计算是有误差的，所以这里不会精确的等于0。</p><h3 id="带参数的图层"><a href="#带参数的图层" class="headerlink" title="带参数的图层"></a>带参数的图层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line"></span><br><span class="line">dense = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">dense.weight</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.7908, -0.2897,  0.5945],</span><br><span class="line">        [ 0.2872,  0.1478,  0.2363],</span><br><span class="line">        [ 0.8848, -0.5880,  1.7398],</span><br><span class="line">        [ 0.9967,  1.3198,  0.6156],</span><br><span class="line">        [-1.3955, -0.3842, -0.5728]], requires_grad=True)</span><br></pre></td></tr></table></figure><h3 id="使用自定义层直接执行正向传播计算"><a href="#使用自定义层直接执行正向传播计算" class="headerlink" title="使用自定义层直接执行正向传播计算"></a>使用自定义层直接执行正向传播计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dense(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0000, 0.4333, 1.6882],</span><br><span class="line">        [0.0000, 0.2149, 1.1064]], grad_fn=&lt;ReluBackward0&gt;)</span><br></pre></td></tr></table></figure><h3 id="使用自定义层构建模型"><a href="#使用自定义层构建模型" class="headerlink" title="使用自定义层构建模型"></a>使用自定义层构建模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    MyLinear(<span class="number">64</span>, <span class="number">8</span>),</span><br><span class="line">    MyLinear(<span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">net(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0000],</span><br><span class="line">        [4.9473]], grad_fn=&lt;ReluBackward0&gt;)</span><br></pre></td></tr></table></figure><h2 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h2><h3 id="加载和保存张量"><a href="#加载和保存张量" class="headerlink" title="加载和保存张量"></a>加载和保存张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x2 = torch.load(<span class="string">&quot;x-file&quot;</span>)</span><br><span class="line">x2</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0, 1, 2, 3])</span><br></pre></td></tr></table></figure><p>同时在当前目录下会生成一个<code>x-file</code>文件。</p><h3 id="存储一个张量列表，然后把它们读回内存"><a href="#存储一个张量列表，然后把它们读回内存" class="headerlink" title="存储一个张量列表，然后把它们读回内存"></a>存储一个张量列表，然后把它们读回内存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))</span><br></pre></td></tr></table></figure><h3 id="写入或读取从字符串映射到张量的字典"><a href="#写入或读取从字符串映射到张量的字典" class="headerlink" title="写入或读取从字符串映射到张量的字典"></a>写入或读取从字符串映射到张量的字典</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;x&#x27;: tensor([0, 1, 2, 3]), &#x27;y&#x27;: tensor([0., 0., 0., 0.])&#125;</span><br></pre></td></tr></table></figure><h3 id="加载和保存模型参数"><a href="#加载和保存模型参数" class="headerlink" title="加载和保存模型参数"></a>加载和保存模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br></pre></td></tr></table></figure><p>将模型的参数存储为一个叫做“mlp.params”的文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure><p>实例化了原始多层感知机模型的一个备份。直接读取文件中存储的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&quot;mlp.params&quot;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MLP(</span><br><span class="line">  (hidden): Linear(in_features=20, out_features=256, bias=True)</span><br><span class="line">  (output): Linear(in_features=256, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y_clone = clone(X)</span><br><span class="line">Y_clone == Y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True, True, True, True, True, True, True, True, True],</span><br><span class="line">        [True, True, True, True, True, True, True, True, True, True]])</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 16 PyTorch 神经网络基础。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 15 Kaggle房价预测 + 课程竞赛：加州2020年房价预测</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/15/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/15/</id>
    <published>2023-10-14T09:00:00.000Z</published>
    <updated>2023-10-14T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="15-Kaggle房价预测-课程竞赛：加州2020年房价预测"><a href="#15-Kaggle房价预测-课程竞赛：加州2020年房价预测" class="headerlink" title="15 Kaggle房价预测 + 课程竞赛：加州2020年房价预测"></a>15 Kaggle房价预测 + 课程竞赛：加州2020年房价预测</h1><h2 id="实战Kaggle比赛：预测房价"><a href="#实战Kaggle比赛：预测房价" class="headerlink" title="实战Kaggle比赛：预测房价"></a>实战Kaggle比赛：预测房价</h2><p>实现几个函数来方便下载数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">DATA_HUB = <span class="built_in">dict</span>()</span><br><span class="line">DATA_URL = <span class="string">&#x27;http://d2l-data.s3-accelerate.amazonaws.com/&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">name, cache_dir=os.path.join(<span class="params"><span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;data&#x27;</span></span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载一个DATA_HUB中的文件，返回本地文件名。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> name <span class="keyword">in</span> DATA_HUB, <span class="string">f&quot;<span class="subst">&#123;name&#125;</span> 不存在于 <span class="subst">&#123;DATA_HUB&#125;</span>&quot;</span></span><br><span class="line">    url, sha1_hash = DATA_HUB[name]</span><br><span class="line">    os.makedirs(cache_dir, exist_ok=<span class="literal">True</span>)</span><br><span class="line">    fname = os.path.join(cache_dir, url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(fname):</span><br><span class="line">        sha1 = hashlib.sha1()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                data = f.read(<span class="number">1048576</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                sha1.update(data)</span><br><span class="line">        <span class="keyword">if</span> sha1.hexdigest() == sha1_hash:</span><br><span class="line">            <span class="keyword">return</span> fname <span class="comment"># 命中缓存</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;正在从<span class="subst">&#123;url&#125;</span>下载<span class="subst">&#123;fname&#125;</span>...&#x27;</span>)</span><br><span class="line">    r = requests.get(url, stream=<span class="literal">True</span>, verify=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fname, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(r.content)</span><br><span class="line">    <span class="keyword">return</span> fname</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_extract</span>(<span class="params">name, folder=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载并解压zip/tar文件。&quot;&quot;&quot;</span></span><br><span class="line">    fname = download(name)</span><br><span class="line">    base_dir = os.path.dirname(fname)</span><br><span class="line">    data_dir, ext = os.path.splitext(fname)</span><br><span class="line">    <span class="keyword">if</span> ext == <span class="string">&#x27;.zip&#x27;</span>:</span><br><span class="line">        fp = zipfile.ZipFile(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> ext <span class="keyword">in</span> (<span class="string">&#x27;.tar&#x27;</span>, <span class="string">&#x27;.gz&#x27;</span>):</span><br><span class="line">        fp = tarfile.<span class="built_in">open</span>(fname, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="literal">False</span>, <span class="string">&#x27;只有zip/tar文件可以被解压缩&#x27;</span></span><br><span class="line">    fp.extractall(base_dir)</span><br><span class="line">    <span class="keyword">return</span> os.path.join(base_dir, folder) <span class="keyword">if</span> folder <span class="keyword">else</span> data_dir</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_all</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载DATA_HUB中的所有文件。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> DATA_HUB:</span><br><span class="line">        download(name)</span><br></pre></td></tr></table></figure><p>使用<code>pandas</code>读入并处理数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_train&#x27;</span>] = (</span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_train.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;585e9cc93e70b39160e7921475f9bcd7d31219ce&#x27;</span></span><br><span class="line">)</span><br><span class="line">DATA_HUB[<span class="string">&#x27;kaggle_house_test&#x27;</span>] = (</span><br><span class="line">    DATA_URL + <span class="string">&#x27;kaggle_house_pred_test.csv&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fa19780a7b011d9b009e8bff8e99922a8ee2eb90&#x27;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_train&#x27;</span>))</span><br><span class="line">test_data = pd.read_csv(download(<span class="string">&#x27;kaggle_house_test&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(train_data.shape)</span><br><span class="line"><span class="built_in">print</span>(test_data.shape)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1460, 81)</span><br><span class="line">(1459, 80)</span><br></pre></td></tr></table></figure><p>查看前四个和最后两个特征，以及相应的标签（房价）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train_data.iloc[<span class="number">0</span>:<span class="number">4</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, -<span class="number">3</span>, -<span class="number">2</span>, -<span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice</span><br><span class="line">0   1          60       RL         65.0       WD        Normal     208500</span><br><span class="line">1   2          20       RL         80.0       WD        Normal     181500</span><br><span class="line">2   3          60       RL         68.0       WD        Normal     223500</span><br><span class="line">3   4          70       RL         60.0       WD       Abnorml     140000</span><br></pre></td></tr></table></figure><p>在每个样本中，第一个特征是ID，我们将其从数据集中删除：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_features = pd.concat((train_data.iloc[:, <span class="number">1</span>:-<span class="number">1</span>], test_data.iloc[:, <span class="number">1</span>:]))</span><br></pre></td></tr></table></figure><p>将所有缺失的值替换为相应特征的平均值。通过将特征重新缩放到零均值和单位方差来标准化数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将所有数值特征挑出来</span></span><br><span class="line">numeric_features = all_features.dtypes[all_features.dtypes != <span class="string">&#x27;object&#x27;</span>].index</span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].apply(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x - x.mean()) / x.std()</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 由于执行了(x - x.mean()) / x.std()操作，所以现在所有数值数据的均值都是0</span></span><br><span class="line"><span class="comment"># 那么将缺失值替换为0，就是将缺失值替换为了均值</span></span><br><span class="line">all_features[numeric_features] = all_features[numeric_features].fillna(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>处理离散值。我们用一次独热编码替换它们：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dummy_na=True将缺失值视为有效的特征值，并为其创建一个特征</span></span><br><span class="line">all_features = pd.get_dummies(all_features, dummy_na=<span class="literal">True</span>)</span><br><span class="line">all_features.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(2919, 331)</span><br></pre></td></tr></table></figure><p>从<code>pandas</code>格式中提取NumPy格式，并将其转换为张量表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_train = train_data.shape[<span class="number">0</span>]</span><br><span class="line">train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)</span><br><span class="line">test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)</span><br><span class="line">train_labels = torch.tensor(train_data.SalePrice.values.reshape(-<span class="number">1</span>, <span class="number">1</span>), dtype=torch.float32)</span><br></pre></td></tr></table></figure><p>训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line">in_features = train_features.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_net</span>():</span><br><span class="line">    net = nn.Sequential(nn.Linear(in_features, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><p>我们更关心相对误差$\frac{y - \hat{y}}{y}$，解决这个问题的一种方式是用价格预测的对数来衡量差异：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">log_rmse</span>(<span class="params">net, features, labels</span>):</span><br><span class="line">    <span class="comment"># 为了在取对数时进一步稳定该值，将小于1的值设置为1</span></span><br><span class="line">    clipped_preds = torch.clamp(net(features), <span class="number">1</span>, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))</span><br><span class="line">    <span class="keyword">return</span> rmse.item()</span><br></pre></td></tr></table></figure><p>我们的训练函数将借助Adam优化器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, train_features, train_labels, test_features, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels), batch_size)</span><br><span class="line">    <span class="comment"># 这里使用的是Adam优化算法</span></span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(),</span><br><span class="line">                                 lr=learning_rate,</span><br><span class="line">                                 weight_decay=weight_decay)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">        train_ls.append(log_rmse(net, train_features, train_labels))</span><br><span class="line">        <span class="keyword">if</span> test_labels <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            test_ls.append(log_rmse(net, test_features, test_labels))</span><br><span class="line">    <span class="keyword">return</span> train_ls, test_ls</span><br></pre></td></tr></table></figure><p>$K$折交叉验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_k_fold_data</span>(<span class="params">k, i, X, y</span>):</span><br><span class="line">    <span class="keyword">assert</span> k &gt; <span class="number">1</span></span><br><span class="line">    fold_size = X.shape[<span class="number">0</span>] // k</span><br><span class="line">    X_train, y_train = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        idx = <span class="built_in">slice</span>(j * fold_size, (j + <span class="number">1</span>) * fold_size)</span><br><span class="line">        X_part, y_part = X[idx, :], y[idx]</span><br><span class="line">        <span class="keyword">if</span> j == i:</span><br><span class="line">            X_valid, y_valid = X_part, y_part</span><br><span class="line">        <span class="keyword">elif</span> X_train <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            X_train, y_train = X_part, y_part</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X_train = torch.cat([X_train, X_part], <span class="number">0</span>)</span><br><span class="line">            y_train = torch.cat([y_train, y_part], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> X_train, y_train, X_valid, y_valid</span><br></pre></td></tr></table></figure><p>返回训练和验证误差的平均值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">k_fold</span>(<span class="params">k, X_train, y_train, num_epochs, learning_rate, weight_decay, batch_size</span>):</span><br><span class="line">    train_l_sum, valid_l_sum = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        data = get_k_fold_data(k, i, X_train, y_train)</span><br><span class="line">        net = get_net()</span><br><span class="line">        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,</span><br><span class="line">                                   weight_decay, batch_size)</span><br><span class="line">        train_l_sum += train_ls[-<span class="number">1</span>]</span><br><span class="line">        valid_l_sum += valid_ls[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            d2l.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>)), [train_ls, valid_ls],</span><br><span class="line">                     xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                     legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;折<span class="subst">&#123;i + <span class="number">1</span>&#125;</span>，训练log rmse<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;验证los rmse<span class="subst">&#123;<span class="built_in">float</span>(valid_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> train_l_sum / k, valid_l_sum / k</span><br></pre></td></tr></table></figure><p>模型选择：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">k, num_epochs, lr, weight_decay, batch_size = <span class="number">5</span>, <span class="number">100</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">64</span></span><br><span class="line">train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs,</span><br><span class="line">                          lr, weight_decay, batch_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;k&#125;</span>-折验证：平均训练log rmse: <span class="subst">&#123;<span class="built_in">float</span>(train_l):f&#125;</span>, &#x27;</span></span><br><span class="line">      <span class="string">f&#x27;平均验证log rmse: <span class="subst">&#123;<span class="built_in">float</span>(valid_l):f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">折1，训练log rmse0.170201, 验证los rmse0.156629</span><br><span class="line">折2，训练log rmse0.162550, 验证los rmse0.192134</span><br><span class="line">折3，训练log rmse0.164213, 验证los rmse0.169010</span><br><span class="line">折4，训练log rmse0.167820, 验证los rmse0.154409</span><br><span class="line">折5，训练log rmse0.163098, 验证los rmse0.182882</span><br><span class="line">5-折验证：平均训练log rmse: 0.165576, 平均验证log rmse: 0.171013</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/15/output1.svg" alt="output1"></p><p>提交你的Kaggle预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_and_pred</span>(<span class="params">train_features, test_features, train_labels, test_data,</span></span><br><span class="line"><span class="params">                   num_epochs, lr, weight_decay, batch_size</span>):</span><br><span class="line">    net = get_net()</span><br><span class="line">    train_ls, _ = train(net, train_features, train_labels, <span class="literal">None</span>, <span class="literal">None</span>,</span><br><span class="line">                        num_epochs, lr, weight_decay, batch_size)</span><br><span class="line">    d2l.plot(np.arange(<span class="number">1</span>, num_epochs + <span class="number">1</span>), [train_ls], xlabel=<span class="string">&#x27;epoch&#x27;</span>,</span><br><span class="line">             ylabel=<span class="string">&#x27;log rmse&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], yscale=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;训练log rmse：<span class="subst">&#123;<span class="built_in">float</span>(train_ls[-<span class="number">1</span>]):f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将网络应用于测试集</span></span><br><span class="line">    preds = net(test_features).detach().numpy()</span><br><span class="line">    <span class="comment"># 将其重新格式化以导出到Kaggle</span></span><br><span class="line">    test_data[<span class="string">&#x27;SalePrice&#x27;</span>] = pd.Series(preds.reshape(<span class="number">1</span>, -<span class="number">1</span>)[<span class="number">0</span>])</span><br><span class="line">    submission = pd.concat([test_data[<span class="string">&#x27;Id&#x27;</span>], test_data[<span class="string">&#x27;SalePrice&#x27;</span>]], axis=<span class="number">1</span>)</span><br><span class="line">    submission.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">训练log rmse：0.162573</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/15/output2.svg" alt="output2"></p><h2 id="课程竞赛：加州2020年房价预测"><a href="#课程竞赛：加州2020年房价预测" class="headerlink" title="课程竞赛：加州2020年房价预测"></a>课程竞赛：加州2020年房价预测</h2><h3 id="规则"><a href="#规则" class="headerlink" title="规则"></a>规则</h3><ul><li>网址：<a href="https://www.kaggle.com/c/california-house-prices/overview">California House Prices | Kaggle</a></li><li>前5个人（团队）将获赠作者签名版《动手学深度学习》第二版</li><li>欢迎分享经验（学习第一，结果第二）</li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 15 Kaggle房价预测 + 课程竞赛：加州2020年房价预测。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 14 数值稳定性 + 模型初始化和激活函数</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/14/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/14/</id>
    <published>2023-10-13T08:00:00.000Z</published>
    <updated>2023-10-14T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="14-数值稳定性-模型初始化和激活函数"><a href="#14-数值稳定性-模型初始化和激活函数" class="headerlink" title="14 数值稳定性 + 模型初始化和激活函数"></a>14 数值稳定性 + 模型初始化和激活函数</h1><h2 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h2><h3 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h3><p>考虑如下有d层的神经网络：</p><script type="math/tex; mode=display">\mathbf{h}^t = f_t(\mathbf{h}^{t - 1}) \quad \text{and} \quad y = \ell \circ f_d \circ \cdots \circ f_1(\mathbf{x})</script><p>计算损失$\ell$关于参数$\mathbf{W}_t$的梯度：</p><script type="math/tex; mode=display">\frac{\partial\ell}{\partial\mathbf{W}^t} = \frac{\partial\ell}{\partial\mathbf{h}^d}\underbrace{\frac{\partial\mathbf{h}^d}{\partial\mathbf{h}^{d - 1}}\cdots\frac{\partial\mathbf{h}^{t + 1}}{\partial\mathbf{h}^t}}_{d - t次矩阵乘法}\frac{\partial\mathbf{h}^t}{\partial\mathbf{W}^t}</script><h3 id="数值稳定性的常见两个问题"><a href="#数值稳定性的常见两个问题" class="headerlink" title="数值稳定性的常见两个问题"></a>数值稳定性的常见两个问题</h3><script type="math/tex; mode=display">\prod_{i = t}^{d - 1}\frac{\partial\mathbf{h}^{i + 1}}{\partial\mathbf{h}^i}</script><p><img src="https://img.karltan.com/notes-out-class/d2l/14/image-20231013163934763.png" alt="image-20231013163934763"></p><h4 id="例子：MLP"><a href="#例子：MLP" class="headerlink" title="例子：MLP"></a>例子：MLP</h4><p>加入如下MLP（为了简单省略了偏移）（这里我认为PPT上$\frac{\partial\mathbf{h}^t}{\partial\mathbf{h}^{t - 1}}$的计算结果是错的，具体计算过程见下文）：</p><script type="math/tex; mode=display">\begin{aligned}f_t(\mathbf{h}^{t - 1}) &= \sigma(\mathbf{W}^t\mathbf{h}^{t - 1}) \quad \sigma是激活函数 \\\frac{\partial\mathbf{h}^t}{\partial\mathbf{h}^{t - 1}} &= \text{diag}\left(\sigma^\prime(\mathbf{W}^t\mathbf{h}^{t - 1})\right)\mathbf{W}^t \quad \sigma^\prime是\sigma的导数函数 \\\prod_{i = t}^{d - 1}\frac{\partial\mathbf{h}^{i + 1}}{\partial\mathbf{h}^i} &= \prod_{i = t}^{d - 1}\text{diag}\left(\sigma^\prime(\mathbf{W}^i\mathbf{h}^{i - 1})\right)\mathbf{W}^i\end{aligned}</script><p>其中：</p><ul><li><p>$\text{diag}$表示对角矩阵</p></li><li><p>$\mathbf{h}^{t - 1}$是第$t - 1$层的输出，也是第$t$层的输入，假设其形状为$m \times 1$</p></li><li><p>$\mathbf{W}^t$是第$t$层的权重，假设其形状为$n \times m$，将$\mathbf{W}^t$写成向量的形式，那么$\mathbf{W}^t = \begin{bmatrix}\mathbf{w}_1 \\ \mathbf{w}_2 \\ \vdots \\ \mathbf{w}_n\end{bmatrix}$，其中$\mathbf{w}_i$是$1 \times m$的行向量</p></li><li><p>那么$\sigma(\mathbf{W}^t\mathbf{h}^{t - 1})$是第$t$层的输出，其形状为$n \times 1$</p></li><li><p>接下来详细讲一下$\frac{\partial\mathbf{h}^t}{\partial\mathbf{h}^{t - 1}}$的计算过程，其中，$\mathbf{h}^t$是第$t$层的输出，且将分母看作标量，然后按照分子布局展开：</p><script type="math/tex; mode=display">  \begin{aligned}  \frac{\partial\mathbf{h}^t}{\partial\mathbf{h}^{t - 1}}  &= \frac{\partial\left(\sigma(\mathbf{W}^t\mathbf{h}^{t - 1})\right)}{\partial\mathbf{h}^{t - 1}} \\  &= \begin{bmatrix}  \frac{\partial\left(\sigma(\mathbf{w}_1^t\mathbf{h}^{t - 1})\right)}{\partial\mathbf{h}^{t - 1}} \\  \frac{\partial\left(\sigma(\mathbf{w}_2^t\mathbf{h}^{t - 1})\right)}{\partial\mathbf{h}^{t - 1}} \\  \vdots \\  \frac{\partial\left(\sigma(\mathbf{w}_n^t\mathbf{h}^{t - 1})\right)}{\partial\mathbf{h}^{t - 1}}  \end{bmatrix}  \end{aligned}</script><p>  然后将每一项的分子看作标量，再用分子布局进行展开：</p><script type="math/tex; mode=display">  \begin{aligned}  \frac{\partial\mathbf{h}^t}{\partial\mathbf{h}^{t - 1}}  &= \begin{bmatrix}  \frac{\partial\left(\sigma(\mathbf{w}_1^t\mathbf{h}^{t - 1})\right)}{\partial\mathbf{h}^{t - 1}} \\  \frac{\partial\left(\sigma(\mathbf{w}_2^t\mathbf{h}^{t - 1})\right)}{\partial\mathbf{h}^{t - 1}} \\  \vdots \\  \frac{\partial\left(\sigma(\mathbf{w}_n^t\mathbf{h}^{t - 1})\right)}{\partial\mathbf{h}^{t - 1}}  \end{bmatrix} \\  &= \begin{bmatrix}  \frac{\partial\left(\sigma(\mathbf{w}_1^t\mathbf{h}^{t - 1})\right)}{\partial h_1^{t - 1}}, & \frac{\partial\left(\sigma(\mathbf{w}_1^t\mathbf{h}^{t - 1})\right)}{\partial h_2^{t - 1}}, & \cdots, & \frac{\partial\left(\sigma(\mathbf{w}_1^t\mathbf{h}^{t - 1})\right)}{\partial h_m^{t - 1}} \\  \frac{\partial\left(\sigma(\mathbf{w}_2^t\mathbf{h}^{t - 1})\right)}{\partial h_1^{t - 1}}, & \frac{\partial\left(\sigma(\mathbf{w}_2^t\mathbf{h}^{t - 1})\right)}{\partial h_2^{t - 1}}, & \cdots, & \frac{\partial\left(\sigma(\mathbf{w}_2^t\mathbf{h}^{t - 1})\right)}{\partial h_m^{t - 1}} \\  \vdots & \vdots & & \vdots \\  \frac{\partial\left(\sigma(\mathbf{w}_n^t\mathbf{h}^{t - 1})\right)}{\partial h_1^{t - 1}}, & \frac{\partial\left(\sigma(\mathbf{w}_n^t\mathbf{h}^{t - 1})\right)}{\partial h_2^{t - 1}}, & \cdots, & \frac{\partial\left(\sigma(\mathbf{w}_n^t\mathbf{h}^{t - 1})\right)}{\partial h_m^{t - 1}}  \end{bmatrix} \\  &= \begin{bmatrix}  \sigma^\prime(\mathbf{w}_1^t\mathbf{h}^{t - 1})w_{11}^t, & \sigma^\prime(\mathbf{w}_1^t\mathbf{h}^{t - 1})w_{12}^t, & \cdots, & \sigma^\prime(\mathbf{w}_1^t\mathbf{h}^{t - 1})w_{1m}^t \\  \sigma^\prime(\mathbf{w}_2^t\mathbf{h}^{t - 1})w_{21}^t, & \sigma^\prime(\mathbf{w}_2^t\mathbf{h}^{t - 1})w_{22}^t, & \cdots, & \sigma^\prime(\mathbf{w}_2^t\mathbf{h}^{t - 1})w_{2m}^t \\  \vdots & \vdots & & \vdots \\  \sigma^\prime(\mathbf{w}_n^t\mathbf{h}^{t - 1})w_{n1}^t, & \sigma^\prime(\mathbf{w}_n^t\mathbf{h}^{t - 1})w_{n2}^t, & \cdots, & \sigma^\prime(\mathbf{w}_n^t\mathbf{h}^{t - 1})w_{nm}^t  \end{bmatrix} \\  &= \begin{bmatrix}  \sigma^\prime(\mathbf{w}_1^t\mathbf{h}^{t - 1}), & 0, & \cdots, & 0 \\  0, & \sigma^\prime(\mathbf{w}_2^t\mathbf{h}^{t - 1}), & \cdots, & 0 \\  \vdots & \vdots & & \vdots \\  0, & 0, & \cdots, & \sigma^\prime(\mathbf{w}_n^t\mathbf{h}^{t - 1})  \end{bmatrix}  \begin{bmatrix}  w_{11}^t, & w_{12}^t, & \cdots, & w_{1m}^t \\  w_{21}^t, & w_{22}^t, & \cdots, & w_{2m}^t \\  \vdots & \vdots & & \vdots \\  w_{n1}^t, & w_{n2}^t, & \cdots, & w_{nm}^t  \end{bmatrix} \\  &= \text{diag}\left(\sigma(\mathbf{W}^t\mathbf{h}^{t - 1})\right)\mathbf{W}^t  \end{aligned}</script><p>  得到结论，计算完毕。</p><p>  综上，我认为PPT上的计算过程是错误的。</p></li></ul><h4 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h4><p>使用ReLU作为激活函数：</p><script type="math/tex; mode=display">\sigma(x) = \max(0, x)\quad\text{and}\quad\sigma^\prime(x)= \begin{cases}\begin{array}{ll}1 & \text{if } x > 0 \\0 & \text{otherwise}\end{array}\end{cases}</script><p>那么在对$\sigma(\mathbf{W}^i\mathbf{h}^{i - 1})$进行求导时，在$\sigma^\prime(\mathbf{W}^i\mathbf{h}^{i - 1})$中会出现很多的0元素，所以$\prod_{i = t}^{d - 1}\frac{\partial\mathbf{h}^{i + 1}}{\partial\mathbf{h}^i} = \prod_{i = t}^{d - 1}\text{diag}\left(\sigma^\prime(\mathbf{W}^i\mathbf{h}^{i - 1})\right)\mathbf{W}^i$中的一些元素会来自于$\prod_{i = t}^{d - 1}\mathbf{W}^i$</p><ul><li>如果$d - t$很大，值将会很大</li></ul><h5 id="梯度爆炸的问题"><a href="#梯度爆炸的问题" class="headerlink" title="梯度爆炸的问题"></a>梯度爆炸的问题</h5><ul><li>值超出值域（infinity）<ul><li>对于16位浮点数尤为严重（数值区间6e-5到6e4）</li></ul></li><li>对学习率敏感<ul><li>如果学习率太大$\to$大参数值$\to$更大的梯度</li><li>如果学习率太小$\to$训练无进展</li><li>我们可能需要在训练过程中不断调整学习率</li></ul></li></ul><h4 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h4><ul><li><p>使用Sigmoid作为激活函数：</p><script type="math/tex; mode=display">  \sigma(x) = \frac{1}{1 + e^{-x}} \quad \sigma^\prime(x) = \sigma(x)(1 - \sigma(x))</script></li><li><p>$\prod_{i = t}^{d - 1}\frac{\partial\mathbf{h}^{i + 1}}{\partial\mathbf{h}^i} = \prod_{i = t}^{d - 1}\text{diag}\left(\sigma^\prime(\mathbf{W}^i\mathbf{h}^{i - 1})\right)W^i$的元素值是$d - t$个小数值的乘积（$0.8^{100} \approx 2 \times 10^{-10}$）</p></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/14/image-20231014113243212.png" alt="image-20231014113243212"></p><h5 id="梯度消失的问题"><a href="#梯度消失的问题" class="headerlink" title="梯度消失的问题"></a>梯度消失的问题</h5><ul><li>梯度值变为0<ul><li>对16位浮点数尤为严重</li></ul></li><li>训练没有进展<ul><li>不管如何选择学习率</li></ul></li><li>对于底部层（靠近输入的一侧）尤为严重<ul><li>仅仅顶部层（靠近输出的一侧）训练的较好</li><li>无法让网络变得更深</li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>当数值过大或者过小会导致数值问题</li><li>常发生在深度模型中，因为会对n个数累乘</li></ul><h2 id="模型初始化和激活函数"><a href="#模型初始化和激活函数" class="headerlink" title="模型初始化和激活函数"></a>模型初始化和激活函数</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/14/image-20231014113841488.png" alt="image-20231014113841488"></p><h3 id="让训练更加稳定"><a href="#让训练更加稳定" class="headerlink" title="让训练更加稳定"></a>让训练更加稳定</h3><ul><li>目标：让梯度值在合理的范围内<ul><li>例如[1e-6, 1e3]</li></ul></li><li>将乘法变加法<ul><li>ResNet，LSTM</li></ul></li><li>归一化<ul><li>梯度归一化，梯度裁剪</li></ul></li><li>合理的权重初始和激活函数</li></ul><h3 id="让每层的方差是一个常数"><a href="#让每层的方差是一个常数" class="headerlink" title="让每层的方差是一个常数"></a>让每层的方差是一个常数</h3><ul><li>将每层的输出和梯度都看作随机变量</li><li>让它们的均值和方差都保持一致<ul><li>对于$\forall i, t$</li><li>正向：$\mathbb{E}\left[h_i^t\right] = 0 \quad \text{Var}\left[h_i^t\right] = a$</li><li>反向：$\mathbb{E}\left[\frac{\partial\ell}{\partial h_i^t}\right] = 0 \quad \text{Var}\left[\frac{\partial\ell}{\partial h_i^t}\right] = b$</li><li>其中$a$和$b$都是常数</li></ul></li></ul><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><ul><li>在合理值区间里随机初始参数</li><li>训练开始的时候更容易有数值不稳定<ul><li>远离最优解的地方损失函数表面可能很复杂</li><li>最优解附近表面会比较平</li></ul></li><li>使用$\mathcal{N}(0, 0.01)$来初始可能对小网络没问题，但不能保证深度神经网络</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/14/image-20231014133248420.png" alt="image-20231014133248420"></p><h4 id="例子：MLP-1"><a href="#例子：MLP-1" class="headerlink" title="例子：MLP"></a>例子：MLP</h4><ul><li><p>假设：</p><ul><li>$w_{i, j}^t$是第$t$层中，第$i$个向量的第$j$个元素，且服从$i.i.d.$</li><li>$\mathbb{E}\left[w_{i, j}^t\right] = 0, \text{Var}\left[w_{i, j}^t\right] = \gamma_t$</li><li>$h_i^{t - 1}$独立于$w_{i, j}^t$</li></ul></li><li><p>假设没有激活函数，那么当前层的输出$\mathbf{h}^t = \mathbf{W}^t\mathbf{h}^{t - 1}$，这里$\mathbf{W}^t \in \mathbb{R}^{n_t \times n_{t - 1}}$，那么有：</p><script type="math/tex; mode=display">  \begin{aligned}  \mathbb{E}\left[h_i^t\right]  &= \mathbb{E}\left[\mathbf{w}_i^t\mathbf{h}^{t - 1}\right] \\  &= \mathbb{E}\left[\sum_jw_{i, j}^th_j^{t - 1}\right] \\  &= \sum_j\mathbb{E}\left[w_{i, j}^t\right]\mathbb{E}\left[h_j^{t - 1}\right] \\  &= 0  \end{aligned}</script></li></ul><h5 id="正向方差"><a href="#正向方差" class="headerlink" title="正向方差"></a>正向方差</h5><script type="math/tex; mode=display">\begin{aligned}\text{Var}\left[h_i^t\right]&= \mathbb{E}\left[(h_i^t)^2\right] - \mathbb{E}\left[h_i^t\right]^2 \\&= \mathbb{E}\left[(h_i^t)^2\right] \\&= \mathbb{E}\left[\left(\sum_jw_{i, j}^th_j^{t - 1}\right)^2\right] \\&= \mathbb{E}\left[\sum_j\left(w_{i, j}^t\right)^2\left(h_j^{t - 1}\right)^2 + \sum_{j \ne k}w_{i, j}^tw_{i, k}^th_j^{t - 1}h_k^{t - 1}\right] \\&= \mathbb{E}\left[\sum_j\left(w_{i, j}^t\right)^2\left(h_j^{t - 1}\right)^2\right] + \mathbb{E}\left[\sum_{j \ne k}w_{i, j}^tw_{i, k}^th_j^{t - 1}h_k^{t - 1}\right] \\&= \mathbb{E}\left[\sum_j\left(w_{i, j}^t\right)^2\left(h_j^{t - 1}\right)^2\right] \\&= \sum_j\mathbb{E}\left[\left(w_{i, j}^t\right)^2\right]\mathbb{E}\left[\left(h_j^{t - 1}\right)^2\right] \\&= \sum_j\left(\mathbb{E}\left[\left(w_{i, j}^t\right)^2\right] - 0\right)\left(\mathbb{E}\left[\left(h_j^{t - 1}\right)^2\right] - 0\right) \\&= \sum_j\left(\mathbb{E}\left[\left(w_{i, j}^t\right)^2\right] - \mathbb{E}\left[w_{i, j}^t\right]^2\right)\left(\mathbb{E}\left[\left(h_j^{t - 1}\right)^2\right] - \mathbb{E}\left[h_j^{t - 1}\right]^2\right) \\&= \sum_j\text{Var}\left[w_{i, j}^t\right]\text{Var}\left[h_j^{t - 1}\right] \\&= n_{t - 1}\gamma_t\text{Var}\left[h_j^{t - 1}\right] \\&\Rightarrow n_{t - 1}\gamma_t = 1\end{aligned}</script><p>下面逐步拆解一下这个式子：</p><ul><li><p>首先，$\text{Var}\left[h_i^t\right] = \mathbb{E}\left[(h_i^t)^2\right] - \mathbb{E}\left[h_i^t\right]^2$是公式，“方差=平方的均值-均值的平方”，且在前面假设了$\mathbb{E}\left[h_i^t\right] = 0$，故$\text{Var}\left[h_i^t\right] = \mathbb{E}\left[(h_i^t)^2\right]$</p></li><li><p>将$\mathbb{E}\left[(h_i^t)^2\right]$写成和的形式，得到$\text{Var}\left[h_i^t\right] = \mathbb{E}\left[\left(\sum_jw_{i, j}^th_j^{t - 1}\right)^2\right]$</p></li><li><p>将平方打开，得到：</p><script type="math/tex; mode=display">  \begin{aligned}  \text{Var}\left[h_i^t\right]  &= \mathbb{E}\left[\sum_j\left(w_{i, j}^t\right)^2\left(h_j^{t - 1}\right)^2 + \sum_{j \ne k}w_{i, j}^tw_{i, k}^th_j^{t - 1}h_k^{t - 1}\right] \\  &= \mathbb{E}\left[\sum_j\left(w_{i, j}^t\right)^2\left(h_j^{t - 1}\right)^2\right] + \mathbb{E}\left[\sum_{j \ne k}w_{i, j}^tw_{i, k}^th_j^{t - 1}h_k^{t - 1}\right]  \end{aligned}</script></li><li><p>由于$\mathbb{E}\left[h_i^t\right] = 0$，故$\text{Var}\left[h_i^t\right] = \mathbb{E}\left[\sum_j\left(w_{i, j}^t\right)^2\left(h_j^{t - 1}\right)^2\right]$</p></li><li><p>做一次变形：$\text{Var}\left[h_i^t\right] = \sum_j\mathbb{E}\left[\left(w_{i, j}^t\right)^2\right]\mathbb{E}\left[\left(h_j^{t - 1}\right)^2\right]$</p></li><li><p>由于假设了$\mathbb{E}\left[w_{i, j}^t\right] = 0, \mathbb{E}\left[h_i^t\right] = 0$，可再做一次变形：</p><script type="math/tex; mode=display">  \begin{aligned}  \text{Var}\left[h_i^t\right]  &= \sum_j\mathbb{E}\left[\left(w_{i, j}^t\right)^2\right]\mathbb{E}\left[\left(h_j^{t - 1}\right)^2\right] \\  &= \sum_j\left(\mathbb{E}\left[\left(w_{i, j}^t\right)^2\right] - 0\right)\left(\mathbb{E}\left[\left(h_j^{t - 1}\right)^2\right] - 0\right) \\  &= \sum_j\left(\mathbb{E}\left[\left(w_{i, j}^t\right)^2\right] - \mathbb{E}\left[w_{i, j}^t\right]^2\right)\left(\mathbb{E}\left[\left(h_j^{t - 1}\right)^2\right] - \mathbb{E}\left[h_j^{t - 1}\right]^2\right)  \end{aligned}</script></li><li><p>回忆方差的公式，那么最终有$\text{Var}\left[h_i^t\right] = \sum_j\text{Var}\left[w_{i, j}^t\right]\text{Var}\left[h_j^{t - 1}\right]$</p></li><li><p>又$\text{Var}\left[w_{i, j}^t\right] = \gamma_t$，且第$t - 1$层的输入的维度为$n_{t - 1}$，故$\text{Var}\left[h_i^t\right] = n_{t - 1}\gamma_t\text{Var}\left[h_j^{t - 1}\right]$</p></li><li><p>由于每一层的方差要相同，所以需要$n_{t - 1}\gamma_t = 1$</p></li><li><p>得出结论，计算完毕</p></li></ul><h5 id="反向均值和方差"><a href="#反向均值和方差" class="headerlink" title="反向均值和方差"></a>反向均值和方差</h5><p>与正向情况类似：</p><script type="math/tex; mode=display">\begin{aligned}&\frac{\partial\ell}{\partial\mathbf{h}^{t - 1}} = \frac{\partial\ell}{\partial\mathbf{h}^t}\mathbf{W}^t \\&\Rightarrow \left(\frac{\partial\ell}{\partial\mathbf{h}^{t - 1}}\right)^\mathrm{T} = \left(\mathbf{W}^t\right)^\mathrm{T}\left(\frac{\partial\ell}{\partial\mathbf{h}^t}\right)^\mathrm{T} \\\end{aligned}</script><script type="math/tex; mode=display">\mathbb{E}\left[\frac{\partial\ell}{\partial h_i^{t - 1}}\right] = 0</script><script type="math/tex; mode=display">\begin{aligned}&\text{Var}\left[\frac{\partial\ell}{\partial h_i^{t - 1}}\right] = n_t\gamma_t\text{Var}\left[\frac{\partial\ell}{\partial h_j^t}\right] \\&\Rightarrow n_t\gamma_t = 1\end{aligned}</script><h4 id="Xavier初始"><a href="#Xavier初始" class="headerlink" title="Xavier初始"></a>Xavier初始</h4><p>在Xavier初始中，权重初始化时的方差是通过输入输出维度来定的。</p><ul><li>难以同时满足$n_{t - 1}\gamma_t = 1$和$n_t\gamma_t = 1$这两个条件，其中：<ul><li>$n_{t - 1}$是上层的输出，也是当前层的输入，$n_{t - 1}\gamma_t = 1$使得每一次前向输出的方差是一致的</li><li>$n_t$是当前层的输出，也是下一层的输入，$n_t\gamma_t = 1$使得每一次的梯度是一样的</li><li>因为$n_{t - 1}$是当前层的输入，$n_t$是当前层的输出，除非输入等于输出，不然无法满足这个条件</li></ul></li><li>由于不能同时满足，那么Xavier做了一个折中，它使得$\frac{\gamma_t(n_{t - 1} + n_t)}{2} = 1$，那么有$\gamma_t = \frac{2}{n_{t - 1} + n_t}$（注意$\gamma_t = \text{Var}\left[w_{i, j}^t\right]$），那么只要给定输入$n_{t - 1}$和输出$n_t$，我就能知道权重需要满足方差的大小</li><li>那么在确定了输入$n_{t - 1}$和输出$n_t$后，我们能够使用如下的随机分布来初始化权重<ul><li>正态分布$\mathcal{N}\left(0, \sqrt{\frac{2}{n_{t - 1} + n_t}}\right)$</li><li>均匀分布$\mathcal{U}\left(-\sqrt{\frac{6}{n_{t - 1} + n_t}}, \sqrt{\frac{6}{n_{t - 1} + n_t}}\right)$<ul><li>注意分布$\mathcal{U}\left[-a, a\right]$的方差是$\frac{a^2}{3}$（均匀分布的方差为$\frac{(b - a)^2}{12}$）</li></ul></li></ul></li><li>适配权重形状变换，特别是$n_t$</li></ul><h3 id="假设线性的激活函数"><a href="#假设线性的激活函数" class="headerlink" title="假设线性的激活函数"></a>假设线性的激活函数</h3><p>假设$\sigma(x) = \alpha x + \beta$，那么：</p><script type="math/tex; mode=display">\mathbf{h}^\prime = \mathbf{W}^t\mathbf{h}^{t - 1} \quad \text{and} \quad \mathbf{h}^t = \sigma(\mathbf{h}^\prime)</script><p>其中：</p><ul><li>$\mathbf{W}^t$是当前层的权重</li><li>$\mathbf{h}^{t - 1}$是上层的输出，当前层的输入</li><li>$\mathbf{h}^\prime$是输入与权重的乘积</li><li>输入与权重的乘积再经过激活函数处理后，就得到了输出$\mathbf{h}^t$</li></ul><h4 id="正向"><a href="#正向" class="headerlink" title="正向"></a>正向</h4><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}\left[h_i^t\right]&= \mathbb{E}\left[\sigma(h_i^\prime)\right] \\&= \mathbb{E}\left[\alpha h_i^\prime + \beta\right] \\&= \mathbb{E}\left[\alpha h_i^\prime\right] + \mathbb{E}\left[\beta\right] \\&= \beta \\&\Rightarrow \beta = 0\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}\text{Var}\left[h_i^t\right]&= \mathbb{E}\left[\left(h_i^t\right)^2\right] - \mathbb{E}\left[h_i^t\right]^2 \\&= \mathbb{E}\left[\left(\alpha h_i^\prime + \beta\right)^2\right] - \beta^2 \\&= \mathbb{E}\left[\alpha^2\left(h_i^\prime\right)^2 + 2\alpha\beta h_i^\prime + \beta^2\right] - \beta^2 \\&= \mathbb{E}\left[\alpha^2\left(h_i^\prime\right)^2\right] + \mathbb{E}\left[2\alpha\beta h_i^\prime\right] + \mathbb{E}\left[\beta^2\right] - \beta^2 \\&= \mathbb{E}\left[\alpha^2\left(h_i^\prime\right)^2\right] + \beta^2 - \beta^2 \\&= \alpha^2\mathbb{E}\left[\left(h_i^\prime\right)^2\right] - 0 \\&= \alpha^2\mathbb{E}\left[\left(h_i^\prime\right)^2\right] - \alpha^2\mathbb{E}\left[h_i^\prime\right]^2 \\&= \alpha^2\left(\mathbb{E}\left[\left(h_i^\prime\right)^2\right] - \mathbb{E}\left[h_i^\prime\right]^2\right) \\&= \alpha^2\text{Var}\left[h_i^\prime\right] \\&\Rightarrow \alpha = 1\end{aligned}</script><p>那么这里的意思是，为了使前向输出的均值为0，方差固定，激活函数只能是$\sigma(x) = x$（这里@ReLU）。</p><h4 id="反向"><a href="#反向" class="headerlink" title="反向"></a>反向</h4><p>假设$\sigma(x) = \alpha x + \beta$，那么：</p><script type="math/tex; mode=display">\frac{\partial\ell}{\partial\mathbf{h}^\prime} = \frac{\partial\ell}{\partial\mathbf{h}^t}\left(\mathbf{W}^t\right)^\mathrm{T} \quad \text{and} \quad \frac{\partial\ell}{\partial\mathbf{h}^{t - 1}} = \alpha\frac{\partial\ell}{\partial\mathbf{h}^\prime}</script><p>那么：</p><script type="math/tex; mode=display">\mathbb{E}\left[\frac{\partial\ell}{\partial h_i^{t - 1}}\right] = 0 \quad \Rightarrow \quad \beta = 0</script><script type="math/tex; mode=display">\text{Var}\left[\frac{\partial\ell}{\partial h_i^{t - 1}}\right] = \alpha^2\text{Var}\left[\frac{\partial\ell}{\partial h_i^\prime}\right] \quad \Rightarrow \quad \alpha = 1</script><h3 id="检查常用激活函数"><a href="#检查常用激活函数" class="headerlink" title="检查常用激活函数"></a>检查常用激活函数</h3><p>使用泰勒展开：</p><script type="math/tex; mode=display">\begin{aligned}\text{sigmoid}(x) &= \frac{1}{2} + \frac{x}{4} - \frac{x^3}{48} + O(x^5) \\\tanh(x) &= 0 + x - \frac{x^3}{3} + O(x^5) \\\text{relu} &= 0 + x \quad \text{for } x \ge 0\end{aligned}</script><p>调整sigmoid：</p><script type="math/tex; mode=display">4 \times \text{sigmoid}(x) - 2</script><p><img src="https://img.karltan.com/notes-out-class/d2l/14/image-20231014161502427.png" alt="image-20231014161502427"></p><p>上图中蓝色的曲线就是调整之后的sigmoid。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>合理的权重初始值和激活函数的选取可以提升数值稳定性。</p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 14 数值稳定性 + 模型初始化和激活函数。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 13 丢弃法</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/13/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/13/</id>
    <published>2023-10-12T13:00:00.000Z</published>
    <updated>2023-10-13T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="13-丢弃法"><a href="#13-丢弃法" class="headerlink" title="13 丢弃法"></a>13 丢弃法</h1><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><ul><li>一个好的模型需要对输入数据的扰动鲁棒<ul><li>使用有噪音的数据等价于Tikhonov正则</li><li>丢弃法：在层之间加入噪音</li></ul></li></ul><h2 id="无偏差的加入噪音"><a href="#无偏差的加入噪音" class="headerlink" title="无偏差的加入噪音"></a>无偏差的加入噪音</h2><ul><li><p>对$\mathbf{x}$加入噪音得到$\mathbf{x}^\prime$，我们希望</p><script type="math/tex; mode=display">  \mathbf{E}[\mathbf{x}^\prime] = \mathbf{x}</script></li><li><p>丢弃法对每个元素进行如下扰动</p><script type="math/tex; mode=display">  x_i^\prime  = \begin{cases}  \begin{array}{ll}  0 & \text{with probablity } p \\  \frac{x_i}{1 - p} & \text{otherwise}  \end{array}  \end{cases}</script><p>  在经过上面的扰动后，可以证明$\mathbf{E}[\mathbf{x}^\prime] = \mathbf{x}$。</p><p>  简单说明一下为什么$\mathbf{E}[\mathbf{x}^\prime] = \mathbf{x}$，$\mathbf{x}$是一个向量，所以只需要证明$\mathrm{E}[x_i^\prime] = x_i$即可，由于$x_i^\prime$的值被$x_i^\prime = \begin{cases}\begin{array}{ll}0 &amp; \text{with probablity } p \\ \frac{x_i}{1 - p} &amp; \text{otherwise}\end{array}\end{cases}$所扰动，其概率可以计算为$\mathrm{E}[x_i^\prime] = p \times 0 + (1 - p) \times \frac{x_i}{1 - p} = x_i$，得到结论，证明完毕。</p></li></ul><h2 id="使用丢弃法"><a href="#使用丢弃法" class="headerlink" title="使用丢弃法"></a>使用丢弃法</h2><ul><li>通常将丢弃法作用在隐藏全连接层的输出上<script type="math/tex; mode=display">  \begin{aligned}  \mathbf{h} &= \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \\  \mathbf{h}^\prime &= \text{dropout}(\mathbf{h}) \\  \mathbf{o} &= \mathbf{W}_2\mathbf{h}^\prime + \mathbf{b}_2 \\  \mathbf{y} &= \text{softmax}(\mathbf{o})  \end{aligned}</script></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/13/image-20231012214331521.png" alt="image-20231012214331521"></p><p>一种说法是可以将dropout看作是模型集成，因为在随机丢的过程中，生成了很多不一样的模型。但是在后续的实验中，发现dropout的效果和正则化是差不多的。</p><h2 id="推理中的丢弃法"><a href="#推理中的丢弃法" class="headerlink" title="推理中的丢弃法"></a>推理中的丢弃法</h2><ul><li><p>正则项只在训练中使用：他们影响模型参数的更新</p></li><li><p>在推理过程中，丢弃法直接返回输入</p><script type="math/tex; mode=display">  \mathbf{h} = \text{dropout}(\mathbf{h})</script><ul><li>这样也能保证确定性的输出</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>丢弃法将一些输出项随机置0来控制模型复杂度</li><li>常作用在多层感知机的隐藏层输出上</li><li>丢弃概率是控制模型复杂度的超参数</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><p>我们实现<code>dropout_layer</code>函数，该函数以<code>dropout</code>的概率丢弃张量输入<code>X</code>中的元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.randn(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure><p>测试<code>dropout_layer</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape(<span class="number">2</span>, <span class="number">8</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;dropout_layer(X, 0.0) =\n&#x27;</span>, dropout_layer(X, <span class="number">0.0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;dropout_layer(X, 0.5) =\n&#x27;</span>, dropout_layer(X, <span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;dropout_layer(X, 1.0) =\n&#x27;</span>, dropout_layer(X, <span class="number">1.0</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span><br><span class="line">dropout_layer(X, 0.0) =</span><br><span class="line"> tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span><br><span class="line">dropout_layer(X, 0.5) =</span><br><span class="line"> tensor([[ 0.,  2.,  0.,  0.,  0.,  0., 12., 14.],</span><br><span class="line">        [ 0.,  0.,  0.,  0., 24.,  0., 28.,  0.]])</span><br><span class="line">dropout_layer(X, 1.0) =</span><br><span class="line"> tensor([[0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line">        [0., 0., 0., 0., 0., 0., 0., 0.]])</span><br></pre></td></tr></table></figure><p>定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs,</span></span><br><span class="line"><span class="params">                 num_hiddens1, num_hiddens2, is_training=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape(-<span class="number">1</span>, self.num_inputs)))</span><br><span class="line">        <span class="comment"># 只有在训练模型时才使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure><p>读取数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure><p>训练和测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/13/output1.svg" alt="output1"></p><h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">    nn.Dropout(dropout1),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">    nn.Dropout(dropout2),</span><br><span class="line">    nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (1): Linear(in_features=784, out_features=256, bias=True)</span><br><span class="line">  (2): ReLU()</span><br><span class="line">  (3): Dropout(p=0.2, inplace=False)</span><br><span class="line">  (4): Linear(in_features=256, out_features=256, bias=True)</span><br><span class="line">  (5): ReLU()</span><br><span class="line">  (6): Dropout(p=0.5, inplace=False)</span><br><span class="line">  (7): Linear(in_features=256, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>对模型进行训练和测试：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/13/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 13 丢弃法。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 12 权重衰退</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/12/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/12/</id>
    <published>2023-10-11T12:00:00.000Z</published>
    <updated>2023-10-12T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="12-权重衰退"><a href="#12-权重衰退" class="headerlink" title="12 权重衰退"></a>12 权重衰退</h1><h2 id="使用均方范数作为硬性限制"><a href="#使用均方范数作为硬性限制" class="headerlink" title="使用均方范数作为硬性限制"></a>使用均方范数作为硬性限制</h2><ul><li><p>通过限制参数值的选择范围来控制模型容量</p><script type="math/tex; mode=display">  \min\ell(\mathbf{w}, b) \quad \text{subject to } \Vert\mathbf{w}\Vert_2^2 \le \theta</script><ul><li>通常不限制偏移$b$（限不限制都差不多）</li><li>小的$\theta$意味着更强的正则项</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/12/image-20231011210046243.png" alt="image-20231011210046243"></p><h2 id="使用均方范数作为柔性限制"><a href="#使用均方范数作为柔性限制" class="headerlink" title="使用均方范数作为柔性限制"></a>使用均方范数作为柔性限制</h2><ul><li><p>对每个$\theta$，都可以找到$\lambda$使得之前的目标函数等价于下面</p><script type="math/tex; mode=display">  \min\ell(\mathbf{w}, b) + \frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2</script><ul><li>可以通过拉格朗日乘子法（回忆一下高数的条件极值！）来证明</li></ul></li><li><p>超参数$\lambda$控制了正则项的重要程度</p><ul><li>$\lambda = 0$：无作用</li><li>$\lambda \to \infty, \mathbf{w}^\ast \to \mathbf{0}$</li></ul></li></ul><h2 id="演示对最优解的影响"><a href="#演示对最优解的影响" class="headerlink" title="演示对最优解的影响"></a>演示对最优解的影响</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/12/image-20231011211149968.png" alt="image-20231011211149968"></p><p>首先，我们将$\ell(\mathbf{w}, b)$看作一个二次函数，当$\tilde{\mathbf{w}}^\ast = \arg\min\ell(\mathbf{w}, b)$时，最优解位于自己的中心（也就是图中绿色同心椭圆的圆心，在图中已经用绿点标出），但是当我们加入$\frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2$后，式子变为$\tilde{\mathbf{w}}^\ast = \arg\min\ell(\mathbf{w}, b) + \frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2$。</p><p>对于$\frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2$来说，我们同样也可以将其看作一个中心为橙色同心圆圆心的二次函数，那么这两个二次函数就会互相“争抢”最优点，也就是将最优点向自己的中心拉动，但是谁的拉力更强呢？</p><p>那么从上图右上角的简图可以看出，当二次函数上的一个点离中心（也就是二次函数的顶点）越远时，其梯度越大，也就是拉力越强，那么原本$\tilde{\mathbf{w}}^\ast = \arg\min\ell(\mathbf{w}, b)$的最优解绿色点就会逐渐被$\frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2$向自己的中心拉，直至拉到$\tilde{\mathbf{w}}^\ast = \arg\min\ell(\mathbf{w}, b) + \frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2$的最优解橙色点（绿色最外侧同心椭圆和橙色最外侧同心圆的切点，被老师笔迹挡住了）处，在该点处两个二次函数的拉力达到平衡，从而得到最优解。</p><p>那么总体来说，$\frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2$的引入导致最优解向原点移动了，那么可以说最优解的值变小了，这就导致参数值的选择范围变小了，这就将模型容量降低了（详见<a href="https://blog.karltan.com/notes-out-class/d2l/11/">动手学深度学习v2 11 模型选择 + 过拟合和欠拟合 | Karl的博客 (karltan.com)</a>）。</p><h2 id="参数更新法则"><a href="#参数更新法则" class="headerlink" title="参数更新法则"></a>参数更新法则</h2><ul><li><p>计算梯度</p><script type="math/tex; mode=display">  \frac{\partial}{\partial\mathbf{w}}\left(\ell(\mathbf{w}, b) + \frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2\right) = \frac{\partial\ell(\mathbf{w}, b)}{\partial\mathbf{w}} + \lambda\mathbf{w}</script></li><li><p>时间t更新参数，其中$\eta$为学习率</p><script type="math/tex; mode=display">  \begin{aligned}  \mathbf{w}_{t + 1}  &= \mathbf{w}_t -\eta\frac{\partial}{\partial\mathbf{w}_t}\left(\ell(\mathbf{w}_t, b_t) + \frac{\lambda}{2}\Vert\mathbf{w}_t\Vert_2^2\right) \\  &= \mathbf{w}_t -\eta\left(\frac{\partial\ell(\mathbf{w}_t, b_t)}{\partial\mathbf{w}_t} + \lambda\mathbf{w}_t\right)\\  &= (1 - \eta\lambda)\mathbf{w}_t - \eta\frac{\partial\ell(\mathbf{w}_t, b_t)}{\partial\mathbf{w}_t}  \end{aligned}</script><ul><li>通常$\eta\lambda &lt; 1$，在深度学习中通常叫做权重衰退</li><li>如果不加$\frac{\lambda}{2}\Vert\mathbf{w}\Vert_2^2$，$\mathbf{w}_{t + 1}$的更新公式为$\mathbf{w}_{t + 1} = \mathbf{w}_t - \eta\frac{\partial\ell(\mathbf{w}_t, b_t)}{\partial\mathbf{w}_t}$，可以看到和上式相比，只是将$\mathbf{w}_t$的权重由$1$改为了$1 - \eta\lambda$，这就称为权重衰退。</li></ul></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度</li><li>正则项权重是控制模型复杂度的超参数</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><p>权重衰退是最广泛使用的正则化技术之一。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>像以前一样生成一些数据：</p><script type="math/tex; mode=display">y = 0.05 + \sum_{i = 1}^d0.01x_i + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.01^2)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span> <span class="comment"># 训练样本越小越容易过拟合</span></span><br><span class="line">true_w, true_b = torch.ones(num_inputs, <span class="number">1</span>) * <span class="number">0.01</span>, <span class="number">0.05</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train)</span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size)</span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure><p>定义$L_2$范数惩罚：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(w ** <span class="number">2</span>) / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>定义训练代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 增加了L2范数惩罚项，</span></span><br><span class="line">            <span class="comment"># 广播机制使l2_penalty(w)成为一个长度为batch_size的向量</span></span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br></pre></td></tr></table></figure><p>忽略正则化直接训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w的L2范数是： 13.940652847290039</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/12/output1.svg" alt="output1"></p><p>使用权重衰减：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w的L2范数是： 0.3739544749259949</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/12/output2.svg" alt="output2"></p><h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># 偏置参数没有衰减</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: net[<span class="number">0</span>].weight, <span class="string">&quot;weight_decay&quot;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>: net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure><p>这些图看起来和从零开始实现权重衰减时的图相同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w的L2范数： 13.023282051086426</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/12/output3.svg" alt="output3"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_concise(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w的L2范数： 0.4676832854747772</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/12/output4.svg" alt="output4"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 12 权重衰退。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 11 模型选择 + 过拟合和欠拟合</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/11/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/11/</id>
    <published>2023-10-11T04:00:00.000Z</published>
    <updated>2023-10-11T10:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="11-模型选择-过拟合和欠拟合"><a href="#11-模型选择-过拟合和欠拟合" class="headerlink" title="11 模型选择 + 过拟合和欠拟合"></a>11 模型选择 + 过拟合和欠拟合</h1><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="预测谁会偿还贷款"><a href="#预测谁会偿还贷款" class="headerlink" title="预测谁会偿还贷款"></a>预测谁会偿还贷款</h3><ul><li>银行雇你来调查谁会偿还贷款<ul><li>你得到了100个申请人的信息</li><li>其中五个人在3年内违约了</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011123458506.png" alt="image-20231011123458506"></p><h4 id="惊讶的发现"><a href="#惊讶的发现" class="headerlink" title="惊讶的发现"></a>惊讶的发现</h4><ul><li>你发现所有的5个人在面试的时候都穿了蓝色衬衫</li><li>你的模型也发现了这个强信号</li><li>这回有什么问题？</li></ul><h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h3><ul><li>训练误差：模型在训练数据上的误差</li><li>泛化误差：模型在新数据上的误差</li><li>例子：根据模考成绩来预测未来考试分数<ul><li>在过去的考试中表现很好（训练误差）不代表未来考试一定会好（泛化误差）</li><li>学生A通过背书在模考中拿到很好成绩</li><li>学生B知道答案后面的原因</li></ul></li></ul><p><strong>全篇金句：《我的GRE很差，复习了两三个星期，结果也就是MIT和CMU这种学校》</strong></p><h3 id="验证数据集和测试数据集"><a href="#验证数据集和测试数据集" class="headerlink" title="验证数据集和测试数据集"></a>验证数据集和测试数据集</h3><ul><li>验证（validation）数据集：一个用来评估模型好坏的数据集<ul><li>例如拿出50%的训练数据</li><li>不要和训练数据混在一起（常犯错误）</li></ul></li><li>测试（test）数据集：只用一次的数据集。例如<ul><li>未来的考试</li><li>我出价的房子的实际成交价</li><li>我在Kaggle私有排行榜中的数据集</li></ul></li></ul><h3 id="K-折交叉验证"><a href="#K-折交叉验证" class="headerlink" title="K-折交叉验证"></a>K-折交叉验证</h3><ul><li>在没有足够多数据时使用（这是常态）</li><li>算法：<ul><li>将训练数据分割为K块</li><li>For i = 1, …, K<ul><li>使用第i块作为验证数据集，其余的作为训练数据集</li></ul></li><li>报告K个验证集误差的平均</li></ul></li><li>常用：K = 5或10</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>训练数据集：训练模型参数</li><li>验证数据集：选择模型超参数</li><li>非大数据集上通常使用K-折交叉验证</li></ul><h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011133207861.png" alt="image-20231011133207861"></p><p>模型容量指的是模型的复杂度。</p><h3 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h3><ul><li>拟合各种函数的能力</li><li>低容量的模型难以拟合训练数据</li><li>高容量的模型可以记住所有的训练数据</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011133724955.png" alt="image-20231011133724955"></p><h4 id="模型容量的影响"><a href="#模型容量的影响" class="headerlink" title="模型容量的影响"></a>模型容量的影响</h4><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011133902045.png" alt="image-20231011133902045"></p><h4 id="估计模型容量"><a href="#估计模型容量" class="headerlink" title="估计模型容量"></a>估计模型容量</h4><ul><li>难以在不同种类的算法之间比较<ul><li>例如树模型和神经网络</li></ul></li><li>给定一个模型种类，将有两个主要因素<ul><li>参数的个数</li><li>参数值的选择范围</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011134506866.png" alt="image-20231011134506866"></p><h3 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h3><ul><li>统计学习理论的一个核心思想</li><li>对于一个分类模型，VC等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型来对它进行完美分类</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011134902698.png" alt="image-20231011134902698"></p><h4 id="线性分类器的VC维"><a href="#线性分类器的VC维" class="headerlink" title="线性分类器的VC维"></a>线性分类器的VC维</h4><ul><li><p>2维输入的感知机，VC维=3</p><ul><li><p>能够分类任何三个点，但不是4个（xor）</p><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011135253950.png" alt="image-20231011135253950"></p></li></ul></li><li><p>支持N维输入的感知机的VC维是$N + 1$</p></li><li><p>一些多层感知机的VC维为$O(N\log_2N)$</p></li></ul><h4 id="VC维的用处"><a href="#VC维的用处" class="headerlink" title="VC维的用处"></a>VC维的用处</h4><ul><li>提供为什么一个模型好的理论依据<ul><li>它可以衡量训练误差和泛化误差之间的间隔</li></ul></li><li>但深度学习中很少使用<ul><li>衡量不是很准确</li><li>计算深度学习模型的VC维很困难</li></ul></li></ul><h3 id="数据复杂度"><a href="#数据复杂度" class="headerlink" title="数据复杂度"></a>数据复杂度</h3><ul><li>多个重要因素<ul><li>样本个数</li><li>每个样本的元素个数</li><li>时间、空间结构</li><li>多样性</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/11/image-20231011135822918.png" alt="image-20231011135822918"></p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>模型容量需要匹配数据复杂度，否则可能导致欠拟合和过拟合</li><li>统计机器学习提供数学工具来衡量模型复杂度</li><li>实际中一般靠观察训练误差和验证误差</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>通过多项式拟合来交互地探索这些概念。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>使用以下三阶多项式来生成训练集和测试数据的标签：</p><script type="math/tex; mode=display">y = 5 + 1.2x - 3.4\frac{x^2}{2!} + 5.6\frac{x^3}{3!} + \epsilon \text{ where } \epsilon \sim \mathcal{N}(0, 0.1^2)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">max_degree = <span class="number">20</span> <span class="comment"># 生成一个20维的向量</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span></span><br><span class="line">true_w = np.zeros(max_degree)</span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])</span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>))</span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + <span class="number">1</span>)</span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)</span><br></pre></td></tr></table></figure><p>看一下前两个样本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">true_w, features, poly_features, labels = [</span><br><span class="line">    torch.tensor(x, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">features[:<span class="number">2</span>], poly_features[:<span class="number">2</span>, :], labels[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.3926],</span><br><span class="line">         [-0.1971]]),</span><br><span class="line"> tensor([[ 1.0000e+00,  3.9256e-01,  7.7052e-02,  1.0083e-02,  9.8951e-04,</span><br><span class="line">           7.7689e-05,  5.0830e-06,  2.8505e-07,  1.3988e-08,  6.1012e-10,</span><br><span class="line">           2.3951e-11,  8.5474e-13,  2.7962e-14,  8.4436e-16,  2.3676e-17,</span><br><span class="line">           6.1962e-19,  1.5202e-20,  3.5105e-22,  7.6561e-24,  1.5818e-25],</span><br><span class="line">         [ 1.0000e+00, -1.9711e-01,  1.9427e-02, -1.2764e-03,  6.2900e-05,</span><br><span class="line">          -2.4797e-06,  8.1462e-08, -2.2939e-09,  5.6520e-11, -1.2379e-12,</span><br><span class="line">           2.4400e-14, -4.3723e-16,  7.1820e-18, -1.0890e-19,  1.5332e-21,</span><br><span class="line">          -2.0148e-23,  2.4821e-25, -2.8780e-27,  3.1516e-29, -3.2696e-31]]),</span><br><span class="line"> tensor([5.2712, 4.6437]))</span><br></pre></td></tr></table></figure><p>实现一个函数来评估模型在给定数据集上的损失：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失。&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>定义训练函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels, num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="number">1</span>, <span class="number">1</span>)), batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="number">1</span>, <span class="number">1</span>)), batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">1e-3</span>, <span class="number">1e2</span>], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure><p>三阶多项式函数拟合（正态）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight: [[ 5.008722   1.2207366 -3.3949623  5.585942 ]]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/11/output1.svg" alt="output1"></p><p>线性函数拟合（欠拟合）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight: [[3.061323  4.0269103]]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/11/output2.svg" alt="output2"></p><p>高阶多项式函数拟合（过拟合）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">weight: [[ 5.0102854   1.3048034  -3.4156542   5.1497645   0.03866126  1.2160044</span><br><span class="line">   0.16914661 -0.10108263 -0.11531459  0.15235905  0.11382856  0.04193605</span><br><span class="line">   0.09186782 -0.03405394  0.21768057  0.1064044  -0.21055214  0.01479841</span><br><span class="line">   0.17359915 -0.09508206]]</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/11/output3.svg" alt="output3"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 11 模型选择 + 过拟合和欠拟合。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 10 多层感知机 + 代码实现</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/10/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/10/</id>
    <published>2023-10-10T13:00:00.000Z</published>
    <updated>2023-10-11T01:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="10-多层感知机-代码实现"><a href="#10-多层感知机-代码实现" class="headerlink" title="10 多层感知机 + 代码实现"></a>10 多层感知机 + 代码实现</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h3 id="感知机定义"><a href="#感知机定义" class="headerlink" title="感知机定义"></a>感知机定义</h3><ul><li><p>给定输入$\mathbf{x}$，权重$\mathbf{w}$和偏移$b$，感知机输出：</p><script type="math/tex; mode=display">  o = \sigma\left(\left\langle\mathbf{w}, \mathbf{x}\right\rangle + b\right)  \quad  \sigma(x)  = \begin{cases}  \begin{array}{ll}  1 & \text{if } x > 0 \\  0 & \text{otherwise}  \end{array}  \end{cases}</script><p>  <img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010203900103.png" alt="image-20231010203900103"></p></li><li><p>如果将输出改写为：</p><script type="math/tex; mode=display">  o = \sigma\left(\left\langle\mathbf{w}, \mathbf{x}\right\rangle + b\right)  \quad  \sigma(x)  = \begin{cases}  \begin{array}{ll}  1 & \text{if } x > 0 \\  -1 & \text{otherwise}  \end{array}  \end{cases}</script><p>  那么这就变为一个输出为-1或1的二分类问题</p><ul><li>Vs. 回归输出实数</li><li>Vs. Softmax回归输出概率</li></ul></li></ul><h3 id="训练感知机"><a href="#训练感知机" class="headerlink" title="训练感知机"></a>训练感知机</h3><p>训练算法伪代码如下：</p><script type="math/tex; mode=display">\begin{array}{ll}\mathbf{initialize\ } w = 0 \text{ and } b = 0 \\\mathbf{repeat} \\\quad \mathbf{if\ } y_i\left[\left\langle w, x_i\right\rangle + b\right] \le 0 \mathbf{\ then} \\\quad \quad w \gets w + y_ix_i \text{ and } b \gets b + y_i \\\quad \mathbf{end\ if} \\\mathbf{until}\text{ all classified correctly}\end{array}</script><p>当$y_i\left[\left\langle w, x_i\right\rangle + b\right] \le 0$时，意味着$y_i$和$\left[\left\langle w, x_i\right\rangle + b\right]$异号，说明分类错误。</p><p>上述算法等价于使用批量大小为1的梯度下降，并使用如下的损失函数：</p><script type="math/tex; mode=display">\ell(y, \mathbf{x}, \mathbf{w}) = \max(0, -y\left\langle\mathbf{w}, \mathbf{x}\right\rangle)</script><blockquote><p>解释一下这个损失函数：</p><ul><li>当分类正确时，需要损失为0，参数不需要更新，那么$\ell(y, \mathbf{x}, \mathbf{w})$应为0，注意到此时$-y\left\langle\mathbf{w}, \mathbf{x}\right\rangle$小于0，那么正好$\max(0, -y\left\langle\mathbf{w}, \mathbf{x}\right\rangle) = 0$，正好对应了不更新参数；</li><li>当分类错误时，需要损失不为0，需要更新参数，那么$\ell(y, \mathbf{x}, \mathbf{w})$应不为0，注意到此时$-y\left\langle\mathbf{w}, \mathbf{x}\right\rangle$大于0，那么正好$\max(0, -y\left\langle\mathbf{w}, \mathbf{x}\right\rangle) = -y\left\langle\mathbf{w}, \mathbf{x}\right\rangle$，可以对参数进行更新。</li></ul></blockquote><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010212450064.png" alt="image-20231010212450064"></p><p>当该数据集中有一只猫和一只狗时，上图中的这条直线可以将猫和狗正确的分类。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010212527574.png" alt="image-20231010212527574"></p><p>但是新来了一只狗后，原来的直线就不能够正确的分类这个新的狗，而是需要进行移动（也就是更新自己的参数），移动后，又能够将猫和狗正确分类。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010212750109.png" alt="image-20231010212750109"></p><p>又增加了一只狗和一只猫，可以发现直线再次进行了移动（参数继续更新）。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010212832613.png" alt="image-20231010212832613"></p><p>继续增加，可以看到，图中的这条直线能够将猫和狗正确分类，所以直线会停止移动（参数停止更新）。</p><h3 id="收敛定理"><a href="#收敛定理" class="headerlink" title="收敛定理"></a>收敛定理</h3><ul><li><p>数据在半径$r$内（下图中的绿色圆形区域）</p><p>  <img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010213412131.png" alt="image-20231010213412131"></p></li><li><p>对于一个分类面，如果其参数满足$\Vert\mathbf{w}\Vert^2 + b^2 \le 1$，且存在一个大于0的余量$\rho$，能够分类两类（$\rho$是上图中两条绿色直线间的距离），即：</p><script type="math/tex; mode=display">  y(\mathbf{x}^\mathrm{T}\mathbf{w} + b) \ge \rho</script><p>  那么感知机保证在$\frac{r^2 + 1}{\rho^2}$步后收敛</p></li><li><p>那么可能会有疑惑：</p><ul><li>半径$r$怎么测量？</li><li><p>余量$\rho$怎么确定？</p><p>答案是确定不了。</p><p>收敛定理是统计中的概念，而机器学习可以看作是统计的计算面，所以我们当然不会知道$r$和$\rho$怎么计算。</p></li></ul></li></ul><h3 id="XOR问题（Minsky-amp-Papert，1969）"><a href="#XOR问题（Minsky-amp-Papert，1969）" class="headerlink" title="XOR问题（Minsky&amp;Papert，1969）"></a>XOR问题（Minsky&amp;Papert，1969）</h3><p>感知机不能拟合XOR函数，它只能产生线性分割面：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010214345115.png" alt="image-20231010214345115"></p><p>上图中，无法找到一条直线将红色点和绿色点分开。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>感知机是一个二分类模型，是最早的AI模型之一</li><li>它的求解算法等价于使用批量大小为1的梯度下降</li><li>它不能拟合XOR函数，导致了第一次AI寒冬</li></ul><h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010214923546.png" alt="image-20231010214923546"></p><h3 id="学习XOR"><a href="#学习XOR" class="headerlink" title="学习XOR"></a>学习XOR</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010215351484.png" alt="image-20231010215351484"></p><p>我们可以学习两条直线，来解决XOR的分类问题。</p><p>首先是蓝色线，蓝色线将左侧的点分类为正类，右侧的点分类为负类。</p><p>其次是橙色线，橙色线将上方的点分类为正类，下方的点分类为负类。</p><p>那么将这两个直线分类的结果结合起来，就能够解决XOR问题，如下表。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010215409539.png" alt="image-20231010215409539"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010215437450.png" alt="image-20231010215437450"></p><p>流程如上图。</p><h3 id="单隐藏层"><a href="#单隐藏层" class="headerlink" title="单隐藏层"></a>单隐藏层</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010220159384.png" alt="image-20231010220159384"></p><h4 id="单分类"><a href="#单分类" class="headerlink" title="单分类"></a>单分类</h4><ul><li><p>输入$\mathbf{x} \in \mathbb{R}^n$（可能$\mathbf{x} \in \mathbb{R}^{n \times 1}$更好理解）</p></li><li><p>隐藏层$\mathbf{W}_1 \in \mathbb{R}^{m \times n}, \mathbf{b}_1 \in \mathbf{R}^m$（$\mathbf{b}_1$的形状就无所谓了，反正跟着$\mathbf{W}_1\mathbf{x}$的形状来，但是$\mathbf{b}_1 \in \mathbb{R}^{m \times 1}$）</p></li><li><p>输出层$\mathbf{w}_2 \in \mathbb{R}^m, b_2 \in \mathbb{R}$（同理，这里我认为$\mathbf{w}_2 \in \mathbb{R}^{m \times 1}$更好理解）</p><script type="math/tex; mode=display">  \begin{aligned}  \mathbf{h} &= \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \\  o &= \mathbf{w}_2^\mathrm{T}\mathbf{h} + b_2  \end{aligned}</script><p>  $\sigma$是按元素的激活函数（大写黑体代表矩阵，小写黑体代表向量，小写非黑体代表标量）</p><p>  下面计算一下形状：</p><ul><li>$\mathbf{W}_1\mathbf{x} \in \mathbb{R}^{m \times 1}$，那么$\mathbf{W}_1\mathbf{x} + \mathbf{b}_1 \in \mathbb{R}^{m \times 1}$，那么$\mathbf{h} = \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \in \mathbb{R}^{m \times 1}$</li><li><p>$\mathbf{w}_2^\mathrm{T}\mathbf{h} \in \mathbb{R}^{1 \times 1}$，那么$o = \mathbf{w}_2^\mathrm{T}\mathbf{h} + b_2$就是一个标量</p><p>为什么需要非线性激活函数？</p><p>我们假设$\sigma(x) = x$，那么该激活函数是一个线性激活函数，那么有：</p><script type="math/tex; mode=display">\begin{aligned}o&= \mathbf{w}_2^\mathrm{T}\mathbf{h} + b_2 \\&= \mathbf{w}_2^\mathrm{T}\sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) + b_2 \\&= \mathbf{w}_2^\mathrm{T}(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) + b_2 \\&= \mathbf{w}_2^\mathrm{T}\mathbf{W}_1\mathbf{x} + \mathbf{w}_2^\mathrm{T}\mathbf{b}_1 + b_2 \\&= \mathbf{w}_2^\mathrm{T}\mathbf{W}_1\mathbf{x} + b^\prime \\&= \mathbf{w}^\prime\mathbf{x} + b^\prime\end{aligned}</script><p>那么这和单层感知机本质上是一样的。</p></li></ul></li></ul><h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><h5 id="Sigmoid激活函数"><a href="#Sigmoid激活函数" class="headerlink" title="Sigmoid激活函数"></a>Sigmoid激活函数</h5><p>将输入投影到$(0, 1)$，是$\sigma(x) = \begin{cases}\begin{array}{ll}1 &amp; \text{if } x &gt; 0 \\ 0 &amp; \text{otherwise}\end{array}\end{cases}$的软版本，其表达式为：</p><script type="math/tex; mode=display">\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}</script><p>图像为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010222753054.png" alt="image-20231010222753054"></p><h5 id="Tanh激活函数"><a href="#Tanh激活函数" class="headerlink" title="Tanh激活函数"></a>Tanh激活函数</h5><p>将输入投影到$(-1, 1)$，其表达式为：</p><script type="math/tex; mode=display">\tanh(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}</script><p>图像为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010223140472.png" alt="image-20231010223140472"></p><h5 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h5><p>ReLU：rectified linear unit，其表达式为：</p><script type="math/tex; mode=display">\text{ReLU}(x) = \max(x, 0)</script><p>图像为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010223347608.png" alt="image-20231010223347608"></p><h4 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h4><script type="math/tex; mode=display">y_1, y_2, \cdots, y_k = \text{softmax}(o_1, o_2, \cdots, o_k)</script><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010220159384.png" alt="image-20231010220159384"></p><ul><li><p>输入$\mathbf{x} \in \mathbb{R}^n$（$\mathbf{x} \in \mathbb{R}^{n \times 1}$）</p></li><li><p>隐藏层$\mathbf{W}_1 \in \mathbb{R}^{m \times n}, \mathbf{b}_1 \in \mathbb{R}^m$（$\mathbf{b}_1 \in \mathbb{R}^{m \times 1}$）</p></li><li><p>输出层$\mathbf{W}_2 \in \mathbb{R}^{m \times k}, \mathbf{b}_2 \in \mathbb{R}^k$（$\mathbf{b}_2 \in \mathbb{R}^{k \times 1}$）</p><script type="math/tex; mode=display">  \begin{aligned}  \mathbf{h} &= \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \\  \mathbf{o} &= \mathbf{W}_2^\mathrm{T}\mathbf{h} + \mathbf{b}_2 \\  \mathbf{y} &= \text{softmax}(\mathbf{o})  \end{aligned}</script><p>  继续计算一下形状：</p><ul><li>$\mathbf{W}_1\mathbf{x} \in \mathbb{R}^{m \times 1}$，那么$\mathbf{W}_1\mathbf{x} + \mathbf{b}_1 \in \mathbb{R}^{m \times 1}$，那么$\mathbf{h} \in \mathbb{R}^{m \times 1}$</li><li>$\mathbf{W}_2^\mathrm{T}\mathbf{h} \in \mathbb{R}^{k \times 1}$，那么$\mathbf{W}_2^\mathrm{T}\mathbf{h} + \mathbf{b}_2 \in \mathbb{R}^{k \times 1}$，那么$\mathbf{o} \in \mathbb{R}^{k \times 1}$</li><li>那么$\mathbf{y} = \text{softmax}(\mathbf{o}) \in \mathbb{R}^{k \times 1}$</li></ul></li></ul><h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/10/image-20231010233541962.png" alt="image-20231010233541962"></p><script type="math/tex; mode=display">\begin{aligned}\mathbf{h}_1 &= \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1) \\\mathbf{h}_2 &= \sigma(\mathbf{W}_2\mathbf{h}_1 + \mathbf{b}_2) \\\mathbf{h}_3 &= \sigma(\mathbf{W}_3\mathbf{h}_2 + \mathbf{b}_3) \\\mathbf{o} &= \mathbf{W}_4\mathbf{h}_3 + \mathbf{b}_4\end{aligned}</script><p>超参数：</p><ul><li>隐藏层数</li><li>每层隐藏层的大小</li></ul><p>如果是单隐藏层，那么该隐藏层的大小可以稍微大些；如果是多隐藏层，那么由输入到输出，隐藏层的大小应逐渐减小。</p><p>对于多隐藏层中隐藏层大小的变化有一个直观的理解：机器学习本质上就是做的压缩，将很复杂的输入压缩到很简单的输出上，那么最好是慢慢的进行压缩，也就对应着隐藏层大小的逐渐减小。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>多层感知机使用隐藏层和激活函数来得到非线性模型</li><li>常用激活函数是Sigmoid，Tanh，ReLU</li><li>使用Softmax来处理多类分类</li><li>超参数为隐藏层数和各个隐藏层的大小</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="多层感知机的从零开始实现"><a href="#多层感知机的从零开始实现" class="headerlink" title="多层感知机的从零开始实现"></a>多层感知机的从零开始实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line">W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure><p>实现ReLU激活函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br></pre></td></tr></table></figure><p>实现我们的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_inputs)</span><br><span class="line">    H = relu(X @ W1 + b1)</span><br><span class="line">    <span class="keyword">return</span> (H @ W2 + b2)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><p>多层感知机的训练过程与softmax回归的训练过程完全相同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">30</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure><p>训练输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/output1.svg" alt="output1"></p><p>我这里训练了30个epoch，必要时可以再多训练若干个epoch。</p><h3 id="多层感知机的简洁实现"><a href="#多层感知机的简洁实现" class="headerlink" title="多层感知机的简洁实现"></a>多层感知机的简洁实现</h3><p>通过高级API更简洁地实现多层感知机：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br></pre></td></tr></table></figure><p>隐藏层包含256个隐藏单元，并使用了ReLU激活函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (1): Linear(in_features=784, out_features=256, bias=True)</span><br><span class="line">  (2): ReLU()</span><br><span class="line">  (3): Linear(in_features=256, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/10/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 10 多层感知机 + 代码实现。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 09 Softmax回归 + 损失函数 + 图片分类数据集</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/09/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/09/</id>
    <published>2023-10-09T08:00:00.000Z</published>
    <updated>2023-10-10T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="09-Softmax回归-损失函数-图片分类数据集"><a href="#09-Softmax回归-损失函数-图片分类数据集" class="headerlink" title="09 Softmax回归 + 损失函数 + 图片分类数据集"></a>09 Softmax回归 + 损失函数 + 图片分类数据集</h1><h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><h3 id="回归vs分类"><a href="#回归vs分类" class="headerlink" title="回归vs分类"></a>回归vs分类</h3><ul><li>回归估计一个连续值</li><li>分类预测一个离散类别</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009162556283.png" alt="image-20231009162556283"></p><h3 id="Kaggle上的分类问题"><a href="#Kaggle上的分类问题" class="headerlink" title="Kaggle上的分类问题"></a>Kaggle上的分类问题</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009162632622.png" alt="image-20231009162632622"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009162654345.png" alt="image-20231009162654345"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009162708433.png" alt="image-20231009162708433"></p><h3 id="从回归到多类分类"><a href="#从回归到多类分类" class="headerlink" title="从回归到多类分类"></a>从回归到多类分类</h3><ul><li><p>回归</p><ul><li>单连续数值输出</li><li>自然区间$\mathbb{R}$</li><li><p>跟真实值的区别作为损失</p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009162943805.png" alt="image-20231009162943805"></p></li></ul></li><li><p>分类</p><ul><li>通常多个输出</li><li><p>输出i是预测为第i类的置信度</p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009163000560.png" alt="image-20231009163000560"></p></li></ul></li></ul><h3 id="从回归到多类分类-1"><a href="#从回归到多类分类-1" class="headerlink" title="从回归到多类分类"></a>从回归到多类分类</h3><h4 id="均方损失"><a href="#均方损失" class="headerlink" title="均方损失"></a>均方损失</h4><ul><li><p>对类别进行一位有效编码（单热点码）</p><script type="math/tex; mode=display">  \begin{aligned}  \mathbf{y}  &= \begin{bmatrix}  y_1, & y_2, & \cdots, & y_n  \end{bmatrix}^\mathrm{T} \\  y_i  &= \begin{cases}  1 \text{ if } i = y \\  0 \text{ otherwise}  \end{cases}  \end{aligned}</script></li><li><p>使用均方损失训练</p></li></ul><h4 id="无校验比例"><a href="#无校验比例" class="headerlink" title="无校验比例"></a>无校验比例</h4><ul><li><p>最大值作为预测</p><script type="math/tex; mode=display">  \hat{y} = \arg \max_{i}o_i</script></li><li><p>需要更置信的识别正确类（大余量），就是预测出来这一类的值要和其他类的值的相差大于一个阈值</p><script type="math/tex; mode=display">  o_y - o_i \ge \Delta(y, i)</script><p>  能够保证将目标类和其他类拉开距离</p></li></ul><h4 id="校验比例"><a href="#校验比例" class="headerlink" title="校验比例"></a>校验比例</h4><ul><li><p>输出匹配概率（非负，和为1）</p><script type="math/tex; mode=display">  \begin{aligned}  \hat{\mathbf{y}} &= \text{softmax}(\mathbf{o}) \\  \hat{y}_i &= \frac{\exp(o_i)}{\sum_k\exp(o_k)}  \end{aligned}</script><p>  注意，这里$\mathbf{o} = \begin{bmatrix}o_1, &amp; o_2, &amp; \cdots, &amp; o_n\end{bmatrix}$。</p></li><li><p>概率$\mathbf{y}$和$\hat{\mathbf{y}}$的区别作为损失</p></li></ul><h3 id="Softmax和交叉熵损失"><a href="#Softmax和交叉熵损失" class="headerlink" title="Softmax和交叉熵损失"></a>Softmax和交叉熵损失</h3><ul><li><p>交叉熵常用来衡量两个概率的区别$H(\mathbf{p}, \mathbf{q}) = \sum_i \left(- p_i\log(q_i)\right)$</p></li><li><p>将它作为损失</p><script type="math/tex; mode=display">  l(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_iy_i\log\hat{y}_i = -\log\hat{y}_y</script><p>  这里求和的解释如下：</p><script type="math/tex; mode=display">  \begin{aligned}  l(\mathbf{y}, \hat{\mathbf{y}})  &= -\sum_iy_i\log\hat{y}_i \\  &= -\left(y_1\log\hat{y_1} + y_2\log\hat{y_2} + \cdots + y_i\log\hat{y_i} + \cdots + y_n\log\hat{y_n}\right)  \end{aligned}</script><p>  由于在向量$\mathbf{y} = \begin{bmatrix}y_1, &amp; y_2, &amp; \cdots, &amp; y_n\end{bmatrix}^\mathrm{T}$中，只有一项为1，那么上式可简化为：$l(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_iy_i\log\hat{y}_i = -\log\hat{y}_y$，表示只对真实类别$y$上预测出来的$\hat{y}$求$-\log$，其余位置上的$\hat{y}$都忽略。</p><p>  并且对分类问题来说，不关心对于非预测类的预测值，只关心对于正确类的预测值。</p></li></ul><h4 id="求梯度-frac-partial-l-mathbf-y-hat-mathbf-y-partial-o-i"><a href="#求梯度-frac-partial-l-mathbf-y-hat-mathbf-y-partial-o-i" class="headerlink" title="求梯度$\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i}$"></a>求梯度$\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i}$</h4><p>其梯度是真实概率和预测概率的区别：</p><script type="math/tex; mode=display">\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i} = \text{softmax}(\mathbf{o})_i - y_i</script><p>注意，$\text{softmax}(\mathbf{o})_i = \hat{y}_i = \frac{\exp(o_i)}{\sum_k\exp(o_k)}$。</p><p>这里计算过程如下：</p><script type="math/tex; mode=display">\begin{aligned}l(\mathbf{y}, \hat{\mathbf{y}})&= -\sum_iy_i\log\hat{y}_i \\&= -\sum_iy_i\log\frac{\exp(o_i)}{\sum_k\exp(o_k)} \\&= -\sum_iy_i\log\frac{e^{o_i}}{\sum_ke^{o_k}} \\&= -\sum_iy_i\left(\log e^{o_i} - \log\left(\sum_ke^{o_k}\right)\right) \\&= -\sum_iy_i\left(o_i - \log\left(\sum_ke^{o_k}\right)\right) \\&= -\sum_iy_io_i + \sum_iy_i\left(\log\left(\sum_ke^{o_k}\right)\right)\end{aligned}</script><p>那么$\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i}$等价于：</p><script type="math/tex; mode=display">\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i} = -\frac{\partial(\sum_iy_io_i)}{\partial o_i} + \frac{\partial\left(\sum_iy_i\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i}</script><p>接下来分别求$\frac{\partial(\sum_iy_io_i)}{\partial o_i}$和$\frac{\partial\left(\sum_iy_i\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i}$。</p><h5 id="求-frac-partial-sum-iy-io-i-partial-o-i"><a href="#求-frac-partial-sum-iy-io-i-partial-o-i" class="headerlink" title="求$\frac{\partial(\sum_iy_io_i)}{\partial o_i}$"></a>求$\frac{\partial(\sum_iy_io_i)}{\partial o_i}$</h5><p>$\frac{\partial(\sum_iy_io_i)}{\partial o_i}$的计算过程如下：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial(\sum_iy_io_i)}{\partial o_i}&= \frac{\partial(y_1o_1 + y_2o_2 + \cdots + y_io_i + \cdots + y_no_n)}{\partial o_i} \\&= y_i\end{aligned}</script><p>至此，$\frac{\partial(\sum_iy_io_i)}{\partial o_i}$计算完毕。</p><h5 id="求-frac-partial-left-sum-iy-i-log-left-sum-ke-o-k-right-right-partial-o-i"><a href="#求-frac-partial-left-sum-iy-i-log-left-sum-ke-o-k-right-right-partial-o-i" class="headerlink" title="求$\frac{\partial\left(\sum_iy_i\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i}$"></a>求$\frac{\partial\left(\sum_iy_i\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i}$</h5><p>$\frac{\partial\left(\sum_iy_i\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i}$的计算过程如下：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial\left(\sum_iy_i\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i}&= \frac{\partial\left(y_1\log\left(\sum_ke^{o_k}\right) + y_2\log\left(\sum_ke^{o_k}\right) + \cdots + y_n\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i} \\&= y_1\frac{e^{o_i}}{\sum_ke^{o_k}} + y_2\frac{e^{o_i}}{\sum_ke^{o_k}} + \cdots + y_n\frac{e^{o_i}}{\sum_ke^{o_k}} \\&= \frac{e^{o_i}}{\sum_ke^{o_k}}(y_1 + y_2 + \cdots + y_n) \\&= \frac{e^{o_i}}{\sum_ke^{o_k}} \\&= \frac{\exp(o_i)}{\sum_k\exp(o_k)} \\&= \text{softmax}(\mathbf{o})_i \\&= \hat{y}_i\end{aligned}</script><p>至此，$\frac{\partial\left(\sum_iy_i\log\left(\sum_ke^{o_k}\right)\right)}{\partial o_i}$计算完毕。</p><p>那么将上面的两个结果代入$\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i}$中：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i}&= -\sum_iy_io_i + \sum_iy_i\left(\log\left(\sum_ke^{o_k}\right)\right) \\&= -y_i + \text{softmax}(\mathbf{o})_i \\&= \text{softmax}(\mathbf{o})_i - y_i\end{aligned}</script><p>得出结论，计算完毕。</p><p>那么可以知道：$\frac{\partial l(\mathbf{y}, \hat{\mathbf{y}})}{\partial o_i} = \text{softmax}(\mathbf{o})_i - y_i = \hat{y}_i - y_i$。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>Softmax回归是一个多类分类模型</li><li>使用Softmax操作子得到每个类的预测置信度</li><li>使用交叉熵来衡量预测与标号的区别</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009193318213.png" alt="image-20231009193318213"></p><h3 id="L2-Loss"><a href="#L2-Loss" class="headerlink" title="L2 Loss"></a>L2 Loss</h3><script type="math/tex; mode=display">l(y, y^\prime) = \frac{1}{2}(y - y^\prime)^2</script><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009193455804.png" alt="image-20231009193455804"></p><ul><li>蓝色曲线是当$y = 0$且$y^\prime$变化时$l(y, y^\prime)$的曲线</li><li>绿色曲线是$l(y, y^\prime)$的似然函数</li><li>橙色曲线是$l(y, y^\prime)$的梯度</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009193853619.png" alt="image-20231009193853619"></p><p>上图红色箭头的方向是参数更新的方向，长度是参数更新幅度的相对大小。</p><p>当所需要的梯度要更小时，可以选用L1 Loss。</p><h3 id="L1-Loss"><a href="#L1-Loss" class="headerlink" title="L1 Loss"></a>L1 Loss</h3><script type="math/tex; mode=display">l(y, y^\prime) = \left|y - y^\prime\right|</script><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009194036403.png" alt="image-20231009194036403"></p><ul><li>蓝色曲线是当$y = 0$且$y^\prime$变化时$l(y, y^\prime)$的曲线</li><li>绿色曲线是$l(y, y^\prime)$的似然函数</li><li>橙色曲线是$l(y, y^\prime)$的梯度</li></ul><p>可以看到，梯度的大小要么是-1，要么是1，这会带来很多稳定性上的好处；但是在零点处是不平滑的。</p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009194452791.png" alt="image-20231009194452791"></p><p>上图红色箭头的方向是参数更新的方向，长度是参数更新幅度的相对大小。</p><p>可以看到，更新幅度的相对大小都是相等的。</p><h3 id="Huber’s-Robust-Loss"><a href="#Huber’s-Robust-Loss" class="headerlink" title="Huber’s Robust Loss"></a>Huber’s Robust Loss</h3><script type="math/tex; mode=display">l(y, y^\prime)= \begin{cases}\begin{array}{ll}\left|y - y^\prime\right| - \frac{1}{2} & \text{if }\left|y - y^\prime\right| > 1 \\\frac{1}{2}(y - y^\prime)^2 & \text{otherwise}\end{array}\end{cases}</script><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009194743087.png" alt="image-20231009194743087"></p><ul><li>蓝色曲线是当$y = 0$且$y^\prime$变化时$l(y, y^\prime)$的曲线</li><li>绿色曲线是$l(y, y^\prime)$的似然函数</li><li>橙色曲线是$l(y, y^\prime)$的梯度</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/09/image-20231009194806434.png" alt="image-20231009194806434"></p><p>上图红色箭头的方向是参数更新的方向，长度是参数更新幅度的相对大小。</p><p>这样就能够结合两个损失函数的优点。</p><p>上述的三个损失函数向我们展示了分析损失函数的方法。</p><h2 id="图片分类数据集"><a href="#图片分类数据集" class="headerlink" title="图片分类数据集"></a>图片分类数据集</h2><p>MNIST数据集是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。我们将使用类似但更复杂的Fashion-MNIST数据集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()</span><br></pre></td></tr></table></figure><p>通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过ToTensor实例将图像数据从PIL类型变换为32位浮点数格式</span></span><br><span class="line"><span class="comment"># 并除以255使得所有像素的数值均在0到1之间</span></span><br><span class="line">trans = transforms.ToTensor()</span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;./data&quot;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(mnist_train), <span class="built_in">len</span>(mnist_test)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(60000, 10000)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([1, 28, 28])</span><br></pre></td></tr></table></figure><p>这里是黑白图，所以第一个通道的维度为1。</p><p>两个可视化数据集的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签。&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [</span><br><span class="line">        <span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;coat&#x27;</span>, <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>, <span class="string">&#x27;sneaker&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span></span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shwo_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制图像列表。&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)</span><br><span class="line">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()</span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment"># 图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><p>几个样本的图像及其对应的标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train, batch_size=<span class="number">18</span>)))</span><br><span class="line">shwo_images(X.reshape(<span class="number">18</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="number">2</span>, <span class="number">9</span>, titles=get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">array([&lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;ankle boot&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;t-shirt&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;t-shirt&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;dress&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;t-shirt&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;pullover&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;sneaker&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;pullover&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;sandal&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;sandal&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;t-shirt&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;ankle boot&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;sandal&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;sandal&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;sneaker&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;ankle boot&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;trouser&#x27;&#125;&gt;,</span><br><span class="line">       &lt;Axes: title=&#123;&#x27;center&#x27;: &#x27;t-shirt&#x27;&#125;&gt;], dtype=object)</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/09/dataset-sample.svg" alt="dataset-sample"></p><p>读取一小批量数据，大小为<code>batch_size</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=get_dataloader_workers())</span><br><span class="line"></span><br><span class="line">timer = d2l.Timer()</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;9.41 sec&#x27;</span><br></pre></td></tr></table></figure><p>定义<code>load_data_fashion_mnist</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br></pre></td></tr></table></figure><h2 id="Softmax回归从零开始实现"><a href="#Softmax回归从零开始实现" class="headerlink" title="Softmax回归从零开始实现"></a>Softmax回归从零开始实现</h2><p>就像我们从零开始实现线性回归一样，你应该知道实现softmax的细节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>这里由于我不想把数据集存储在上级目录下，所以就直接把上一个视频的<code>load_data_fashion_mnist</code>函数抄过来了，这个函数中，我将数据存放在了<code>./data</code>目录下。</p><p>将展平每个图像，将他们视为长度为784的向量。因为我们的数据集有10个类别，所以网络输出维度为10：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>给定一个矩阵<code>X</code>，我们可以对所有元素求和：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]]) <span class="comment"># X的形状为(2, 3)</span></span><br><span class="line">X.<span class="built_in">sum</span>(<span class="number">0</span>, keepdim=<span class="literal">True</span>), X.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 形状分别为(1, 3) (2, 1)</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[5., 7., 9.]]),</span><br><span class="line"> tensor([[ 6.],</span><br><span class="line">         [15.]]))</span><br></pre></td></tr></table></figure><p>实现softmax：</p><script type="math/tex; mode=display">\text{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k\exp(\mathbf{X}_{ik})}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition <span class="comment"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure><p>我们将每个元素变成一个非负数。此外，依据概率原理，每行总和为1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">X_prob = softmax(X)</span><br><span class="line">X_prob, X_prob.<span class="built_in">sum</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0.2243, 0.1884, 0.0316, 0.5224, 0.0333],</span><br><span class="line">         [0.1058, 0.5632, 0.0817, 0.1372, 0.1121]]),</span><br><span class="line"> tensor([1.0000, 1.0000]))</span><br></pre></td></tr></table></figure><p>实现softmax回归模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape(-<span class="number">1</span>, W.shape[<span class="number">0</span>]), W) + b)</span><br></pre></td></tr></table></figure><p>创建一个数据<code>y_hat</code>，其中包含2个样本在3个类别的预测概率，使用<code>y</code>作为<code>y_hat</code>中概率的索引：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tensor([<span class="number">0</span>, <span class="number">2</span>]) <span class="comment"># 表示2个真实的标号</span></span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]]) <span class="comment"># 预测值，2个样本在3类上的预测值</span></span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y] <span class="comment"># 第0个样本的第0个预测值 和 第1个样本的第2个预测值</span></span><br></pre></td></tr></table></figure><p>实现交叉熵损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="comment"># 在y_hat中，拿出真实标号代表的值</span></span><br><span class="line">    <span class="comment"># y_hat[range(len(y_hat)), y]表示在y_hat的所有行中，</span></span><br><span class="line">    <span class="comment"># 以y为索引，在y_hat（形状为(batch_size, num_outputs)）的第i行中</span></span><br><span class="line">    <span class="comment"># 拿出第y[i]（形状为(num_outputs, 1)）个值</span></span><br><span class="line">    <span class="comment"># 最后再求-log值</span></span><br><span class="line">    <span class="keyword">return</span> -torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line">cross_entropy(y_hat, y)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([2.3026, 0.6931])</span><br></pre></td></tr></table></figure><p>将预测类别与真实<code>y</code>元素进行比较：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># len(y_hat.shape) &gt; 1判断是否为二维</span></span><br><span class="line">        <span class="comment"># y_hat.shape[1] &gt; 1判断列数</span></span><br><span class="line">        <span class="comment"># 这里如果y_hat不止一行，argmax(axis=1)会自动扩展为列向量</span></span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)</span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y <span class="comment"># 进行比较</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>()) <span class="comment"># 返回相同的个数</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">关于这里为什么不在accuracy函数中就除len(y)</span></span><br><span class="line"><span class="string">因为取到最后可能取出数据的数量不足一个batch_size</span></span><br><span class="line"><span class="string">这样会使得每个数据的权重不同</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">accuracy(y_hat, y) / <span class="built_in">len</span>(y) <span class="comment"># 再除y的个数，就是预测正确的概率</span></span><br></pre></td></tr></table></figure><p>我们可以评估在任意模型<code>net</code>的准确率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>() <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">        <span class="comment"># 不设置也没关系，但是需要一个好习惯</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>) <span class="comment"># 正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><code>Accumulator</code>实例中创建了2个变量，用于分别存储正确预测的数量和预测的总数量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;在n个变量上累加。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        self.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(self.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        self.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[idx]</span><br><span class="line"></span><br><span class="line">evaluate_accuracy(net, test_iter)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.1925</span><br></pre></td></tr></table></figure><p>Softmax回归的训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.train()</span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            updater.step()</span><br><span class="line">            metric.add(</span><br><span class="line">                <span class="built_in">float</span>(l) * <span class="built_in">len</span>(y), accuracy(y_hat, y),</span><br><span class="line">                y.size.numel())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])</span><br><span class="line">            metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br></pre></td></tr></table></figure><p>定义一个在动画中绘制数据的实用程序类：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g--&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="comment"># 增量地绘制多条线</span></span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            self.axes = [self.axes, ]</span><br><span class="line">        <span class="comment"># 使用lambda函数捕获参数</span></span><br><span class="line">        self.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(</span><br><span class="line">            self.axes[<span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        self.X, self.Y, self.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="comment"># 向图表中添加多个数据点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.X:</span><br><span class="line">            self.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.Y:</span><br><span class="line">            self.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.X[i].append(a)</span><br><span class="line">                self.Y[i].append(b)</span><br><span class="line">        self.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(self.X, self.Y, self.fmts):</span><br><span class="line">            self.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        self.config_axes()</span><br><span class="line">        display.display(self.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>训练函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型。&quot;&quot;&quot;</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br></pre></td></tr></table></figure><p>小批量随机梯度下降来优化模型的损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br></pre></td></tr></table></figure><p>训练模型10个迭代周期：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/output1.svg" alt="output1"></p><p>对图像进行分类预测：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [<span class="string">&#x27;true:&#x27;</span> + true + <span class="string">&#x27;\n&#x27;</span> + <span class="string">&#x27;pred:&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(X[<span class="number">0</span>:n].reshape(n, <span class="number">28</span>, <span class="number">28</span>), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><p>输出为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/predict.svg" alt="predict"></p><h2 id="Softmax回归简洁实现"><a href="#Softmax回归简洁实现" class="headerlink" title="Softmax回归简洁实现"></a>Softmax回归简洁实现</h2><p>通过深度学习框架的高级API能够使实现softmax回归变得更加容易。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用四个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize:</span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=trans, download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>这里由于我不想把数据集存储在上级目录下，所以就直接把上一个视频的<code>load_data_fashion_mnist</code>函数抄过来了，这个函数中，我将数据存放在了<code>./data</code>目录下。</p><p>Softmax回归是一个全连接层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PyTorch不会隐式地调整输入的形状</span></span><br><span class="line"><span class="comment"># 因此，我们定义了展平层（flatten）在线性层前调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sequential(</span><br><span class="line">  (0): Flatten(start_dim=1, end_dim=-1)</span><br><span class="line">  (1): Linear(in_features=784, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><code>nn.Flatten()</code>可以将任何维度的Tensor变为一个2D的Tensor，其中第0维保留，其余维度全部展成一个向量。</p><p>那么在展平后，原本的输入形状是<code>(batch_size, 1, 28, 28)</code>，在经过<code>nn.Flatten()</code>后，就变为了<code>(batch_size, 1 * 28 * 28) = (batch_size, 784)</code>。</p><p>在交叉熵损失函数中传递未归一化的预测，并同时计算softmax及其对数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><p>注意，这里和B站视频上的代码不一样。</p><p>使用学习率为0.1的小批量随机梯度下降作为优化算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>调用之前定义的训练函数来训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p>如果报错说没有<code>train_ch3</code>这个模块，可以运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install d2l==0.17.6</span><br></pre></td></tr></table></figure><p>但是注意需要自己解决环境的依赖冲突问题。</p><p>最后的曲线为：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/09/output2.svg" alt="output2"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 09 Softmax回归 + 损失函数 + 图片分类数据集。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 08 线性回归 + 基础优化算法</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/08/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/08/</id>
    <published>2023-10-07T05:00:00.000Z</published>
    <updated>2023-10-08T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="08-线性回归-基础优化算法"><a href="#08-线性回归-基础优化算法" class="headerlink" title="08 线性回归 + 基础优化算法"></a>08 线性回归 + 基础优化算法</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="如何在美国买房"><a href="#如何在美国买房" class="headerlink" title="如何在美国买房"></a>如何在美国买房</h3><ul><li>看中一个房，参观了解</li><li>估计一个价格，出价</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231007115904125.png" alt="image-20231007115904125"></p><h3 id="房价预测"><a href="#房价预测" class="headerlink" title="房价预测"></a>房价预测</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231007120104651.png" alt="image-20231007120104651"></p><h3 id="一个简化模型"><a href="#一个简化模型" class="headerlink" title="一个简化模型"></a>一个简化模型</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231007120125504.png" alt="image-20231007120125504"></p><ul><li><p>假设1：影响房价的关键因素是卧室个数，卫生间个数和居住面积，记为$x_1, x_2, x_3$</p></li><li><p>假设2：成交价是关键因素的加权和</p><script type="math/tex; mode=display">  y = w_1x_1 + w_2x_2 + w_3x_3 + b</script><p>  权重和偏差的实际值在后面决定</p></li></ul><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><ul><li><p>给定n维输入$\mathbf{x} = \begin{bmatrix}x_1, &amp; x_2, &amp; \cdots, &amp; x_n\end{bmatrix}^\mathrm{T}$</p></li><li><p>线性模型有一个n维权重和一个标量偏差</p><script type="math/tex; mode=display">  \mathbf{w} = \begin{bmatrix}w_1, & w_2, & \cdots, & w_n\end{bmatrix}^\mathrm{T}, \quad b</script></li><li><p>输出是输入的加权和</p><script type="math/tex; mode=display">  y = w_1x_1 + w_2x_2 + \cdots + w_nx_n + b</script><p>  向量版本：$y = \left\langle\mathbf{w}, \mathbf{x}\right\rangle + b$</p></li></ul><h3 id="线性模型可以看作是单层神经网络"><a href="#线性模型可以看作是单层神经网络" class="headerlink" title="线性模型可以看作是单层神经网络"></a>线性模型可以看作是单层神经网络</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231007120922080.png" alt="image-20231007120922080"></p><h3 id="神经网络源于神经科学"><a href="#神经网络源于神经科学" class="headerlink" title="神经网络源于神经科学"></a>神经网络源于神经科学</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231007121212904.png" alt="image-20231007121212904"></p><h3 id="衡量预估质量"><a href="#衡量预估质量" class="headerlink" title="衡量预估质量"></a>衡量预估质量</h3><ul><li><p>比较真实值和预估值，例如房屋售价和估价</p></li><li><p>假设$y$是真实值，$\hat{y}$是估计值，我们可以比较</p><script type="math/tex; mode=display">  \ell(y, \hat{y}) = \frac{1}{2}(y - \hat{y})^2</script><p>  这个叫做平方损失</p></li></ul><h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><ul><li><p>收集一些数据点来决定参数值（权重和偏差），例如过去6个月卖的房子</p></li><li><p>这背称之为训练数据</p></li><li><p>通常越多越好</p></li><li><p>假设我们有n个样本</p><script type="math/tex; mode=display">  \mathbf{X} = \begin{bmatrix}\mathbf{x}_1, & \mathbf{x}_2, & \cdots, & \mathbf{x}_n\end{bmatrix}^\mathrm{T} \quad \mathbf{y} = \begin{bmatrix}y_1, & y_2, & \cdots, & y_n\end{bmatrix}^\mathrm{T}</script></li></ul><h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><ul><li><p>训练损失</p><script type="math/tex; mode=display">  \begin{aligned}  \ell(\mathbf{X}, \mathbf{y}, \mathbf{w}, b)  &= \frac{1}{2n}\sum_{i = 1}^{n}(y_i - \left\langle\mathbf{x}_i, \mathbf{w}\right\rangle - b)^2 \\  &= \frac{1}{2n}\Vert\mathbf{y} - \mathbf{Xw} - b\Vert^2  \end{aligned}</script></li><li><p>最小化损失来学习参数</p><script type="math/tex; mode=display">  \mathbf{w^\ast}, \mathbf{b^\ast} = \arg \min_{\mathbf{w}, b}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}, b)</script></li></ul><h3 id="显式解"><a href="#显式解" class="headerlink" title="显式解"></a>显式解</h3><h4 id="损失函数导数-ell-mathbf-X-mathbf-y-mathbf-w-的求法"><a href="#损失函数导数-ell-mathbf-X-mathbf-y-mathbf-w-的求法" class="headerlink" title="损失函数导数$\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})$的求法"></a>损失函数导数$\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})$的求法</h4><p>首先将偏差加入权重$\mathbf{X} \gets \begin{bmatrix}\mathbf{X}, &amp; \mathbf{1}\end{bmatrix} \quad \mathbf{w} \gets \begin{bmatrix}\mathbf{w} \\ b\end{bmatrix}$。</p><p>那么损失函数$\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})$可以表示为：</p><script type="math/tex; mode=display">\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}) = \frac{1}{2n}\Vert\mathbf{y} - \mathbf{Xw}\Vert^2</script><p>对$\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})$关于$\mathrm{w}$求偏导：</p><script type="math/tex; mode=display">\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}) = -\frac{1}{n}(\mathbf{y} - \mathbf{Xw})^\mathrm{T}\mathbf{X}</script><h5 id="求法1"><a href="#求法1" class="headerlink" title="求法1"></a>求法1</h5><p>假设：</p><ul><li><p>$\mathbf{x}_i \in \mathbb{R}^{1 \times (m + 1)}$，即$\mathbf{x}_i = \begin{bmatrix}x_{i1}, &amp; x_{i2}, &amp; \cdots, &amp; x_{im}, &amp; 1\end{bmatrix}$即一个样本有$m$维特征，再在末尾加上偏置项的$1$维，共$m + 1$维，是一个行向量；</p></li><li><p>$\mathbf{X} \in \mathbb{R}^{n \times (m + 1)}$，即$\mathbf{X} = \begin{bmatrix}\mathbf{x}_1 \\ \mathbf{x}_2 \\ \vdots \\ \mathbf{x}_n\end{bmatrix}$，即有$n$个样本，是一个矩阵，即：</p><script type="math/tex; mode=display">  \mathbf{X}  =  \begin{bmatrix}  \mathbf{x}_1 \\  \mathbf{x}_2 \\  \vdots \\  \mathbf{x}_n  \end{bmatrix}  =  \begin{bmatrix}  x_{11}, & x_{12}, & \cdots, & x_{1m}, & 1 \\  x_{21}, & x_{22}, & \cdots, & x_{2m}, & 1 \\  \vdots & \vdots & & \vdots & \vdots \\  x_{n1}, & x_{n2}, & \cdots, & x_{nm}, & 1  \end{bmatrix}</script></li><li><p>$\mathbf{w} \in \mathbb{R}^{(m + 1) \times 1}$，即$\mathbf{w} = \begin{bmatrix}w_1, &amp; w_2, &amp; \cdots, &amp; w_m, &amp; b\end{bmatrix}^\mathrm{T}$，是一个列向量；</p></li><li><p>$\mathbf{y} \in \mathbb{R}^{n \times 1}$，即$\mathbf{y} = \begin{bmatrix}y_1, &amp; y_2, &amp; \cdots, &amp; y_n\end{bmatrix}^\mathrm{T}$，是一个列向量。</p></li></ul><p>那么$\mathbf{y} - \mathbf{Xw} \in \mathbb{R}^{n \times 1}$，且为一个列向量，并且这里要注意，$\Vert\mathbf{y} - \mathbf{Xw}\Vert^2$是一个标量。</p><p>在此基础上，我们逐层进行求导：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})&= \frac{\partial}{\partial\mathbf{w}}\frac{1}{2n}\Vert\mathbf{y} - \mathbf{Xw}\Vert^2 \\&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}\Vert\mathbf{y} - \mathbf{Xw}\Vert^2 \\&= \frac{1}{2n}\frac{\partial\Vert\mathbf{y} - \mathbf{Xw}\Vert^2}{\partial(\mathbf{y} - \mathbf{Xw})}\frac{\partial(\mathbf{y} - \mathbf{Xw})}{\partial\mathbf{w}}\end{aligned}</script><h6 id="求-frac-partial-Vert-mathbf-y-mathbf-Xw-Vert-2-partial-mathbf-y-mathbf-Xw"><a href="#求-frac-partial-Vert-mathbf-y-mathbf-Xw-Vert-2-partial-mathbf-y-mathbf-Xw" class="headerlink" title="求$\frac{\partial\Vert\mathbf{y} - \mathbf{Xw}\Vert^2}{\partial(\mathbf{y} - \mathbf{Xw})}$"></a>求$\frac{\partial\Vert\mathbf{y} - \mathbf{Xw}\Vert^2}{\partial(\mathbf{y} - \mathbf{Xw})}$</h6><p>对于$\frac{\partial\Vert\mathbf{y} - \mathbf{Xw}\Vert^2}{\partial(\mathbf{y} - \mathbf{Xw})}$的求法，我们按照分子布局展开：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial\Vert\mathbf{y} - \mathbf{Xw}\Vert^2}{\partial(\mathbf{y} - \mathbf{Xw})}&= \frac{\partial(\sum_{i = 1}^{n}(y_i - \left\langle\mathbf{x}_i, \mathbf{w}\right\rangle)^2)}{\partial(\mathbf{y} - \mathbf{Xw})} \\&= \begin{bmatrix}\frac{\partial(\sum_{i = 1}^{n}(y_i - \left\langle\mathbf{x}_i, \mathbf{w}\right\rangle)^2)}{\partial(y_1 - \left\langle\mathbf{x}_1, \mathbf{w}\right\rangle)}, & \frac{\partial(\sum_{i = 1}^{n}(y_i - \left\langle\mathbf{x}_i, \mathbf{w}\right\rangle)^2)}{\partial(y_2 - \left\langle\mathbf{x}_2, \mathbf{w}\right\rangle)}, & \cdots, & \frac{\partial(\sum_{i = 1}^{n}(y_i - \left\langle\mathbf{x}_i, \mathbf{w}\right\rangle)^2)}{\partial(y_n - \left\langle\mathbf{x}_n, \mathbf{w}\right\rangle)}\end{bmatrix} \\&= \begin{bmatrix}2(y_1 - \left\langle\mathbf{x}_1, \mathbf{w}\right\rangle), & 2(y_2 - \left\langle\mathbf{x}_2, \mathbf{w}\right\rangle), & \cdots, & 2(y_n - \left\langle\mathbf{x}_n, \mathbf{w}\right\rangle)\end{bmatrix} \\&= 2\begin{bmatrix}y_1 - \left\langle\mathbf{x}_1, \mathbf{w}\right\rangle, & y_2 - \left\langle\mathbf{x}_2, \mathbf{w}\right\rangle, & \cdots, & y_n - \left\langle\mathbf{x}_n, \mathbf{w}\right\rangle\end{bmatrix} \\&= 2(\mathbf{y} - \mathbf{Xw})^\mathrm{T}\end{aligned}</script><p>至此，$\frac{\partial\Vert\mathbf{y} - \mathbf{Xw}\Vert^2}{\partial(\mathbf{y} - \mathbf{Xw})}$计算完毕。</p><h6 id="求-frac-partial-mathbf-y-mathbf-Xw-partial-mathbf-w"><a href="#求-frac-partial-mathbf-y-mathbf-Xw-partial-mathbf-w" class="headerlink" title="求$\frac{\partial(\mathbf{y} - \mathbf{Xw})}{\partial\mathbf{w}}$"></a>求$\frac{\partial(\mathbf{y} - \mathbf{Xw})}{\partial\mathbf{w}}$</h6><p>对于$\frac{\partial(\mathbf{y} - \mathbf{Xw})}{\partial\mathbf{w}}$的求法，我们按照分子布局展开：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial(\mathbf{y} - \mathbf{Xw})}{\partial\mathbf{w}}&= \begin{bmatrix}\frac{\partial(y_1 - \left\langle\mathbf{x}_1, \mathbf{w}\right\rangle)}{\partial\mathbf{w}}, & \frac{\partial(y_2 - \left\langle\mathbf{x}_2, \mathbf{w}\right\rangle)}{\partial\mathbf{w}}, & \cdots, & \frac{\partial(y_n - \left\langle\mathbf{x}_n, \mathbf{w}\right\rangle)}{\partial\mathbf{w}}\end{bmatrix}^\mathrm{T} \\&= \begin{bmatrix}\frac{\partial(y_1 - \mathbf{x}_1^\mathrm{T}\mathbf{w})}{\partial\mathbf{w}}, & \frac{\partial(y_2 - \mathbf{x}_2^\mathrm{T}\mathbf{w})}{\partial\mathbf{w}}, & \cdots, & \frac{\partial(y_n - \mathbf{x}_n^\mathrm{T}\mathbf{w})}{\partial\mathbf{w}}\end{bmatrix}^\mathrm{T} \\&= \begin{bmatrix}-\mathbf{x}_1^\mathrm{T}, & -\mathbf{x}_2^\mathrm{T}, & \cdots, & -\mathbf{x}_n^\mathrm{T}\end{bmatrix}^\mathrm{T} \\&= \begin{bmatrix}\mathbf{x}_1^\mathrm{T}, & \mathbf{x}_2^\mathrm{T}, & \cdots, & \mathbf{x}_n^\mathrm{T}\end{bmatrix}^\mathrm{T} \\&= -\left(\mathbf{X}^\mathrm{T}\right)^\mathrm{T} \\&= -\mathbf{X}\end{aligned}</script><p>至此，$\frac{\partial(\mathbf{y} - \mathbf{Xw})}{\partial\mathbf{w}}$计算完毕。</p><p>将上述计算结果代入$\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})$中：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})&= \frac{1}{2n}\left(2\left(\mathbf{y} - \mathbf{Xw}\right)^\mathrm{T}\right)\left(-\mathbf{X}\right) \\&= -\frac{1}{n}(\mathbf{y} - \mathbf{Xw})^\mathrm{T}\mathbf{X}\end{aligned}</script><p>得出结论，计算完毕。</p><h5 id="求法2"><a href="#求法2" class="headerlink" title="求法2"></a>求法2</h5><p>关于$\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})$的求法2：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})&= \frac{\partial}{\partial\mathbf{w}}\frac{1}{2n}\Vert\mathbf{y} - \mathbf{Xw}\Vert^2 \\&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}\Vert\mathbf{y} - \mathbf{Xw}\Vert^2 \\&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}(\mathbf{y} - \mathbf{Xw})^\mathrm{T}(\mathbf{y} - \mathbf{Xw}) \\&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}(\mathbf{y}^\mathbf{T} - \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T})(\mathbf{y} - \mathbf{Xw}) \\&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}(\mathbf{y}^\mathbf{T}\mathbf{y} - \mathbf{y}^\mathbf{T}\mathbf{Xw} - \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{y} + \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})\end{aligned}</script><p>然后可以注意到，$\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{y} = (\mathbf{y}^\mathbf{T}\mathbf{Xw})^\mathbf{T}$，且$\mathbf{y}^\mathbf{T}$的维度为$1 \times n$，$\mathbf{X}$的维度为$n \times (m + 1)$，$\mathbf{w}$的维度为$(m + 1) \times 1$，故$\mathbf{y}^\mathbf{T}\mathbf{Xw}$的维度为$1 \times 1$，为一个标量，那么易知，$\mathbf{y}^\mathbf{T}\mathbf{Xw}$，$(\mathbf{y}^\mathbf{T}\mathbf{Xw})^\mathbf{T}$和$\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{y}$都为标量，且值相等，那么有$\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{y} = \mathbf{y}^\mathbf{T}\mathbf{Xw}$，那么可以将上式中的$- \mathbf{y}^\mathbf{T}\mathbf{Xw} - \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{y}$进行合并：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}(\mathbf{y}^\mathbf{T}\mathbf{y} - \mathbf{y}^\mathbf{T}\mathbf{Xw} - \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{y} + \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw}) \\&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}(\mathbf{y}^\mathbf{T}\mathbf{y} - 2\mathbf{y}^\mathbf{T}\mathbf{Xw} + \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw}) \\&= \frac{1}{2n}\frac{\partial}{\partial\mathbf{w}}(-2\mathbf{y}^\mathbf{T}\mathbf{Xw} + \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw}) \\&= \frac{1}{2n}\left(-2\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}} + \frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}\right)\end{aligned}</script><h6 id="求-frac-partial-mathbf-y-mathbf-T-mathbf-Xw-partial-mathbf-w"><a href="#求-frac-partial-mathbf-y-mathbf-T-mathbf-Xw-partial-mathbf-w" class="headerlink" title="求$\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$"></a>求$\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$</h6><p>对于$\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$的求法，我们首先明确各个向量的维度：</p><ul><li>$\mathbf{w}^\mathbf{T} \in \mathbb{R}^{1 \times (m + 1)}$</li><li>$\mathbf{X}^\mathbf{T} \in \mathbb{R}^{(m + 1) \times n}$</li><li>$\mathbf{x}^\mathbf{T} \in \mathbb{R}^{(m + 1) \times 1}$</li><li>$\mathbf{y}^\mathbf{T} \in \mathbb{R}^{1 \times n}$</li><li>$\mathbf{X} \in \mathbb{R}^{n \times (m + 1)}$</li><li>$\mathbf{w} \in \mathbb{R}^{(m + 1) \times 1}$</li><li>$\mathbf{x} \in \mathbb{R}^{1 \times (m + 1)}$</li><li>$\mathbf{y} \in \mathbb{R}^{n \times 1}$</li></ul><p>然后计算$\mathbf{y}^\mathbf{T}\mathbf{Xw}$：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{y}^\mathbf{T}\mathbf{Xw}&= \begin{bmatrix}y_1, & y_2, & \cdots, & y_n\end{bmatrix}\begin{bmatrix}x_{11}, & x_{12}, & \cdots, & x_{1m}, & 1 \\x_{21}, & x_{22}, & \cdots, & x_{2m}, & 1\\\vdots & \vdots & & \vdots & \vdots \\x_{n1}, & x_{n2}, & \cdots, & x_{nm}, & 1\end{bmatrix}\begin{bmatrix}w_1 \\w_2 \\\vdots \\w_m \\b\end{bmatrix} \\&= \begin{bmatrix}y_1, & y_2, & \cdots, & y_n\end{bmatrix}\begin{bmatrix}\sum_{i = 1}^{m + 1}w_ix_{1i} \\\sum_{i = 1}^{m + 1}w_ix_{2i} \\\vdots \\\sum_{i = 1}^{m + 1}w_ix_{ni}\end{bmatrix} \\&= y_1\sum_{i = 1}^{m + 1}w_ix_{1i} + y_2\sum_{i = 1}^{m + 1}w_ix_{2i} + \cdots + y_n\sum_{i = 1}^{m + 1}w_ix_{ni} \\&= \sum_{i = 1}^{n}y_i\sum_{j = 1}^{m + 1}w_jx_{ij}\end{aligned}</script><p>那么继续，按照分子布局展开：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}&= \frac{\partial(\sum_{i = 1}^{n}y_i\sum_{j = 1}^{m + 1}w_jx_{ij})}{\partial\mathbf{w}} \\&= \begin{bmatrix}\frac{\partial(\sum_{i = 1}^{n}y_i\sum_{j = 1}^{m + 1}w_jx_{ij})}{\partial w_1}, & \frac{\partial(\sum_{i = 1}^{n}y_i\sum_{j = 1}^{m + 1}w_jx_{ij})}{\partial w_2}, & \cdots, & \frac{\partial(\sum_{i = 1}^{n}y_i\sum_{j = 1}^{m + 1}w_jx_{ij})}{\partial w_{(m + 1)}}\end{bmatrix} \\&= \begin{bmatrix}y_1x_{11} + y_2x_{21} + \cdots + y_nx_{n1} \\y_1x_{12} + y_2x_{22} + \cdots + y_nx_{n2} \\\vdots \\y_1x_{1(m + 1)} + y_2x_{2(m + 1)} + \cdots + y_nx_{n(m + 1)}\end{bmatrix}^\mathbf{T} \\&= \begin{bmatrix}\sum_{i = 1}^ny_ix_{i1}, & \sum_{i = 1}^ny_iw_{i2}, & \cdots, & \sum_{i = 1}^ny_iw_{i(m + 1)}\end{bmatrix} \\&= \mathbf{y}^\mathbf{T}\mathbf{X}\end{aligned}</script><p>至此，$\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$计算完毕。</p><h6 id="求-frac-partial-mathbf-w-mathbf-T-mathbf-X-mathbf-T-mathbf-Xw-partial-mathbf-w"><a href="#求-frac-partial-mathbf-w-mathbf-T-mathbf-X-mathbf-T-mathbf-Xw-partial-mathbf-w" class="headerlink" title="求$\frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$"></a>求$\frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$</h6><p>对于$\frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$的求法，首先计算$\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw}$：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw}&= \begin{bmatrix}w_1 & w_2 & \cdots & w_m & b\end{bmatrix}\begin{bmatrix}x_{11}, & x_{21}, & \cdots, & x_{n1} \\x_{12}, & x_{22}, & \cdots, & x_{n2} \\\vdots & \vdots & & \vdots \\x_{1m}, & x_{2m}, & \cdots, & x_{nm} \\1, & 1, & \cdots, & 1\end{bmatrix}\begin{bmatrix}x_{11}, & x_{12}, & \cdots, & x_{1m}, & 1 \\x_{21}, & x_{22}, & \cdots, & x_{2m}, & 1\\\vdots & \vdots & & \vdots & \vdots \\x_{n1}, & x_{n2}, & \cdots, & x_{nm}, & 1\end{bmatrix}\begin{bmatrix}w_1 \\w_2 \\\vdots \\w_m \\b\end{bmatrix} \\&= \begin{bmatrix}w_1 & w_2 & \cdots & w_m & b\end{bmatrix}\begin{bmatrix}\sum_{i = 1}^nx_{i1}x_{i1}, & \sum_{i = 1}^nx_{i1}x_{i2}, & \cdots, & \sum_{i = 1}^nx_{i1}x_{im}, & \sum_{i = 1}^nx_{i1} \\\sum_{i = 1}^nx_{i2}x_{i1}, & \sum_{i = 1}^nx_{i2}x_{i2}, & \cdots, & \sum_{i = 1}^nx_{i2}x_{im}, & \sum_{i = 1}^nx_{i2} \\\vdots & \vdots & & \vdots & \vdots \\\sum_{i = 1}^nx_{im}x_{i1}, & \sum_{i = 1}^nx_{im}x_{i2}, & \cdots, & \sum_{i = 1}^nx_{im}x_{im}, & \sum_{i = 1}^nx_{im} \\\sum_{i = 1}^nx_{i1}, & \sum_{i = 1}^nx_{i2}, & \cdots, & \sum_{i = 1}^nx_{im}, & n\end{bmatrix}\begin{bmatrix}w_1 \\w_2 \\\vdots \\w_m \\b\end{bmatrix}\end{aligned}</script><p>易知，中间的$(m + 1) \times (m + 1)$矩阵$\mathbf{X}^\mathbf{T}\mathbf{X}$中不含$\mathbf{w}$，且是一个对称矩阵，故令：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{X}^\mathbf{T}\mathbf{X}&= \begin{bmatrix}\sum_{i = 1}^nx_{i1}x_{i1}, & \sum_{i = 1}^nx_{i1}x_{i2}, & \cdots, & \sum_{i = 1}^nx_{i1}x_{im}, & \sum_{i = 1}^nx_{i1} \\\sum_{i = 1}^nx_{i2}x_{i1}, & \sum_{i = 1}^nx_{i2}x_{i2}, & \cdots, & \sum_{i = 1}^nx_{i2}x_{im}, & \sum_{i = 1}^nx_{i2} \\\vdots & \vdots & & \vdots & \vdots \\\sum_{i = 1}^nx_{im}x_{i1}, & \sum_{i = 1}^nx_{im}x_{i2}, & \cdots, & \sum_{i = 1}^nx_{im}x_{im}, & \sum_{i = 1}^nx_{im} \\\sum_{i = 1}^nx_{i1}, & \sum_{i = 1}^nx_{i2}, & \cdots, & \sum_{i = 1}^nx_{im}, & n\end{bmatrix} \\&= \begin{bmatrix}a_{11}, & a_{12}, & \cdots, & a_{1(m + 1)} \\a_{21}, & a_{22}, & \cdots, & a_{2(m + 1)} \\\vdots & \vdots & & \vdots \\a_{(m + 1)1}, & a_{(m + 1)2}, & \cdots, & a_{(m + 1)(m + 1)}\end{bmatrix} \\&= \mathbf{A}\end{aligned}</script><p>那么$\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw}$可表示为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw}&= \mathbf{w}^\mathbf{T}\mathbf{Aw} \\&= \begin{bmatrix}w_1, & w_2, & \cdots, & w_m, & b\end{bmatrix}\begin{bmatrix}a_{11}, & a_{12}, & \cdots, & a_{1(m + 1)} \\a_{21}, & a_{22}, & \cdots, & a_{2(m + 1)} \\\vdots & \vdots & & \vdots \\a_{(m + 1)1}, & a_{(m + 1)2}, & \cdots, & a_{(m + 1)(m + 1)}\end{bmatrix}\begin{bmatrix}w_1 \\w_2 \\\vdots \\w_m \\b\end{bmatrix} \\&= \begin{bmatrix}\sum_{i = 1}^{m + 1}w_ia_{i1}, & \sum_{i = 1}^{m + 1}w_ia_{i2}, & \cdots, & \sum_{i = 1}^{m + 1}w_ia_{i(m + 1)}\end{bmatrix}\begin{bmatrix}w_1 \\w_2 \\\vdots \\w_m \\b\end{bmatrix} \\&= \sum_{i = 1}^{m + 1}\sum_{j = 1}^{m + 1}a_{ij}w_iw_j\end{aligned}</script><p>那么继续，按照分子布局展开：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}&= \frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{Aw})}{\partial\mathbf{w}} \\&= \frac{\partial\left(\sum_{i = 1}^{m + 1}\sum_{j = 1}^{m + 1}a_{ij}w_iw_j\right)}{\partial\mathbf{w}} \\&= \begin{bmatrix}\frac{\partial\left(\sum_{i = 1}^{m + 1}\sum_{j = 1}^{m + 1}a_{ij}w_iw_j\right)}{\partial w_1}, & \frac{\partial\left(\sum_{i = 1}^{m + 1}\sum_{j = 1}^{m + 1}a_{ij}w_iw_j\right)}{\partial w_2}, & \cdots, & \frac{\partial\left(\sum_{i = 1}^{m + 1}\sum_{j = 1}^{m + 1}a_{ij}w_iw_j\right)}{\partial w_{(m + 1)}}\end{bmatrix} \\&= \begin{bmatrix}\sum_{j = 1}^{m + 1}a_{1j}w_j + \sum_{i = 1}^{m + 1}a_{i1}w_i \\\sum_{j = 1}^{m + 1}a_{2j}w_j + \sum_{i = 1}^{m + 1}a_{i2}w_i \\\vdots \\\sum_{j = 1}^{m + 1}a_{(m + 1)j}w_j + \sum_{i = 1}^{m + 1}a_{i(m + 1)}w_i\end{bmatrix}^\mathbf{T} \\&= \begin{bmatrix}\sum_{j = 1}^{m + 1}a_{1j}w_j, & \sum_{j = 1}^{m + 1}a_{2j}w_j, & \cdots, \sum_{j = 1}^{m + 1}a_{(m + 1)j}w_j\end{bmatrix} \\&\quad +\begin{bmatrix}\sum_{i = 1}^{m + 1}a_{i1}w_i, & \sum_{i = 1}^{m + 1}a_{i2}w_i, & \cdots, & \sum_{i = 1}^{m + 1}a_{i(m + 1)}w_i\end{bmatrix} \\&= \mathbf{w}^\mathbf{T}\mathbf{A}^\mathbf{T} + \mathbf{w}^\mathbf{T}\mathbf{A} \\&= \mathbf{w}^\mathbf{T}(\mathbf{A}^\mathbf{T} + \mathbf{A})\end{aligned}</script><p>又$\mathbf{A}$是一个对称矩阵，所以有$\mathbf{A} = \mathbf{A}^\mathbf{T}$，故：</p><script type="math/tex; mode=display">\frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}} = \mathbf{w}^\mathbf{T}(\mathbf{A}^\mathbf{T} + \mathbf{A}) = 2\mathbf{w}^\mathbf{T}\mathbf{A}</script><blockquote><p>为什么这里选$\mathbf{A}$而不选$\mathbf{A}^\mathbf{T}$——因为维度要对齐。</p></blockquote><p>又$\mathbf{A} = \mathbf{X}^\mathbf{T}\mathbf{X}$，故：</p><script type="math/tex; mode=display">\frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}} = \mathbf{w}^\mathbf{T}(\mathbf{A}^\mathbf{T} + \mathbf{A}) = 2\mathbf{w}^\mathbf{T}\mathbf{A} = 2\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{X}</script><p>至此，$\frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}$计算完毕。</p><p>将上述计算结果代入代入$\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}) = \frac{1}{2n}\left(-2\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}} + \frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}\right)$中：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w})&= \frac{1}{2n}\left(-2\frac{\partial(\mathbf{y}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}} + \frac{\partial(\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{Xw})}{\partial\mathbf{w}}\right) \\&= \frac{1}{2n}\left(-2\mathbf{y}^\mathbf{T}\mathbf{X} + 2\mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{X}\right) \\&= -\frac{1}{n}\left(\mathbf{y}^\mathbf{T} - \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\right)\mathbf{X}\end{aligned}</script><p>得出结论，计算完毕（这里的结果与第一种方法得到的结果$-\frac{1}{n}(\mathbf{y} - \mathbf{Xw})^\mathrm{T}\mathbf{X}$不同，但是仍然可以得到相同的$\mathbf{w^\ast}$）。</p><h4 id="求出最优解-mathbf-w-ast"><a href="#求出最优解-mathbf-w-ast" class="headerlink" title="求出最优解$\mathbf{w^\ast}$"></a>求出最优解$\mathbf{w^\ast}$</h4><p>损失是凸函数，所以最优解满足：</p><script type="math/tex; mode=display">\begin{aligned}&\quad \frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}) = 0 \\&\Leftrightarrow -\frac{1}{n}(\mathbf{y} - \mathbf{Xw})^\mathbf{T}\mathbf{X} = 0 \\&\Leftrightarrow \mathbf{w^\ast} = (\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}\mathbf{X}^\mathrm{T}\mathbf{y}\end{aligned}</script><p>关于$\frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}) = 0$的求法：</p><script type="math/tex; mode=display">\begin{aligned}&\quad \frac{\partial}{\partial\mathbf{w}}\ell(\mathbf{X}, \mathbf{y}, \mathbf{w}) = 0 \\&\Leftrightarrow -\frac{1}{n}(\mathbf{y} - \mathbf{Xw})^\mathbf{T}\mathbf{X} = 0 \\&\Leftrightarrow (\mathbf{y} - \mathbf{Xw})^\mathbf{T}\mathbf{X} = 0 \\&\Leftrightarrow (\mathbf{y}^\mathbf{T} - \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T})\mathbf{X} = 0 \\&\Leftrightarrow \mathbf{y}^\mathbf{T}\mathbf{X} - \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{X} = 0 \\&\Leftrightarrow \mathbf{w}^\mathbf{T}\mathbf{X}^\mathbf{T}\mathbf{X} = \mathbf{y}^\mathbf{T}\mathbf{X} \\&\Leftrightarrow \mathbf{X}^\mathbf{T}\mathbf{X}\mathbf{w} = \mathbf{X}^\mathbf{T}\mathbf{y} \\&\Leftrightarrow \mathbf{w^\ast} = (\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}\mathbf{X}^\mathrm{T}\mathbf{y}\end{aligned}</script><p>计算完毕。</p><blockquote><p>凸函数：</p><p>当二阶导大于0时，则切线递增，函数为凹函数；当二阶导小于0时，则切线递减，函数为凸函数，如下图左图是凹函数，右图是凸函数：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231007143105239.png" alt="image-20231007143105239"></p></blockquote><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>线性回归是对n维输入的加权，外加偏差</li><li>使用平方损失来衡量预测值和真实值的差异</li><li>线性回归有显式解</li><li>线性回归可以看作是单层神经网络</li></ul><h2 id="基础优化方法"><a href="#基础优化方法" class="headerlink" title="基础优化方法"></a>基础优化方法</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231008162132856.png" alt="image-20231008162132856"></p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><ul><li><p>挑选一个初始值$\mathbf{w}_0$</p></li><li><p>重复迭代参数$t = 1, 2, 3$</p><script type="math/tex; mode=display">  \mathbf{w}_t = \mathbf{w}_{t - 1} - \eta\frac{\partial\ell}{\partial\mathbf{w}_{t - 1}}</script></li><li><p>沿梯度方向将增加损失函数值</p></li><li><p>学习率：步长的超参数</p></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231008162609837.png" alt="image-20231008162609837"></p><h3 id="选择学习率"><a href="#选择学习率" class="headerlink" title="选择学习率"></a>选择学习率</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/08/image-20231008162916345.png" alt="image-20231008162916345"></p><h3 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h3><ul><li><p>在整个训练集上算梯度太贵</p><p>  一个深度神经网络可能需要数分钟至数小时</p></li><li><p>我们可以随机采样b个样本$i_1, i_2, \cdots, i_b$来近似损失</p><script type="math/tex; mode=display">  \frac{1}{b}\sum_{i \in I_b}\ell(\mathbf{x}_i, y_i, \mathbf{w})</script><p>  b是批量大小，另一个重要的超参数</p></li></ul><h3 id="选择批量大小"><a href="#选择批量大小" class="headerlink" title="选择批量大小"></a>选择批量大小</h3><ul><li><p>不能太小</p><p>  每次计算量太小，不适合并行来最大利用计算资源</p></li><li><p>不能太大</p><p>  内存消耗增加，浪费计算，例如如果所有样本都是相同的</p></li></ul><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ul><li>梯度下降通过不断沿着反梯度方向更新参数求解</li><li>小批量随机梯度下降是深度学习默认的求解算法</li><li>两个重要的超参数是批量大小和学习率</li></ul><h2 id="线性回归的从零开始实现"><a href="#线性回归的从零开始实现" class="headerlink" title="线性回归的从零开始实现"></a>线性回归的从零开始实现</h2><p>我们将从零开始实现整个方法，包括数据流水线、模型、损失函数和小批量随机梯度下降优化器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>根据带有噪声的线性模型构造一个人造数据集。我们使用线性模型参数$\mathbf{w} = \begin{bmatrix}2, &amp; -3.4\end{bmatrix}^\mathbf{T}$、$b = 4.2$和噪声项$\epsilon$生成数据集及其标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成 y = Xw + b + 噪声。&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w))) <span class="comment"># 均值为1，标准差为1</span></span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p><code>features</code>中的每一行都包含一个二维数据样本，<code>labels</code>中的每一行都包含一维标签值（一个标量）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>, features[<span class="number">0</span>], <span class="string">&#x27;\nlabels:&#x27;</span>, labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">features: tensor([0.5193, 1.6783])</span><br><span class="line">labels: tensor([-0.4736])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.set_figsize()</span><br><span class="line">d2l.plt.scatter(features[:, <span class="number">1</span>].detach().numpy(),</span><br><span class="line">                labels.detach().numpy(), <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;matplotlib.collections.PathCollection at 0x1cd64102310&gt;</span><br></pre></td></tr></table></figure><p><img src="https://img.karltan.com/notes-out-class/d2l/08/output.svg" alt="output"></p><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>定义一个<code>data_iter</code>函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为<code>batch_size</code>的小批量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices) <span class="comment"># 得到一个随机顺序，然后按这个顺序取出，得到随机取出的效果</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size): <span class="comment"># 从0开始，到num_examples - 1为止，步长为batch_size</span></span><br><span class="line">        batch_indices = torch.tensor(indices[i:<span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices] <span class="comment"># yield不会立即返回</span></span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-0.6880, -0.3479],</span><br><span class="line">        [ 1.0066,  1.2217],</span><br><span class="line">        [ 0.4324, -1.2991],</span><br><span class="line">        [-0.1217,  1.5604],</span><br><span class="line">        [ 1.6267, -1.3457],</span><br><span class="line">        [-1.4481,  1.3394],</span><br><span class="line">        [ 0.8386,  0.1982],</span><br><span class="line">        [ 1.1044, -2.1684],</span><br><span class="line">        [-0.1196,  0.8331],</span><br><span class="line">        [ 0.8636,  1.3469]])</span><br><span class="line"> tensor([[ 4.0034],</span><br><span class="line">        [ 2.0714],</span><br><span class="line">        [ 9.4983],</span><br><span class="line">        [-1.3506],</span><br><span class="line">        [12.0259],</span><br><span class="line">        [-3.2358],</span><br><span class="line">        [ 5.2271],</span><br><span class="line">        [13.7814],</span><br><span class="line">        [ 1.1067],</span><br><span class="line">        [ 1.3650]])</span><br></pre></td></tr></table></figure><p>当<code>batch_size</code>小时，取出的这个batch就不能很好的代表整体，但是这对神经网络是有好处的，可以减少过拟合。</p><p>这里能够保证将所有数据都取过。</p><p>定义初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>定义模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b</span><br></pre></td></tr></table></figure><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>定义损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>关于这里为什么用的是均方误差而不是绝对值误差——因为绝对值不可导。</p><h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p>定义优化算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 更新时不需要计算梯度</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params: <span class="comment"># 对于每个参数</span></span><br><span class="line">            param -= lr * param.grad / batch_size <span class="comment"># 将梯度取平均</span></span><br><span class="line">            param.grad.zero_() <span class="comment"># 清零梯度</span></span><br></pre></td></tr></table></figure><p>注意，这里直接除<code>batch_size</code>是不严谨的，应该除样本的个数。</p><p>这里梯度取平均可以忽略样本规模对梯度的影响（因为梯度是累加的）。</p><p>SGD——随机梯度下降，随机的意思是在数据集中随机的采样<code>batch_size</code>个样本。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels): <span class="comment"># 取X，y的一个小批量</span></span><br><span class="line">        l = loss(net(X, w, b), y) <span class="comment"># 对这一个小批量求loss</span></span><br><span class="line">        <span class="comment"># 这里X - (batch_size, 2)</span></span><br><span class="line">        <span class="comment"># w - (2, 1)</span></span><br><span class="line">        <span class="comment"># 计算出的形状为(batch_size, 1)，是一个向量</span></span><br><span class="line">        <span class="comment"># 所以要先累加，也就是sum()，然后再反向传播求梯度</span></span><br><span class="line">        l.<span class="built_in">sum</span>().backward() <span class="comment"># 反向传播求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size) <span class="comment"># 随机梯度下降更新参数</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment"># 打印训练损失，不需要记录梯度</span></span><br><span class="line">        train_l = loss(net(features, w, b), labels) <span class="comment"># 对整个数据集计算损失</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>) <span class="comment"># 打印损失</span></span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.000115</span><br><span class="line">epoch 2, loss 0.000047</span><br><span class="line">epoch 3, loss 0.000047</span><br></pre></td></tr></table></figure><p>比较真实参数和通过训练学到的参数来评估训练的成功程度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w的估计误差: tensor([-0.0007,  0.0002], grad_fn=&lt;SubBackward0&gt;)</span><br><span class="line">b的估计误差: tensor([-0.0005], grad_fn=&lt;RsubBackward1&gt;)</span><br></pre></td></tr></table></figure><p>关于为什么使用梯度下降（一阶导）而不是牛顿法（二阶导）——因为一阶导好算（一阶导是向量），二阶导不好算（二阶导是梯度）。</p><h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><p>通过使用深度学习框架来简洁地实现线性回归模型。</p><h3 id="数据集-1"><a href="#数据集-1" class="headerlink" title="数据集"></a>数据集</h3><p>生成数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><h3 id="读取数据-1"><a href="#读取数据-1" class="headerlink" title="读取数据"></a>读取数据</h3><p>调用框架中现有的API来读取数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器。&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"></span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[tensor([[-0.4919,  0.0578],</span><br><span class="line">         [ 1.1658,  0.8898],</span><br><span class="line">         [ 0.7835, -1.1982],</span><br><span class="line">         [-0.2297,  0.8767],</span><br><span class="line">         [-0.0499,  1.9528],</span><br><span class="line">         [-0.2559,  0.4991],</span><br><span class="line">         [-0.6673, -2.0153],</span><br><span class="line">         [-1.0123,  0.1060],</span><br><span class="line">         [ 0.5949, -0.6779],</span><br><span class="line">         [ 0.3853, -0.5347]]),</span><br><span class="line"> tensor([[ 3.0052],</span><br><span class="line">         [ 3.4968],</span><br><span class="line">         [ 9.8504],</span><br><span class="line">         [ 0.7449],</span><br><span class="line">         [-2.5389],</span><br><span class="line">         [ 1.9955],</span><br><span class="line">         [ 9.7290],</span><br><span class="line">         [ 1.8039],</span><br><span class="line">         [ 7.6897],</span><br><span class="line">         [ 6.7938]])]</span><br></pre></td></tr></table></figure><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p>使用框架的预定义好的层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># nn.Sequential()是一个layer的容器</span></span><br></pre></td></tr></table></figure><p>初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于nn.Sequential()只有一层，所以可以通过net[0]来访问到这一层</span></span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>) <span class="comment"># 使用正态分布，均值为0，标准差为0.01</span></span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0.])</span><br></pre></td></tr></table></figure><h3 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h3><p>计算均方误差使用的是<code>MSELoss</code>类，也称$L_2$平方范数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure><h3 id="优化算法-1"><a href="#优化算法-1" class="headerlink" title="优化算法"></a>优化算法</h3><p>实例化<code>SGD</code>实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><p>训练过程的代码与我们从零开始实现时所做的非常相似：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs): <span class="comment"># 将整个数据集遍历3遍</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X), y) <span class="comment"># net()的参数w, b在之前设置过了，这里不需要再输入了</span></span><br><span class="line">        trainer.zero_grad() <span class="comment"># 清零梯度</span></span><br><span class="line">        l.backward() <span class="comment"># 计算梯度，这里默认会执行sum()</span></span><br><span class="line">        trainer.step() <span class="comment"># 更新参数</span></span><br><span class="line">    l = loss(net(features), labels) <span class="comment"># 这里只进行了前向传播，没有涉及到梯度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss 0.000100</span><br><span class="line">epoch 2, loss 0.000109</span><br><span class="line">epoch 3, loss 0.000112</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 08 线性回归 + 基础优化算法。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 07 自动求导</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/07/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/07/</id>
    <published>2023-10-05T13:00:00.000Z</published>
    <updated>2023-10-05T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="07-自动求导"><a href="#07-自动求导" class="headerlink" title="07 自动求导"></a>07 自动求导</h1><h2 id="向量链式法则"><a href="#向量链式法则" class="headerlink" title="向量链式法则"></a>向量链式法则</h2><ul><li><p>标量链式法则</p><script type="math/tex; mode=display">  y = f(u), u = g(x) \quad \frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial x}</script></li><li><p>拓展到向量</p><script type="math/tex; mode=display">  \begin{array}{c}  \frac{\partial y}{\partial\mathbf{x}} = \frac{\partial y}{\partial u} \frac{\partial u}{\partial\mathbf{x}} \\  (1, n) \quad (1,)(1, n) \\  \frac{\partial y}{\partial\mathbf{x}} = \frac{\partial y}{\partial\mathbf{u}} \frac{\partial\mathbf{u}}{\partial\mathbf{x}} \\  (1, n) \quad (1, k)(k, n) \\  \frac{\partial\mathbf{y}}{\partial\mathbf{x}} = \frac{\partial\mathbf{y}}{\partial\mathbf{u}} \frac{\partial\mathbf{u}}{\partial\mathbf{x}} \\  (m, n) \quad (m, k)(k, n)  \end{array}</script></li></ul><h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h3><ul><li><p>假设</p><script type="math/tex; mode=display">  \mathbf{x}, \mathbf{w} \in \mathbb{R}^n, y \in \mathbb{R}, z = \left(\left\langle\mathbf{x}, \mathbf{w}\right\rangle - y\right)^2</script><p>  $\left\langle\mathbf{x}, \mathbf{w}\right\rangle$表示向量$\mathbf{x}$和向量$\mathbf{w}$做内积，且$\left\langle\mathbf{x}, \mathbf{w}\right\rangle = \mathbf{x}^\mathrm{T}\mathbf{w} = x_1w_1 + x_2w_2 + \cdots x_nw_n$，是一个标量。</p></li><li><p>计算$\frac{\partial z}{\partial\mathbf{w}}$</p></li><li><p>计算步骤如下</p><ol><li><p>令$a = \left\langle\mathbf{x}, \mathbf{w}\right\rangle, b = a - y, z = b^2$</p></li><li><p>那么</p><script type="math/tex; mode=display"> \begin{aligned} \frac{\partial z}{\partial\mathbf{w}} &= \frac{\partial z}{\partial b}\frac{\partial b}{\partial a}\frac{\partial a}{\partial\mathbf{w}} \\ &= \frac{\partial b^2}{\partial b}\frac{\partial(a - y)}{\partial a}\frac{\partial\left\langle\mathbf{x}, \mathbf{w}\right\rangle}{\partial\mathbf{w}} \\ &= 2b \cdot 1 \cdot \mathbf{x}^\mathrm{T} \\ &= 2(\left\langle\mathbf{x}, \mathbf{w}\right\rangle - y)\mathbf{x}^\mathrm{T} \end{aligned}</script></li></ol></li></ul><h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h3><ul><li><p>假设</p><script type="math/tex; mode=display">  \mathbf{X} \in \mathbb{R}^{m \times n}, \mathbf{w} \in \mathbb{R}^{n}, \mathbf{y} \in \mathbb{R}^{m}, z = \Vert\mathbf{X}\mathbf{w} - \mathbf{y}\Vert^2</script></li><li><p>计算$\frac{\partial z}{\partial\mathbf{w}}$</p></li><li><p>计算步骤如下</p><ol><li><p>令$\mathbf{a} = \mathbf{Xw}, \mathbf{b} = \mathbf{a} - \mathbf{y}, z = \Vert\mathbf{b}\Vert^2$</p></li><li><p>那么</p><script type="math/tex; mode=display"> \begin{aligned} \frac{\partial z}{\partial\mathbf{w}} &= \frac{\partial z}{\partial\mathbf{b}}\frac{\partial\mathbf{b}}{\partial\mathbf{a}}\frac{\partial\mathbf{a}}{\partial\mathbf{w}} \\ &= \frac{\partial\Vert\mathbf{b}\Vert^2}{\partial\mathbf{b}}\frac{\partial(\mathbf{a} - \mathbf{y})}{\partial\mathbf{a}}\frac{\partial\mathbf{Xw}}{\partial\mathbf{w}} \\ &= 2\mathbf{b}^\mathrm{T} \times \mathbf{E} \times \mathbf{X} \\ &= 2(\mathbf{Xw} - \mathbf{y})^\mathrm{T}\mathbf{X} \end{aligned}</script></li></ol></li></ul><h2 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h2><ul><li><p>自动求导计算一个函数在指定值上的导数</p></li><li><p>它有别于</p><ul><li><p>符号求导</p><script type="math/tex; mode=display">  \text{D}[4x^3 + x^2 + 3, x] = 2x + 12x^2</script></li><li><p>数值求导</p><script type="math/tex; mode=display">  \frac{\partial f(x)}{\partial x} = \lim_{h \to 0}\frac{f(x + h) - f(x)}{h}</script></li></ul></li></ul><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><ul><li>将代码分成操作子</li><li>将计算表示为一个无环图</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/07/image-20231005205626984.png" alt="image-20231005205626984"></p><ul><li>显式构造<ul><li>Tensorflow/Theano/MXNet</li></ul></li><li>隐式构造<ul><li>PyTorch/MXNet</li></ul></li></ul><h2 id="自动求导的两种模式"><a href="#自动求导的两种模式" class="headerlink" title="自动求导的两种模式"></a>自动求导的两种模式</h2><ul><li><p>链式法则：</p><script type="math/tex; mode=display">  \frac{\partial y}{\partial x} = \frac{\partial y}{\partial u_n}\frac{\partial u_n}{\partial u_{n - 1}}\cdots\frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x}</script></li><li><p>正向累积：</p><script type="math/tex; mode=display">  \frac{\partial y}{\partial x} = \frac{\partial y}{\partial u_n}\left(\frac{\partial u_n}{\partial u_{n - 1}}\left(\cdots\left(\frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x}\right)\right)\right)</script></li><li><p>反向累积、又称反向传递：</p><script type="math/tex; mode=display">  \frac{\partial y}{\partial x} = \left(\left(\left(\frac{\partial y}{\partial u_n}\frac{\partial u_n}{\partial u_{n - 1}}\right)\cdots\right)\frac{\partial u_2}{\partial u_1}\right)\frac{\partial u_1}{\partial x}</script></li></ul><h2 id="反向累积"><a href="#反向累积" class="headerlink" title="反向累积"></a>反向累积</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/07/image-20231005210917533.png" alt="image-20231005210917533"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/07/image-20231005211003972.png" alt="image-20231005211003972"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>构造计算图</li><li>前向：执行图，存储中间结果</li><li>反向：从相反方向执行图<ul><li>去除不需要的枝</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/07/image-20231005211116617.png" alt="image-20231005211116617"></p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><ul><li>计算复杂度：$O(n)$，$n$是操作子个数<ul><li>通常正向和反向的代价类似</li></ul></li><li>内存复杂度：$O(n)$，因为需要存储正向的所有中间结果</li><li>跟正向累积对比：<ul><li>$O(n)$计算复杂度用来计算一个变量的梯度</li><li>$O(1)$内存复杂度</li></ul></li></ul><h2 id="自动求导实现"><a href="#自动求导实现" class="headerlink" title="自动求导实现"></a>自动求导实现</h2><p>假设我们想对函数$y = 2\mathbf{x}^\mathrm{T}\mathbf{x}$关于列向量$\mathbf{x}$求导：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 1., 2., 3.])</span><br></pre></td></tr></table></figure><p>在我们计算$y$关于$\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.requires_grad_(<span class="literal">True</span>) <span class="comment"># 等价于x = torch.arange(4.0, requires_grad=True)</span></span><br><span class="line">x.grad <span class="comment"># 默认值是None</span></span><br></pre></td></tr></table></figure><p>这里没有输出。</p><p>现在让我们计算$y$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(28., grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><p>通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0.,  4.,  8., 12.])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad == <span class="number">4</span> * x</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure><p>现在计算<code>x</code>的另一个函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 1., 1., 1.])</span><br></pre></td></tr></table></figure><p>深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x <span class="comment"># 逐元素相乘</span></span><br><span class="line"><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0., 2., 4., 6.])</span><br></pre></td></tr></table></figure><p>将某些计算移动到记录的计算图之外：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u</span><br></pre></td></tr></table></figure><p>这里是将<code>u</code>看成了一个常数，并且这个常数的值为<code>x * x</code>，那么在计算<code>z = u * x</code>关于<code>x</code>的导数的时候，其值为<code>u</code>，最后的输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span> * x</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([True, True, True, True])</span><br></pre></td></tr></table></figure><p>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br><span class="line"></span><br><span class="line">a.grad == d / a</span><br></pre></td></tr></table></figure><p>在<code>f(a)</code>函数中对<code>a</code>的操作都是线性的，所以<code>d = f(a) = ka</code>，所以梯度为<code>d / a</code>，那么上面代码的输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(True)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 07 自动求导。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 06 矩阵计算</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/06/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/06/</id>
    <published>2023-10-05T05:00:00.000Z</published>
    <updated>2023-10-07T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="06-矩阵计算"><a href="#06-矩阵计算" class="headerlink" title="06 矩阵计算"></a>06 矩阵计算</h1><h2 id="标量导数"><a href="#标量导数" class="headerlink" title="标量导数"></a>标量导数</h2><div class="table-container"><table><thead><tr><th style="text-align:center">$y$</th><th style="text-align:center">$a$</th><th style="text-align:center">$x^n$</th><th style="text-align:center">$\exp(x)$</th><th style="text-align:center">$\log(x)$（这里的$\log$默认是$\ln$）</th><th style="text-align:center">$\sin(x)$</th></tr></thead><tbody><tr><td style="text-align:center">$\frac{\mathrm{d}y}{\mathrm{d}x}$</td><td style="text-align:center">$0$</td><td style="text-align:center">$nx^{n - 1}$</td><td style="text-align:center">$\exp(x)$</td><td style="text-align:center">$\frac{1}{x}$</td><td style="text-align:center">$\cos(x)$</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center">$y$</th><th style="text-align:center">$u + v$</th><th style="text-align:center">$uv$</th><th style="text-align:center">$y = f(u), u = g(x)$</th></tr></thead><tbody><tr><td style="text-align:center">$\frac{\mathrm{d}y}{\mathrm{d}x}$</td><td style="text-align:center">$\frac{\mathrm{d}u}{\mathrm{d}x} + \frac{\mathrm{d}v}{\mathrm{d}x}$</td><td style="text-align:center">$\frac{\mathrm{d}u}{\mathrm{d}x}v + \frac{\mathrm{d}v}{\mathrm{d}x}u$</td><td style="text-align:center">$\frac{\mathrm{d}y}{\mathrm{d}u}\frac{\mathrm{d}u}{\mathrm{d}x}$</td></tr></tbody></table></div><p>导数是切线的斜率：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/06/image-20231004124032678.png" alt="image-20231004124032678"></p><h2 id="亚导数"><a href="#亚导数" class="headerlink" title="亚导数"></a>亚导数</h2><ul><li>将导数拓展到不可微的函数</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/06/image-20231004124312940.png" alt="image-20231004124312940"></p><script type="math/tex; mode=display">\frac{\partial\left|x\right|}{\partial x} =\begin{cases}\begin{array}{ll}1 & \text { if } x > 0 \\-1 & \text { if } x < 0 \\a & \text { if } x = 0, \quad a \in [-1,1]\end{array}\end{cases}</script><ul><li>另一个例子</li></ul><script type="math/tex; mode=display">\frac{\partial}{\partial x}\max(x, 0) =\begin{cases}\begin{array}{ll}1 & \text{ if } x > 0 \\0 & \text{ if } x < 0 \\a & \text{ if } x = 0, \quad a \in \left[-1, 1\right]\end{array}\end{cases}</script><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><ul><li><p>将导数拓展到向量</p><p>  关键是要搞清楚形状。</p></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/06/image-20231004125227523.png" alt="image-20231004125227523"></p><h3 id="frac-partial-y-partial-mathbf-x-（-y-是标量，-mathbf-x-是向量）"><a href="#frac-partial-y-partial-mathbf-x-（-y-是标量，-mathbf-x-是向量）" class="headerlink" title="$\frac{\partial y}{\partial \mathbf{x}}$（$y$是标量，$\mathbf{x}$是向量）"></a>$\frac{\partial y}{\partial \mathbf{x}}$（$y$是标量，$\mathbf{x}$是向量）</h3><p>$\frac{\partial y}{\partial \mathbf{x}}$是行向量，形状为$1 \times n$，结果的第一维大小与分子的第一维大小相同，是分子布局，且在这门课程后续的叙述中，如没有特殊说明，那么用的基本都是分子布局。</p><script type="math/tex; mode=display">\mathbf{x} =\begin{bmatrix}x_1 \\x_2 \\\vdots \\x_n\end{bmatrix}</script><script type="math/tex; mode=display">\frac{\partial y}{\partial\mathbf{x}} = \begin{bmatrix}\frac{\partial y}{\partial x_1}, & \frac{\partial y}{\partial x_2}, & \cdots, & \frac{\partial y}{\partial x_n}\end{bmatrix}</script><p>令$y = x_1^2 + 2x_2^2$，则：</p><script type="math/tex; mode=display">\frac{\partial y}{\partial\mathbf{x}} = \frac{\partial}{\partial\mathbf{x}}x_1^2 + 2x_2^2 = \begin{bmatrix}2x_1, & 4x_2\end{bmatrix}</script><p>将函数理解为一个等高线：</p><p><img src="https://img.karltan.com/notes-out-class/d2l/06/image-20231004125930130.png" alt="image-20231004125930130"></p><p>且点$(1, 1)$在函数上，将$(1, 1)$代入$\frac{\partial y}{\partial\mathbf{x}}$中，得到向量$(2, 4)$，那么这个向量$(2, 4)$是与等高线（函数）正交（垂直）的，且这个向量指向的方向也是值变化最大的方向。</p><div class="table-container"><table><thead><tr><th style="text-align:center">$y$</th><th style="text-align:center">$a$</th><th style="text-align:center">$au$</th><th style="text-align:center">$\text{sum}(x)$</th><th style="text-align:center">$\Vert\mathbf{x}\Vert^2$</th></tr></thead><tbody><tr><td style="text-align:center">$\frac{\partial y}{\partial\mathbf{x}}$</td><td style="text-align:center">$\mathbf{0}^{\mathrm{T}}$（0向量）</td><td style="text-align:center">$a\frac{\partial u}{\partial\mathbf{x}}$</td><td style="text-align:center">$\mathbf{1}^{\mathrm{T}}$（全1向量）</td><td style="text-align:center">$2\mathbf{x}^{\mathrm{T}}$</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center">$y$</th><th style="text-align:center">$u + v$</th><th style="text-align:center">$uv$</th><th style="text-align:center">$\left\langle\mathbf{u}, \mathbf{v}\right\rangle$</th></tr></thead><tbody><tr><td style="text-align:center">$\frac{\partial y}{\partial\mathbf{x}}$</td><td style="text-align:center">$\frac{\partial u}{\partial\mathbf{x}} + \frac{\partial v}{\partial\mathbf{x}}$</td><td style="text-align:center">$\frac{\partial u}{\partial\mathbf{x}}v + \frac{\partial v}{\partial\mathbf{x}}u$</td><td style="text-align:center">$\mathbf{u}^{\mathrm{T}}\frac{\partial\mathbf{v}}{\partial\mathbf{x}} + \mathbf{v}^{\mathrm{T}}\frac{\partial\mathbf{u}}{\partial\mathbf{x}}$</td></tr></tbody></table></div><h3 id="frac-partial-mathbf-y-partial-x-（-mathbf-y-是向量，-x-是标量）"><a href="#frac-partial-mathbf-y-partial-x-（-mathbf-y-是向量，-x-是标量）" class="headerlink" title="$\frac{\partial\mathbf{y}}{\partial x}$（$\mathbf{y}$是向量，$x$是标量）"></a>$\frac{\partial\mathbf{y}}{\partial x}$（$\mathbf{y}$是向量，$x$是标量）</h3><p>$\frac{\partial\mathbf{y}}{\partial x}$是列向量，形状为$m \times 1$，结果的第一维大小与分子的第一维大小相同，是分子布局。</p><script type="math/tex; mode=display">\mathbf{y} =\begin{bmatrix}y_1 \\y_2 \\\vdots \\y_m\end{bmatrix}</script><script type="math/tex; mode=display">\frac{\partial\mathbf{y}}{\partial x} =\begin{bmatrix}\frac{\partial y_1}{\partial x} \\\frac{\partial y_2}{\partial x} \\\vdots \\\frac{\partial y_m}{\partial x}\end{bmatrix}</script><h3 id="frac-partial-mathbf-y-partial-mathbf-x-（-mathbf-x-mathbf-y-都是向量）"><a href="#frac-partial-mathbf-y-partial-mathbf-x-（-mathbf-x-mathbf-y-都是向量）" class="headerlink" title="$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$（$\mathbf{x}, \mathbf{y}$都是向量）"></a>$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$（$\mathbf{x}, \mathbf{y}$都是向量）</h3><p>$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$是矩阵，形状为$m \times n$，结果的第一维大小与分子的第一维大小相同，是分子布局。</p><script type="math/tex; mode=display">\mathbf{x} =\begin{bmatrix}x_1 \\x_2 \\\vdots \\x_n\end{bmatrix}\quad\mathbf{y} =\begin{bmatrix}y_1 \\y_2 \\\vdots \\y_m\end{bmatrix}</script><script type="math/tex; mode=display">\frac{\partial\mathbf{y}}{\partial\mathbf{x}} =\begin{bmatrix}\frac{\partial y_1}{\partial\mathbf{x}} \\\frac{\partial y_2}{\partial\mathbf{x}} \\\vdots \\\frac{\partial y_m}{\partial\mathbf{x}}\end{bmatrix} =\begin{bmatrix}\frac{\partial y_1}{\partial x_1}, & \frac{\partial y_1}{\partial x_2}, & \cdots, & \frac{\partial y_1}{\partial x_n} \\\frac{\partial y_2}{\partial x_1}, & \frac{\partial y_2}{\partial x_2}, & \cdots, & \frac{\partial y_2}{\partial x_n} \\\vdots & \vdots & & \vdots \\\frac{\partial y_m}{\partial x_1}, & \frac{\partial y_m}{\partial x_2}, & \cdots, & \frac{\partial y_m}{\partial x_n}\end{bmatrix}</script><p>这里的计算是基于分子布局的计算方法来计算的，将$\mathbf{y}$看作是列向量，$\mathbf{x}$看作是标量，那么可以通过分子布局的计算方式直接展开为$\begin{bmatrix} \frac{\partial y_1}{\partial\mathbf{x}} \\ \frac{\partial y_2}{\partial\mathbf{x}} \\ \vdots \\ \frac{\partial y_m}{\partial\mathbf{x}} \end{bmatrix}$，然后再根据分子布局的计算方法，将$\frac{\partial y_1}{\partial\mathbf{x}}$展开为$\begin{bmatrix}\frac{\partial y_1}{\partial x_1}, &amp; \frac{\partial y_1}{\partial x_2}, &amp; \cdots, &amp; \frac{\partial y_1}{\partial x_n}\end{bmatrix}$（剩下的$m - 1$行同理展开），由此得到结果。</p><p>下表基于$\mathbf{x} \in \mathbb{R}^n, \mathbf{y} \in \mathbb{R}^m, \frac{\partial\mathbf{y}}{\partial\mathbf{x}} \in \mathbb{R}^{m \times n}$。</p><div class="table-container"><table><thead><tr><th style="text-align:center">$\mathbf{y}$</th><th style="text-align:center">$\mathbf{a}$</th><th style="text-align:center">$\mathbf{x}$</th><th style="text-align:center">$\mathbf{Ax}$</th><th style="text-align:center">$\mathbf{x}^\mathrm{T}\mathbf{A}$</th></tr></thead><tbody><tr><td style="text-align:center">$\frac{\partial\mathbf{y}}{\partial\mathbf{x}}$</td><td style="text-align:center">$\mathbf{0}$（0矩阵）</td><td style="text-align:center">$\mathbf{E}$（单位矩阵）</td><td style="text-align:center">$\mathbf{A}$</td><td style="text-align:center">$\mathbf{A}^\mathrm{T}$</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center">$\mathbf{y}$</th><th style="text-align:center">$a\mathbf{u}$</th><th style="text-align:center">$\mathbf{Au}$</th><th style="text-align:center">$\mathbf{u} + \mathbf{v}$</th></tr></thead><tbody><tr><td style="text-align:center">$\frac{\partial\mathbf{y}}{\partial\mathbf{x}}$</td><td style="text-align:center">$a\frac{\partial\mathbf{u}}{\partial\mathbf{x}}$</td><td style="text-align:center">$\mathbf{A}\frac{\partial\mathbf{u}}{\partial\mathbf{x}}$</td><td style="text-align:center">$\frac{\partial\mathbf{u}}{\partial\mathbf{x}} + \frac{\partial\mathbf{v}}{\partial\mathbf{x}}$</td></tr></tbody></table></div><h2 id="拓展到矩阵"><a href="#拓展到矩阵" class="headerlink" title="拓展到矩阵"></a>拓展到矩阵</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/06/image-20231004140521250.png" alt="image-20231004140521250"></p><p>右下角的维度为$(m, l, k, n)$。</p><p>这一节主要的目的不是为了计算导数，而是要知道输入形状和导数形状的变化关系。</p><h2 id="分子布局和分母布局"><a href="#分子布局和分母布局" class="headerlink" title="分子布局和分母布局"></a>分子布局和分母布局</h2><p><a href="https://www.bilibili.com/video/BV1xk4y1B7RQ?p=3">【手推机器学习】矩阵求导3—向量函数与矩阵求导初印象 哔哩哔哩 bilibili</a></p><p><a href="https://zhuanlan.zhihu.com/p/263777564">矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)</a></p><p><a href="https://zhuanlan.zhihu.com/p/273729929">矩阵求导公式的数学推导（矩阵求导——基础篇） - 知乎 (zhihu.com)</a></p><p><a href="https://www.matrixcalculus.org/">Matrix Calculus</a></p><h3 id="函数的种类"><a href="#函数的种类" class="headerlink" title="函数的种类"></a>函数的种类</h3><p>在讲分子分母布局的时候，先讲一下函数的种类。</p><ol><li><p>标量函数</p><p> 顾名思义，标量函数就是输出为标量的函数。</p><p> 比如$f(x) = x^2$，这个函数的输入为$x$，输出为$x^2$，输入到输出是从$\mathbb{R} \to \mathbb{R}$。</p><p> 再比如$f(x) = x_1^2 + x_2^2$，这个函数的输入为一个向量$\begin{bmatrix}x_1 \\ x_2\end{bmatrix}$，输出为一个标量$x_1^2 + x_2^2$，输入到输出是从$\mathbb{R}^2 \to \mathbb{R}$，虽然函数的输入是一个向量，但是输出是一个标量，所以也是标量函数。</p></li><li><p>向量函数</p><p> 同理，向量函数就是输出为向量的函数。</p><p> 比如$f(x) = \begin{bmatrix}f_1(x) = x \\ f_2(x) = x^2\end{bmatrix}$，输入是一个标量$x$，但是输出是一个向量$\begin{bmatrix}x \\ x^2\end{bmatrix}$，输入到输出是从$\mathbb{R} \to \mathbb{R}^2$。</p></li><li><p>矩阵函数</p><p> 那么继续推广可以得到矩阵函数，同理，矩阵函数就是输出为矩阵的函数。</p><p> 比如$f(x) = \begin{bmatrix}f_{11}(x) = x, &amp; f_{12}(x) = x^2 \\ f_{21}(x) = x^3, &amp; f_{22}(x) = x^4\end{bmatrix}$，输入是一个标量$x$，但是输出是一个矩阵$\begin{bmatrix}x, &amp; x^2 \\ x^3, &amp; x^4\end{bmatrix}$，输入到输出是从$\mathbb{R} \to \mathbb{R}^{2 \times 2}$。</p><p> 再比如$f(x) = \begin{bmatrix}f_{11}(x) = x_1 + x_2, &amp; f_{12}(x) = x_1^2 + x_2^2 \\ f_{21}(x) = x_1^3 + x_2^3, &amp; f_{22}(x) = x_1^4 + x_2^4\end{bmatrix}$，输入是一个向量$\begin{bmatrix}x_1 \\ x_2\end{bmatrix}$，输出是一个矩阵$\begin{bmatrix}x_1 + x_2, &amp; x_1^2 + x_2^2 \\ x_1^3 + x_2^3, &amp; x_1^4 + x_2^4\end{bmatrix}$，输入到输出是从$\mathbb{R}^2 \to \mathbb{R}^{2 \times 2}$。</p></li></ol><h3 id="矩阵求导的本质"><a href="#矩阵求导的本质" class="headerlink" title="矩阵求导的本质"></a>矩阵求导的本质</h3><script type="math/tex; mode=display">\frac{\mathrm{d}\mathbf{A}}{\mathrm{d}\mathbf{B}}</script><p>矩阵求导的本质就是矩阵$\mathbf{A}$中的每一个元素对矩阵$\mathbf{B}$中的每一个元素求导。</p><h3 id="求导后元素的个数"><a href="#求导后元素的个数" class="headerlink" title="求导后元素的个数"></a>求导后元素的个数</h3><ul><li>如果矩阵$\mathbf{A} \in \mathbb{R}^{1 \times 1}$，$\mathbf{B} \in \mathbb{R}^{1 \times 1}$，那么$\frac{\mathrm{d}\mathbf{A}}{\mathrm{d}\mathbf{B}}$中只有1个元素；</li><li>如果矩阵$\mathbf{A} \in \mathbb{R}^{1 \times p}$，$\mathbf{B} \in \mathbb{R}^{1 \times n}$，那么$\frac{\mathrm{d}\mathbf{A}}{\mathrm{d}\mathbf{B}}$中会有$p \times n$个元素；</li><li>如果矩阵$\mathbf{A} \in \mathbb{R}^{q \times p}$，$\mathbf{B} \in \mathbb{R}^{m \times n}$，那么$\frac{\mathrm{d}\mathbf{A}}{\mathrm{d}\mathbf{B}}$中就会有$p \times q \times m \times n$个元素。</li></ul><p>那么问题就来了，这些元素怎么放呢？</p><p>这里就需要引出分子布局和分母布局。</p><h3 id="分子布局"><a href="#分子布局" class="headerlink" title="分子布局"></a>分子布局</h3><p>分子布局就是分子的第一个维度大小会作为求导矩阵的第一个维度大小。</p><p>我们结合例子说明。</p><ul><li><p>假设$x \in \mathbb{R}^1, \mathbf{y} \in \mathbb{R}^m$：</p><script type="math/tex; mode=display">  \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}  =  \begin{bmatrix}  \frac{\mathrm{d}y_1}{\mathrm{d}x} \\  \frac{\mathrm{d}y_2}{\mathrm{d}x} \\  \vdots \\  \frac{\mathrm{d}y_m}{\mathrm{d}x}  \end{bmatrix}</script><p>  可以看出，$x$中有$1 \times 1 = 1$个元素，$\mathbf{y}$中有$m \times 1 = m$个元素，那么$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}$会有$1 \times m = m$个元素，且根据我们的结果，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x} \in \mathbb{R}^{m \times 1}$，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}$第一个维度的大小为$m$，与分子$\mathbf{y}$的第一个维度大小相等（默认为列向量）。</p></li><li><p>假设$\mathbf{x} \in \mathbb{R}^n, y \in \mathbb{R}^1$：</p><script type="math/tex; mode=display">  \frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}} = \begin{bmatrix}\frac{\mathrm{d}y}{\mathrm{d}x_1}, & \frac{\mathrm{d}y}{\mathrm{d}x_2}, & \cdots, & \frac{\mathrm{d}y}{\mathrm{d}x_n}\end{bmatrix}</script><p>  可以看出，$\mathbf{x}$中有$n \times 1 = 1$个元素，$y$中有$1 \times 1 = 1$个元素，那么$\frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}}$会有$n \times 1 = n$个元素，且根据我们的结果，$\frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}} \in \mathbb{R}^{1 \times n}$，$\frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}}$第一个维度的大小为$1$，与分子$y$的第一个维度大小相等（默认为列向量）。</p></li><li><p>假设$\mathbf{x} \in \mathbb{R}^n, \mathbf{y} \in \mathbb{R}^m$：</p><script type="math/tex; mode=display">  \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}}  =  \begin{bmatrix}  \frac{\mathrm{d}y_1}{\mathrm{d}\mathbf{x}} \\  \frac{\mathrm{d}y_2}{\mathrm{d}\mathbf{x}} \\  \vdots \\  \frac{\mathrm{d}y_m}{\mathrm{d}\mathbf{x}}  \end{bmatrix}  =  \begin{bmatrix}  \frac{\mathrm{d}y_1}{\mathrm{d}x_1}, & \frac{\mathrm{d}y_1}{\mathrm{d}x_2}, & \cdots, & \frac{\mathrm{d}y_1}{\mathrm{d}x_n} \\  \frac{\mathrm{d}y_2}{\mathrm{d}x_1}, & \frac{\mathrm{d}y_2}{\mathrm{d}x_2}, & \cdots, & \frac{\mathrm{d}y_2}{\mathrm{d}x_n} \\  \vdots & \vdots & & \vdots \\  \frac{\mathrm{d}y_m}{\mathrm{d}x_1}, & \frac{\mathrm{d}y_m}{\mathrm{d}x_2}, & \cdots, & \frac{\mathrm{d}y_m}{\mathrm{d}x_n}  \end{bmatrix}</script><p>  可以看出，$\mathbf{x}$中有$n \times 1 = 1$个元素，$\mathbf{y}$中有$m \times 1 = m$个元素，那么$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}}$会有$n \times m$个元素，且根据我们的结果，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}} \in \mathbb{R}^{m \times n}$，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}}$第一个维度的大小为$m$，与分子$\mathbf{y}$的第一个维度大小相等（默认为列向量）。</p></li></ul><p>以上便是分子布局。</p><h3 id="分母布局"><a href="#分母布局" class="headerlink" title="分母布局"></a>分母布局</h3><p>分母布局就是分母的第一个维度大小会作为求导矩阵的第一个维度大小。</p><p>我们结合例子说明。</p><ul><li><p>假设$x \in \mathbb{R}^1, \mathbf{y} \in \mathbb{R}^m$：</p><script type="math/tex; mode=display">  \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x} = \begin{bmatrix}\frac{\mathrm{d}y_1}{\mathrm{d}x}, & \frac{\mathrm{d}y_2}{\mathrm{d}x}, & \cdots, & \frac{\mathrm{d}y_m}{\mathrm{d}x}\end{bmatrix}</script><p>  可以看出，$x$中有$1 \times 1 = 1$个元素，$\mathbf{y}$中有$m \times 1 = m$个元素，那么$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}$会有$1 \times m = m$个元素，且根据我们的结果，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x} \in \mathbb{R}^{1 \times m}$，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x}$第一个维度的大小为$1$，与分母$x$的第一个维度大小相等（默认为列向量）。</p></li><li><p>假设$\mathbf{x} \in \mathbb{R}^n, y \in \mathbb{R}^1$：</p><script type="math/tex; mode=display">  \frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}}  =  \begin{bmatrix}  \frac{\mathrm{d}y}{\mathrm{d}x_1} \\  \frac{\mathrm{d}y}{\mathrm{d}x_2} \\  \vdots \\  \frac{\mathrm{d}y}{\mathrm{d}x_n}  \end{bmatrix}</script><p>  可以看出，$\mathbf{x}$中有$n \times 1 = n$个元素，$y$中有$1 \times 1 = 1$个元素，那么$\frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}}$会有$n \times 1 = n$个元素，且根据我们的结果，$\frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}} \in \mathbb{R}^{n \times 1}$，$\frac{\mathrm{d}y}{\mathrm{d}\mathbf{x}}$第一个维度的大小为$n$，与分母$x$的第一个维度大小相等（默认为列向量）。</p></li><li><p>假设$\mathbf{x} \in \mathbb{R}^n, \mathbf{y} \in \mathbb{R}^m$：</p><script type="math/tex; mode=display">  \begin{aligned}  \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}}  &= \begin{bmatrix}  \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x_1} \\  \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x_2} \\  \vdots \\  \frac{\mathrm{d}\mathbf{y}}{\mathrm{d}x_n}  \end{bmatrix} \\  &= \begin{bmatrix}  \frac{\mathrm{d}y_1}{\mathrm{d}x_1}, & \frac{\mathrm{d}y_2}{\mathrm{d}x_1}, & \cdots, & \frac{\mathrm{d}y_m}{\mathrm{d}x_1} \\  \frac{\mathrm{d}y_1}{\mathrm{d}x_2}, & \frac{\mathrm{d}y_2}{\mathrm{d}x_2}, & \cdots, & \frac{\mathrm{d}y_m}{\mathrm{d}x_2} \\  \vdots & \vdots & & \vdots \\  \frac{\mathrm{d}y_1}{\mathrm{d}x_n}, & \frac{\mathrm{d}y_2}{\mathrm{d}x_n}, & \cdots, & \frac{\mathrm{d}y_m}{\mathrm{d}x_n}  \end{bmatrix}  \end{aligned}</script><p>  可以看出，$\mathbf{x}$中有$n \times 1 = 1$个元素，$\mathbf{y}$中有$m \times 1 = m$个元素，那么$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}}$会有$n \times m$个元素，且根据我们的结果，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}} \in \mathbb{R}^{n \times m}$，$\frac{\mathrm{d}\mathbf{y}}{\mathrm{d}\mathbf{x}}$第一个维度的大小为$n$，与分母$\mathbf{x}$的第一个维度大小相等（默认为列向量）。</p></li></ul><p>以上便是分母布局。</p><h2 id="常见矩阵求导结论"><a href="#常见矩阵求导结论" class="headerlink" title="常见矩阵求导结论"></a>常见矩阵求导结论</h2><p>通常，$(分子布局)^\mathrm{T} = (分母布局)$。</p><h3 id="frac-mathrm-d-mathbf-A-mathrm-T-mathbf-x-mathrm-d-mathbf-x-frac-mathrm-d-mathbf-x-mathrm-T-mathbf-A-mathrm-d-mathbf-x-mathbf-A-mathrm-T-（分子布局）"><a href="#frac-mathrm-d-mathbf-A-mathrm-T-mathbf-x-mathrm-d-mathbf-x-frac-mathrm-d-mathbf-x-mathrm-T-mathbf-A-mathrm-d-mathbf-x-mathbf-A-mathrm-T-（分子布局）" class="headerlink" title="$\frac{\mathrm{d}\mathbf{A}^\mathrm{T}\mathbf{x}}{\mathrm{d}\mathbf{x}} = \frac{\mathrm{d}\mathbf{x}^\mathrm{T}\mathbf{A}}{\mathrm{d}\mathbf{x}} = \mathbf{A}^\mathrm{T}$（分子布局）"></a>$\frac{\mathrm{d}\mathbf{A}^\mathrm{T}\mathbf{x}}{\mathrm{d}\mathbf{x}} = \frac{\mathrm{d}\mathbf{x}^\mathrm{T}\mathbf{A}}{\mathrm{d}\mathbf{x}} = \mathbf{A}^\mathrm{T}$（分子布局）</h3><p>其中：</p><script type="math/tex; mode=display">\mathbf{A}=\begin{bmatrix}a_1 \\a_2 \\\vdots \\a_n\end{bmatrix}\quad\mathbf{x}=\begin{bmatrix}x_1 \\x_2 \\\vdots \\x_n\end{bmatrix}</script><p>分子是一个标量，按照分子布局展开为行向量，计算过程如下：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\mathrm{d}\mathbf{A}^\mathrm{T}\mathbf{x}}{\mathrm{d}\mathbf{x}}&= \frac{\mathrm{d}(\sum_{i = 1}^na_ix_i)}{\mathrm{d}\mathbf{x}} \\&= \begin{bmatrix}\frac{\mathrm{d}(\sum_{i = 1}^na_ix_i)}{\mathrm{d}x_1}, & \frac{\mathrm{d}(\sum_{i = 1}^na_ix_i)}{\mathrm{d}x_2}, & \cdots, & \frac{\mathrm{d}(\sum_{i = 1}^na_ix_i)}{\mathrm{d}x_n}\end{bmatrix} \\&= \begin{bmatrix}a_1, & a_2, & \cdots, & a_n\end{bmatrix} \\&= \mathbf{A}^\mathrm{T}\end{aligned}</script><p>并且由于这里$\mathbf{A}^\mathrm{T}\mathbf{x}$是一个标量，那么有$\mathbf{A}^\mathrm{T}\mathbf{x} = (\mathbf{A}^\mathrm{T}\mathbf{x})^\mathrm{T} = \mathbf{x}^\mathrm{T}\mathbf{A} = \sum_{i = 1}^na_ix_i$。</p><h3 id="frac-mathrm-d-mathbf-x-mathrm-T-mathbf-Ax-mathrm-d-mathbf-x-mathbf-x-mathrm-T-mathbf-A-mathbf-A-mathrm-T-（分子布局）"><a href="#frac-mathrm-d-mathbf-x-mathrm-T-mathbf-Ax-mathrm-d-mathbf-x-mathbf-x-mathrm-T-mathbf-A-mathbf-A-mathrm-T-（分子布局）" class="headerlink" title="$\frac{\mathrm{d}\mathbf{x}^\mathrm{T}\mathbf{Ax}}{\mathrm{d}\mathbf{x}} = \mathbf{x}^\mathrm{T}(\mathbf{A} + \mathbf{A}^\mathrm{T})$（分子布局）"></a>$\frac{\mathrm{d}\mathbf{x}^\mathrm{T}\mathbf{Ax}}{\mathrm{d}\mathbf{x}} = \mathbf{x}^\mathrm{T}(\mathbf{A} + \mathbf{A}^\mathrm{T})$（分子布局）</h3><p>其中：</p><script type="math/tex; mode=display">\mathbf{x}=\begin{bmatrix}x_1 \\x_2 \\\vdots \\x_n\end{bmatrix}\quad\mathbf{A}=\begin{bmatrix}a_{11}, & a_{12}, & \cdots, & a_{1n} \\a_{21}, & a_{22}, & \cdots, & a_{2n} \\\vdots & \vdots & & \vdots \\a_{n1}, & a_{n2}, & \cdots, & a_{nn}\end{bmatrix}</script><p>首先计算一下分子：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{x}^\mathrm{T}\mathbf{Ax}&= \begin{bmatrix}x_1, & x_2, & \cdots, & x_n\end{bmatrix}\begin{bmatrix}a_{11}, & a_{12}, & \cdots, & a_{1n} \\a_{21}, & a_{22}, & \cdots, & a_{2n} \\\vdots & \vdots & & \vdots \\a_{n1}, & a_{n2}, & \cdots, & a_{nn}\end{bmatrix}\begin{bmatrix}x_1 \\x_2 \\\vdots \\x_n\end{bmatrix} \\&= \begin{bmatrix}\sum_{i = 1}^na_{i1}x_i, & \sum_{i = 1}^na_{i2}x_i, & \cdots, & \sum_{i = 1}^na_{in}x_i\end{bmatrix}\begin{bmatrix}x_1 \\x_2 \\\vdots \\x_n\end{bmatrix} \\&= \sum_{i = 1}^n\sum_{j = 1}^na_{ij}x_ix_j\end{aligned}</script><p>那么可在计算中将分子替换为$\sum_{i = 1}^n\sum_{j = 1}^na_{ij}x_ix_j$，分子是一个标量，按照分子布局展开为行向量，最后计算过程如下：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\mathrm{d}\mathbf{x}^\mathrm{T}\mathbf{Ax}}{\mathrm{d}\mathbf{x}}&= \frac{\mathrm{d}(\sum_{i = 1}^n\sum_{j = 1}^na_{ij}x_ix_j)}{\mathrm{d}\mathbf{x}} \\&= \begin{bmatrix}\frac{\mathrm{d}(\sum_{i = 1}^n\sum_{j = 1}^na_{ij}x_ix_j)}{\mathrm{d}x_1}, & \frac{\mathrm{d}(\sum_{i = 1}^n\sum_{j = 1}^na_{ij}x_ix_j)}{\mathrm{d}x_2}, & \cdots, & \frac{\mathrm{d}(\sum_{i = 1}^n\sum_{j = 1}^na_{ij}x_ix_j)}{\mathrm{d}x_n}\end{bmatrix} \\&= \begin{bmatrix}\sum_{j = 1}^na_{1j}x_j + \sum_{i = 1}^na_{i1}x_i, & \sum_{j = 1}^na_{2j}x_j + \sum_{i = 1}^na_{i2}x_i, & \cdots, & \sum_{j = 1}^na_{nj}x_j + \sum_{i = 1}^na_{in}x_i\end{bmatrix} \\&= \begin{bmatrix}\sum_{j = 1}^na_{1j}x_j, & \sum_{j = 1}^na_{2j}x_j, & \cdots, & \sum_{j = 1}^na_{nj}x_j\end{bmatrix} \\&\quad +\begin{bmatrix}\sum_{i = 1}^na_{i1}x_i, & \sum_{i = 1}^na_{i2}x_i, & \cdots, & \sum_{i = 1}^na_{in}x_i\end{bmatrix} \\&= \begin{bmatrix}x_1, & x_2, & \cdots, & x_n\end{bmatrix}\begin{bmatrix}a_{11}, & a_{21}, & \cdots, & a_{n1} \\a_{12}, & a_{22}, & \cdots, & a_{n2} \\\vdots & \vdots & & \vdots \\a_{1n}, & a_{2n}, & \cdots, & a_{nn}\end{bmatrix} \\&\quad +\begin{bmatrix}x_1, & x_2, & \cdots, & x_n\end{bmatrix}\begin{bmatrix}a_{11}, & a_{12}, & \cdots, & a_{1n} \\a_{21}, & a_{22}, & \cdots, & a_{2n} \\\vdots & \vdots & & \vdots \\a_{n1}, & a_{n2}, & \cdots, & a_{nn}\end{bmatrix} \\&= \mathbf{x}^\mathrm{T}\mathbf{A}^\mathrm{T} + \mathbf{x}^\mathrm{T}\mathbf{A} \\&= \mathbf{x}^\mathrm{T}(\mathbf{A} + \mathbf{A}^\mathrm{T})\end{aligned}</script><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>首先：</p><script type="math/tex; mode=display">\mathbf{u}=\begin{bmatrix}u_1(x) \\u_2(x) \\\vdots \\u_n(x)\end{bmatrix}_{n \times 1}\quad\mathbf{v}=\begin{bmatrix}v_1(x) \\v_2(x) \\\vdots \\v_n(x)\end{bmatrix}_{n \times 1}\quad\mathbf{x}=\begin{bmatrix}x_1 \\x_2 \\\vdots \\x_n\end{bmatrix}_{n \times 1}</script><h3 id="乘法公式"><a href="#乘法公式" class="headerlink" title="乘法公式"></a>乘法公式</h3><p>那么有乘法公式：</p><script type="math/tex; mode=display">\frac{\mathrm{d}\mathbf{u}^\mathrm{T}\mathbf{v}}{\mathrm{d}\mathbf{x}} = \frac{\partial\mathbf{u}}{\partial\mathbf{x}}\mathbf{v} + \frac{\partial\mathbf{v}}{\partial\mathbf{x}}\mathbf{u}</script><h3 id="加法公式"><a href="#加法公式" class="headerlink" title="加法公式"></a>加法公式</h3><p>那么有加法公式：</p><script type="math/tex; mode=display">\frac{\mathrm{d}(\mathbf{u} + \mathbf{v})}{\mathrm{d}\mathbf{x}} = \frac{\mathrm{d}\mathbf{u}}{\mathrm{d}\mathbf{x}} + \frac{\mathrm{d}\mathbf{v}}{\mathrm{d}\mathbf{x}}</script>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 06 矩阵计算。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 05 线性代数</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/05/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/05/</id>
    <published>2023-10-02T03:00:00.000Z</published>
    <updated>2023-10-02T14:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="05-线性代数"><a href="#05-线性代数" class="headerlink" title="05 线性代数"></a>05 线性代数</h1><h2 id="标量"><a href="#标量" class="headerlink" title="标量"></a>标量</h2><ul><li><p>简单操作</p><script type="math/tex; mode=display">  \begin{aligned}  c &= a + b \\  c &= a \cdot b \\  c &= \sin a  \end{aligned}</script></li><li><p>长度</p><script type="math/tex; mode=display">  \begin{aligned}  \left|a\right|  &=  \begin{aligned}  \begin{cases}  a &\text{if } a > 0, \\  -a &\text{otherwise}  \end{cases}  \end{aligned} \\  \left|a + b\right| &\le \left|a\right| + \left|b\right| \\  \left|a \cdot b\right| &= \left|a\right| \cdot \left|b\right|  \end{aligned}</script></li></ul><h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><ul><li><p>简单操作</p><script type="math/tex; mode=display">  \begin{array}{ll}  c = a + b & \text {where } c_i = a_i + b_i \\  c = a \cdot b & \text {where } c_i = a_ib_i \\  c = \sin a & \text {where } c_{i} = \sin a_i  \end{array}</script></li><li><p>长度</p><script type="math/tex; mode=display">  \begin{aligned}  \Vert a\Vert_2 &= \left[\sum_{i = 1}^m a_i^2\right]^{\frac{1}{2}} \\  \Vert a\Vert & \ge 0 \text{ for all } a \\  \Vert a + b\Vert & \le \Vert a\Vert + \Vert b\Vert \\  \Vert a \cdot b\Vert &= \left|a\right| \cdot \Vert b\Vert  \end{aligned}</script></li><li><p>点乘</p><script type="math/tex; mode=display">  a^\mathrm{T}b = \sum_i a_ib_i</script></li><li><p>正交</p><script type="math/tex; mode=display">  a^\mathrm{T}b = \sum_i a_ib_i = 0</script></li></ul><h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><ul><li><p>简单操作</p><script type="math/tex; mode=display">  \begin{array}{ll}  C = A + B &\text{where } C_{ij} = A_{ij} + B_{ij} \\  C = \alpha \cdot B &\text{where } C_{ij} = \alpha B_{ij} \\  C = \sin A &\text{where } C_{ij} = \sin A_{ij}  \end{array}</script></li><li><p>乘法（矩阵乘以向量）</p><script type="math/tex; mode=display">  c = Ab \text{ where } c_i = \sum_j A_{ij} b_j</script></li><li><p>乘法（矩阵乘以矩阵）</p><script type="math/tex; mode=display">  C = AB \text{ where } C_{ik} = \sum_j A_{ij} B_{jk}</script></li><li><p>范数</p><script type="math/tex; mode=display">  c = A \cdot b \text{ hence } \Vert c\Vert \le \Vert A\Vert \cdot \Vert b\Vert</script><ul><li>取决于如何衡量b和c的长度</li></ul></li><li><p>常见范数</p><ul><li><p>矩阵范数：最小的满足上面公式的值</p></li><li><p>Frobenius范数</p><script type="math/tex; mode=display">  \Vert A\Vert_{\text{Frob}} = \left[\sum_{ij} A_{ij}^2\right]^\frac{1}{2}</script></li></ul></li><li><p>特殊矩阵</p><ul><li><p>对称和反对称</p><script type="math/tex; mode=display">  A_{ij} = A_{ji} \text{ and } A_{ij} = -A_{ji}</script></li><li><p>正定</p><script type="math/tex; mode=display">  \Vert x\Vert^2 = x^\mathrm{T}x \ge 0 \text{ generalizes to } x^\mathrm{T}Ax \ge 0</script></li><li><p>正交矩阵</p><ul><li>所有行都相互正交</li><li>所有行都有单位长度</li><li>可以写成$UU^\mathrm{T} = E$</li></ul></li><li><p>置换矩阵</p><ul><li>置换矩阵是正交矩阵<script type="math/tex; mode=display">  P \text{ where } P_{ij} = 1 \text{ if and only if } j = \pi(i)</script></li></ul></li></ul></li><li><p>特征向量和特征值</p><ul><li><p>不被矩阵改变方向的向量</p><script type="math/tex; mode=display">  Ax = \lambda x</script></li><li><p>对称矩阵总是可以找到特征向量</p></li></ul></li></ul><h2 id="线性代数实现"><a href="#线性代数实现" class="headerlink" title="线性代数实现"></a>线性代数实现</h2><p>标量由只有一个元素的张量表示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">3.0</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line">x + y, x * y, x / y, x ** y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))</span><br></pre></td></tr></table></figure><p>你可以将向量视为标量值组成的列表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([0, 1, 2, 3])</span><br></pre></td></tr></table></figure><p>通过张量的索引来访问任一元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(3)</span><br></pre></td></tr></table></figure><p>访问张量的长度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(x)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure><p>只有一个轴的张量，形状只有一个元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([4])</span><br></pre></td></tr></table></figure><p>通过指定两个分量$m$和$n$来创建一个形状为$m \times n$的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">A</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11],</span><br><span class="line">        [12, 13, 14, 15],</span><br><span class="line">        [16, 17, 18, 19]])</span><br></pre></td></tr></table></figure><p>矩阵的转置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  4,  8, 12, 16],</span><br><span class="line">        [ 1,  5,  9, 13, 17],</span><br><span class="line">        [ 2,  6, 10, 14, 18],</span><br><span class="line">        [ 3,  7, 11, 15, 19]])</span><br></pre></td></tr></table></figure><p>对称矩阵（symmetric matrix）$\mathbf{A}$等于其转置：$\mathbf{A} = \mathbf{A}^\mathrm{T}$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">B</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [2, 0, 4],</span><br><span class="line">        [3, 4, 5]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">B == B.T</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[True, True, True],</span><br><span class="line">        [True, True, True],</span><br><span class="line">        [True, True, True]])</span><br></pre></td></tr></table></figure><p>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0,  1,  2,  3],</span><br><span class="line">         [ 4,  5,  6,  7],</span><br><span class="line">         [ 8,  9, 10, 11]],</span><br><span class="line"></span><br><span class="line">        [[12, 13, 14, 15],</span><br><span class="line">         [16, 17, 18, 19],</span><br><span class="line">         [20, 21, 22, 23]]])</span><br></pre></td></tr></table></figure><p>给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">B = A.clone() <span class="comment"># 通过分配新内存，将A的一个副本分配给B</span></span><br><span class="line">A, A + B</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.],</span><br><span class="line">         [12., 13., 14., 15.],</span><br><span class="line">         [16., 17., 18., 19.]]),</span><br><span class="line"> tensor([[ 0.,  2.,  4.,  6.],</span><br><span class="line">         [ 8., 10., 12., 14.],</span><br><span class="line">         [16., 18., 20., 22.],</span><br><span class="line">         [24., 26., 28., 30.],</span><br><span class="line">         [32., 34., 36., 38.]]))</span><br></pre></td></tr></table></figure><p>两个矩阵的按元素乘法称为哈达玛积（Hadamard product）（数学符号$\odot$）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  0.,   1.,   4.,   9.],</span><br><span class="line">        [ 16.,  25.,  36.,  49.],</span><br><span class="line">        [ 64.,  81., 100., 121.],</span><br><span class="line">        [144., 169., 196., 225.],</span><br><span class="line">        [256., 289., 324., 361.]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">2</span></span><br><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a + X, (a * X).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[[ 2,  3,  4,  5],</span><br><span class="line">          [ 6,  7,  8,  9],</span><br><span class="line">          [10, 11, 12, 13]],</span><br><span class="line"></span><br><span class="line">         [[14, 15, 16, 17],</span><br><span class="line">          [18, 19, 20, 21],</span><br><span class="line">          [22, 23, 24, 25]]]),</span><br><span class="line"> torch.Size([2, 3, 4]))</span><br></pre></td></tr></table></figure><p>计算其元素的和：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">x, x.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0., 1., 2., 3.]), tensor(6.))</span><br></pre></td></tr></table></figure><p>表示任意形状张量的元素和：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span> * <span class="number">2</span>).reshape(<span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">A.shape, A.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([2, 5, 4]), tensor(780))</span><br></pre></td></tr></table></figure><p>指定求和汇总张量的轴：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[20, 22, 24, 26],</span><br><span class="line">         [28, 30, 32, 34],</span><br><span class="line">         [36, 38, 40, 42],</span><br><span class="line">         [44, 46, 48, 50],</span><br><span class="line">         [52, 54, 56, 58]]),</span><br><span class="line"> torch.Size([5, 4]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 40,  45,  50,  55],</span><br><span class="line">         [140, 145, 150, 155]]),</span><br><span class="line"> torch.Size([2, 4]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>]), A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>]).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([180, 190, 200, 210]), torch.Size([4]))</span><br></pre></td></tr></table></figure><p>一个与求和相关的量是平均值（mean或average）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.mean(dtype=torch.float32), A.<span class="built_in">sum</span>() / A.numel()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(19.5000), tensor(19.5000))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>, dtype=torch.float32), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[10., 11., 12., 13.],</span><br><span class="line">         [14., 15., 16., 17.],</span><br><span class="line">         [18., 19., 20., 21.],</span><br><span class="line">         [22., 23., 24., 25.],</span><br><span class="line">         [26., 27., 28., 29.]]),</span><br><span class="line"> tensor([[10., 11., 12., 13.],</span><br><span class="line">         [14., 15., 16., 17.],</span><br><span class="line">         [18., 19., 20., 21.],</span><br><span class="line">         [22., 23., 24., 25.],</span><br><span class="line">         [26., 27., 28., 29.]]))</span><br></pre></td></tr></table></figure><p>计算总和或均值时保持轴数不变：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">sum_A, sum_A.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 6],</span><br><span class="line">         [22],</span><br><span class="line">         [38],</span><br><span class="line">         [54],</span><br><span class="line">         [70]]),</span><br><span class="line"> torch.Size([5, 1]))</span><br></pre></td></tr></table></figure><p>通过广播将<code>A</code>除以<code>sum_A</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, A / sum_A</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0.0000, 0.1667, 0.3333, 0.5000],</span><br><span class="line">        [0.1818, 0.2273, 0.2727, 0.3182],</span><br><span class="line">        [0.2105, 0.2368, 0.2632, 0.2895],</span><br><span class="line">        [0.2222, 0.2407, 0.2593, 0.2778],</span><br><span class="line">        [0.2286, 0.2429, 0.2571, 0.2714]])</span><br></pre></td></tr></table></figure><p>某个轴计算<code>A</code>元素的累积总和：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, A.cumsum(axis=<span class="number">0</span>), A.cumsum(axis=<span class="number">0</span>).shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0,  1,  2,  3],</span><br><span class="line">         [ 4,  5,  6,  7],</span><br><span class="line">         [ 8,  9, 10, 11],</span><br><span class="line">         [12, 13, 14, 15],</span><br><span class="line">         [16, 17, 18, 19]]),</span><br><span class="line"> tensor([[ 0,  1,  2,  3],</span><br><span class="line">         [ 4,  6,  8, 10],</span><br><span class="line">         [12, 15, 18, 21],</span><br><span class="line">         [24, 28, 32, 36],</span><br><span class="line">         [40, 45, 50, 55]]),</span><br><span class="line"> torch.Size([5, 4]))</span><br></pre></td></tr></table></figure><p>点积是相同位置的按元素乘积的和：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.ones(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">x, y, torch.dot(x, y)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))</span><br></pre></td></tr></table></figure><p>我们可以通过执行按元素乘法，然后求和来表示两个向量的点积：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(6.)</span><br></pre></td></tr></table></figure><p>矩阵向量积$\mathrm{Ax}$是一个长度为$m$的列向量，其$i^\text{th}$元素是点积$\mathrm{a}_i^\mathbf{T}\mathrm{x}$：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A, x)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))</span><br></pre></td></tr></table></figure><p>我们可以将矩阵-矩阵乘法$\mathbf{AB}$看作是简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \times m$矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B = torch.ones(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">A, torch.mm(A, B)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.],</span><br><span class="line">         [12., 13., 14., 15.],</span><br><span class="line">         [16., 17., 18., 19.]]),</span><br><span class="line"> tensor([[ 6.,  6.,  6.],</span><br><span class="line">         [22., 22., 22.],</span><br><span class="line">         [38., 38., 38.],</span><br><span class="line">         [54., 54., 54.],</span><br><span class="line">         [70., 70., 70.]]))</span><br></pre></td></tr></table></figure><p>$L_2$范数是向量元素平方和的平方根：</p><script type="math/tex; mode=display">\Vert\mathbf{x}\Vert_2 = \sqrt{\sum_{i = 1}^n x_i^2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(5.)</span><br></pre></td></tr></table></figure><p>$L_1$范数是向量元素的绝对值之和：</p><script type="math/tex; mode=display">\Vert\mathbf{x}\Vert = \sum_{i = 1}^n \left|x_i\right|</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(7.)</span><br></pre></td></tr></table></figure><p>矩阵的弗罗贝尼乌斯范数（Frobenius norm）是矩阵元素的平方和的平方根：</p><script type="math/tex; mode=display">\Vert\mathbf{X}\Vert_{F} = \sqrt{\sum_{i = 1}^m \sum_{j = 1}^n x_{ij}^2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones(<span class="number">4</span>, <span class="number">9</span>))</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(6.)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 05 线性代数。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 04 数据操作 + 数据预处理</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/04/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/04/</id>
    <published>2023-10-02T02:00:00.000Z</published>
    <updated>2023-10-02T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="04-数据操作-数据预处理"><a href="#04-数据操作-数据预处理" class="headerlink" title="04 数据操作 + 数据预处理"></a>04 数据操作 + 数据预处理</h1><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><h3 id="N维数组样例"><a href="#N维数组样例" class="headerlink" title="N维数组样例"></a>N维数组样例</h3><ul><li>N维数组是机器学习和神经网络的主要数据结构</li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/04/image-20231002105507321.png" alt="image-20231002105507321"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/04/image-20231002105525454.png" alt="image-20231002105525454"></p><h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h3><ul><li>创建数组需要<ul><li>形状：例如3 x 4矩阵</li><li>每个元素的数据类型：例如32位浮点数</li><li>每个元素的值：例如全是0，或者随机数</li></ul></li></ul><p><img src="https://img.karltan.com/notes-out-class/d2l/04/image-20231002105753869.png" alt="image-20231002105753869"></p><h3 id="访问元素"><a href="#访问元素" class="headerlink" title="访问元素"></a>访问元素</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/04/image-20231002110145573.png" alt="image-20231002110145573"></p><h2 id="数据操作实现"><a href="#数据操作实现" class="headerlink" title="数据操作实现"></a>数据操作实现</h2><p>首先导入<code>torch</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>张量表示一个数值组成的数组，这个数组可能有多个维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</span><br></pre></td></tr></table></figure><p>我们可以通过张量的<code>shape</code>属性来访问张量的形状和张量中元素的总数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([12])</span><br></pre></td></tr></table></figure><p>或者：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.numel()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12</span><br></pre></td></tr></table></figure><p>要改变一个张量的形状而不改变元素数量和元素值，我们可以调用<code>reshape</code>函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = x.reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0,  1,  2,  3],</span><br><span class="line">        [ 4,  5,  6,  7],</span><br><span class="line">        [ 8,  9, 10, 11]])</span><br></pre></td></tr></table></figure><p>使用全0、全1、其他常量或者从特定分布中随机采样的数字：</p><ul><li><p>全0：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>  输出为：</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]],</span><br><span class="line"></span><br><span class="line">        [[0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.],</span><br><span class="line">         [0., 0., 0., 0.]]])</span><br></pre></td></tr></table></figure></li><li><p>全1：</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>  输出为：</p>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.],</span><br><span class="line">         [1., 1., 1., 1.]]])</span><br></pre></td></tr></table></figure></li></ul><p>通过提供包含数值的Python列表（或嵌套列表）来为所需张量中的每个元素赋予确定值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[2, 1, 4, 3],</span><br><span class="line">        [1, 2, 3, 4],</span><br><span class="line">        [4, 3, 2, 1]])</span><br></pre></td></tr></table></figure><p>常见的标准算数运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">x + y, x - y, x * y, x / y, x ** y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 3.,  4.,  6., 10.]),</span><br><span class="line"> tensor([-1.,  0.,  2.,  6.]),</span><br><span class="line"> tensor([ 2.,  4.,  8., 16.]),</span><br><span class="line"> tensor([0.5000, 1.0000, 2.0000, 4.0000]),</span><br><span class="line"> tensor([ 1.,  4., 16., 64.]))</span><br></pre></td></tr></table></figure><p>按元素方式应用更多的计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</span><br></pre></td></tr></table></figure><p>我们也可以把多个张量连接在一起：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), torch.cat((X, Y), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.],</span><br><span class="line">         [ 2.,  1.,  4.,  3.],</span><br><span class="line">         [ 1.,  2.,  3.,  4.],</span><br><span class="line">         [ 4.,  3.,  2.,  1.]]),</span><br><span class="line"> tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],</span><br><span class="line">         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],</span><br><span class="line">         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</span><br></pre></td></tr></table></figure><p>通过逻辑运算符构建二元张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X == Y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[False,  True, False,  True],</span><br><span class="line">        [False, False, False, False],</span><br><span class="line">        [False, False, False, False]])</span><br></pre></td></tr></table></figure><p>对张量中的所有元素进行求和会产生一个只有一个元素的张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(66.)</span><br></pre></td></tr></table></figure><p>即使形状不同，我们仍然可以通过调用广播机制（boardcasting mechanism）来执行按元素操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[0],</span><br><span class="line">         [1],</span><br><span class="line">         [2]]),</span><br><span class="line"> tensor([[0, 1]]))</span><br></pre></td></tr></table></figure><p>然后：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + b</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 1],</span><br><span class="line">        [1, 2],</span><br><span class="line">        [2, 3]])</span><br></pre></td></tr></table></figure><p>可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[-<span class="number">1</span>], X[<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tensor([ 8.,  9., 10., 11.]),</span><br><span class="line"> tensor([[ 4.,  5.,  6.,  7.],</span><br><span class="line">         [ 8.,  9., 10., 11.]]))</span><br></pre></td></tr></table></figure><p>除读取外，我们还可以通过指定索引来将元素写入矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.,  1.,  2.,  3.],</span><br><span class="line">        [ 4.,  5.,  9.,  7.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure><p>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[12., 12., 12., 12.],</span><br><span class="line">        [12., 12., 12., 12.],</span><br><span class="line">        [ 8.,  9., 10., 11.]])</span><br></pre></td></tr></table></figure><p>运行一些操作可能会导致为新结果分配内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line"><span class="built_in">id</span>(Y) == before</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><p>执行原地操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br></pre></td></tr></table></figure><p>输出为（注意，你的输出可能会和下面的输出不同，但是两个<code>id(Z)</code>应相等）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">id(Z): 2247481849472</span><br><span class="line">id(Z): 2247481849472</span><br></pre></td></tr></table></figure><p>如果在后续计算中没有重复使用<code>X</code>，我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(X)</span><br><span class="line">X += Y</span><br><span class="line"><span class="built_in">id</span>(X) == before</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure><p>转换为<code>NumPy</code>张量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(numpy.ndarray, torch.Tensor)</span><br></pre></td></tr></table></figure><p>将大小为1的张量转换为Python标量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([3.5000]), 3.5, 3.5, 3)</span><br></pre></td></tr></table></figure><h2 id="数据预处理实现"><a href="#数据预处理实现" class="headerlink" title="数据预处理实现"></a>数据预处理实现</h2><p>创建一个人工数据集，并存储在csv（逗号分隔值）文件中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)   <span class="comment"># 列名</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)         <span class="comment"># 每行表示一个数据样本</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p>从创建的csv文件中加载原始数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms Alley   Price</span><br><span class="line">0       NaN  Pave  127500</span><br><span class="line">1       2.0   NaN  106000</span><br><span class="line">2       4.0   NaN  178100</span><br><span class="line">3       NaN   NaN  140000</span><br></pre></td></tr></table></figure><p>为了处理缺失的数据，典型的方法包括插值和删除，这里我们将考虑插值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line"><span class="comment"># inputs = inputs.fillna(inputs.mean()) # 会报错</span></span><br><span class="line">inputs[<span class="string">&#x27;NumRooms&#x27;</span>] = inputs[<span class="string">&#x27;NumRooms&#x27;</span>].fillna(inputs[<span class="string">&#x27;NumRooms&#x27;</span>].mean())</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><blockquote><p>这里要使用<code>inputs[&#39;NumRooms&#39;]</code>限定修改的是<code>&#39;NumRooms&#39;</code>这一列，然后后面的修改也要基于<code>inputs[&#39;NumRooms&#39;]</code>进行。</p></blockquote><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms Alley</span><br><span class="line">0       3.0  Pave</span><br><span class="line">1       2.0   NaN</span><br><span class="line">2       4.0   NaN</span><br><span class="line">3       3.0   NaN</span><br></pre></td></tr></table></figure><p>对于<code>inputs</code>中的类别值或离散值，我们将<code>NaN</code>视为一个类别：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>).astype(<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><blockquote><p>这里要在最后调用一个<code>.astype(float)</code>方法，将其中所有的值转换为<code>float</code>型。</p></blockquote><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms  Alley_Pave  Alley_nan</span><br><span class="line">0       3.0         1.0        0.0</span><br><span class="line">1       2.0         0.0        1.0</span><br><span class="line">2       4.0         0.0        1.0</span><br><span class="line">3       3.0         0.0        1.0</span><br></pre></td></tr></table></figure><p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[3., 1., 0.],</span><br><span class="line">         [2., 0., 1.],</span><br><span class="line">         [4., 0., 1.],</span><br><span class="line">         [3., 0., 1.]], dtype=torch.float64),</span><br><span class="line"> tensor([127500, 106000, 178100, 140000]))</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 04 数据操作 + 数据预处理。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 03 安装</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/03/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/03/</id>
    <published>2023-10-01T15:00:00.000Z</published>
    <updated>2023-10-01T15:30:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="03-安装"><a href="#03-安装" class="headerlink" title="03 安装"></a>03 安装</h1><p>可直接查看<a href="https://zh-v2.d2l.ai/chapter_installation/index.html">安装 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a>进行安装，如果在实操过程中出现问题可在该博客下留言，或者直接给我发邮件。</p><p>我这里直接以本地Ubuntu环境作为示例。</p><ol><li><p>首先更新软件源：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><blockquote><p>注意，如果这里更新失败，那么需要更新软件源：</p><p>这里提供两个好用的软件源：</p><ol><li><a href="https://mirror.tuna.tsinghua.edu.cn/help/ubuntu/">ubuntu | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror</a></li><li><a href="https://mirrors.ustc.edu.cn/help/ubuntu.html">Ubuntu 源使用帮助 — USTC Mirror Help 文档</a></li></ol><p>具体来说就是替换<code>/etc/apt/sources.list</code>文件中的内容（注意备份）。</p></blockquote></li><li><p>安装必要依赖：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential python3 python3-pip</span><br></pre></td></tr></table></figure></li><li><p>安装Miniconda：</p><p> 在<a href="https://docs.conda.io/projects/miniconda/en/latest/">Miniconda — miniconda documentation</a>中找到适合的版本，使用<code>wget</code>命令下载到本地，然后使用<code>bash 软件包名</code>的方式运行。</p><blockquote><p>注意，国内用户最好再进行一次换源，这里同样提供两个好用的源：</p><ol><li><a href="https://mirror.tuna.tsinghua.edu.cn/help/anaconda/">anaconda | 镜像站使用帮助 | 清华大学开源软件镜像站 | Tsinghua Open Source Mirror</a></li><li><a href="https://mirrors.ustc.edu.cn/help/anaconda.html">Anaconda 源使用帮助 — USTC Mirror Help 文档</a></li></ol><p>简单来说就是修改<code>~/.condarc</code>文件中的内容。</p></blockquote></li><li><p>使用conda创建虚拟环境：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name d2l python=3.8</span><br></pre></td></tr></table></figure><blockquote><p>注意，创建后，在安装软件包时，一定要记得使用<code>conda activate d2l</code>命令激活虚拟环境。</p></blockquote></li><li><p>在<code>d2l</code>虚拟环境中安装包：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install d2l torch torchvision torchaudio</span><br></pre></td></tr></table></figure><blockquote><p>注意，在安装<code>torch</code>，<code>torchvision</code>和<code>torchaudio</code>时，如果下载速度过慢，可在<a href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a>中下载。</p></blockquote></li><li><p>最后在VSCode中使用刚刚安装好的环境。</p></li></ol>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 03 安装。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 02 深度学习介绍</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/02/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/02/</id>
    <published>2023-10-01T14:00:00.000Z</published>
    <updated>2023-10-01T15:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="02-深度学习介绍"><a href="#02-深度学习介绍" class="headerlink" title="02 深度学习介绍"></a>02 深度学习介绍</h1><h2 id="AI地图"><a href="#AI地图" class="headerlink" title="AI地图"></a>AI地图</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224522285.png" alt="image-20231001224522285"></p><h2 id="图片分类"><a href="#图片分类" class="headerlink" title="图片分类"></a>图片分类</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224550031.png" alt="image-20231001224550031"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224609284.png" alt="image-20231001224609284"></p><h2 id="物体检测和分割"><a href="#物体检测和分割" class="headerlink" title="物体检测和分割"></a>物体检测和分割</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224632042.png" alt="image-20231001224632042"></p><h2 id="样式迁移"><a href="#样式迁移" class="headerlink" title="样式迁移"></a>样式迁移</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224743638.png" alt="image-20231001224743638"></p><h2 id="人脸合成"><a href="#人脸合成" class="headerlink" title="人脸合成"></a>人脸合成</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224757199.png" alt="image-20231001224757199"></p><h2 id="文字生成图片"><a href="#文字生成图片" class="headerlink" title="文字生成图片"></a>文字生成图片</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224809692.png" alt="image-20231001224809692"></p><h2 id="文字生成"><a href="#文字生成" class="headerlink" title="文字生成"></a>文字生成</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224835338.png" alt="image-20231001224835338"></p><h2 id="无人驾驶"><a href="#无人驾驶" class="headerlink" title="无人驾驶"></a>无人驾驶</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224850612.png" alt="image-20231001224850612"></p><h2 id="案例研究-广告点击"><a href="#案例研究-广告点击" class="headerlink" title="案例研究 - 广告点击"></a>案例研究 - 广告点击</h2><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224912749.png" alt="image-20231001224912749"></p><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224930327.png" alt="image-20231001224930327"></p><h3 id="预测与训练"><a href="#预测与训练" class="headerlink" title="预测与训练"></a>预测与训练</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001224947734.png" alt="image-20231001224947734"></p><h3 id="完整的故事"><a href="#完整的故事" class="headerlink" title="完整的故事"></a>完整的故事</h3><p><img src="https://img.karltan.com/notes-out-class/d2l/02/image-20231001225007273.png" alt="image-20231001225007273"></p>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 02 深度学习介绍。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 01 课程安排</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/01/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/01/</id>
    <published>2023-10-01T08:00:00.000Z</published>
    <updated>2023-10-01T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="01-课程安排"><a href="#01-课程安排" class="headerlink" title="01 课程安排"></a>01 课程安排</h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul><li>介绍深度学习经典和最新模型<ul><li>LeNet，ResNet，LSTM，BERT，…</li></ul></li><li>机器学习基础<ul><li>损失函数、目标函数、过拟合、优化</li></ul></li><li>实践<ul><li>使用Pytorch实现介绍的知识点</li><li>在真实数据上体验算法效果</li></ul></li></ul><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ul><li>深度学习基础 - 线性神经网络，多层感知机</li><li>卷积神经网络 - LeNet，AlexNet，VGG，Inception，ResNet</li><li>循环神经网络 - RNN，GRU，LSTM，seq2seq</li><li>注意力机制 - Attention，Transformer</li><li>优化算法 - SGD，Momentum，Adam</li><li>高性能计算 - 并行，多GPU，分布式</li><li>计算机视觉 - 目标检测，语义分割</li><li>自然语言处理 - 词嵌入，BERT</li></ul><h2 id="形式"><a href="#形式" class="headerlink" title="形式"></a>形式</h2><ul><li>每周六、日下午1:00 - 2:30直播</li><li>每次直播讲~4个小节</li><li>每个小节讲算法（10min），代码实现（10min），问答（5min）</li><li>每个月一次Kaggle竞赛</li><li>视频回放在B站和Youtube</li></ul><h2 id="你将学到什么"><a href="#你将学到什么" class="headerlink" title="你将学到什么"></a>你将学到什么</h2><ul><li>What - AI相关从业人员<ul><li>深度学习里有哪些技术</li></ul></li><li>How - 数据科学家、工程师<ul><li>如何实现和调参</li></ul></li><li>Why - 研究员、学生<ul><li>背后的原因（直觉、数学）</li></ul></li></ul><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><ul><li>课程主页：<a href="https://courses.d2l.ai/zh-v2/">课程安排 - 动手学深度学习课程 (d2l.ai)</a></li><li>教材：<a href="https://zh-v2.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li><li>课程论坛讨论：<a href="https://discuss.d2l.ai/c/chinese-version/16">Latest 中文版 topics - D2L Discussion</a></li><li>Pytorch论坛：<a href="https://discuss.pytorch.org/">PyTorch Forums</a></li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 01 课程安排。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>动手学深度学习v2 00 预告</title>
    <link href="http://blog.karltan.com/notes-out-class/d2l/00/"/>
    <id>http://blog.karltan.com/notes-out-class/d2l/00/</id>
    <published>2023-10-01T04:00:00.000Z</published>
    <updated>2023-10-01T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="00-预告"><a href="#00-预告" class="headerlink" title="00 预告"></a>00 预告</h1><p>学习深度学习关键是<strong>动手</strong></p><ul><li>深度学习是人工智能最重要的领域</li><li>核心是神经网络</li><li>神经网络是一门语言</li><li>应该像学习Python/C++一样学习深度学习</li></ul><p>什么是《动手学深度学习》</p><ul><li>是一本深度学习<strong>教科书</strong><ul><li>覆盖90年代至今的重要模型</li></ul></li><li>每一章都提供一个<strong>Jupyter记事本</strong><ul><li>提供所有模型的完整实现</li><li>在真实数据上运行</li></ul></li><li>内容<strong>免费</strong><ul><li><a href="https://zh.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li></ul></li></ul><p>项目起源于2017年的直播分享</p><p>中文版v1</p><ul><li>网页浏览用户65万+<ul><li><a href="https://zh.d2l.ai/">《动手学深度学习》 — 动手学深度学习 2.0.0 documentation (d2l.ai)</a></li></ul></li><li>源代码Github star 19.8k<ul><li><a href="https://github.com/d2l-ai/d2l-zh">d2l-ai/d2l-zh: 《动手学深度学习》：面向中文读者、能运行、可讨论。中英文版被70多个国家的500多所大学用于教学。 (github.com)</a></li></ul></li><li>实体版于19年出版<ul><li>5万本+销量</li></ul></li></ul><p>2019年推出英文版</p><ul><li>重写了所有章节</li><li>新加了50%新内容<ul><li>例如大红大紫的Transformer</li></ul></li><li>新增了Numpy/MXNet，Pytoch和TensorFlow 2.0的代码实现</li></ul><p>动手学深度学习v2</p><ul><li>2021年3月20日直播开课<ul><li>每周六、日下午1:00到2:30</li></ul></li><li>针对数据科学家、工程师和在校学生<ul><li>只需要最基础的数学和Python编程</li></ul></li><li>讲述算法、实现，在线答疑，课内比赛<ul><li>使用Pytorch代码实现</li></ul></li><li><a href="https://courses.d2l.ai/zh-v2/">课程安排 - 动手学深度学习课程 (d2l.ai)</a></li></ul>]]></content>
    
    
    <summary type="html">李沐《动手学深度学习v2》 00 预告。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="d2l" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/d2l/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="李沐" scheme="http://blog.karltan.com/tags/%E6%9D%8E%E6%B2%90/"/>
    
  </entry>
  
  <entry>
    <title>在线扩容阿里云服务器云盘</title>
    <link href="http://blog.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/"/>
    <id>http://blog.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/</id>
    <published>2023-09-27T02:00:00.000Z</published>
    <updated>2023-09-27T04:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在线扩容阿里云服务器云盘"><a href="#在线扩容阿里云服务器云盘" class="headerlink" title="在线扩容阿里云服务器云盘"></a>在线扩容阿里云服务器云盘</h1><h2 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h2><p>在购买这个服务器的时候，由于贪图便宜，所以只买了20G的硬盘，创建了几个<code>docker</code>容器之后，容量就不够用了，所以想扩容到40G。</p><p>然后服务器的操作系统是Ubuntu22.04。</p><h2 id="扩容过程"><a href="#扩容过程" class="headerlink" title="扩容过程"></a>扩容过程</h2><h3 id="在阿里云中购买云盘"><a href="#在阿里云中购买云盘" class="headerlink" title="在阿里云中购买云盘"></a>在阿里云中购买云盘</h3><ol><li><p>点击扩容：</p><p> <img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927122100861.png" alt="image-20230927122100861"></p></li><li><p>确认扩容：</p><p> <img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927122221607.png" alt="image-20230927122221607"></p></li><li><p>选择扩容后的容量：</p><p> <img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927122416075.png" alt="image-20230927122416075"></p><p> 点击确定扩容后付款即可，这里我是扩容结束后才截的图，并且后来又扩容到了100G，故这里显示的起始容量是101G。</p></li></ol><h3 id="确定云盘信息"><a href="#确定云盘信息" class="headerlink" title="确定云盘信息"></a>确定云盘信息</h3><p>首先执行<code>sudo fdisk -lu</code>，确认待扩容云盘及其分区信息：</p><p><img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927122750013.png" alt="image-20230927122750013"></p><p>注意下面的图：</p><p><img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927123920776.png" alt="image-20230927123920776"></p><p>其中最上方的红框是一个报错，在我们正确扩容分区和文件系统后，再次执行<code>sudo fdisk -lu</code>命令，该报错会消失。</p><p>对于②处，值为<code>dos</code>表示是MBR分区，值为<code>gpt</code>表示GPT分区。</p><p>然后执行<code>lsblk</code>，查看哪个分区待扩容：</p><p><img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927124410966.png" alt="image-20230927124410966"></p><p>可以看到是<code>/vda/vda3</code>这个分区待扩容。</p><h3 id="扩容分区"><a href="#扩容分区" class="headerlink" title="扩容分区"></a>扩容分区</h3><p>执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cloud-guest-utils gdisk</span><br><span class="line">sudo LC_ALL=en_US.UTF-8 growpart /dev/vda 3</span><br></pre></td></tr></table></figure><p>这里最后一条命令的参数<code>/dev/vda 3</code>与使用<code>lsblk</code>命令得到的<code>/vda/vda3</code>对应。</p><p>当命令执行输出<code>CHANGE</code>时，说明扩容成功：</p><p><img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927124910747.png" alt="image-20230927124910747"></p><p>但是执行<code>df -Th</code>查看存储空间大小时，会发现<code>/dev/vda3</code>分区的大小没有改变：</p><p><img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927125147940.png" alt="image-20230927125147940"></p><p>所以还需要扩容文件系统。</p><h3 id="扩容文件系统"><a href="#扩容文件系统" class="headerlink" title="扩容文件系统"></a>扩容文件系统</h3><p>由于需要调整<code>/dev/vda3</code>文件系统的大小，执行：<code>sudo resize2fs /dev/vda3</code>：</p><p><img src="https://img.karltan.com/environment/online-expansion-of-aliyun-server-cloud-disk/image-20230927125521238.png" alt="image-20230927125521238"></p><p>至此扩容结束。</p>]]></content>
    
    
    <summary type="html">本文记录了在阿里云Ubuntu22.04服务器上扩容云盘的过程。</summary>
    
    
    
    <category term="环境配置" scheme="http://blog.karltan.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="Ubuntu" scheme="http://blog.karltan.com/tags/Ubuntu/"/>
    
    <category term="阿里云" scheme="http://blog.karltan.com/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
    <category term="扩容" scheme="http://blog.karltan.com/tags/%E6%89%A9%E5%AE%B9/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5017. 垦田计划</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5017/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5017/</id>
    <published>2023-09-25T07:00:00.000Z</published>
    <updated>2023-09-25T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5017-垦田计划"><a href="#AcWing-5017-垦田计划" class="headerlink" title="AcWing 5017. 垦田计划"></a>AcWing 5017. 垦田计划</h1><p><a href="https://www.acwing.com/problem/content/5020/">5017. 垦田计划 - AcWing题库</a></p><p>顿顿总共选中了$n$，块区域准备开垦田地，由于各块区域大小不一，开垦所需时间也不尽相同。</p><p>据估算，其中第$i$块（$1 \le i \le n$）区域的开垦耗时为$t_i$天。</p><p>这$n$块区域可以同时开垦，所以总耗时$t_{Total}$取决于耗时最长的区域，即：</p><script type="math/tex; mode=display">t_{Total} = \max\{t_1, t_2, \cdots, t_n\}</script><p>为了加快开垦进度，顿顿准备在部分区域投入额外资源来缩短开垦时间。</p><p>具体来说：</p><ul><li>在第$i$块区域<strong>每</strong>投入$c_i$单位资源，便可将其开垦耗时缩短$1$天；</li><li>耗时缩短天数以整数记，即第$i$块区域投入资源数量必须是$c_i$的整数倍；</li><li>在第$i$块区域最多可投入$c_i \times (t_i - k)$单位资源，将其开垦耗时缩短为$k$天；</li><li>这里的$k$表示开垦一块区域的最少天数，满足$0 &lt; k \le \min\{t_1, t_2, \cdots, t_n\}$；换言之，如果无限制地投入资源，所有区域都可以用$k$天完成开垦。</li></ul><p>现在顿顿手中共有$m$单位资源可供使用，试计算开垦$n$块区域最少需要多少天？</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>输入共$n + 1$行。</p><p>输入的第一行包含空格分隔的三个正整数$n, m, k$，分别表示待开垦的区域总数、顿顿手上的资源数量和每块区域的最少开垦天数。</p><p>接下来$n$行，每行包含空格分隔的两个正整数$t_i$和$c_i$，分别表示第$i$块区域开垦耗时和将耗时缩短$1$天所需资源数量。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>输出一个整数，表示开垦$n$块区域的最少耗时。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&70\%的测试数据满足: 0 < n, t_i, c_i \le 100 且 0 < m \le 10^6; \\&全部的测试数据满足: 0 < n, t_i, c_i \le 10^5 且 0 < m \le 10^9。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4 9 2</span><br><span class="line">6 1</span><br><span class="line">5 1</span><br><span class="line">6 2</span><br><span class="line">7 1</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>如下表所示，投入$5$单位资源即可将总耗时缩短至$5$天。此时顿顿手中还剩余$4$单位资源，但无论如何安排，也无法使总耗时进一步缩短。</p><div class="table-container"><table><thead><tr><th style="text-align:center">$i$</th><th style="text-align:center">基础耗时</th><th style="text-align:center">$t_i$缩减$1$天所需资源</th><th style="text-align:center">$c_i$投入资源数量</th><th style="text-align:center">实际耗时</th></tr></thead><tbody><tr><td style="text-align:center">$1$</td><td style="text-align:center">$6$</td><td style="text-align:center">$1$</td><td style="text-align:center">$1$</td><td style="text-align:center">$5$</td></tr><tr><td style="text-align:center">$2$</td><td style="text-align:center">$5$</td><td style="text-align:center">$1$</td><td style="text-align:center">$0$</td><td style="text-align:center">$5$</td></tr><tr><td style="text-align:center">$3$</td><td style="text-align:center">$6$</td><td style="text-align:center">$2$</td><td style="text-align:center">$2$</td><td style="text-align:center">$5$</td></tr><tr><td style="text-align:center">$4$</td><td style="text-align:center">$7$</td><td style="text-align:center">$1$</td><td style="text-align:center">$2$</td><td style="text-align:center">$5$</td></tr></tbody></table></div><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">4 30 2</span><br><span class="line">6 1</span><br><span class="line">5 1</span><br><span class="line">6 2</span><br><span class="line">7 1</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>投入$20$单位资源，恰好可将所有区域开垦耗时均缩短为$k = 2$天；受限于$k$，剩余的单位资源无法使耗时进一步缩短。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>这是一个可以得70分的版本。</p><p>这里我写上来的原因是因为这个结构体比较代码，这里我想用一个最大堆来暴力解，那么最大堆涉及到堆内元素的排序，而这里堆内的元素是<code>pair</code>（因为要把天数和缩短一天所需的资源一起存储），所以需要自定义排序代码，代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; pii;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">compare</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> pii &amp;a, <span class="type">const</span> pii &amp;b)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a.first &lt; b.first;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n, m, k;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m &gt;&gt; k;</span><br><span class="line">    priority_queue&lt;pii, vector&lt;pii&gt;, compare&gt; heap;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> t, c;</span><br><span class="line">        cin &gt;&gt; t &gt;&gt; c;</span><br><span class="line">        heap.<span class="built_in">push</span>(&#123;t, c&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (heap.<span class="built_in">top</span>().first &gt;= k &amp;&amp; m &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        pii temp = heap.<span class="built_in">top</span>();</span><br><span class="line">        heap.<span class="built_in">pop</span>();</span><br><span class="line">        <span class="keyword">if</span> (m - temp.second &gt;= <span class="number">0</span> &amp; heap.<span class="built_in">top</span>().first - <span class="number">1</span> &gt;= k)</span><br><span class="line">        &#123;</span><br><span class="line">            temp.first -= <span class="number">1</span>;</span><br><span class="line">            m -= temp.second;</span><br><span class="line">            heap.<span class="built_in">push</span>(temp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            heap.<span class="built_in">push</span>(temp);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; heap.<span class="built_in">top</span>().first &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><h4 id="二分-O-n-log-n"><a href="#二分-O-n-log-n" class="headerlink" title="二分$O(n\log n)$"></a>二分$O(n\log n)$</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line"><span class="type">int</span> t[N], c[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 检查mid是否满足条件</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">(<span class="type">int</span> mid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 如果 该块田需要的天数t[i] &gt; 想达到的天数mid</span></span><br><span class="line">        <span class="keyword">if</span> (t[i] &gt; mid)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 需要加上代价</span></span><br><span class="line">            res += (t[i] - mid) * c[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 条件是 所用资源res &lt;= 总资源m</span></span><br><span class="line">    <span class="keyword">if</span> (res &lt;= m)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m &gt;&gt; k;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        cin &gt;&gt; t[i] &gt;&gt; c[i];</span><br><span class="line">    <span class="comment">// 二分</span></span><br><span class="line">    <span class="type">int</span> l = k, r = <span class="number">1e5</span>;</span><br><span class="line">    <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> mid = l + r &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 如果mid点满足条件，说明总时间还有可能减少</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>(mid))</span><br><span class="line">            r = mid;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            l = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; r &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="差分-O-n"><a href="#差分-O-n" class="headerlink" title="差分$O(n)$"></a>差分$O(n)$</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> LL;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n, m, k;</span><br><span class="line">LL b[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;n, &amp;m, &amp;k);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> t, c;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;t, &amp;c);</span><br><span class="line">        b[<span class="number">1</span>] += c, b[t + <span class="number">1</span>] -= c;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; N; i ++ ) b[i] += b[i - <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    LL cost = <span class="number">0</span>;</span><br><span class="line">    <span class="type">int</span> i = N - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> (i &gt; k &amp;&amp; cost + b[i] &lt;= m) cost += b[i -- ];</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, i);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="另外的思路-O-t-Total"><a href="#另外的思路-O-t-Total" class="headerlink" title="另外的思路$O(t_{Total})$"></a>另外的思路$O(t_{Total})$</h3><p><a href="https://www.acwing.com/solution/content/189158/">AcWing 5017. 垦田计划（非二分做法） - AcWing</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1e5</span> + <span class="number">10</span>;</span><br><span class="line"><span class="type">int</span> n, k, total, maxx; <span class="comment">// maxx 是最大的天数，也就是题目中的 t_&#123;Total&#125;</span></span><br><span class="line"><span class="type">int</span> m[N];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; total &gt;&gt; k;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; ++ i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> t, c;</span><br><span class="line">        cin &gt;&gt; t &gt;&gt; c;</span><br><span class="line">        m[t] += c; <span class="comment">// 合并天数为t的区域</span></span><br><span class="line">        maxx = <span class="built_in">max</span>(maxx, t);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 从最大天数开始找</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = maxx; i &gt;= k; -- i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 如果不够投入了，这就是最终答案，输出，结束程序</span></span><br><span class="line">        <span class="keyword">if</span> (total &lt; m[i])</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; i &lt;&lt; endl;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        total -= m[i];</span><br><span class="line">        <span class="comment">// 天数为i的区域投入资源后就变成了天数为i-1的区域</span></span><br><span class="line">        m[i - <span class="number">1</span>] += m[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 不能低于k天</span></span><br><span class="line">    cout &lt;&lt; k &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5017. 垦田计划。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5165. CCC单词搜索</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5165/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5165/</id>
    <published>2023-09-25T07:00:00.000Z</published>
    <updated>2023-09-25T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5165-CCC单词搜索"><a href="#AcWing-5165-CCC单词搜索" class="headerlink" title="AcWing 5165. CCC单词搜索"></a>AcWing 5165. CCC单词搜索</h1><p><a href="https://www.acwing.com/problem/content/5168/">5165. CCC单词搜索 - AcWing题库</a></p><p>给定一个$R \times C$的大写字母矩阵。</p><p>请你在其中寻找目标单词$W$。</p><p>已知，目标单词$W$由若干个<strong>不同的</strong>大写字母构成。</p><p>目标单词可以遵循以下两种规则，出现在矩阵的水平、垂直或斜$45$度线段中：</p><ul><li>单词出现在一条线段上。</li><li>单词出现在两条相互垂直且存在公共端点的线段上。也就是说，单词首先出现在某线段上，直到某个字母后，转向$90$度，其余部分出现在另一条线段上。</li></ul><p>具体可以参照图例。</p><p>请你计算，目标单词在给定矩阵中一共出现了多少次。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含一个由若干个<strong>不同的</strong>大写字母构成的字符串，表示单词$W$。</p><p>第二行包含整数$R$。</p><p>第三行包含整数$C$。</p><p>接下来$R$行，每行包含$C$个大写字母，表示给定字母矩阵。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>一个整数，表示目标单词在给定矩阵中的出现次数。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}2 &\le \left|W\right| \le 6, \\1 &\le R, C \le 100。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MENU</span><br><span class="line">5</span><br><span class="line">7</span><br><span class="line">F T R U B L K</span><br><span class="line">P M N A X C U</span><br><span class="line">A E R C N E O</span><br><span class="line">M N E U A R M</span><br><span class="line">M U N E M N S</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>目标单词一共出现$3$次，如下图所示。</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5165_case1.png" alt="acwing5165_case1"></p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">NATURE</span><br><span class="line">6</span><br><span class="line">9</span><br><span class="line">N A T S F E G Q N</span><br><span class="line">S A I B M R H F A</span><br><span class="line">C F T J C U C L T</span><br><span class="line">K B H U P T A N U</span><br><span class="line">D P R R R J D I R</span><br><span class="line">I E E K M E G B E</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>目标单词一共出现$4$次，如下图所示。</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5165_case2.png" alt="acwing5165_case2"></p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">110</span>;</span><br><span class="line"><span class="type">int</span> r, c;</span><br><span class="line"><span class="type">int</span> res;</span><br><span class="line">string s;</span><br><span class="line"><span class="type">char</span> g[N][N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从上中开始</span></span><br><span class="line"><span class="type">int</span> dx[] = &#123;<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>&#125;;</span><br><span class="line"><span class="type">int</span> dy[] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dfs</span><span class="params">(<span class="type">int</span> x, <span class="type">int</span> y, <span class="type">int</span> d, <span class="type">int</span> pos, <span class="type">int</span> is_turn)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 首先判断搜索的位置是否符合要求</span></span><br><span class="line">    <span class="keyword">if</span> (x &lt; <span class="number">0</span> || x &gt;= r || y &lt; <span class="number">0</span> || y &gt;= c || g[x][y] != s[pos])</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">// 如果当前已经搜到了最后一位，则返回</span></span><br><span class="line">    <span class="keyword">if</span> (pos == s.<span class="built_in">size</span>() - <span class="number">1</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        res ++;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 搜索新位置</span></span><br><span class="line">    <span class="built_in">dfs</span>(x + dx[d], y + dy[d], d, pos + <span class="number">1</span>, is_turn);</span><br><span class="line">    <span class="comment">// 注意，这里在第0个位置时是不能转向的，不然方案会重复</span></span><br><span class="line">    <span class="comment">// 并且只能转向一次</span></span><br><span class="line">    <span class="keyword">if</span> (pos != <span class="number">0</span> &amp;&amp; is_turn == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 注意这里的转向方式</span></span><br><span class="line">        <span class="comment">// 由于一共有8个方向，故可用3位二进制数来表示这8个方向</span></span><br><span class="line">        <span class="comment">// 000 - 上</span></span><br><span class="line">        <span class="comment">// 001 - 右上</span></span><br><span class="line">        <span class="comment">// 010 - 右</span></span><br><span class="line">        <span class="comment">// 011 - 右下</span></span><br><span class="line">        <span class="comment">// 100 - 下</span></span><br><span class="line">        <span class="comment">// 101 - 左下</span></span><br><span class="line">        <span class="comment">// 110 - 左</span></span><br><span class="line">        <span class="comment">// 111 - 左上</span></span><br><span class="line">        <span class="comment">// 举个例子，上(000)与左(110)和右(010)的关系</span></span><br><span class="line">        <span class="comment">// 可以看到，</span></span><br><span class="line">        <span class="comment">// （从第0位开始，最右边为第0位）第1位都是1，与原位置相反</span></span><br><span class="line">        <span class="comment">// 且第2位01交替</span></span><br><span class="line">        <span class="comment">// 那么^ 2可实现第2位与原位置相反</span></span><br><span class="line">        <span class="comment">// ^ (i &lt;&lt; 2)可实现第2位01交替</span></span><br><span class="line">        <span class="comment">// 故遍历方式如下</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; i ++)</span><br><span class="line">            <span class="built_in">dfs</span>(x, y, d ^ <span class="number">2</span> ^ (i &lt;&lt; <span class="number">2</span>), pos, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 输入</span></span><br><span class="line">    cin &gt;&gt; s;</span><br><span class="line">    cin &gt;&gt; r &gt;&gt; c;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; r; i ++)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; c; j ++)</span><br><span class="line">            cin &gt;&gt; g[i][j];</span><br><span class="line">    <span class="comment">// 遍历每个位置的每个方向进行搜索</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; r; i ++)</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; c; j ++)</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> d = <span class="number">0</span>; d &lt; <span class="number">8</span>; d ++)</span><br><span class="line">                <span class="built_in">dfs</span>(i, j, d, <span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">    <span class="comment">// 输出结果</span></span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5165. CCC单词搜索。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu20.04中MySQL8.0安装教程</title>
    <link href="http://blog.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/"/>
    <id>http://blog.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/</id>
    <published>2023-09-24T09:00:00.000Z</published>
    <updated>2023-09-24T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ubuntu20-04中MySQL8-0安装教程"><a href="#Ubuntu20-04中MySQL8-0安装教程" class="headerlink" title="Ubuntu20.04中MySQL8.0安装教程"></a>Ubuntu20.04中MySQL8.0安装教程</h1><p>首先运行<code>sudo apt-get update</code>命令更新软件源，必要时可以编辑<code>/etc/apt/sources.list</code>，将国外的软件源修改为国内高校的镜像软件源。</p><h2 id="安装与安装结果检查"><a href="#安装与安装结果检查" class="headerlink" title="安装与安装结果检查"></a>安装与安装结果检查</h2><p>然后运行<code>sudo apt-get install mysql-server</code>安装MySQL，运行截图如下：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924194933841.png" alt="安装mysql-server"></p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195059341.png" alt="安装完毕"></p><p>等待安装结束。</p><p>检查<code>MySQL</code>的运行状态，执行<code>sudo service mysql status</code>：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195132598.png" alt="MySQL的运行状态"></p><p>说明服务已经启动。</p><p>检查<code>MySQL</code>的版本，执行<code>mysql --version</code>：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195155868.png" alt="MySQL的版本"></p><h2 id="安全设置"><a href="#安全设置" class="headerlink" title="安全设置"></a>安全设置</h2><p>然后执行<code>sudo mysql_secure_installation</code>（注意这里一定要加<code>sudo</code>）编辑<code>MySQL</code>的安全设置：</p><ol><li><p>第一个设置：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195327992.png" alt="是否安装验证密码组件"></p><p> 这里的大意是“验证密码组件可以被用来测试密码并且提升密码的安全性。它可以检查密码的强度并且强制用户去设置足够安全的密码。你希望安装验证密码组件吗？”，这里我选择安装：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195424410.png" alt="选择最小的密码强度"></p><p> 有三个选项，分别是低中高，这里我选低：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195522083.png" alt="选择密码强度"></p><p> 然后这里说“默认为<code>root</code>用户使用<code>auth_socket</code>的方式来跳过密码，可以使用<code>ALTER_USER</code>命令来使用密码进行验证。”，这里先不管，后面会改<code>root</code>用户的密码。</p></li><li><p>第二个设置：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195641152.png" alt="是否移除匿名用户"></p><p> 这里的大意是“根据默认设置，在<code>MySQL</code>的安装过程中有匿名用户，匿名用户允许任何人不需要创建用户就可以登录<code>MySQL</code>。匿名用户的存在是为了调试的方便，也是为了安装过程更加的顺利。你应该在进入生产环境前移除他们。”，这里要我们选择是否移除匿名用户，这里选择移除：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195720812.png" alt="选择移除"></p></li><li><p>第三个设置：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195739330.png" alt="是否能够远程连接root用户"></p><p> 这里的大意是“在一般情况下，<code>root</code>用户一般只能够在本地进行连接。这保证了人们不能在网络上对<code>root</code>的密码进行尝试。”，然后问你是否要“不允许远程登录<code>root</code>”，这里选是：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195813979.png" alt="选择不可远程连接"></p></li><li><p>第四个设置：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195832188.png" alt="是否移除test数据库"></p><p> 这里的大意是“默认情况下，<code>MySQL</code>会有一个名为<code>test</code>的任何人能够访问的数据库。这也是为了测试需要，并且需要在进入生产环境前移除。”，然后问你需不需要移除，这里我们选移除：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195903660.png" alt="选择移除"></p></li><li><p>第五个设置：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924195924769.png" alt="是否重新加载权限表"></p><p> 这里的大意是“重新加载权限表可以保证所有的更改立即生效。”，然后问我们是否要重新加载权限表，这里我们选重新加载：</p><p> <img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924200020898.png" alt="选择重新加载"></p><p> 到此，安全设置结束。</p></li></ol><h2 id="更改root用户的密码"><a href="#更改root用户的密码" class="headerlink" title="更改root用户的密码"></a>更改root用户的密码</h2><p>注意，在我们安装的时候，是没有对<code>root</code>用户的密码进行设置的，并且<code>root</code>用户的默认登录验证方式是<code>auth_socket</code>（身份验证套接字），这会导致服务器无法使用密码对用户进行身份验证。它不仅会引发安全问题，而且还会使用户无法使用外部程序（如<code>phpMyAdmin</code>）访问数据库。我们需要将身份验证方法从<code>auth_socket</code>更改为使用<code>mysql_native_password</code>。</p><p>为此，我们需要执行<code>sudo mysql</code>，直接登录<code>MySQL</code>：</p><p>然后检查数据库对不同用户使用的身份验证方法，执行<code>SELECT user,authentication_string,plugin,host FROM mysql.user;</code>：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924200154917.png" alt="查看root用户的身份验证方式"></p><p>可以看到，<code>root</code>用户确实使用<code>auth_socket</code>进行了身份验证，我们需要使用<code>ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;your_password&#39;;</code>命令来切换到使用密码验证，<code>your_password</code>是你想设置的密码：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924200345750.png" alt="更改root用户的身份验证方式"></p><p>这里为了方便，我直接使用了<code>12345678</code>作为密码（密码的强度要和第一个安全设置处设置的相符合）。</p><p>在修改密码后，需要执行<code>FLUSH PRIVILEGES;</code>刷新权限表：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924200418836.png" alt="刷新权限表"></p><p>再次执行<code>SELECT user,authentication_string,plugin,host FROM mysql.user;</code>，查看<code>root</code>用户的身份验证方式：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924200451755.png" alt="再次查看root用户的身份验证方式"></p><p>可以看到，身份验证方式变为了<code>mysql_native_password</code>。</p><p>最后，执行<code>quit</code>，退出<code>MySQL</code>的命令行界面，在后续的使用中，可以通过<code>mysql -u root -p</code>来作为<code>root</code>用户进行登录：</p><p><img src="https://img.karltan.com/environment/mysql8.0-installation-tutorial-in-ubuntu20.04/image-20230924200633973.png" alt="不使用sudo登录root用户"></p><p>至此，安装完毕，可以正常的使用<code>mysql -u root -p</code>登录。</p>]]></content>
    
    
    <summary type="html">本文记录了在Ubuntu20.04中安装MySQL8.0的过程。</summary>
    
    
    
    <category term="环境配置" scheme="http://blog.karltan.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="Ubuntu" scheme="http://blog.karltan.com/tags/Ubuntu/"/>
    
    <category term="MySQL" scheme="http://blog.karltan.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Windows11中CUDA安装教程</title>
    <link href="http://blog.karltan.com/environment/cuda-installation-tutorial-in-windows11/"/>
    <id>http://blog.karltan.com/environment/cuda-installation-tutorial-in-windows11/</id>
    <published>2023-09-19T09:00:00.000Z</published>
    <updated>2023-09-19T10:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Windows11中CUDA安装教程"><a href="#Windows11中CUDA安装教程" class="headerlink" title="Windows11中CUDA安装教程"></a>Windows11中CUDA安装教程</h1><h2 id="CUDA-Toolkit和cuDNN下载"><a href="#CUDA-Toolkit和cuDNN下载" class="headerlink" title="CUDA Toolkit和cuDNN下载"></a>CUDA Toolkit和cuDNN下载</h2><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit Archive | NVIDIA Developer</a></p><p><a href="https://developer.nvidia.com/rdp/cudnn-download">cuDNN Download | NVIDIA Developer</a></p><p>对于Windows来说，以笔者的电脑为例，下载<code>CUDA Toolkit</code>时，请在<code>NVIDIA</code>控制面板中查看自己的<code>CUDA</code>版本：</p><p><img src="https://img.karltan.com/environment/cuda-installation-tutorial-in-windows11/image-20230919171625170.png" alt="NVIDIA控制面板截图"></p><p>那么在下载时，就应选择红框中的版本：</p><p><img src="https://img.karltan.com/environment/cuda-installation-tutorial-in-windows11/image-20230919172049723.png" alt="CUDA版本截图"></p><p>下载<code>cuDNN</code>时，请选择后缀为<code>zip</code>的文件，且要选对版本：</p><p><img src="https://img.karltan.com/environment/cuda-installation-tutorial-in-windows11/image-20230919172129220.png" alt="cuDNN版本截图"></p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="CUDA-Toolkit"><a href="#CUDA-Toolkit" class="headerlink" title="CUDA Toolkit"></a>CUDA Toolkit</h3><p>直接打开<code>exe</code>，然后按照流程走，注意选择自定义，把能选上的全部选上。</p><p>安装后可在终端中执行<code>nvcc -V</code>来查看是否安装成功。</p><h3 id="cuDNN"><a href="#cuDNN" class="headerlink" title="cuDNN"></a>cuDNN</h3><p>将cuDNN解压后，其中会有<code>LICENSE</code>文件、<code>bin</code>、<code>include</code>和<code>lib</code>三个文件夹，将三个文件夹中的文件分别复制到到<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\</code>中的<code>bin</code>、<code>include</code>和<code>lib</code>文件夹中（这个文件夹中已经有<code>bin</code>，<code>include</code>和<code>lib</code>三个文件夹了，并且路径可能会有所不同，这取决于你的安装过程）。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.7\extras\demo_suite\</code>目录下打开终端，执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bandwidthTest.exe</span><br><span class="line">./deviceQuery.exe</span><br></pre></td></tr></table></figure><p>如果能够在两次执行的末尾都看到<code>PASS</code>字样，说明安装成功。</p>]]></content>
    
    
    <summary type="html">本文记录了在Windows11中安装CUDA的过程。</summary>
    
    
    
    <category term="环境配置" scheme="http://blog.karltan.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="深度学习" scheme="http://blog.karltan.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Windows" scheme="http://blog.karltan.com/tags/Windows/"/>
    
    <category term="CUDA" scheme="http://blog.karltan.com/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 5081. 重复局面</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5081/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5081/</id>
    <published>2023-09-19T02:00:00.000Z</published>
    <updated>2023-09-19T11:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-5081-重复局面"><a href="#AcWing-5081-重复局面" class="headerlink" title="AcWing 5081. 重复局面"></a>AcWing 5081. 重复局面</h1><p><a href="https://www.acwing.com/problem/content/5084/">5081. 重复局面 - AcWing题库</a></p><p>国际象棋在对局时，同一局面连续或间断出现$3$次或$3$次以上，可由任意一方提出和棋。</p><p>国际象棋每一个局面可以用大小为$8 \times 8$的字符数组来表示，其中每一位对应棋盘上的一个格子。</p><p>六种棋子王、后、车、象、马、兵分别用字母$k\text{、}q\text{、}r\text{、}b\text{、}n\text{、}p$表示，其中大写字母对应白方、小写字母对应黑方。</p><p>棋盘上无棋子处用字符<code>*</code>表示。</p><p>两个字符数组的每一位均相同则说明对应同一局面。</p><p>现已按上述方式整理好了每步棋后的局面，试统计每个局面分别是第几次出现。</p><p><img src="https://img.karltan.com/ac-diary/acwing/autumn-daily-question-2023/acwing5081.png" alt="acwing5081"></p><p>注意：判断重复局面仅涉及字符串比较，无需考虑国际象棋实际行棋规则。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>输入的第一行包含一个正整数$n$，表示这盘棋总共有$n$步。</p><p>接下来$8 \times n$行，依次输入第$1$到$n$步棋后的局面。具体来说每行包含一个长度为$8$的字符串，每$8$行字符串共$64$个字符对应一个局面。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>输出共$n$行，每行一个整数，表示该局面是第几次出现。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">1 \le n \le 100。</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">8</span><br><span class="line">********</span><br><span class="line">******pk</span><br><span class="line">*****r*p</span><br><span class="line">p*pQ****</span><br><span class="line">********</span><br><span class="line">**b*B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br><span class="line">********</span><br><span class="line">******pk</span><br><span class="line">*****r*p</span><br><span class="line">p*pQ****</span><br><span class="line">*b******</span><br><span class="line">****B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br><span class="line">********</span><br><span class="line">******pk</span><br><span class="line">*****r*p</span><br><span class="line">p*p*****</span><br><span class="line">*b**Q***</span><br><span class="line">****B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br><span class="line">******k*</span><br><span class="line">******p*</span><br><span class="line">*****r*p</span><br><span class="line">p*p*****</span><br><span class="line">*b**Q***</span><br><span class="line">****B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br><span class="line">******k*</span><br><span class="line">******p*</span><br><span class="line">*****r*p</span><br><span class="line">p*pQ****</span><br><span class="line">*b******</span><br><span class="line">****B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br><span class="line">********</span><br><span class="line">******pk</span><br><span class="line">*****r*p</span><br><span class="line">p*pQ****</span><br><span class="line">*b******</span><br><span class="line">****B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br><span class="line">********</span><br><span class="line">******pk</span><br><span class="line">*****r*p</span><br><span class="line">p*p*****</span><br><span class="line">*b**Q***</span><br><span class="line">****B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br><span class="line">********</span><br><span class="line">******pk</span><br><span class="line">******rp</span><br><span class="line">p*p*****</span><br><span class="line">*b**Q***</span><br><span class="line">****B*PP</span><br><span class="line">****qP**</span><br><span class="line">**R***K*</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">2</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>第$6\text{、}7$步后的局面分别与第$2\text{、}3$步后的局面相同。第$8$步后的局面与上图相对应。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string s, temp;</span><br><span class="line">    unordered_map&lt;string, <span class="type">int</span>&gt; res;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">8</span> * n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; temp;</span><br><span class="line">        s += temp;</span><br><span class="line">        <span class="keyword">if</span> (i % <span class="number">8</span> == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            res[s] ++;</span><br><span class="line">            cout &lt;&lt; res[s] &lt;&lt; endl;</span><br><span class="line">            s.<span class="built_in">clear</span>();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><h4 id="做法1：枚举-O-64n-2"><a href="#做法1：枚举-O-64n-2" class="headerlink" title="做法1：枚举$O(64n^2)$"></a>做法1：枚举$O(64n^2)$</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">110</span>;</span><br><span class="line"></span><br><span class="line">string g[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        string line;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">8</span>; j ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; line;</span><br><span class="line">            g[i] += line;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; i; j ++ )</span><br><span class="line">            <span class="keyword">if</span> (g[j] == g[i])</span><br><span class="line">                cnt ++ ;</span><br><span class="line"></span><br><span class="line">        cout &lt;&lt; cnt + <span class="number">1</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="做法2：哈希表-O-64n"><a href="#做法2：哈希表-O-64n" class="headerlink" title="做法2：哈希表$O(64n)$"></a>做法2：哈希表$O(64n)$</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_map&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">110</span>;</span><br><span class="line"></span><br><span class="line">string g[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line"></span><br><span class="line">    unordered_map&lt;string, <span class="type">int</span>&gt; cnt;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        string g, line;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">8</span>; j ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; line;</span><br><span class="line">            g += line;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; ++ cnt[g] &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing秋季每日一题2023——AcWing 5081. 重复局面。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="秋季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E7%A7%8B%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>张宇考研数学基础30讲 高等数学分册 第1讲</title>
    <link href="http://blog.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/"/>
    <id>http://blog.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/</id>
    <published>2023-07-20T07:37:00.000Z</published>
    <updated>2023-07-24T10:43:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第1讲-高等数学预备知识"><a href="#第1讲-高等数学预备知识" class="headerlink" title="第1讲 高等数学预备知识"></a>第1讲 高等数学预备知识</h1><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><h3 id="第一部分-考研数学的深刻性"><a href="#第一部分-考研数学的深刻性" class="headerlink" title="第一部分 考研数学的深刻性"></a>第一部分 考研数学的深刻性</h3><p>$高三 \to 大一$VS$大四 \to 研一$。</p><blockquote><p>【例】：</p><p>设$x \to 0$时，$f(x) = x - \sin x$与$g(x) = c x^k$是等价无穷小量，则$c = \underline{}, k = \underline{}$。</p><p>【解】：</p><p>由题目条件等价无穷小，有$\lim_{x \to 0} \frac{x - \sin x}{c x^k} =^{\frac{0}{0}} 1$。</p><blockquote><p>【注】：</p><p>$=^{\frac{0}{0}}$意思是0比0型。</p><p>【注】：</p><p>中间插一条结论：任何可导函数泰勒展开后，有：</p><script type="math/tex; mode=display">f(x) = \sum a_n x^n</script><p>且每个$f(x)$都唯一对应一组$a_n$。</p></blockquote><p>那么这里可以使用泰勒展开来展开$\sin x$：</p><script type="math/tex; mode=display">\sin x = x - \frac{1}{3!}x^3 + o(x^3)</script><blockquote><p>【注】：</p><p>其中，$o(x^3)$表示的是比$x^3$更高阶的无穷小量。</p></blockquote><p>由泰勒展开的结果可知，$\sin x$与$x - \frac{1}{3!}x^3$相比，其中差了一个高阶无穷小量$o(x^3)$，所以$\sin x$和$x - \frac{1}{3!}x^3$并不相等，而是等价，但是在实际运算中可以忽略掉这个高阶无穷小量，所以称$\sin x$和$x - \frac{1}{3!}x^3$互为等价无穷小，记为$\sin x \sim x - \frac{1}{3!}x^3$。</p><p>那么：</p><script type="math/tex; mode=display">\lim_{x \to 0} \frac{x - \sin x}{c x^k} = \lim_{x \to 0} \frac{x - (x - \frac{1}{3!}x^3)}{c x^k} = \lim_{x \to 0} \frac{\frac{1}{6} x^3}{c x^k} = 1</script><p>所以$c = \frac{1}{6}, k = 3$。</p></blockquote><p>这里补几个数学家的百度百科：</p><p><a href="https://baike.baidu.com/item/泰勒/10285875?fromModule=lemma_search-box">泰勒 百度百科 (baidu.com)</a></p><p><a href="https://baike.baidu.com/item/雅各布·伯努利/7261223">雅各布·伯努利_百度百科 (baidu.com)</a></p><p><a href="https://baike.baidu.com/item/艾萨克·牛顿/1119240">艾萨克·牛顿_百度百科 (baidu.com)</a></p><p><a href="https://baike.baidu.com/item/戈特弗里德·威廉·莱布尼茨/5028927?fromtitle=戈特弗里德·莱布尼茨&amp;fromid=5746021">戈特弗里德·威廉·莱布尼茨_百度百科 (baidu.com)</a></p><p><a href="https://baike.baidu.com/item/洛必达/2948230">洛必达_百度百科 (baidu.com)</a></p><h3 id="第二部分-知识结构"><a href="#第二部分-知识结构" class="headerlink" title="第二部分 知识结构"></a>第二部分 知识结构</h3><h4 id="高等数学部分"><a href="#高等数学部分" class="headerlink" title="高等数学部分"></a>高等数学部分</h4><ol><li><p>高等数学预备知识</p><p> 考研数学默认本科以下（包括本科）的所有结论都可以使用，所以需要补充。</p></li><li><p>数列极限</p></li><li><p>函数极限与连续性</p></li><li><p>一元函数微分学的概念与计算</p></li><li><p>一元函数微分学的几何应用（长什么样子）</p></li><li><p>中值定理</p></li><li><p>零点问题与微分不等式</p></li><li><p>一元函数积分学的概念与计算</p></li><li><p>一元函数积分学的几何应用（面积是多少）</p></li><li><p>积分等式与积分不等式</p></li><li><p>多元函数微分学</p></li><li><p>二重积分</p></li><li><p>常微分方程</p></li><li><p>无穷级数（仅数学一、数学三要求）</p></li><li><p>数学一、数学二专题内容</p></li><li><p>数学三专题内容</p></li><li><p>多元函数积分学的基础知识（仅数学一要求）</p></li><li><p>三重积分、曲线曲面积分（仅数学一要求）</p></li></ol><p>其中1-10为高数上部分，11-18为高数下部分。</p><h2 id="基础知识结构解读"><a href="#基础知识结构解读" class="headerlink" title="基础知识结构解读"></a>基础知识结构解读</h2><script type="math/tex; mode=display">\begin{cases}函数的概念与特性\begin{cases}函数y = f(x) \\反函数y = f^{-1}(x) \\复合函数y = f[g(x)] \\四种特性(有7个重要结论)\begin{cases}有界性 \\单调性 \\奇偶性 \\周期性\end{cases}\end{cases}\\函数的图像\begin{cases}直角坐标系下的图像(f(x, y) = 0)\begin{cases}常见图像\begin{cases}基本初等函数与初等函数 \\分段函数\end{cases}\\图像变换\begin{cases}平移变换 \\对称变换 \\伸缩变换\end{cases}\end{cases}\\极坐标下的图像(g(r, \theta) = 0)\begin{cases}用描点法画常见图像\begin{cases}心形线(外摆线) \\玫瑰线 \\阿基米德螺线 \\伯努利双纽线\end{cases}\\用直角坐标系的观点画极坐标系下图像\end{cases}\\参数法 - 参数方程\left(\begin{cases}x = x(t) \\y = y(t)\end{cases}\right)\begin{cases}摆线(平摆线) \\星形线(内摆线)\end{cases}\end{cases}\end{cases}</script><script type="math/tex; mode=display">常用基础知识\begin{cases}数列 \\三角函数 \\指数运算法则 \\对数运算法则 \\一元二次方程基础 \\因式分解公式 \\阶乘与双阶乘 \\常用不等式\end{cases}</script><h2 id="一、函数的概念与特性"><a href="#一、函数的概念与特性" class="headerlink" title="一、函数的概念与特性"></a>一、函数的概念与特性</h2><h3 id="（一）函数"><a href="#（一）函数" class="headerlink" title="（一）函数"></a>（一）函数</h3><p>对于每一个$x \in D$，按照一定的法则$f$，有一个确定的值$y$与其对应。</p><p>这里说明可以一对一、一对多，但是不能多对一。</p><h3 id="（二）反函数"><a href="#（二）反函数" class="headerlink" title="（二）反函数"></a>（二）反函数</h3><p>对于函数$y = 2x$，其反向求出$y$后为$x = \frac{1}{2}y$，但是要注意的是，这两个函数的图像是一样的，只有将$x = \frac{1}{2}y$中的$x, y$对调，即变为$y = \frac{1}{2}x$时，$y = 2x$和$y = \frac{1}{2}x$才互为反函数，其图像才关于$y = x$对称。</p><p>并且严格单调函数必有反函数，比如$y = x^2$在$x \in [0, +\infty)$是严格单调函数，所以它有反函数$y = \sqrt{x}$。</p><blockquote><p>【注】：</p><p>但是有反函数的函数不一定是单调函数，比如$f(x) = \begin{aligned}\begin{cases}x, &amp;x \ge 0, \\ \frac{1}{x}, &amp;x &lt; 0,\end{cases}\end{aligned}$其图像如图1-1所示，</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230721002108930.png" alt="image-20230721002108930"></p><p>显然这个函数的反函数就是自己，但是这个函数不是单调的。</p></blockquote><h3 id="（三）复合函数"><a href="#（三）复合函数" class="headerlink" title="（三）复合函数"></a>（三）复合函数</h3><p>设函数$y = f(u)$的定义域为$D_1$，函数$u = g(x)$在$D$上有定义，且$g(D) \subset D_1$，则由</p><script type="math/tex; mode=display">y = f[g(x)], x \in D</script><p>确定的函数，称为由函数$u = g(x)$和函数$y = f(u)$构成的复合函数，它的定义域为$D$，$u$称为中间变量，要掌握复合的方法，具体见例1.4。</p><h4 id="例1-4"><a href="#例1-4" class="headerlink" title="例1.4"></a>例1.4</h4><blockquote><p>【例1.4】：</p><p>设$f(x) = \begin{aligned}\begin{cases}\ln \sqrt{x}, &amp;x \ge 1, \\ 2x - 1, &amp;x &lt; 1\end{cases}\end{aligned}$。求$f[f(x)]$。</p><p>【解】：</p><p>这道题书上给出的方式是三步：</p><ol><li><p>第1步，广义化：$f[f(x)] = \begin{aligned}\begin{cases}\ln \sqrt{f(x)}, &amp;f(x) \ge 1, \\ 2f(x) - 1, &amp;f(x) &lt; 1。\end{cases}\end{aligned}$</p></li><li><p>第2步，画图1-43并分析：</p><ol><li>当$f(x) \ge 1$时，$x \ge e^2$，此时$f(x) = \ln \sqrt{x}$；</li><li>当$f(x) &lt; 1$时：<ul><li>可能$1 \le x &lt; e^2$，此时$f(x) = \ln \sqrt{x}$；</li><li>可能$x &lt; 1$，此时$f(x) = 2x - 1$。</li></ul></li></ol></li><li><p>第3步，写答案：</p><script type="math/tex; mode=display"> f[f(x)] = \begin{aligned} \begin{cases} \ln \sqrt{\ln \sqrt{x}}, &x \ge e^2, \\ 2 \ln \sqrt{x} - 1, &1 \le x < e^2, \\ 2(2x - 1) - 1, &x < 1 \end{cases} = \begin{cases} \frac{1}{2} \ln (\ln \sqrt{x}), &x \ge e^2, \\ \ln x - 1, &1 \le x < e^2, \\ 4x - 3, &x < 1。 \end{cases} \end{aligned}</script><p> <img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230721005628436.png" alt="image-20230721005628436"></p><p> 但是我认为可能直接令$t = f(x)$，然后转为求$f(t)$的表达式会更高中一点，毕竟高中貌似是有这种题目的，但是这个题的难度已经是这种题的上限了。</p></li></ol></blockquote><h3 id="（四）函数的四种特性及重要结论"><a href="#（四）函数的四种特性及重要结论" class="headerlink" title="（四）函数的四种特性及重要结论"></a>（四）函数的四种特性及重要结论</h3><h4 id="1-有界性"><a href="#1-有界性" class="headerlink" title="1. 有界性"></a>1. 有界性</h4><p>需要指明区间，在某个区间内，$\left|f(x)\right| \le M$，说$f(x)$在这个区间上有界。</p><h4 id="2-单调性"><a href="#2-单调性" class="headerlink" title="2. 单调性"></a>2. 单调性</h4><p>定义如下：</p><ol><li>在某一区间内，当$x_1 &lt; x_2$时，恒有$f(x_1) &lt; f(x_2)$，则称$f(x)$在区间内单调增；</li><li>在某一区间内，当$x_1 &lt; x_2$时，恒有$f(x_1) &lt; f(x_2)$，则称$f(x)$在区间内单调减。</li></ol><blockquote><p>【注】：</p><p>也可以使用求导的方式来讨论函数在某个区间上的单调性，但是定义法不可忘记。</p><p>除此之外，下面的形式也很常用：</p><p>对任意$x_1, x_2 \in D, x_1 \ne x_2$，有：</p><ol><li>$f(x)$是单调增函数$\Leftrightarrow(x_1 - x_2)[f(x_1) - f(x_2)] &gt; 0$；</li><li>$f(x)$是单调减函数$\Leftrightarrow(x_1 - x_2)[f(x_1) - f(x_2)] &lt; 0$；</li><li>$f(x)$是单调不减函数$\Leftrightarrow(x_1 - x_2)[f(x_1) - f(x_2)] \ge 0$；</li><li>$f(x)$是单调不增函数$\Leftrightarrow(x_1 - x_2)[f(x_1) - f(x_2)] \le 0$。</li></ol></blockquote><h4 id="3-奇偶性"><a href="#3-奇偶性" class="headerlink" title="3. 奇偶性"></a>3. 奇偶性</h4><p>首先定义域要关于原点对称。</p><p>当$f(-x) = f(x)$时，$f(x)$为偶函数，当$f(-x) = -f(x)$时，$f(x)$为奇函数。</p><blockquote><p>【注】：</p><p>设$f(x)$是定义在$[-l, l]$上的任意函数，则：</p><ol><li><p>$F_1(x) = f(x) - f(-x)$必为奇函数；</p></li><li><p>$F_2(x) = f(x) + f(-x)$必为偶函数。</p><blockquote><p>【注注】：</p><p>设$f(x) = e^x$，<br>$\varphi_1(x) = \frac{1}{2} F_1(x) = \frac{e^x - e^{-x}}{2}$为奇函数，这个函数也叫双曲正弦函数；<br>$\varphi_2(x) = \frac{1}{2} F_2(x) = \frac{e^x + e^{-x}}{2}$为偶函数，这个函数也叫双曲余弦函数。</p></blockquote></li></ol><p>除此之外有几个结论：</p><ol><li><p>奇函数$y = f(x)$的图像关于坐标原点对称，当$f(x)$在$x = 0$处有定义时，必有$f(0) = 0$；</p></li><li><p>偶函数$y = f(x)$的图像关于$y$轴对称，且当$f^{\prime}(0)$存在时，必有$f^{\prime}(0) = 0$；</p></li><li><p>函数$y = f(x)$与$y = -f(x)$的图形关于$x$轴对称，函数$y = f(x)$与$y = f(-x)$的图形关于$y$轴对称，函数$y = f(x)$与$y = -f(-x)$的图形关于原点对称；</p></li><li><p>函数$y = f(x)$的图形关于直线$x = T$对称的充分必要条件是$f(x) = f(2T - x) 或 f(T + x) = f(T - x)$。</p><blockquote><p>【注注】：</p><p>隐藏的01：</p><ol><li>当见到$f - f$时，要想到拉格朗日中值定理，但是题目可以只有一个$f$，此时需要构建出一个$f - 0$，转换为$f - f$的形式；</li><li>当见到$e^b - 1$时，可以转换为$e^b - e^0$，从而构建出$f - f$的形式；</li><li>当见到$f(x) &gt; f(1)x$时，可以转换为$\frac{f(x)}{x} &gt; \frac{f(1)}{1}$，然后令$F(x) = \frac{f(x)}{x}$，构建出$F - F$的形式。</li></ol></blockquote></li></ol></blockquote><h4 id="4-周期性"><a href="#4-周期性" class="headerlink" title="4. 周期性"></a>4. 周期性</h4><ol><li>若$f(x)$是周期为$T$的周期函数，则$\int_{0}^{T}f(x)\mathrm{d}t = \int_{a}^{a + T}f(x)\mathrm{d}t$；</li><li>若$f(x)$是周期为$T$的奇函数，则$\int_{0}^{T}f(x)\mathrm{d}t = \int_{a}^{a + T}f(x)\mathrm{d}t = \int_{-\frac{T}{2}}^{\frac{T}{2}}f(x)\mathrm{d}t = 0$。</li></ol><h4 id="5-重要结论"><a href="#5-重要结论" class="headerlink" title="5. 重要结论"></a>5. 重要结论</h4><p>事实上，关于$f^{\prime}(x)$和$\int_{a}^{T}f(t)\mathrm{d}t$的性质才是这里的重点，先提前总结如下：</p><ol><li><p>若$f(x)$是可导的偶函数，则$f^{\prime}(x)$为奇函数；</p></li><li><p>若$f(x)$是可导的奇函数，则$f^{\prime}(x)$为偶函数；</p><blockquote><p>【注】：</p><p>求导后原函数和导函数的奇偶性是有规律的。</p></blockquote></li><li><p>若$f(x)$是可导的周期为$T$的周期函数，则$f^{\prime}(x)$也是以$T$为周期的周期函数；</p></li><li><p>连续的奇函数的一切原函数都是偶函数；</p></li><li><p>连续的偶函数的原函数中仅有一个原函数是奇函数；</p><blockquote><p>【注】：</p><p>因为积分之后有一个常数$C$：</p><script type="math/tex; mode=display">\int f(x)\mathrm{d}x = F(x) + C</script><p>所以只有当$C$的取值满足$F(0) + C = 0$时，原函数才是奇函数。</p></blockquote></li><li><p>若连续函数$f(x)$以$T$为周期且$\int_{0}^{T}f(x)\mathrm{d}x = 0$，则$f(x)$的一切原函数也以$T$为周期；</p></li><li><p>若$f(x)$在$(a, b)$内可导且$f^{\prime}(x)$有界，则$f(x)$在$(a, b)$内有界。</p><blockquote><p>【注】：</p><p>证明：</p><p>因为$f(x)$在$(a, b)$内可导，所以$f(x)$在$(a, b)$内连续，</p><blockquote><p>【注】：</p><p>可导必连续。</p></blockquote><p>因此对任意$x_0 \in (a, b)$，$f(x_0)$存在，对任意的$x \in (a, b)$，不妨设$x &lt; x_0$，对$f(x)$在$[x, x_0]$上应用拉格朗日中值定理，有：</p><script type="math/tex; mode=display">f(x) - f(x_0) = f^{\prime}(\xi)(x - x_0), \xi \in (x, x_0),</script><p>则$\left|f(x)\right| \le \left|f(x_0)\right| + \left|f^{\prime}(\xi)\right| \left|x - x_0\right|$，由于$f^{\prime}(\xi)$有界，故存在$k &gt; 0$，使得对任意$x \in (a, b)$，有$\left|f^{\prime}(x)\right| \le k$，则$f^{\prime}(\xi) \le k$，故：</p><script type="math/tex; mode=display">\left|f(x)\right| \le \left|f(x_0)\right| + k\left|x - x_0\right| < \left|f(x_0)\right| + k(b - a),</script><p>令$M = \left|f(x_0)\right| + k(b - a)$，则有$\left|f(x)\right| &lt; M$，则$f(x)$在$(a, b)$内有界。</p></blockquote></li></ol><h5 id="例1-2"><a href="#例1-2" class="headerlink" title="例1.2"></a>例1.2</h5><blockquote><p>【例1.2】：</p><p>设$f(x) = x^2$，$f[\varphi(x)] = -x^2 + 2x + 3$，且$\varphi(x) \ge 0$，求$\varphi(x)$及其定义域与值域。</p><p>【解】：</p><p>由题设条件知，$f(x) = x^2$，那么$f[\varphi(x)] = \varphi^2(x) = -x^2 + 2x + 3$，并且由于$\varphi^2(x) = -x^2 + 2x + 3$，所以$-x^2 + 2x + 3 \ge 0$，所以$\varphi(x) = \sqrt{-x^2 + 2x + 3}$。</p><p>又$-x^2 + 2x + 3 \ge 0$，即$(x - 3)(x + 1) \le 0$，所以$\varphi(x)$的定义域是$[-1, 3]$。</p><p>又$\sqrt{-x^2 + 2x + 3} = \sqrt{-(x - 1)^2 + 4}$，当$x = 1$时，$\varphi(x) = 2$为最大值，$x = -1, 3$时，$\varphi(-1) = \varphi(3) = 0$为最小值，故$\varphi(x)$的值域为$[0, 2]$。</p></blockquote><h5 id="例1-3"><a href="#例1-3" class="headerlink" title="例1.3"></a>例1.3</h5><blockquote><p>【例1.3】：</p><p>求函数$y = f(x) = \ln (x + \sqrt{x^2 + 1})$的反函数$f^{-1}(x)$的表达式及其定义域。</p><p>【解】：</p><p>直接由$y = \ln (x + \sqrt{x^2 + 1})$解出$x = f^{-1}(y)$会很麻烦，现采用下述方法：</p><script type="math/tex; mode=display">\begin{aligned}-y&= -\ln (x + \sqrt{x^2 + 1}) = \ln \frac{1}{x + \sqrt{x^2 + 1}} \\&= \ln \frac{\sqrt{x^2 + 1} - x}{(\sqrt{x^2 + 1} + x)(\sqrt{x^2 + 1} - x)} = \ln (\sqrt{x^2 + 1} - x),\end{aligned}</script><p>所以$e^{-y} = \sqrt{x^2 + 1} - x$，再由$y = f(x)$的表达式有$e^y = \sqrt{x^2 + 1} + x$，解出$x = \frac{e^y - e^{-y}}{2}$，再交换其中$x, y$的位置，得到$y = f(x)$的反函数：</p><script type="math/tex; mode=display">y = f^{-1}(x) = \frac{e^x - e^{-x}}{2}, -\infty < x < +\infty。</script><blockquote><p>【注】：</p><ol><li><p>函数$y = \ln (x + \sqrt{x^2 + 1})$叫做反双曲正弦函数，其图像如图1-40所示。</p><p> 函数$y = \frac{e^x - e^{-x}}{2}$叫做双曲正弦函数，其图像如图1-41所示。</p><p> 要记住这两个函数的图像。</p><p> <img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230722154231248.png" alt="image-20230722154231248"></p><p> 这两个函数都是奇函数，于是$\int_{-1}^{1}\left[\ln (x + \sqrt{x^2 + 1}) + x^2\right]  = \int_{-1}^{1}x^2\mathrm{d}x = \frac{2}{3}$。</p></li><li><p>考生还应知道，$\left[\ln (x + \sqrt{x^2 + 1}) + x^2\right]^{\prime} = \frac{1}{\sqrt{x^2 + 1}}$，于是$\int\frac{1}{\sqrt{x^2 + 1}}\mathrm{d}x = \ln (x + \sqrt{x^2 + 1}) + C$。</p></li><li><p>$y = \frac{e^x + e^{-1}}{2}$叫做双曲余弦函数，其图像如图1-42所示，它是偶函数，是一种特殊的悬链线。</p><p> <img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230722154950155.png" alt="image-20230722154950155"></p></li></ol></blockquote></blockquote><h2 id="二、函数的图像"><a href="#二、函数的图像" class="headerlink" title="二、函数的图像"></a>二、函数的图像</h2><p>数一需要掌握直角坐标系，极坐标系，参数方程，空间图形。</p><h3 id="（一）直角坐标系下的图像-f-x-y-0"><a href="#（一）直角坐标系下的图像-f-x-y-0" class="headerlink" title="（一）直角坐标系下的图像$(f(x, y) = 0)$"></a>（一）直角坐标系下的图像$(f(x, y) = 0)$</h3><h4 id="1-常见图像"><a href="#1-常见图像" class="headerlink" title="1. 常见图像"></a>1. 常见图像</h4><h5 id="1-基本初等函数与初等函数"><a href="#1-基本初等函数与初等函数" class="headerlink" title="(1) 基本初等函数与初等函数"></a>(1) 基本初等函数与初等函数</h5><p>基本初等函数：常数函数、幂函数、指数函数、对数函数、三角函数、反三角函数。</p><h6 id="①-常数函数"><a href="#①-常数函数" class="headerlink" title="① 常数函数"></a>① 常数函数</h6><p>$y = A$，$A$为常数，其图形为平行于$x$轴的水平直线（见图1-2）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230722165442386.png" alt="image-20230722165442386"></p><blockquote><p>常数函数的用途：</p><ol><li>$y = A$，$A$为常数，为偶函数；</li><li>用$y = A$找交点个数；</li><li>用$y = A$在概率论中求分布。</li></ol></blockquote><h6 id="②-幂函数"><a href="#②-幂函数" class="headerlink" title="② 幂函数"></a>② 幂函数</h6><p>$y = x^{\mu}$（$\mu$为实数）。</p><blockquote><p>【注】：</p><ol><li><p>$y = x^{\mu}$的定义域和值域取决于$\mu$的值，当$x &gt; 0$时，$y = x^{\mu}$都有定义。</p></li><li><p>常用的幂函数（见图1-3(a)-(c)）。</p><script type="math/tex; mode=display"> y = x, y = x^2, y = \sqrt{x}, y = x^3, y = \sqrt[3]{x}, y = \frac{1}{x}。</script><p> <img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230722170345145.png" alt="image-20230722170345145"></p></li><li><p>当$x &gt; 0$时，由$y = x$与$y = \sqrt{x}, y = \sqrt[3]{x}, y = \ln x$（见图1-5(b)）具有相同的单调性且与$y = \frac{1}{x}$具有相反的单调性，故：</p><ol><li>见到$\sqrt{u}, \sqrt[3]{u}$时，可用$u$来研究最值；</li><li>见到$\left|u\right|$时，由$\left|u\right| = \sqrt{u^2}$，可用$u^2$来研究最值；</li><li>见到$u_1u_2u_3$时，可用$\ln (u_1u_2u_3) = \ln u_1 + \ln u_2 + \ln u_3$来研究最值；</li><li><p>见到$\frac{1}{u}$时，可用$u$来研究最值（结论相反，即$\frac{1}{u}$与$u$的最大值点、最小值点相反）。</p><p>以上$1 \sim 4$，可使得计算简单方便。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230722170954408.png" alt="image-20230722170954408"></p></li></ol></li></ol></blockquote><h6 id="③-指数函数"><a href="#③-指数函数" class="headerlink" title="③ 指数函数"></a>③ 指数函数</h6><p>$y = a^x(a &gt; 0, a \ne 1)$（见图1-4(a)）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230722203806438.png" alt="image-20230722203806438"></p><blockquote><p>【注】：</p><ol><li><p>定义域：$(-\infty, +\infty)$。值域：$(0, +\infty)$。</p></li><li><p>单调性：当$a &gt; 1$时，$y = a^x$单调增加；当$0 &lt; a &lt; 1$时，$y = a^x$单调减少。</p></li><li><p>常用的指数函数：$y = e^x$（见图1-4(b)）。</p></li><li><p>极限：$\lim_{x \to -\infty} e^x = 0, \lim_{x \to +\infty} e^x = +\infty$。</p><blockquote><p>【注注】：</p><ol><li>$\lim_{x \to \infty} e^x$不存在，因为在$x \to -\infty$和$x \to +\infty$时$e^x$的极限不相等。</li><li>$\int_{-\infty}^{0} e^x = 1$，即$y = e^x$的图像与$x$负半轴、$y$正半轴围成的图形面积为1。</li></ol></blockquote></li><li><p>特殊函数值：$a^0 = 1, e^0 = 1$。</p></li></ol></blockquote><h6 id="④-对数函数"><a href="#④-对数函数" class="headerlink" title="④ 对数函数"></a>④ 对数函数</h6><p>$y = \log_a x(a &gt; 0, a \ne 1)$（见图1-5(a)），是$y = a^x$的反函数。</p><blockquote><p>【注】：</p><ol><li><p>定义域：$(0, +\infty)$。值域：$(-\infty, +\infty)$。</p></li><li><p>单调性：当$a &gt; 1$时，$y = \log_a x$单调增加；当$0 &lt; a &lt; 1$时，$y = \log_a x$单调减少。</p></li><li><p>常用的对数函数：$y = \ln x$（自然对数：$\ln x = \log_e x, e = 2.71828\cdots$）（见图1-5(b)）。</p></li><li><p>特殊函数值：$\log_a 1 = 0, \log_a a = 1, \ln 1 = 0, \ln e = 1$。</p></li><li><p>极限：$\lim_{x \to 0^+}\ln x = -\infty, \lim_{x \to +\infty}\ln x = +\infty$。</p></li><li><p>常用公式：$x = e^{\ln x}(x &gt; 0), u^v = e^{\ln u^v} = e^{v\ln u}(u &gt; 0)$。</p><blockquote><p>【注注】：</p><p>这里的$u^v$指的是$u(x)^{v(x)}$。</p><p>$x^x$被称为幂指函数。</p></blockquote></li></ol></blockquote><h6 id="⑤-三角函数"><a href="#⑤-三角函数" class="headerlink" title="⑤ 三角函数"></a>⑤ 三角函数</h6><p><strong>(i) 正弦函数与余弦函数</strong></p><p>正弦函数$y = \sin x$（见图1-6(a)），余弦函数$y = \cos x$（见图1-6(b)）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230722210112970.png" alt="image-20230722210112970"></p><blockquote><p>【注】：</p><ol><li><p>定义域：$(-\infty, +\infty)$。值域：$[-1, 1]$。</p></li><li><p>奇偶性：$y = \sin x$是奇函数，$y = \cos x$是偶函数，$x \in (-\infty, +\infty)$。</p></li><li><p>周期性：$y = \sin x$和$y = \cos x$均以$2 \pi$为最小正周期，$x \in (-\infty, +\infty)$。</p></li><li><p>有界性：$\left|\sin x\right| \le 1, \left|\cos x\right| \le 1$。</p></li><li><p>特殊函数值：</p><script type="math/tex; mode=display"> \begin{matrix} \sin 0 = 0, & \sin \frac{\pi}{6} = \frac{1}{2}, & \sin \frac{\pi}{4} = \frac{\sqrt{2}}{2}, & \sin \frac{\pi}{3} = \frac{\sqrt{3}}{2}, \\ \sin \frac{\pi}{2} = 1, & \sin \pi = 0, & \sin \frac{3\pi}{2} = -1, & \sin 2\pi = 0, \\ \cos 0 = 1, & \cos \frac{\pi}{6} = \frac{\sqrt{3}}{2}, & \cos \frac{\pi}{4} = \frac{\sqrt{2}}{2}, & \cos \frac{\pi}{3} = \frac{1}{2}, \\ \cos \frac{\pi}{2} = 0, & \cos \pi = -1, & \cos \frac{3\pi}{2} = 0, & \cos 2\pi = 1。 \end{matrix}</script></li></ol><blockquote><p>【注注】：</p><ol><li><p>正弦、余弦函数一拱的面积为2，即$\int_{0}^{\pi}\sin x \mathrm{d}x = 2$，且$\int_{\frac{\pi}{4}}^{\frac{3\pi}{4}}\sin x \mathrm{d}x = \sqrt{2}$。</p></li><li><p>在$x = 0$的邻域中，$\left|\sin x\right| &lt; \left|x\right|$；当$x \to 0^+$时，$\sin x &lt; x$。</p></li><li><p>真题：</p><p> 求$\int_{\frac{3\pi}{4}}^{\frac{5\pi}{4}}\left|\sin x\right|\mathrm{d}x = \underline{}$。</p><p> 答案：$2 - \sqrt{2}$。</p><p> 直接使用1中的结论然后画图即可。</p></li></ol></blockquote></blockquote><p><strong>(ii) 正切函数与余切函数</strong></p><p>正切函数$y = \tan x$（见图1-7(a)），余切函数$y = \cot x$（见图1-7(b)）。</p><script type="math/tex; mode=display">\tan x = \frac{\sin x}{\cos x}, \cot x = \frac{\cos x}{\sin x} = \frac{1}{\tan x}。</script><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723142845703.png" alt="image-20230723142845703"></p><blockquote><p>【注】：</p><ol><li><p>定义域：</p><p> $y = \tan x$的定义域为$x \ne k\pi + \frac{\pi}{2}(k \in \mathbf{Z})$的一切实数$x$；</p><p> $y = \cot x$的定义域为$x \ne k\pi(k \in \mathbf{Z})$的一切实数$x$。</p><p> 值域：$(-\infty, +\infty)$。</p></li><li><p>奇偶性：$y = \tan x$和$y = \cot x$均为奇函数（在其定义域内）。</p></li><li><p>周期性：$y = \tan x$和$y = \cot x$均以$\pi$为最小正周期（在其定义域内）。</p></li><li><p>特殊函数值：</p><script type="math/tex; mode=display"> \begin{matrix} \tan 0 = 0, & \tan \frac{\pi}{6} = \frac{\sqrt{3}}{3}, & \tan \frac{\pi}{4} = 1, & \tan \frac{\pi}{3} = \sqrt{3}, \\ \lim_{x \to \frac{\pi}{2}}\tan x = \infty, & \tan \pi = 0, & \lim_{x \to \frac{3\pi}{2}}\tan x = \infty, & \tan 2\pi = 0, \\ \lim_{x \to 0}\cot x = \infty, & \cot \frac{\pi}{6} = \sqrt{3}, & \cot \frac{\pi}{4} = 1, & \cot \frac{\pi}{3} = \frac{\sqrt{3}}{3}, \\ \cot \frac{\pi}{2} = 0, & \lim_{x \to \pi}\cot x = \infty, & \cot \frac{3\pi}{2} = 0, & \lim_{x \to 2\pi}\cot x = \infty。 \end{matrix}</script></li></ol></blockquote><p><strong>(iii) 正割函数与余割函数</strong></p><p>正割函数$y = \sec x$（见图1.8(a)），余割函数$y = \csc x$（见图1-8(b)）</p><script type="math/tex; mode=display">\sec x = \frac{1}{\cos x}, \csc x = \frac{1}{\sin x}。</script><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723144101627.png" alt="image-20230723144101627"></p><blockquote><p>【注】：</p><ol><li><p>定义域：</p><p> $y = \sec x$的定义域为$x \ne k\pi + \frac{\pi}{2}(k \in \mathbf{Z})$的一切实数$x$；</p><p> $y = \csc x$的定义域为$x \ne k\pi(k \in \mathbf{Z})$的一切实数$x$；</p><p> 值域：$(-\infty, -1] \cup [1, +\infty)$。</p></li><li><p>奇偶性：$y = \sec x$为偶函数，$y = \csc x$为奇函数（在其定义域内）。</p></li><li><p>周期性：$y = \sec x$和$y = \csc x$均以$2\pi$为最小正周期（在其定义域内）。</p></li></ol></blockquote><h6 id="⑥-反三角函数"><a href="#⑥-反三角函数" class="headerlink" title="⑥ 反三角函数"></a>⑥ 反三角函数</h6><p><strong>(i) 反正弦函数与反余弦函数</strong></p><p>反正弦函数$y = \arcsin x$（见图1-9(a)），反余弦函数$y = \arccos x$（见图1-9(b)）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723144559198.png" alt="image-20230723144559198"></p><p>$y = \arcsin x$是$y = \sin x \left(-\frac{\pi}{2} \le x \le \frac{\pi}{2}\right)$的反函数，$y = \arccos x$是$y = \cos x(0 \le x \le \pi)$的反函数。</p><blockquote><p>【注】：</p><ol><li><p>定义域：$[-1, 1]$。</p><p> 值域：$y = \arcsin x$的值域为$\left[-\frac{\pi}{2}, \frac{\pi}{2}\right]$，$y = \arccos x$的值域为$[0, \pi]$。</p></li><li><p>单调性：$y = \arcsin x$单调增加，$y = \arccos x$单调减少。</p></li><li><p>奇偶性：$y = \arcsin x$为奇函数（在其定义域内）。</p></li><li><p>有界性：两个函数在其定义域内有界，$-\frac{\pi}{2} \le \arcsin x \le \frac{\pi}{2}, 0 \le \arccos x \le \pi$。</p></li><li><p>性质：$\arcsin x + \arccos x = \frac{\pi}{2}(-1 \le x \le 1)$。</p><blockquote><p>【证明】：</p><p>令$f(x) = \arcsin x + \arccos x, -1 \le x \le 1$，则$f^{\prime}(x) = \frac{1}{\sqrt{1 - x^2}} - \frac{1}{\sqrt{1 - x^2}} = 0$，于是$f(x) = C(常数)$，又$f(0) = \frac{\pi}{2}$，证毕。（由$f^{\prime}(x) = 0$可知，函数的变化率为0，所以$f(x)$不变化，为一常数）</p><blockquote><p>【证注】：</p><p>拉格朗日定理的公式为：</p><script type="math/tex; mode=display">f(b) - f(a) = f^{\prime}(\xi)(b - a)</script><p>当$f^{\prime}(x) = 0$时，$f(b) - f(a) = 0$，所以函数值不变化。</p></blockquote></blockquote></li><li><p>特殊函数值：</p><script type="math/tex; mode=display"> \begin{matrix} \arcsin 0 = 0, & \arcsin\frac{1}{2} = \frac{\pi}{6}, & \arcsin\frac{\sqrt{2}}{2} = \frac{\pi}{4}, & \arcsin\frac{\sqrt{3}}{2} = \frac{\pi}{3}, & \arcsin 1 = \frac{\pi}{2}, \\ \arccos 1 = 0, & \arccos\frac{\sqrt{3}}{2} = \frac{\pi}{6}, & \arccos\frac{\sqrt{2}}{2} = \frac{\pi}{4}, & \arccos\frac{1}{2} = \frac{\pi}{3}, & \arccos 0 = \frac{\pi}{2}。 \end{matrix}</script></li></ol></blockquote><p><strong>(ii) 反正切函数与反余切函数</strong></p><p>反正切函数$y = \arctan x$（见图1-10(a)），反余切函数$y = \operatorname{arccot} x$（见图1-10(b)）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723150118120.png" alt="image-20230723150118120"></p><p>$y = \arctan x$是$y = \tan x\left(-\frac{\pi}{2} &lt; x &lt; \frac{\pi}{2}\right)$的反函数，$y = \operatorname{arccot} x$是$y = \cot x(0 &lt; x &lt; \pi)$的反函数。</p><blockquote><p>【注】：</p><ol><li><p>定义域：$(-\infty, +\infty)$。</p><p> 值域：$y = \arctan x$的值域为$\left(-\frac{\pi}{2}, \frac{\pi}{2}\right)$，$y = \operatorname{arccot} x$的值域为$(0, \pi)$。</p></li><li><p>单调性：$y = \arctan x$单调增加，$y = \operatorname{arccot} x$单调减少。</p></li><li><p>奇偶性：$y = \arctan x$为奇函数（在其定义域内）。</p></li><li><p>有界性：两个函数在其定义域内有界，$-\frac{\pi}{2} &lt; \arctan x &lt; \frac{\pi}{2}, 0 &lt; \operatorname{arccot} x &lt; \pi$。</p></li><li><p>性质：$\arctan x + \operatorname{arccot} x = \frac{\pi}{2}(-\infty \le x \le \infty)$。</p><blockquote><p>【证明】：</p><p>$(\arctan x + \operatorname{arccot} x)^{\prime} = \frac{1}{1 + x^2} - \frac{1}{1 + x^2} = 0$，其余同理。</p></blockquote></li><li><p>特殊函数值：</p><script type="math/tex; mode=display"> \begin{matrix} \arctan 0 = 0, & \arctan\frac{\sqrt{3}}{3} = \frac{\pi}{6}, & \arctan 1 = \frac{\pi}{4}, & \arctan\sqrt{3} = \frac{\pi}{3}, \\ \operatorname{arccot} 0 = \frac{\pi}{2}, & \operatorname{arccot}\sqrt{3} = \frac{\pi}{6}, & \operatorname{arccot} 1 = \frac{\pi}{4}, & \operatorname{arccot}\frac{\sqrt{3}}{3} = \frac{\pi}{3}。 \end{matrix}</script></li><li><p>极限：</p><script type="math/tex; mode=display"> \begin{aligned} &\lim_{x \to -\infty} \arctan x = -\frac{\pi}{2}, \\ &\lim_{x \to +\infty} \arctan x = \frac{\pi}{2}, \\ &\lim_{x \to -\infty} \operatorname{arccot} x = \pi, \\ &\lim_{x \to +\infty} \operatorname{arccot} x = 0。 \end{aligned}</script><p> 注意，这里两者$\lim_{x \to \infty}$时的极限不存在（因为极限不唯一）。</p></li></ol></blockquote><h6 id="⑦-初等函数"><a href="#⑦-初等函数" class="headerlink" title="⑦ 初等函数"></a>⑦ 初等函数</h6><p>由基本初等函数经有限次的四则运算，以及有限次的复合步骤所构成的并且可以由一个式子所表示的函数称为初等函数。</p><blockquote><p>【注】：</p><ol><li><p>初等函数的定义域可以是一个区间，也可以时几个区间的并集，甚至可以是一些孤立的点，例如，$y = \sqrt{\cos \pi x - 1}$的定义域是$x = 0, \pm2, \pm4, \cdots$。</p></li><li><p>幂指函数$u(x)^{v(x)} = e^{v(x)\ln u(x)}$也是初等函数，如$x &gt; 0$时，$f(x) = x^x = e^{x \ln x}$时初等函数，其图形如图1-11所示。</p><p> <img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723152618145.png" alt="image-20230723152618145"></p><blockquote><p>【注注】：</p><p>画草图的过程如下：</p><ol><li><p>求两端：</p><script type="math/tex; mode=display"> \begin{aligned} \lim_{x \to 0^+} x^x &= \lim_{x \to 0^+} e^{x \ln x} = 1, \\ \lim_{x \to +\infty} x^x &= +\infty。 \end{aligned}</script></li><li><p>单调性：</p><ol><li><p>直接求导：</p><script type="math/tex; mode=display"> (x^x)^{\prime} = (e^{x \ln x})^{\prime} = (x^x)(\ln x + 1)。</script></li><li><p>令导数为0（导数为0的点称为驻点）：</p><script type="math/tex; mode=display"> (x^x)(\ln x + 1) = 0,</script><p> 解出$x = \frac{1}{e}$，且导数在$(0, \frac{1}{e})$上小于0，在$(\frac{1}{e}, +\infty)$上大于0，可得到函数的单调性。</p></li></ol></li></ol></blockquote></li></ol></blockquote><h5 id="2-分段函数"><a href="#2-分段函数" class="headerlink" title="(2) 分段函数"></a>(2) 分段函数</h5><p>在自变量的不同变化范围中，对应法则用不同式子来表示的函数称为分段函数。需要强调一句，分段函数是用几个式子来表示的一个（不是几个）函数，一般说来它不是初等函数。分段函数的典型形式如下：</p><script type="math/tex; mode=display">f(x) =\begin{aligned}\begin{cases}\varphi_1(x), &x > x_0, \\a, &x = x_0, \\\varphi_2(x), &x < x_0,\end{cases}\end{aligned}或f(x) =\begin{aligned}\begin{cases}\varphi(x), &x \ne x_0, \\a, &x = x_0。\end{cases}\end{aligned}</script><p>分段函数很重要，原因在于其形式的复杂性所带来的命题的丰富性。后面会看到，不论是求极限、求导数，还是求积分，出现最多的研究对象之一便是分段函数。</p><p>下面列出三个重要的分段函数。</p><h6 id="①-绝对值函数"><a href="#①-绝对值函数" class="headerlink" title="① 绝对值函数"></a>① 绝对值函数</h6><p>$y = \left|x\right|$称为绝对值函数。</p><script type="math/tex; mode=display">y = \left|x\right| =\begin{aligned}\begin{cases}x, &x \ge 0, \\-x, & x < 0。\end{cases}\end{aligned}</script><p>如图1-12所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723155316200.png" alt="image-20230723155316200"></p><h6 id="②-符号函数"><a href="#②-符号函数" class="headerlink" title="② 符号函数"></a>② 符号函数</h6><p>$y = \text{sgn} x$称为符号函数。</p><script type="math/tex; mode=display">y = \text{sgn} x =\begin{aligned}\begin{cases}1, &x \ge 0, \\0, &x = 0, \\-1, & x < 0。\end{cases}\end{aligned}</script><p>如图1-13所示，对于任何实数$x$，有$x = \left|x\right| \text{sgn} x$。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723155328541.png" alt="image-20230723155328541"></p><h6 id="③-取整函数"><a href="#③-取整函数" class="headerlink" title="③ 取整函数"></a>③ 取整函数</h6><p>$y = [x]$称为取整函数。给出定义：设$x$任一实数，不超过$x$的最大整数称为$x$的整数部分，记作$[x]$。如</p><script type="math/tex; mode=display">[0.99] = 0, [\pi] = 3, [-1] = -1, [-1.99] = -2。</script><p>因此，取整函数$y = [x]$的定义域为$\mathbf{R}$，值域为$\mathbf{Z}$。它的图形如图1-14所示，在$x$为整数值处图形发生跳跃。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723155708352.png" alt="image-20230723155708352"></p><p>从定义出发，以下两点需要读者注意。</p><ol><li>$x - 1 \le [x] \le x$；</li><li>$\lim_{x \to 0^+}[x] = 0, \lim_{x \to 0^-}[x] = -1$。</li></ol><h4 id="2-图像变换"><a href="#2-图像变换" class="headerlink" title="2. 图像变换"></a>2. 图像变换</h4><p>图像变换方式一般有如下三种。</p><h5 id="1-平移变换"><a href="#1-平移变换" class="headerlink" title="(1) 平移变换"></a>(1) 平移变换</h5><h6 id="①-左加右减"><a href="#①-左加右减" class="headerlink" title="① 左加右减"></a>① 左加右减</h6><p>将函数$y = f(x)$的图像沿$x$轴向左平移$x_0(x_0 &gt; 0)$个单位长度，得到函数$f(x + x_0)$的图像（见图1-15）；将函数$y = f(x)$的图像沿$x$轴向右平移$x_0(x_0 &gt; 0)$个单位长度，得到函数$f(x - x_0)$的图像（见图1-16）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723160124808.png" alt="image-20230723160124808"></p><h6 id="②-上加下减"><a href="#②-上加下减" class="headerlink" title="② 上加下减"></a>② 上加下减</h6><p>将函数$y = f(x)$的图像沿$y$轴向上平移$y_0(y_0 &gt; 0)$个单位长度，得到函数$f(x) + y_0$的图像（见图1-17）；将函数$y = f(x)$的图像沿$y$轴向下平移$y_0(y_0 &gt; 0)$个单位长度，得到函数$f(x) - y_0$的图像（见图1-18）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723160139994.png" alt="image-20230723160139994"></p><h5 id="2-对称变换"><a href="#2-对称变换" class="headerlink" title="(2) 对称变换"></a>(2) 对称变换</h5><p>谁对称，谁不变。</p><h6 id="①-关于-x-轴对称"><a href="#①-关于-x-轴对称" class="headerlink" title="① 关于$x$轴对称"></a>① 关于$x$轴对称</h6><p>将函数$y = f(x)$的图像关于$x$轴对称，得到函数$y = -f(x)$的图像（见图1-19）。</p><h6 id="②-关于-y-轴对称"><a href="#②-关于-y-轴对称" class="headerlink" title="② 关于$y$轴对称"></a>② 关于$y$轴对称</h6><p>将函数$y = f(x)$的图像关于$y$轴对称，得到函数$y = f(-x)$的图像（见图1-20）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723160208780.png" alt="image-20230723160208780"></p><h6 id="③-关于原点对称"><a href="#③-关于原点对称" class="headerlink" title="③ 关于原点对称"></a>③ 关于原点对称</h6><p>将函数$y = f(x)$的图像关于原点对称，得到函数$y = -f(-x)$的图像（见图1-21）。</p><h6 id="④-关于直线-y-x-对称"><a href="#④-关于直线-y-x-对称" class="headerlink" title="④ 关于直线$y = x$对称"></a>④ 关于直线$y = x$对称</h6><p>将函数$y = f(x)$的图像关于直线$y = x$对称，得到函数$y = f^{-1}(x)$的图像（见图1-22）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723160227921.png" alt="image-20230723160227921"></p><h6 id="⑤-函数值绝对值对称"><a href="#⑤-函数值绝对值对称" class="headerlink" title="⑤ 函数值绝对值对称"></a>⑤ 函数值绝对值对称</h6><p>保留函数$y = f(x)$在$x$轴及$x$轴上方的部分，把$x$轴下方的部分关于$x$轴对称到$x$轴上方并去掉原来下方的部分，得到函数$y = \left|f(x)\right|$的图像（见图1-23）。</p><h6 id="⑥-自变量绝对值对称"><a href="#⑥-自变量绝对值对称" class="headerlink" title="⑥ 自变量绝对值对称"></a>⑥ 自变量绝对值对称</h6><p>保留函数$y = f(x)$在$y$轴及$y$轴右侧的部分，把$y$轴右侧的部分关于$y$轴对称到$y$轴左侧并去掉原来左侧的部分，得到函数$y = f(\left|x\right|)$的图像（见图1-24）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723160243368.png" alt="image-20230723160243368"></p><blockquote><p>【注】：</p><p>在$x = 0$的小邻域中，有$\left|\sin x\right| = \sin \left|x\right|$。</p></blockquote><h5 id="3-伸缩变换"><a href="#3-伸缩变换" class="headerlink" title="(3) 伸缩变换"></a>(3) 伸缩变换</h5><h6 id="①-水平伸缩"><a href="#①-水平伸缩" class="headerlink" title="① 水平伸缩"></a>① 水平伸缩</h6><p>$y = f(kx)(k &gt; 1)$的图像，可由$y = f(x)$的图像上每点的横坐标缩短到原来的$\frac{1}{k}$倍且纵坐标不变得到（见图1-25）；$y = f(kx)(0 &lt; k &lt; 1)$的图像，可由$y = f(x)$的图像上每点的横坐标伸长到原来的$\frac{1}{k}$倍且纵坐标不变得到。</p><h6 id="②-垂直伸缩"><a href="#②-垂直伸缩" class="headerlink" title="② 垂直伸缩"></a>② 垂直伸缩</h6><p>$y = kf(x)(k &gt; 1)$的图像，可由$y = f(x)$的图像上每点的纵坐标伸长到原来的$k$倍且横坐标不变得到（见图1-26）；$y = kf(x)(0 &lt; k &lt; 1)$的图像，可由$y = f(x)$的图像上每点的纵坐标缩短到原来的$k$倍且横坐标不变得到。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723160258275.png" alt="image-20230723160258275"></p><h3 id="（二）极坐标系下的图像-g-r-theta-0"><a href="#（二）极坐标系下的图像-g-r-theta-0" class="headerlink" title="（二）极坐标系下的图像$(g(r, \theta) = 0)$"></a>（二）极坐标系下的图像$(g(r, \theta) = 0)$</h3><h4 id="1-用描点法画常见图像"><a href="#1-用描点法画常见图像" class="headerlink" title="1. 用描点法画常见图像"></a>1. 用描点法画常见图像</h4><h5 id="1-心形线（外摆线）"><a href="#1-心形线（外摆线）" class="headerlink" title="(1) 心形线（外摆线）"></a>(1) 心形线（外摆线）</h5><p>表达式为$r = a(1 - \cos \theta)(a &gt; 0)$，图像如图1-27所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723163226087.png" alt="image-20230723163226087"></p><h5 id="2-玫瑰线"><a href="#2-玫瑰线" class="headerlink" title="(2) 玫瑰线"></a>(2) 玫瑰线</h5><p>表达式为$r = a\sin 3\theta(a &gt; 0)$，图像如图1-28所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723163417752.png" alt="image-20230723163417752"></p><h5 id="3-阿基米德螺线"><a href="#3-阿基米德螺线" class="headerlink" title="(3) 阿基米德螺线"></a>(3) 阿基米德螺线</h5><p>表达式为$r = a\theta(a &gt; 0, \theta \ge 0)$，图像如图1-29所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723163459298.png" alt="image-20230723163459298"></p><h5 id="4-伯努利双纽线"><a href="#4-伯努利双纽线" class="headerlink" title="(4) 伯努利双纽线"></a>(4) 伯努利双纽线</h5><p>表达式为$r^2 = a^2\cos 2\theta(a &gt; 0)$，图像如图1-30所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723163548406.png" alt="image-20230723163548406"></p><p>表达式为$r^2 = a^2\sin 2\theta(a &gt; 0)$，图像如图1-31所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723163610056.png" alt="image-20230723163610056"></p><p>绘制顺序如下。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723163717559.png" alt="image-20230723163717559"></p><h4 id="2-用直角系观点画极坐标下图像"><a href="#2-用直角系观点画极坐标下图像" class="headerlink" title="2. 用直角系观点画极坐标下图像"></a>2. 用直角系观点画极坐标下图像</h4><p>意思就是将$r = f(\theta)$中的$r$看作$y$、$\theta$看作$x$，然后将$y = f(x)$的图像在直角坐标系中画出来，便于求出特殊点的值。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723163946719.png" alt="image-20230723163946719"></p><p>比如$r = 2(1 + \cos \theta)$，如果直接在极坐标中求值，相较于直角坐标系而言没有那么直观，所以直接画到直角坐标系中求特殊值。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723164133752.png" alt="image-20230723164133752"></p><h3 id="（三）参数法-参数方程-left-begin-cases-x-x-t-y-y-t-end-cases-right"><a href="#（三）参数法-参数方程-left-begin-cases-x-x-t-y-y-t-end-cases-right" class="headerlink" title="（三）参数法-参数方程$\left(\begin{cases}x = x(t) \\ y = y(t)\end{cases}\right)$"></a>（三）参数法-参数方程$\left(\begin{cases}x = x(t) \\ y = y(t)\end{cases}\right)$</h3><h4 id="1-摆线（平摆线）"><a href="#1-摆线（平摆线）" class="headerlink" title="1. 摆线（平摆线）"></a>1. 摆线（平摆线）</h4><p>当一个圆沿一条定直线做纯滚动时，动圆圆周上一个定点的轨迹叫做摆线（见图1-37）。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723204315524.png" alt="image-20230723204315524"></p><p>现取已给的这条定直线为$x$轴，其正方向就是圆滚动的方向，当圆与直线在圆上的定点$A$处相切时，就取该点为原点$O$。取半径$\left|CO\right|$旋转的角度$t$为参数。对于所求运动轨迹上的任意一定点$A(x, y)$，由图1-38容易看出</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723204616129.png" alt="image-20230723204616129"></p><script type="math/tex; mode=display">\begin{aligned}&x = \left|OP\right| = \left|OQ\right| - \left|PQ\right|, \\&\left|OQ\right| = 圆弧\stackrel\frown{QA}的长度 = rt, \\&\left|PQ\right| = \left|AC^{\prime}\right|\sin t = r\sin t,\end{aligned}</script><p>故得：</p><script type="math/tex; mode=display">x = rt - r\sin t。</script><p>由图1-38也容易看出</p><script type="math/tex; mode=display">y = \left|PA\right| = \left|QC^{\prime}\right| - \left|DC^{\prime}\right| = r - r\cos t。</script><p>因此，所求定点$A$的运动轨迹的参数方程为</p><script type="math/tex; mode=display">\begin{cases}x = r(t - \sin t), \\y = r(1 - \cos t)。\end{cases}</script><blockquote><p>【注】：</p><p>上面的推导过程只适用于$0 \le t &lt; \frac{\pi}{2}$的情况，当$t$取其他任意值时，推到的方法是相仿的，所得结果与$\begin{cases}x = r(t - \sin t), \\ y = r(1 - \cos t)。\end{cases}$完全一样。因此$\begin{cases}x = r(t - \sin t), \\ y = r(1 - \cos t)。\end{cases}$中没有写出$t$的变化范围，这就意味着$t$可取任意实数值。</p><p>摆线的图形具有周期性，当$t$增加$2\pi$时，也就是说，圆滚动一周时，摆线上的点的横坐标增加了$2\pi r$，纵坐标不变。圆继续滚动，圆上的定点$A$就描绘出一拱接一拱的图形。容易看出，从原点开始的第一拱以直线$x = \pi r$为对称轴，拱顶的坐标是$(\pi r, 2r)$。</p><p>要从$\begin{cases}x = r(t - \sin t), \\ y = r(1 - \cos t)。\end{cases}$式中消去参数$t$并不困难，但所得$x, y$间的函数表达式较复杂。因此我们常通过$\begin{cases}x = r(t - \sin t), \\ y = r(1 - \cos t)。\end{cases}$来直接研究摆线。</p></blockquote><h4 id="2-星形线（内摆线）"><a href="#2-星形线（内摆线）" class="headerlink" title="2. 星形线（内摆线）"></a>2. 星形线（内摆线）</h4><p>如图1-39(a)所示，一个小圆在一个固定的大圆内部作纯滚动，如果大圆半径是小圆半径的4倍，那么小圆圆周上任一点$M$的轨迹称为星形线，如图1-39(b)所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723210337892.png" alt="image-20230723210337892"></p><p>此轨迹方程的推导过程要用到繁杂的几何知识与三角公式，不作要求，读者记住它的参数方程表达式即可，其表达式为</p><script type="math/tex; mode=display">\begin{cases}x = r\cos^3 t, \\y = r\sin^3 t。\end{cases}</script><p>若消去$t$，可得$x^{\frac{2}{3}} + y^{\frac{2}{3}} = r^{\frac{2}{3}}$，此为直角坐标方程。</p><h2 id="三、常用基础知识"><a href="#三、常用基础知识" class="headerlink" title="三、常用基础知识"></a>三、常用基础知识</h2><h3 id="（一）数列"><a href="#（一）数列" class="headerlink" title="（一）数列"></a>（一）数列</h3><h4 id="1-等差数列"><a href="#1-等差数列" class="headerlink" title="1. 等差数列"></a>1. 等差数列</h4><p>首项为$a_1$，公差为$d(d \ne 0)$的数列$a_1, a_1 + d, a_1 + 2d, \cdots, a_1 + (n - 1)d, \cdots$。</p><ol><li>通项公式$a_n = a_1 + (n - 1)d$。</li><li>前$n$项的和$S_n = \frac{n}{2}[2a_1 + (n - 1)d] = \frac{n}{2}(a_1 + a_n)$。</li></ol><h4 id="2-等比数列"><a href="#2-等比数列" class="headerlink" title="2. 等比数列"></a>2. 等比数列</h4><p>首项为$a_1$，公比为$r(r \ne 0)$的数列$a_1, a_1r, a_1r^2, \cdots, a_1r^{n - 1}, \cdots$。</p><ol><li><p>通项公式$a_n = a_1r^{n - 1}$。</p></li><li><p>前$n$项的和$S_n = \begin{aligned}\begin{cases}na_1, &amp;r = 1, \\ \frac{a_1(1 - r^n)}{1 - r}, &amp;r \ne 1\end{cases}\end{aligned}$。</p></li><li><p>常用$1 + r + r^2 + \cdots + r^{n - 1} = \frac{1 - r^n}{1 - r}$。</p><blockquote><p>【注】：</p><p>若$\left|r\right| &lt; 1$，则$\sum_{n = 1}^{\infty}r^{n - 1} = \lim_{n \to \infty}\frac{1 - r^n}{1 - r} = \frac{1}{1 - r}$，故$\sum_{n = 1}^{\infty}r^{n - 1} = \frac{1}{1 - r}(\left|r\right| &lt; 1)$。</p></blockquote></li></ol><h4 id="3-一些常见数列前-n-项的和"><a href="#3-一些常见数列前-n-项的和" class="headerlink" title="3. 一些常见数列前$n$项的和"></a>3. 一些常见数列前$n$项的和</h4><ol><li>$\sum_{k = 1}^{n}k = 1 + 2 + 3 + \cdots + n = \frac{n(n + 1)}{2}$。</li><li>$\sum_{k = 1}^{n}k^2 = 1^2 + 2^2 + 3^2 + \cdots + n^2 = \frac{n(n + 1)(2n + 1)}{6}$。</li><li>$\sum_{k = 1}^{n}\frac{1}{k(k + 1)} = \frac{1}{1 \times 2} + \frac{1}{2 \times 3} + \frac{1}{3 \times 4} + \cdots + \frac{1}{n(n + 1)} = \frac{n}{n + 1}$。</li></ol><h3 id="（二）三角函数"><a href="#（二）三角函数" class="headerlink" title="（二）三角函数"></a>（二）三角函数</h3><h4 id="1-三角函数基本关系"><a href="#1-三角函数基本关系" class="headerlink" title="1. 三角函数基本关系"></a>1. 三角函数基本关系</h4><script type="math/tex; mode=display">\begin{matrix}\csc \alpha = \frac{1}{\sin \alpha}, & \sec \alpha = \frac{1}{\cos \alpha}, & \cot \alpha = \frac{1}{\tan \alpha}, \\\tan \alpha = \frac{\sin \alpha}{\cos \alpha}, & \cot \alpha = \frac{\cos \alpha}{\sin \alpha},\end{matrix}</script><script type="math/tex; mode=display">\begin{aligned}&\sin^2 \alpha + \cos^2 \alpha = 1, \\&1 + \tan^2 \alpha = \sec^2 \alpha, \\&1 + \cot^2 \alpha = \csc^2 \alpha。\end{aligned}</script><h4 id="2-诱导公式"><a href="#2-诱导公式" class="headerlink" title="2. 诱导公式"></a>2. 诱导公式</h4><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723212409645.png" alt="image-20230723212409645"></p><blockquote><p>【注】：</p><p>如上表所示，奇变偶不变，符号看象限（因任一角度均可表示为$\frac{k\pi}{2} + \alpha, k \in \mathbf{Z}, \left|\alpha\right| \le \frac{\pi}{4}$，故$k$为奇数时得角$\alpha$的异名函数值，$k$为偶数时得角$\alpha$的同名函数值，然后在前面加上一个把角$\alpha$看作锐角时原来函数值的符号）。</p></blockquote><p>三角函数在四个象限中的符号表如下表所示。</p><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723212710933.png" alt="image-20230723212710933"></p><blockquote><p>【注】：</p><p>对于直角坐标系中一点$A(x_A, y_A)$，其与$x$轴正半轴所成角度为$\alpha$，那么显然，$x_A$的值在第一、四象限为正，$y_A$的值在第二、三象限为正。</p><p>令$r = \sqrt{x^2_A + y^2_A}$，又$\sin \alpha = \frac{y}{r}, \cos \alpha = \frac{x}{r}$，所以$\sin \alpha$在第一、二象限为正，$\cos \alpha$在第二、三象限为正。</p></blockquote><h4 id="3-特殊的三角函数值如下表所示"><a href="#3-特殊的三角函数值如下表所示" class="headerlink" title="3. 特殊的三角函数值如下表所示"></a>3. 特殊的三角函数值如下表所示</h4><p><img src="https://img.karltan.com/notes-out-class/taking-the-postgraduate-entrance-examination/calculus/01/image-20230723213134479.png" alt="image-20230723213134479"></p><blockquote><p>【注】：</p><ol><li>$\sec \alpha$和$\csc \alpha$的函数值可由$\frac{1}{\cos \alpha}$和$\frac{1}{\sin \alpha}$得出。</li><li>表格中的“$\infty$”均是指极限结果，如$\tan 90^{\circ}$为“$\infty$”，是指$\lim_{x \to 90^{\circ}}\tan x = \infty$。</li></ol></blockquote><h4 id="4-重要公式"><a href="#4-重要公式" class="headerlink" title="4. 重要公式"></a>4. 重要公式</h4><h5 id="1-倍角公式"><a href="#1-倍角公式" class="headerlink" title="(1) 倍角公式"></a>(1) 倍角公式</h5><script type="math/tex; mode=display">\begin{aligned}\sin 2\alpha &= 2\sin \alpha \cos \alpha, \\\cos 2\alpha&= \cos^2 \alpha - \sin^2 \alpha \\&= 1 - 2\sin^2 \alpha \\&= 2\cos^2 \alpha - 1, \\\sin 3\alpha &= -4\sin^3 \alpha + 3\sin \alpha, \\\cos 3\alpha &= 4\cos^3 \alpha - 3\cos \alpha, \\\tan 2\alpha &= \frac{2\tan \alpha}{1 - \tan^2 \alpha}, \\\cot 2\alpha &= \frac{\cot^2 \alpha - 1}{2\cot \alpha}。\end{aligned}</script><h5 id="2-半角公式"><a href="#2-半角公式" class="headerlink" title="(2) 半角公式"></a>(2) 半角公式</h5><script type="math/tex; mode=display">\begin{aligned}\sin^2 \frac{\alpha}{2} &= \frac{1}{2}(1 - \cos \alpha),(降幂公式) \\\cos^2 \frac{\alpha}{2} &= \frac{1}{2}(1 + \cos \alpha),(降幂公式) \\\sin \frac{\alpha}{2} &= \pm\sqrt{\frac{1 - \cos \alpha}{2}}, \\\cos \frac{\alpha}{2} &= \pm\sqrt{\frac{1 + \cos \alpha}{2}}, \\\tan \frac{\alpha}{2}&= \frac{1 - \cos \alpha}{\sin \alpha} \\&= \frac{\sin \alpha}{1 + \cos \alpha} \\&= \pm\sqrt{\frac{1 - \cos \alpha}{1 + \cos \alpha}}, \\\cot \frac{\alpha}{2}&= \frac{\sin \alpha}{1 - \cos \alpha} \\&= \frac{1 + \cos \alpha}{\sin \alpha} \\&= \pm\sqrt{\frac{1 + \cos \alpha}{1 - \cos \alpha}}。\end{aligned}</script><h5 id="3-和差公式"><a href="#3-和差公式" class="headerlink" title="(3) 和差公式"></a>(3) 和差公式</h5><script type="math/tex; mode=display">\begin{aligned}\sin (\alpha \pm \beta) &= \sin \alpha \cos \beta \pm \cos \alpha \sin \beta, \\\cos (\alpha \pm \beta) &= \cos \alpha \cos \beta \mp \sin \alpha \sin \beta, \\\tan (\alpha \pm \beta) &= \frac{\tan \alpha \pm \tan \beta}{1 \mp \tan \alpha \tan \beta}, \\\cot (\alpha \pm \beta) &= \frac{\cot \alpha \cot \beta \mp 1}{\cot \beta \pm \cot \alpha}。\end{aligned}</script><h5 id="4-积化和差与和差化积公式"><a href="#4-积化和差与和差化积公式" class="headerlink" title="(4) 积化和差与和差化积公式"></a>(4) 积化和差与和差化积公式</h5><h6 id="①-积化和差公式"><a href="#①-积化和差公式" class="headerlink" title="① 积化和差公式"></a>① 积化和差公式</h6><script type="math/tex; mode=display">\begin{aligned}\sin \alpha \cos \beta &= \frac{1}{2}[\sin (\alpha + \beta) + \sin (\alpha - \beta)], \\\cos \alpha \sin \beta &= \frac{1}{2}[\sin (\alpha + \beta) - \sin (\alpha - \beta)], \\\cos \alpha \cos \beta &= \frac{1}{2}[\cos (\alpha + \beta) + \cos (\alpha - \beta)], \\\sin \alpha \sin \beta &= \frac{1}{2}[\cos (\alpha - \beta) - \cos (\alpha + \beta)]。\end{aligned}</script><h6 id="②-和差化积公式"><a href="#②-和差化积公式" class="headerlink" title="② 和差化积公式"></a>② 和差化积公式</h6><script type="math/tex; mode=display">\begin{aligned}\sin \alpha + \sin \beta &= 2 \sin \frac{\alpha + \beta}{2} \cos \frac{\alpha - \beta}{2}, \\\sin \alpha - \sin \beta &= 2 \sin \frac{\alpha - \beta}{2} \cos \frac{\alpha + \beta}{2}, \\\cos \alpha + \cos \beta &= 2 \cos \frac{\alpha + \beta}{2} \cos \frac{\alpha - \beta}{2}, \\\cos \alpha - \cos \beta &= -2 \sin \frac{\alpha + \beta}{2} \sin \frac{\alpha - \beta}{2}。\end{aligned}</script><h5 id="5-万能公式"><a href="#5-万能公式" class="headerlink" title="(5) 万能公式"></a>(5) 万能公式</h5><p>若$u = \tan \frac{x}{2}(-\pi &lt; x &lt; \pi)$，则$\sin x = \frac{2u}{1 + u^2}, \cos x = \frac{1 - u^2}{1 + u^2}$。</p><h3 id="（三）指数运算法则"><a href="#（三）指数运算法则" class="headerlink" title="（三）指数运算法则"></a>（三）指数运算法则</h3><script type="math/tex; mode=display">\begin{aligned}a^{\alpha} \cdot a^{\beta} &= a^{\alpha + \beta}, \\\frac{a^{\alpha}}{a^{\beta}} &= a^{\alpha - \beta}, \\(a^{\alpha})^{\beta} &= a^{\alpha\beta}, \\(ab)^{\alpha} &= a^{\alpha}b^{\alpha}, \\\left(\frac{a}{b}\right)^{\alpha} &= \frac{a^{\alpha}}{b^{\alpha}}。\end{aligned}</script><p>其中$a, b$是正实数，$\alpha, \beta$是任意实数。</p><h3 id="（四）对数运算法则"><a href="#（四）对数运算法则" class="headerlink" title="（四）对数运算法则"></a>（四）对数运算法则</h3><ol><li>$\log_a(MN) = \log_aM + \log_aN$（积的对数$=$对数的和）。</li><li>$\log_a\frac{M}{N} = \log_aM - \log_aN$（商的对数$=$对数的差）。</li><li>$\log_aM^n = n\log_aM$（幂的对数$=$对数的倍数）。</li><li>$\log_a\sqrt[n]{M} = \frac{1}{n}\log_aM$。</li></ol><blockquote><p>【注】：</p><p>常考：</p><ol><li>$\ln \sqrt{x} = \frac{1}{2}\ln x$；</li><li>$\ln \frac{1}{x} = -\ln x$；</li><li>$\ln (1 + \frac{1}{x}) = \ln \frac{x + 1}{x} = \ln (x + 1) - \ln x$。</li></ol><p>【例】：</p><p>当$x &gt; 0$时，证明$\frac{1}{1 + x} &lt; \ln(1 + \frac{1}{x}) &lt; \frac{1}{x}$。</p><p>【解】：</p><p>$\ln(1 + \frac{1}{x}) = \ln(x + 1) - \ln x$，令$f(x) = \ln x$，在$[x, x + 1]$上使用拉格朗日中值定理，得$f(x + 1) - f(x) = f^{\prime}(\xi)(x + 1 - x) = f^{\prime}(\xi) = \frac{1}{\xi}$，所以$\ln (1 + \frac{1}{x}) = \frac{1}{\xi}$，又$\xi \in [x, x + 1]$，所以$\frac{1}{\xi} \in \left[\frac{1}{x + 1}, \frac{1}{x}\right]$，所以$\ln(1 + \frac{1}{x}) \in \left[\frac{1}{x + 1}, \frac{1}{x}\right]$。</p></blockquote><h3 id="（五）一元二次方程基础"><a href="#（五）一元二次方程基础" class="headerlink" title="（五）一元二次方程基础"></a>（五）一元二次方程基础</h3><ol><li><p>一元二次方程$ax^2 + bx + c = 0(a \ne 0)$。</p></li><li><p>根的公式$x_{1, 2} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$。</p></li><li><p>根与系数的关系（韦达定理）$x_1 + x_2 = -\frac{b}{a}, x_1x_2 = \frac{c}{a}$。</p></li><li><p>判别式$\Delta = b^2 - 4ac$。</p><p> $\Delta &gt; 0$，方程有两个不等的实根；$\Delta = 0$，方程有两个相等的实根；$\Delta &lt; 0$，方程有两个共轭的复根。</p></li><li><p>抛物线$y = ax^2 + bx + c$的顶点$\left(-\frac{b}{2a}, c - \frac{b^2}{4a}\right)$。</p></li></ol><h3 id="（六）因式分解公式"><a href="#（六）因式分解公式" class="headerlink" title="（六）因式分解公式"></a>（六）因式分解公式</h3><ol><li><p>$(a + b)^2 = a^2 + 2ab + b^2$。</p></li><li><p>$(a - b)^2 = a^2 - 2ab + b^2$。</p></li><li><p>$(a + b)^3 = a^3 + 3a^2b + 3ab^2 + b^3$。</p></li><li><p>$(a - b)^3 = a^3 - 3a^2b + 3ab^2 - b^3$。</p></li><li><p>$a^2 - b^2 = (a + b)(a - b)$。</p></li><li><p>$a^3 - b^3 = (a - b)(a^2 + ab + b^2)$。</p></li><li><p>$a^3 + b^3 = (a + b)(a^2 - ab + b^2)$。</p></li><li><p>$a^n - b^n = (a - b)(a^{n - 1} + a^{n - 2}b + \cdots + ab^{n - 2} + b^{n - 1})$（$n$为正整数）。</p></li><li><p>$n$为正偶数时，$a^n - b^n = (a + b)(a^{n - 1} - a^{n - 2}b + \cdots + ab^{n - 2} - b^{n - 1})$。</p></li><li><p>$n$为正奇数时，$a^n + b^n = (a + b)(a^{n - 1} - a^{n - 2}b + \cdots - ab^{n - 2} + b^{n - 1})$。</p></li><li><p>二项式定理</p><script type="math/tex; mode=display">\begin{aligned}(a + b)^n&= \sum_{k = 0}^{n}C_{n}^{k}a^{n - k}b^{k} \\&= a^n + na^{n - 1}b + \frac{n(n + 1)}{2!}a^{n - 2}b^2 + \cdots + \frac{n(n - 1)\cdots(n - k + 1)}{k!}a^{n - k}b^{k} + \cdots + nab^{n - 1} + b^n。\end{aligned}</script></li></ol><blockquote><p>【注】：</p><p>杨辉三角：</p><p><a href="https://baike.baidu.com/item/杨辉三角/215098">杨辉三角_百度百科 (baidu.com)</a></p><script type="math/tex; mode=display">\begin{matrix}1 & & & & & & & n = 0 & C_{0}^{0} \\1 & 1 & & & & & & n = 1 & C_{1}^{0} & C_{1}^{1} \\1 & 2 & 1 & & & & & n = 2 & C_{2}^{0} & C_{2}^{1} & C_{2}^{2} \\1 & 3 & 3 & 1 & & & & n = 3 & C_{3}^{0} & C_{3}^{1} & C_{3}^{2} & C_{3}^{3} \\1 & 4 & 6 & 4 & 1 & & & n = 4 & C_{4}^{0} & C_{4}^{1} & C_{4}^{2} & C_{4}^{3} & C_{4}^{4} \\1 & 5 & 10 & 10 & 5 & 1 & & n = 5 & C_{5}^{0} & C_{5}^{1} & C_{5}^{2} & C_{5}^{3} & C_{5}^{4} & C_{5}^{5} \\1 & 6 & 15 & 20 & 15 & 6 & 1 & n = 6 & C_{6}^{0} & C_{6}^{1} & C_{6}^{2} & C_{6}^{3} & C_{6}^{4} & C_{6}^{5} & C_{6}^{6}\end{matrix}</script><p>【注】：</p><p>排列数与组合数：</p><p><a href="https://blog.csdn.net/qq_43539599/article/details/104467386">排列数A(n, m)与组合数C(n, m)的求法-CSDN博客</a></p><ol><li>排列数：$A_{n}^{m} = n(n - 1)\cdots(n - m + 1) = \frac{n!}{(n - m)!}$，从$n$个元素中选出$m$进行排列，共有$A_{n}^{m}$种排列方法。</li><li>组合数：$C_{n}^{m} = \frac{A_{n}^{m}}{m!} = \frac{n!}{m!(n - m)!} = C_{n}^{n - m}$，从$n$个元素中选出$m$个元素，共有$C_{n}^{m}$种选法。</li></ol></blockquote><h3 id="（七）阶乘与双阶乘"><a href="#（七）阶乘与双阶乘" class="headerlink" title="（七）阶乘与双阶乘"></a>（七）阶乘与双阶乘</h3><ol><li>$n! = 1 \times 2 \times 3 \times \cdots \times n$，规定$0! = 1$。</li><li>$(2n)!! = 2 \times 4 \times 6 \times \cdots \times (2n) = 2^n \times n!$。</li><li>$(2n - 1)!! = 1 \times 3 \times 5 \times \cdots \times (2n - 1)$。</li></ol><blockquote><p>【注】：</p><p>点火公式：</p><ol><li>$\int_{0}^{\frac{\pi}{2}}\sin^{10} x\mathrm{d}x = \frac{9}{10} \times \frac{7}{8} \times \frac{5}{6} \times \frac{3}{4} \times \frac{1}{2} \times \frac{\pi}{2} = \frac{9!!}{10!!} \times \frac{\pi}{2}$（偶数次方）。</li><li>$\int_{0}^{\frac{\pi}{2}}\cos^{9} x\mathrm{d}x = \frac{8}{9} \times \frac{6}{7} \times \frac{4}{5} \times \frac{2}{3} = \frac{8!!}{9!!}$（奇数次方）。</li></ol></blockquote><h3 id="（八）常用不等式"><a href="#（八）常用不等式" class="headerlink" title="（八）常用不等式"></a>（八）常用不等式</h3><ol><li><p>设$a, b$为实数，则</p><ol><li><p>$\left|a \pm b\right| \le \left|\left|a\right| + \left|b\right|\right|$；</p></li><li><p>$\left|\left|a\right| - \left|b\right|\right| \le \left|a - b\right|$。</p><blockquote><p>【注】：</p><p>可以将上述不等式1推广为</p><ul><li><p>离散情况：设$a_1, a_2, \cdots, a_n$为实数，则</p><script type="math/tex; mode=display">  \left|a_1 \pm a_2 \pm \cdots \pm a_n\right| \le \left|a_1\right| + \left|a_2\right| + \cdots + \left|a_n\right|。</script></li><li><p>连续情况：设$f(x)$在$[a, b](a &lt; b)$上可积，则</p><script type="math/tex; mode=display">  \left|\int_{a}^{b}f(x)\mathrm{d}x\right| \le \int_{a}^{b}\left|f(x)\right|\mathrm{d}x。</script></li></ul></blockquote></li></ol></li><li><p>几何平均值$\le$算数平均值$\le$均方根</p><ol><li><p>$\sqrt{ab} \le \frac{a + b}{2} \le \sqrt{\frac{a^2 + b^2}{2}}(a, b &gt; 0)$；</p><blockquote><p>【注】：</p><p>还有$\left|ab\right| \le \frac{a^2 + b^2}{2}$，在考研中考过：若$u_n &gt; 0$，则$\frac{u_n}{n} = u_n \times \frac{1}{n} \le \frac{u_n^2 + \frac{1}{n^2}}{2}$。</p></blockquote></li><li><p>$\sqrt[3]{abc} \le \frac{a + b + c}{3} \le \sqrt{\frac{a^2 + b^2 + c^2}{3}}(a, b, c &gt; 0)$。</p></li></ol></li><li><p>设$a &gt; b &gt; 0$，则$\begin{cases}当n &gt; 0时, a^n &gt; b^n, \\ 当n &lt; 0时, a^n &lt; b ^n\end{cases}$。</p></li><li><p>若$0 &lt; a &lt; x &lt; b, 0 &lt; c &lt; y &lt; d$，则$\frac{c}{b} &lt; \frac{y}{x} &lt; \frac{d}{a}$。</p><blockquote><p>【注】：</p><p>考研中考过：当$n\pi &lt; x &lt; (n + 1)\pi, 2n &lt; S(x) &lt; 2(n + 1)$时，$\frac{2n}{(n + 1)\pi} &lt; \frac{S(x)}{x} &lt; \frac{2(n + 1)}{n\pi}$。</p></blockquote></li><li><p>$\sin x &lt; x &lt; \tan x(0 &lt; x &lt; \frac{\pi}{2})$。</p></li><li><p>$\sin x &lt; x(x &gt; 0)$。</p><blockquote><p>【注】：</p><p>考研中考过：当$x_n &gt; 0$时，$x_{n + 1} = \sin x_n &lt; x_n$，故$\{x_n\}$单调减少。</p></blockquote></li><li><p>$\arctan x \le x \le \arcsin x(0 \le x \le 1)$。</p></li><li><p>$e^x \ge x + 1(\forall x)$。</p><blockquote><p>【注】：</p><p>可考：当$x_{n + 1} = e^{x_n} - 1$时，由$e^{x_n} - 1 \ge x_n$，得$x_{n + 1} \ge x_n$，即$\{x_n\}$单调不减。</p></blockquote></li><li><p>$x - 1 \ge \ln x(x &gt; 0)$。</p><blockquote><p>【注】：</p><p>可考：当$x_n &gt; 0$时，若$x_{n + 1} = \ln x_n + 1$，由$\ln x_n + 1 \le x_n$，得$x_{n + 1} \le x_n$，即$\{x_n\}$单调不增。</p></blockquote></li><li><p>$\frac{1}{1 + x} &lt; \ln (1 + \frac{1}{x}) &lt; \frac{1}{x}(x &gt; 0)$。</p><blockquote><p>【注】：</p><p>证明：</p><p>令$f(x) = \ln x$，并在区间$[x, x + 1]$上对其应用拉格朗日中值定理，有</p><script type="math/tex; mode=display">\ln (1 + \frac{1}{x}) = \ln (1 + x) - \ln x = \frac{1}{\xi},</script><p>其中$0 &lt; x &lt; \xi &lt; x + 1$。因此，对任意的$x &gt; 0$，有$\frac{1}{1 + x} &lt; \ln (1 + \frac{1}{x}) = \frac{1}{\xi} &lt; \frac{1}{x}$。</p></blockquote></li></ol>]]></content>
    
    
    <summary type="html">张宇考研数学基础30讲 高等数学分册 第1讲 高等数学预备知识。</summary>
    
    
    
    <category term="课外笔记" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/"/>
    
    <category term="考研" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/%E8%80%83%E7%A0%94/"/>
    
    <category term="高等数学" scheme="http://blog.karltan.com/categories/%E8%AF%BE%E5%A4%96%E7%AC%94%E8%AE%B0/%E8%80%83%E7%A0%94/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="笔记" scheme="http://blog.karltan.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="数学" scheme="http://blog.karltan.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
    <category term="高等数学" scheme="http://blog.karltan.com/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 3326. 最大硬币数</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3326/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3326/</id>
    <published>2023-07-16T08:00:00.000Z</published>
    <updated>2023-07-16T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-3326-最大硬币数"><a href="#AcWing-3326-最大硬币数" class="headerlink" title="AcWing 3326. 最大硬币数"></a>AcWing 3326. 最大硬币数</h1><p><a href="https://www.acwing.com/problem/content/3329/">3326. 最大硬币数 - AcWing题库</a></p><p>Mike有一个$N$行$N$列的方格矩阵。</p><p>位于第$i$行第$j$列的方格的位置坐标表示为$(i, j)$。</p><p>矩阵左上角方格的坐标即为$(1, 1)$。</p><p>每个方格中都包含一定数量的硬币，Mike只有到达一个方格内时，方可收集方格中的硬币。</p><p>$C_{i, j}$表示第$i$行第$j$列的方格中的硬币数量。</p><p>当Mike处于方格$(i, j)$时，他可以选择移动至方格$(i - 1, j - 1)$或方格$(i + 1, j + 1)$中，前提是所选择的方格位于矩阵边界内，且之前没有到达过。</p><p>Mike可以选择从任意方格开始移动，也可以选择在移动至任意方格时结束移动。</p><p>Mike希望尽可能多的收集硬币。</p><p>请帮助他确定他可以收集的最大硬币数量。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含整数$N$。</p><p>接下来$N$行，每行包含$N$个整数，其中第$i$行第$j$列的整数表示$C_{i, j}$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为可以收集的最大硬币数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&60\%的数据满足, 1 \le T \le 100, 1 \le N \le 100。\\&另外40\%的数据满足, 1 \le T \le 10, 1 \le N \le 1000。\\&100\%的数据满足, 0 \le C_{i, j} \le 10^7。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">3</span><br><span class="line">1 2 5</span><br><span class="line">3 6 1</span><br><span class="line">12 2 7</span><br><span class="line">5</span><br><span class="line">0 0 0 0 0</span><br><span class="line">1 1 1 1 0</span><br><span class="line">2 2 2 8 0</span><br><span class="line">1 1 1 0 0</span><br><span class="line">0 0 0 0 0</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 14</span><br><span class="line">Case #2: 9</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>对于测试数据$1$，Mike可以选择的行进路径为$(1, 1) \to (2, 2) \to (3, 3)$，可收集到的最大硬币数量为$14$。</p><p>对于测试数据$2$，Mike可以选择的行进路径为$(2, 3) \to (3, 4)$，可收集到的最大硬币数量为$9$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1010</span>;</span><br><span class="line"><span class="type">int</span> g[N][N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j ++)</span><br><span class="line">            &#123;</span><br><span class="line">                cin &gt;&gt; g[i][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 我分为了上下两个三角矩阵</span></span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 这里是下三角矩阵</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> row = <span class="number">1</span>; row &lt;= n; row ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> i = row;</span><br><span class="line">            <span class="type">int</span> j = <span class="number">1</span>;</span><br><span class="line">            <span class="type">long</span> <span class="type">long</span> temp = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (i &lt;= n &amp;&amp; j &lt;= n)</span><br><span class="line">            &#123;</span><br><span class="line">                temp += g[i][j];</span><br><span class="line">                i ++;</span><br><span class="line">                j ++;</span><br><span class="line">            &#125;</span><br><span class="line">            res = <span class="built_in">max</span>(res, temp);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里是上三角矩阵</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> col = <span class="number">2</span>; col &lt;= n; col ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> i = <span class="number">1</span>;</span><br><span class="line">            <span class="type">int</span> j = col;</span><br><span class="line">            <span class="type">long</span> <span class="type">long</span> temp = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (i &lt;= n &amp;&amp; j &lt;= n)</span><br><span class="line">            &#123;</span><br><span class="line">                temp += g[i][j];</span><br><span class="line">                i ++;</span><br><span class="line">                j ++;</span><br><span class="line">            &#125;</span><br><span class="line">            res = <span class="built_in">max</span>(res, temp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输出long long的时候记得使用%lld</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lld\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> LL;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">int</span> w[N][N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j ++ )</span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;w[i][j]);</span><br><span class="line"></span><br><span class="line">        LL res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            LL sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> x = i, y = <span class="number">1</span>; x &lt;= n &amp;&amp; y &lt;= n; x ++, y ++ )</span><br><span class="line">                sum += w[x][y];</span><br><span class="line">            res = <span class="built_in">max</span>(res, sum);</span><br><span class="line"></span><br><span class="line">            sum = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">1</span>, y = i; x &lt;= n &amp;&amp; y &lt;= n; x ++, y ++ )</span><br><span class="line">                sum += w[x][y];</span><br><span class="line">            res = <span class="built_in">max</span>(res, sum);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lld\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 3326. 最大硬币数。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4114. 垃圾桶</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4114/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4114/</id>
    <published>2023-07-16T02:00:00.000Z</published>
    <updated>2023-07-16T03:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4114-垃圾桶"><a href="#AcWing-4114-垃圾桶" class="headerlink" title="AcWing 4114. 垃圾桶"></a>AcWing 4114. 垃圾桶</h1><p><a href="https://www.acwing.com/problem/content/4117/">4114. 垃圾桶 - AcWing题库</a></p><p>一条很长的街道上有$N$个房子。</p><p>第一个房子在位置$1$，第二个房子在位置$2$，以此类推。</p><p>任意一对房子$i$和$j$之间的距离为$\left| i - j \right|$。</p><p>一些房子的位置处有垃圾桶。</p><p>每个房子的主人都要倒垃圾。</p><p>如果自己房子前面有垃圾桶，则无需移动，直接倒垃圾即可。</p><p>如果自己房子前面没有垃圾桶，则前往距离自己最近的垃圾桶处倒垃圾，如果这样的垃圾桶不唯一，则任意前往一个即可。</p><p>请计算，所有房子的主人倒垃圾需要行走的总距离之和。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含整数$N$。</p><p>第二行包含一个长度为$N$的$01$字符串，第$i$个字符如果为$1$，则表示第$i$个房屋门前有垃圾桶，如果为$0$，则表示第$i$个房屋门前没有垃圾桶。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为所有房子的主人倒垃圾需要行走的总距离之和。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&1 \le T \le 100, \\&1 \le N \le 5 \times 10^5, \\&字符串中至少包含一个1。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">3</span><br><span class="line">111</span><br><span class="line">6</span><br><span class="line">100100</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 0</span><br><span class="line">Case #2: 5</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>对于Case 1，每个房子门前都有垃圾桶，所以大家都不用移动。</p><p>对于Case 2，第$1, 4$个房子的门前有垃圾桶，这两家主人不用移动，第$2$个房子的主人去第$1$个房子处倒垃圾，第$3, 5, 6$个房子的主人去第$2$个房子处倒垃圾。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">500010</span>;</span><br><span class="line"><span class="comment">// l[i]存储的是第i个房子左边最近垃圾桶的下标</span></span><br><span class="line"><span class="comment">// r[i]存储的是第i个房子右边最近垃圾桶的下标</span></span><br><span class="line"><span class="type">int</span> l[N], r[N];</span><br><span class="line"><span class="type">char</span> s[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="comment">// 这里cin过不了</span></span><br><span class="line">        <span class="comment">// cin &gt;&gt; s;</span></span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>, s);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 假设左边不存在垃圾桶</span></span><br><span class="line">        <span class="type">int</span> d = <span class="number">-2100000000</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果位置i上有垃圾桶</span></span><br><span class="line">            <span class="comment">// 更新左边最近垃圾桶的坐标</span></span><br><span class="line">            <span class="keyword">if</span> (s[i] == <span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                d = i;</span><br><span class="line">            &#125;</span><br><span class="line">            l[i] = d;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 假设右边不存在垃圾桶</span></span><br><span class="line">        d = <span class="number">2100000000</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i --)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果位置i上有垃圾桶</span></span><br><span class="line">            <span class="comment">// 更新右边最近垃圾桶的坐标</span></span><br><span class="line">            <span class="keyword">if</span> (s[i] == <span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                d = i;</span><br><span class="line">            &#125;</span><br><span class="line">            r[i] = d;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里如果用int会爆int</span></span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 在两个距离中取小值</span></span><br><span class="line">            res += <span class="built_in">min</span>(i - l[i], r[i] - i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lld\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4114. 垃圾桶。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 3325. Kick_Start</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3325/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3325/</id>
    <published>2023-07-16T01:00:00.000Z</published>
    <updated>2023-07-16T02:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-3325-Kick-Start"><a href="#AcWing-3325-Kick-Start" class="headerlink" title="AcWing 3325. Kick_Start"></a>AcWing 3325. Kick_Start</h1><p><a href="https://www.acwing.com/problem/content/3328/">3325. Kick_Start - AcWing题库</a></p><p>Ksenia非常喜欢读书，因此每天她都会从自己最喜欢的书中选取一段内容进行阅读，然后再开始她早晨的其他活动。</p><p>一段内容可以看作是整个文本中的一个子字符串。</p><p>Ksenia有点迷信，她坚信如果阅读的这段内容是以字符串<code>KICK</code>开头，然后中间包含$0$​个或更多个字符，最后以字符串<code>START</code>结尾，即使这段内容没什么意义，她的一天也会非常的幸运。</p><p>给定这本书的全部文本内容，请你数一数在这本书变得破旧不堪，Ksenia不得不再买新书之前，共有多少个幸运片段可供她阅读。</p><p>只要两个片段的起始位置或结束位置不同，就认为这两个片段是不同的，即使它们包含的内容完全相同。</p><p>还需注意，不同片段之间可能会有重叠部分。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据共一行，包含一个仅由大写字母构成的字符串$S$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为幸运片段的数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\1 &\le \left| S \right| \le 10^5。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">AKICKSTARTPROBLEMNAMEDKICKSTART</span><br><span class="line">STARTUNLUCKYKICK</span><br><span class="line">KICKXKICKXSTARTXKICKXSTART</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 3</span><br><span class="line">Case #2: 0</span><br><span class="line">Case #3: 5</span><br></pre></td></tr></table></figure><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>看了别人的题解之后，豁然开朗。</p><p>大概的思路就是统计当前<code>KICK</code>出现的次数，然后遇到<code>START</code>时就在答案中加上当前<code>KICK</code>出现的次数。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        string s;</span><br><span class="line">        cin &gt;&gt; s;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// num_kick是当前KICK出现的次数</span></span><br><span class="line">        <span class="type">int</span> num_kick = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; s.<span class="built_in">length</span>(); i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果是KICK</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">3</span> &lt; s.<span class="built_in">length</span>()</span><br><span class="line">                &amp;&amp; s[i] == <span class="string">&#x27;K&#x27;</span></span><br><span class="line">                &amp;&amp; s[i + <span class="number">1</span>] == <span class="string">&#x27;I&#x27;</span></span><br><span class="line">                &amp;&amp; s[i + <span class="number">2</span>] == <span class="string">&#x27;C&#x27;</span></span><br><span class="line">                &amp;&amp; s[i + <span class="number">3</span>] == <span class="string">&#x27;K&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                num_kick ++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果是START</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">4</span> &lt; s.<span class="built_in">length</span>()</span><br><span class="line">                &amp;&amp; s[i] == <span class="string">&#x27;S&#x27;</span></span><br><span class="line">                &amp;&amp; s[i + <span class="number">1</span>] == <span class="string">&#x27;T&#x27;</span></span><br><span class="line">                &amp;&amp; s[i + <span class="number">2</span>] == <span class="string">&#x27;A&#x27;</span></span><br><span class="line">                &amp;&amp; s[i + <span class="number">3</span>] == <span class="string">&#x27;R&#x27;</span></span><br><span class="line">                &amp;&amp; s[i + <span class="number">4</span>] == <span class="string">&#x27;T&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                res += num_kick;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">char</span> str[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>, str);</span><br><span class="line">        string s = str;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, cnt = <span class="number">0</span>; i &lt; s.<span class="built_in">size</span>(); i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (s.<span class="built_in">substr</span>(i, <span class="number">4</span>) == <span class="string">&quot;KICK&quot;</span>) cnt ++ ;</span><br><span class="line">            <span class="keyword">if</span> (s.<span class="built_in">substr</span>(i, <span class="number">5</span>) == <span class="string">&quot;START&quot;</span>) res += cnt;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 3325. Kick_Start。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 3748. 递增子串</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3748/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3748/</id>
    <published>2023-07-16T00:00:00.000Z</published>
    <updated>2023-07-16T01:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-3748-递增子串"><a href="#AcWing-3748-递增子串" class="headerlink" title="AcWing 3748. 递增子串"></a>AcWing 3748. 递增子串</h1><p><a href="https://www.acwing.com/problem/content/3751/">3748. 递增子串 - AcWing题库</a></p><p>你的朋友约翰刚刚度假归来，他迫不及待地想要跟你分享他了解到的关于字符串的一个新性质。</p><p>他了解到，如果一个长度为$L$的大写字母构成的字符串$C$，满足对于每对索引$i, j$（$1 \le i &lt; j \le L$，索引编号$1 \sim L$），位置$i$处的字符均小于位置$j$处的字符，则该字符串是严格递增的。</p><p>例如，字符串<code>ABC</code>和<code>ADF</code>是严格递增的，而字符串<code>ACC</code>和<code>FDA</code>则不是。</p><p>在教给你这个关于字符串的新性质后，他打算考一考你：</p><p>给定一个长度为$N$的字符串$S$，请你计算对于每个位置$i$（$1 \le i \le N$），以该位置结束的最长严格递增子串的长度是多少？</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据占两行，第一行包含整数$N$，第二行包含一个长度为$N$的由大写字母构成的字符串$S$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y1 y2 ... yn</code>，其中$x$为组别编号（从$1$开始），$y_i$为以位置$i$结束的最长严格递增子串的长度。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&全部数据: 1 \le T \le 100。\\&测试点1(小数据测试点): 1 \le N \le 200。\\&测试点2(大数据测试点): 1 \le N \le 2 \times 10^5。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">4</span><br><span class="line">ABBC</span><br><span class="line">6</span><br><span class="line">ABACDA</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 1 2 1 2</span><br><span class="line">Case #2: 1 2 1 2 3 1</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>对于测试数据$1$，在位置$1$、$2$、$3$和$4$处结束的最长严格递增子串分别为<code>A</code>、<code>AB</code>、<code>B</code>和<code>BC</code>。</p><p>对于测试数据$2$，在位置$1 \sim 6$处结束的最长严格递增子串分别为<code>A</code>、<code>AB</code>、<code>A</code>、<code>AC</code>、<code>ACD</code>和<code>A</code>。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>我还以为会TLE，没想到过了。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        string s;</span><br><span class="line">        cin &gt;&gt; s;</span><br><span class="line"></span><br><span class="line">        vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">        <span class="comment">// 遍历每一个可能的结尾</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i --)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 最短的递增字串长度为1，即某元素本身</span></span><br><span class="line">            <span class="type">int</span> temp = <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 向前遍历</span></span><br><span class="line">            <span class="comment">// 看每两个相邻的元素之间是否为升序</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j --)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 如果是升序，长度++</span></span><br><span class="line">                <span class="keyword">if</span> (s[j + <span class="number">1</span>] &gt; s[j])</span><br><span class="line">                &#123;</span><br><span class="line">                    temp ++;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 否则直接结束</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 将结果存到res中</span></span><br><span class="line">            res.<span class="built_in">push_back</span>(temp);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = res.<span class="built_in">size</span>() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i --)</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; res[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">200010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">char</span> s[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%s&quot;</span>, &amp;n, s);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, f = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (!i || s[i - <span class="number">1</span>] &lt; s[i]) f ++ ;</span><br><span class="line">            <span class="keyword">else</span> f = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, f);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">puts</span>(<span class="string">&quot;&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 3748. 递增子串。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4118. 狗和猫</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4118/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4118/</id>
    <published>2023-07-15T12:00:00.000Z</published>
    <updated>2023-07-15T13:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4118-狗和猫"><a href="#AcWing-4118-狗和猫" class="headerlink" title="AcWing 4118. 狗和猫"></a>AcWing 4118. 狗和猫</h1><p><a href="https://www.acwing.com/problem/content/4121/">4118. 狗和猫 - AcWing题库</a></p><p>你在动物收容所工作，负责喂养动物。</p><p>你一共准备了$D$份狗粮和$C$份猫粮。</p><p>一共有$N$只动物排队等候用餐，有的是狗，有的是猫。</p><p>当然，也有可能全都是狗或者全都是猫。</p><p>我们可以用一个长度为$N$的由大写字母$C$和$D$组成的字符串$S$来表示队列中猫狗的顺序。</p><p>如果队列中第$i$只动物是猫，则第$i$个字符为$C$。</p><p>如果队列中第$i$只动物是狗，则第$i$个字符为$D$。</p><p>动物们严格按照排队顺序依次进食。</p><p>每只狗吃一份狗粮，每只猫吃一份猫粮。</p><p>此外，你还有额外的猫粮。</p><p>每当一条狗吃完一份狗粮，你就会为猫多提供$M$份猫粮。</p><p>每只动物都只会在排在其前面的所有动物都进食完毕后，才肯进食。</p><p>这也就意味着，当轮到某只动物进食，但是却没有相应的食物时，它和排在它后面的所有动物都会因此无法进食。</p><p>请问，在这种情况下，队列中的<strong>所有狗</strong>能否都得到喂食。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含四个整数$N, D, C, M$。</p><p>第二行包含一个长度为$N$的由大写字母$C$和$D$组成的字符串$S$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），如果所有狗都能得到喂食，则$y$为<code>YES</code>，否则$y$为<code>NO</code>。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\1 &\le N \le 10^4, \\0 &\le D, C \le 10^6, \\0 &\le M \le 10^6。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">6 10 4 0</span><br><span class="line">CCDCDD</span><br><span class="line">4 1 2 0</span><br><span class="line">CCCC</span><br><span class="line">4 2 1 0</span><br><span class="line">DCCD</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case #1: YES</span><br><span class="line">Case #2: YES</span><br><span class="line">Case #3: NO</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>在Case 1中，一共有$10$份狗粮和$4$份猫粮。</p><ol><li>前两只动物是猫，喂食它们后，还剩下$2$份猫粮。</li><li>接下来是一只狗，喂食它后，还剩下$9$份狗粮。</li><li>然后是一只猫，喂食它后，还剩下$1$份猫粮。</li><li>最后是两只狗，喂食它们后，还剩下$7$份狗粮。</li></ol><p>所有狗都被喂食。</p><p>在Case 2中，没有狗，因此，所有狗（$0$只）都被喂食了。</p><p>在Case 3中，第二只狗前面的猫得不到喂食，所有第二只狗也没法得到喂食。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">12 4 2 2</span><br><span class="line">CDCCCDCCDCDC</span><br><span class="line">8 2 1 3</span><br><span class="line">DCCCCCDC</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: YES</span><br><span class="line">Case #2: NO</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>在Case 1中，每只狗喂食完毕后，都会额外得到两份猫粮。</p><ol><li>首先是一只猫，喂食它后，还剩下$1$份猫粮。</li><li>接下来是一只狗，喂食它后，还剩下$3$份狗粮和$3$份猫粮。</li><li>接下来是三只猫，喂食它们后，还剩下$3$份狗粮和$0$份猫粮。</li><li>接下来是一只狗，喂食它后，还剩下$2$份狗粮和$2$份猫粮。</li><li>接下来是两只猫，喂食它们后，还剩下$2$份狗粮和$0$份猫粮。</li><li>接下来是一只狗，喂食它后，还剩下$1$份狗粮和$2$份猫粮。</li><li>接下来是一只猫，喂食它后，还剩下$1$份狗粮和$1$份猫粮。</li><li>接下来是最后一只狗，喂食它后，还剩下$0$份狗粮和$3$份猫粮。</li></ol><p>所有狗都被喂食。</p><p>在Case 2中，第二只狗前面的猫得不到喂食，所有第二只狗也没法得到喂食。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 这里开long long才能过</span></span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> n, d, c, m;</span><br><span class="line">        cin &gt;&gt; n &gt;&gt; d &gt;&gt; c &gt;&gt; m;</span><br><span class="line">        string s;</span><br><span class="line">        cin &gt;&gt; s;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 标记结果</span></span><br><span class="line">        <span class="type">int</span> res = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果是猫</span></span><br><span class="line">            <span class="keyword">if</span> (s[i] == <span class="string">&#x27;C&#x27;</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 且有猫粮</span></span><br><span class="line">                <span class="keyword">if</span> (c &gt; <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 吃</span></span><br><span class="line">                    c --;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 否则</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 看这只猫后面是不是还有狗</span></span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; n; j ++)</span><br><span class="line">                    &#123;</span><br><span class="line">                        <span class="comment">// 如果有狗</span></span><br><span class="line">                        <span class="keyword">if</span> (s[j] == <span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="comment">// 那么这只狗是吃不到狗粮的</span></span><br><span class="line">                            res = <span class="number">0</span>;</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果是狗</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 且有狗粮</span></span><br><span class="line">                <span class="keyword">if</span> (d &gt; <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    d --;</span><br><span class="line">                    c += m;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 否则直接吃不到</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    res = <span class="number">0</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %s\n&quot;</span>, cases, res ? <span class="string">&quot;YES&quot;</span> : <span class="string">&quot;NO&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">10010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">int</span> d, c;</span><br><span class="line"><span class="type">char</span> s[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">check</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (s[i] == <span class="string">&#x27;D&#x27;</span>)  <span class="comment">// 狗</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (!d) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            d -- ;</span><br><span class="line">            c += m;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>  <span class="comment">// 猫</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (c) c -- ;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> j = i + <span class="number">1</span>; j &lt; n; j ++ )</span><br><span class="line">                    <span class="keyword">if</span> (s[j] == <span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">                        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d%d&quot;</span>, &amp;n, &amp;d, &amp;c, &amp;m);</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>, s);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">check</span>()) <span class="built_in">puts</span>(<span class="string">&quot;YES&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">puts</span>(<span class="string">&quot;NO&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4118. 狗和猫。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 3752. 更小的字符串</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3752/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3752/</id>
    <published>2023-07-15T11:00:00.000Z</published>
    <updated>2023-07-15T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-3752-更小的字符串"><a href="#AcWing-3752-更小的字符串" class="headerlink" title="AcWing 3752. 更小的字符串"></a>AcWing 3752. 更小的字符串</h1><p><a href="https://www.acwing.com/problem/content/3755/">3752. 更小的字符串 - AcWing题库</a></p><p>给定一个整数$K$和一个长度为$N$的字符串$S$。</p><p>已知，字符串$S$是由前$K$个小写字母组成。</p><p>现在，请你找出满足下列条件的<strong>回文字符串</strong>的数量：</p><ol><li>长度为$N$。</li><li>字典序上小于$S$。</li><li>由前$K$个小写字母组成。</li></ol><p>由于满足条件的字符串数量可能很大，所以输出对$10^9 + 7$取模后的答案。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有组测试数据。</p><p>每组数据第一行包含两个整数$N$和$K$。</p><p>第二行包含一个长度为$N$的由小写字母组成的字符串$S$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为对$10^9 + 7$取模后的答案。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\1 &\le N \le 10^5, \\1 &\le K \le 26。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">2 3</span><br><span class="line">bc</span><br><span class="line">5 5</span><br><span class="line">abcdd</span><br><span class="line">1 5</span><br><span class="line">d</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 2</span><br><span class="line">Case #2: 8</span><br><span class="line">Case #3: 3</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>对于样例$1$，满足条件的回文串为<code>aa</code>，<code>bb</code>。</p><p>对于样例$2$，满足条件的回文串为<code>aaaaa</code>，<code>aabaa</code>，<code>aacaa</code>，<code>aadaa</code>，<code>aaeaa</code>，<code>ababa</code>，<code>abbba</code>，<code>abcba</code>。</p><p>对于样例$3$，满足条件的回文串为<code>a</code>，<code>b</code>，<code>c</code>。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>假设现在分析的字符串是<code>bcbae</code>，那么可以画出一棵树：</p><script type="math/tex; mode=display">\begin{cases}a\_\ \_\ \_\ \_, 选a后, 第1, 2位字母(从0开始)可以任选, 有5^2种选法 \\b\_\ \_\ \_\ \_, 选b后, 无法确定该串的字典序是否小于原串\begin{cases}ba\_\ \_\ \_, 选a后, 第2位字母(从0开始)可以任选, 有5种选法 \\bb\_\ \_\ \_, 选b后, 第2位字母(从0开始)可以任选, 有5种选法 \\bc\_\ \_\ \_, 同理, 继续选\begin{cases}bca\_\ \_, 选a后, 因为整个串是一个回文串 \\所以这个串已经确定了, 为bcacb \\且此时字典序是比原串小的, 所以该串是一种方案 \\bcb\_\ \_, 选b后，该串也确定了, 为bcbcb, \\那么显然, 这个串是不符合要求的\end{cases}\end{cases}\end{cases}</script><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> MOD = <span class="number">1e9</span> + <span class="number">7</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// p[i]是用来存储k的i次幂的</span></span><br><span class="line"><span class="comment">// 这样可以加速运算</span></span><br><span class="line"><span class="type">int</span> p[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n, k;</span><br><span class="line">        cin &gt;&gt; n &gt;&gt; k;</span><br><span class="line">        string s;</span><br><span class="line">        cin &gt;&gt; s;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化k的0次幂为1</span></span><br><span class="line">        p[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            p[i] = ((<span class="type">long</span> <span class="type">long</span>)p[i - <span class="number">1</span>] * k) % MOD;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化结果</span></span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 只需要选择前一半的字母</span></span><br><span class="line">        <span class="type">int</span> mid = (n + <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; mid; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// s[i] - &#x27;a&#x27;是该位上有多少个字母比原串中的字母小</span></span><br><span class="line">            <span class="comment">// p[mid - i - 1]是剩余可自由选择的位数</span></span><br><span class="line">            <span class="comment">// 一共有mid位可自由选择，即0 ~ mid - 1</span></span><br><span class="line">            <span class="comment">// 当前已经到了第i位，所以还有i + 1 ~ mid - 1可选</span></span><br><span class="line">            <span class="comment">// 所以有mid - 1 - (i + 1) + 1 = mid - i - 1个</span></span><br><span class="line">            <span class="comment">// 即k的mid - i - 1次方</span></span><br><span class="line">            res += ((<span class="type">long</span> <span class="type">long</span>)(s[i] - <span class="string">&#x27;a&#x27;</span>) * p[mid - i - <span class="number">1</span>]) % MOD;</span><br><span class="line">            <span class="comment">// 最后再将结果取模</span></span><br><span class="line">            res %= MOD;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 现在看最后一个方案是否可行</span></span><br><span class="line">        <span class="type">int</span> flag = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 从原串的mid - 1位开始反向遍历</span></span><br><span class="line">        <span class="comment">// 相当于从新串的mid + 1位开始正向遍历</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = mid - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i --)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 取出原串的对应位</span></span><br><span class="line">            <span class="type">int</span> j = n - <span class="number">1</span> - i;</span><br><span class="line">            <span class="comment">// 如果这两位不等的话，需要比较大小</span></span><br><span class="line">            <span class="keyword">if</span> (s[i] != s[j])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 如果新串小于原串</span></span><br><span class="line">                <span class="keyword">if</span> (s[i] &lt; s[j])</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 则最后一种方案有效</span></span><br><span class="line">                    flag = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 加上可能存在的最后一种方案</span></span><br><span class="line">        res = (res + flag) % MOD;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 3752. 更小的字符串。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4443. 无限区域</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4443/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4443/</id>
    <published>2023-07-15T08:00:00.000Z</published>
    <updated>2023-07-15T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4443-无限区域"><a href="#AcWing-4443-无限区域" class="headerlink" title="AcWing 4443. 无限区域"></a>AcWing 4443. 无限区域</h1><p><a href="https://www.acwing.com/problem/content/4446/">4443. 无限区域 - AcWing题库</a></p><p>给定一个无限大的二维平面，设点$S$为该平面的中心点。</p><p>设经过点$S$的垂直方向的直线为$P$，如果直线$P$是一个圆的切线，且切点恰好为点$S$，那么：</p><ul><li>如果该圆位于直线$P$的右侧，则称之为右圆。</li><li>如果该圆位于直线$P$的左侧，则称之为左圆。</li></ul><p>现在，给定三个整数$R, A, B$，你需要按照右圆、左圆、右圆、左圆…的顺序不断画圆，具体要求如下：</p><ul><li>第一个右圆的半径等于$R$。</li><li>每个左圆的半径等于你画的<strong>上一个圆</strong>的半径乘以$A$。</li><li>每个右圆（第一个除外）的半径等于你画的<strong>上一个圆</strong>的半径除以$B$（向下取整）。</li><li>当你要画的圆的半径等于$0$时，绘画停止。</li></ul><p><img src="https://img.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4443.png" alt="InfinityAreaImage"></p><p>请你计算，所有画出的圆的面积之和。</p><p>保证绘画会在有限数量的步骤后停止。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据占一行，包含三个整数$R, A, B$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为面积和（实数）。</p><p>$y$在正确答案的$10^{-6}$的绝对或相对误差范围内，则视为正确。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&1 \le T \le 100, \\&1 \le R \le 10^5, \\&1 \le A \le 500, \\&2 \times A \le B \le 1000。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">1 3 6</span><br><span class="line">5 2 5</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 31.415927</span><br><span class="line">Case #2: 455.530935</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>对于Case 1，首先画一个半径为$1$的右圆，然后画一个半径为$1 \times 3 = 3$的左圆，随后停止绘画，因为下一个右圆的半径为$\left \lfloor \frac{3}{6} \right \rfloor = 0$。</p><p>对于Case 2：</p><ol><li>第一步画一个半径为$5$的右圆；</li><li>第二步画一个半径为$5 \times 2 = 10$的左圆；</li><li>第三步画一个半径为$\left \lfloor \frac{10}{5} \right \rfloor = 2$的右圆；</li><li>第四步画一个半径为$2 \times 2 = 4$的左圆；</li><li>停止绘画，因为下一个右圆的半径为$\left \lfloor \frac{4}{5} \right \rfloor = 0$。</li></ol><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PI 3.14159265</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> r, a, b;</span><br><span class="line">        cin &gt;&gt; r &gt;&gt; a &gt;&gt; b;</span><br><span class="line">        <span class="type">int</span> R = r;</span><br><span class="line">        <span class="type">double</span> res = <span class="number">0.0</span>;</span><br><span class="line">        <span class="comment">// true表示右圆</span></span><br><span class="line">        <span class="type">bool</span> type = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">while</span> (R)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 计算面积</span></span><br><span class="line">            res += PI * R * R;</span><br><span class="line">            <span class="comment">// 如果是右圆，执行左圆的操作</span></span><br><span class="line">            <span class="keyword">if</span> (type == <span class="literal">true</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                R *= a;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果是左圆，执行右圆的操作</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                R /= b;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 更改类型</span></span><br><span class="line">            type = !type;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lf\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">double</span> PI = <span class="built_in">acos</span>(<span class="number">-1</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">double</span> <span class="title">area</span><span class="params">(<span class="type">double</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> r * r * PI;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> r, a, b;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;r, &amp;a, &amp;b);</span><br><span class="line"></span><br><span class="line">        <span class="type">double</span> res = <span class="built_in">area</span>(r);</span><br><span class="line">        <span class="keyword">while</span> (r)</span><br><span class="line">        &#123;</span><br><span class="line">            r *= a;</span><br><span class="line">            res += <span class="built_in">area</span>(r);</span><br><span class="line">            r /= b;</span><br><span class="line">            res += <span class="built_in">area</span>(r);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lf\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4443. 无限区域。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4122. 字符串转换</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4122/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4122/</id>
    <published>2023-07-15T07:00:00.000Z</published>
    <updated>2023-07-15T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4122-字符串转换"><a href="#AcWing-4122-字符串转换" class="headerlink" title="AcWing 4122. 字符串转换"></a>AcWing 4122. 字符串转换</h1><p><a href="https://www.acwing.com/problem/content/4125/">4122. 字符串转换 - AcWing题库</a></p><p>给定一个由小写字母构成的字符串$S$。</p><p>再给定一个由若干个<strong>各不相同</strong>的小写字母按<strong>字典序</strong>排序构成的字符串$F$。</p><p>现在，你可以对字符串$S$进行字符转换操作。</p><p>每次选中其中一个字符（即某个小写字母），将其转换为一个按照字母顺序与其相邻（上一个或下一个）的小写字母。</p><p>例如，$c$可以转换为$b$或$d$。</p><p>额外的，我们将按照循环顺序考虑字母，即我们认为$a$的上一个字母为$z$，$z$的下一个字母为$a$。</p><p>请问，至少需要进行多少次操作，可以使得字符串$S$中的每个字母都出现在字符串$F$中。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含一个字符串$S$。</p><p>第二行包含一个字符串$F$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为最少操作次数。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&1 \le T \le 100, \\&1 \le \left| S \right| \le 10^5, \\&1 \le \left| F \right| \le 26。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">abcd</span><br><span class="line">a</span><br><span class="line">pppp</span><br><span class="line">p</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 6</span><br><span class="line">Case #2: 0</span><br></pre></td></tr></table></figure><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">pqrst</span><br><span class="line">ou</span><br><span class="line">abd</span><br><span class="line">abd</span><br><span class="line">aaaaaaaaaaaaaaab</span><br><span class="line">aceg</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 9</span><br><span class="line">Case #2: 0</span><br><span class="line">Case #3: 1</span><br></pre></td></tr></table></figure><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;set&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        string s, f;</span><br><span class="line">        cin &gt;&gt; s &gt;&gt; f;</span><br><span class="line">        <span class="comment">// 字典，存储f中的所有字母</span></span><br><span class="line">        set&lt;<span class="type">char</span>&gt; dic;</span><br><span class="line">        <span class="comment">// 初始化字典</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; f.<span class="built_in">size</span>(); i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            dic.<span class="built_in">insert</span>(f[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 遍历s中的字母</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; s.<span class="built_in">size</span>(); i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> mmin = <span class="number">2100000000</span>;</span><br><span class="line">            <span class="comment">// 遍历字典中的字母</span></span><br><span class="line">            <span class="comment">// 为s[i]找出一个最小的转换方案</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">auto</span> j : dic)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// dist是s[i]到字典中当前字母的距离</span></span><br><span class="line">                <span class="type">int</span> dist = s[i] - j;</span><br><span class="line">                <span class="comment">// 当距离大于0时</span></span><br><span class="line">                <span class="keyword">if</span> (dist &gt; <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 可以有两种转换方式</span></span><br><span class="line">                    <span class="comment">// 取两种中的小值</span></span><br><span class="line">                    dist = <span class="built_in">min</span>(dist, <span class="number">26</span> - dist);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (dist &lt; <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 可以有两种转换方式</span></span><br><span class="line">                    <span class="comment">// 取两种中的小值</span></span><br><span class="line">                    dist = <span class="built_in">min</span>(-dist, <span class="number">26</span> + dist);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 一些来自verilog的习惯</span></span><br><span class="line">                <span class="comment">// 一定要用else闭合if</span></span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;&#125;</span><br><span class="line">                <span class="comment">// 更新最小方案</span></span><br><span class="line">                mmin = <span class="built_in">min</span>(mmin, dist);</span><br><span class="line">            &#125;</span><br><span class="line">            res += mmin;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>, M = <span class="number">30</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="type">char</span> s[N], f[M];</span><br><span class="line"><span class="type">int</span> cnt[M];</span><br><span class="line"><span class="type">bool</span> st[M];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%s%s&quot;</span>, s, f);</span><br><span class="line">        <span class="built_in">memset</span>(st, <span class="number">0</span>, <span class="keyword">sizeof</span> st);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; f[i]; i ++ )</span><br><span class="line">            st[f[i] - <span class="string">&#x27;a&#x27;</span>] = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">26</span>; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> l = <span class="number">0</span>, r = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (!st[(i + r) % <span class="number">26</span>]) r ++ ;</span><br><span class="line">            <span class="keyword">while</span> (!st[(i - l + <span class="number">26</span>) % <span class="number">26</span>]) l ++ ;</span><br><span class="line">            cnt[i] = <span class="built_in">min</span>(l, r);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; s[i]; i ++ )</span><br><span class="line">            res += cnt[s[i] - <span class="string">&#x27;a&#x27;</span>];</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4122. 字符串转换。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 3321. ATM队列</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3321/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing3321/</id>
    <published>2023-07-15T06:00:00.000Z</published>
    <updated>2023-07-15T07:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-3321-ATM队列"><a href="#AcWing-3321-ATM队列" class="headerlink" title="AcWing 3321. ATM队列"></a>AcWing 3321. ATM队列</h1><p><a href="https://www.acwing.com/problem/content/3324/">3321. ATM队列 - AcWing题库</a></p><p>$N$个人（编号$1 \sim N$），排成一队在ATM机前准备取钱。</p><p>初始时，队列按编号升序的顺序排列。</p><p>第$i$个人需要取$A_i$元钱。</p><p>一个人一次最多可以取$X$元钱。</p><p>当轮到某个人取钱时，如果其需要的钱的数量大于$X$，则只能先取$X$元钱，然后去队尾重新排队，等待下次轮到他取钱时，继续去取。</p><p>当一个人取够钱时，他就会拿着钱离开队列。</p><p>现在，请你确定所有人离开队列的顺序。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含两个整数$N$和$X$。</p><p>第二行包含$N$个整数$A_i$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$是空格隔开的$N$个整数，表示所有人离开队列的顺序列表。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&60\%数据满足, 1 \le T \le 100, 1 \le N \le 100, \\&另外40\%数据满足, 1 \le T \le 10, 1 \le N \le 10^5, \\&100\%数据满足, 1 \le A_i \le 10^9, 1 \le X \le 10^9。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">3 3</span><br><span class="line">2 7 4</span><br><span class="line">5 6</span><br><span class="line">9 10 4 7 2</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 1 3 2</span><br><span class="line">Case #2: 3 5 1 2 4</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>对于测试数据$1$，共$3$个人，单轮最大取钱数量为$3$。</p><p>取钱过程如下：</p><ol><li>初始时，队列为$[1, 2, 3]$，第一个人需要$2$元钱，取完离队。</li><li>此时，队列为$[2, 3]$，第二个人需要$7$元钱，所以先取$3$元，然后去队尾继续排队。</li><li>此时，队列为$[3, 2]$，第三个人需要$4$元钱，所以先取$3$元，然后去队尾继续排队。</li><li>此时，队列为$[2, 3]$，第二个人需要$4$元钱，所以先取$3$元，然后去队尾继续排队。</li><li>此时，队列为$[3, 2]$，第三个人需要$1$元钱，取完离队。</li><li>此时，队列为$[2]$，第二个人需要$1$元钱，取完离队。</li><li>队列为空。</li></ol><p>离队顺序为$[1, 3, 2]$。</p><p>对于测试数据$2$，共$5$个人，单轮最大取钱数量为$6$。</p><p>取钱过程如下：</p><ol><li>初始时，队列为$[1, 2, 3, 4, 5]$，第一个人需要$9$元钱，所以先取$6$元，然后去队尾继续排队。</li><li>此时，队列为$[2, 3, 4, 5, 1]$，第二个人需要$10$元钱，所以先取$6$元，然后去队尾继续排队。</li><li>此时，队列为$[3, 4, 5, 1, 2]$，第三个人需要$4$元钱，取完离队。</li><li>此时，队列为$[4, 5, 1, 2]$，第四个人需要$7$元钱，所以先取$6$元，然后去队尾继续排队。</li><li>此时，队列为$[5, 1, 2, 4]$，第五个人需要$2$元钱，取完离队。</li><li>此时，队列为$[1, 2, 4]$，这三个人还需取得的钱数均不超过$6$元，所以可以顺次取完离队。</li></ol><p>离队顺序为$[3, 5, 1, 2, 4]$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><h4 id="先是一个非常暴力的TLE版本"><a href="#先是一个非常暴力的TLE版本" class="headerlink" title="先是一个非常暴力的TLE版本"></a>先是一个非常暴力的TLE版本</h4><p>就是直接强行循环。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> a[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; res;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n, x;</span><br><span class="line">        cin &gt;&gt; n &gt;&gt; x;</span><br><span class="line">        res.<span class="built_in">clear</span>();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; a[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (a[i % n] &gt; <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                a[i % n] -= x;</span><br><span class="line">                <span class="keyword">if</span> (a[i % n] &lt; <span class="number">0</span>)</span><br><span class="line">                &#123;</span><br><span class="line">                    a[i % n] = <span class="number">0</span>;</span><br><span class="line">                    count ++;</span><br><span class="line">                    res.<span class="built_in">push_back</span>(i % n);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (count == n)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            i ++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> y : res)</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; y + <span class="number">1</span> &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="能过的版本"><a href="#能过的版本" class="headerlink" title="能过的版本"></a>能过的版本</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> a[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">compare</span><span class="params">(<span class="type">const</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; &amp;a, <span class="type">const</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// first存储的是序号</span></span><br><span class="line">    <span class="comment">// second存储的是取钱的次数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 当取钱的次数不等时，按照取钱的次数排序</span></span><br><span class="line">    <span class="keyword">if</span> (a.second != b.second)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> a.second &lt; b.second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 否则按照序号排序</span></span><br><span class="line">    <span class="keyword">return</span> a.first &lt; b.first;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    vector&lt;pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; res;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n, x;</span><br><span class="line">        cin &gt;&gt; n &gt;&gt; x;</span><br><span class="line">        res.<span class="built_in">clear</span>();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> temp;</span><br><span class="line">            cin &gt;&gt; temp;</span><br><span class="line">            <span class="comment">// 如果能够正好在某一次取完</span></span><br><span class="line">            <span class="keyword">if</span> (temp % x == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 取的次数是temp / x</span></span><br><span class="line">                res.<span class="built_in">push_back</span>(&#123;i + <span class="number">1</span>, temp / x&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 否则取的次数是temp / x + 1</span></span><br><span class="line">                res.<span class="built_in">push_back</span>(&#123;i + <span class="number">1</span>, temp / x + <span class="number">1</span>&#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 排序</span></span><br><span class="line">        <span class="built_in">sort</span>(res.<span class="built_in">begin</span>(), res.<span class="built_in">end</span>(), compare);</span><br><span class="line">        <span class="comment">// 打印</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> y : res)</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; y.first &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> x first</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> y second</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt; PII;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line">PII q[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;n, &amp;m);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> a;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;a);</span><br><span class="line">            q[i] = &#123;(a + m - <span class="number">1</span>) / m, i&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">sort</span>(q + <span class="number">1</span>, q + n + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, q[i].y);</span><br><span class="line">        <span class="built_in">puts</span>(<span class="string">&quot;&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 3321. ATM队列。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4738. 快乐子数组</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4738/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4738/</id>
    <published>2023-07-15T05:00:00.000Z</published>
    <updated>2023-07-15T06:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4738-快乐子数组"><a href="#AcWing-4738-快乐子数组" class="headerlink" title="AcWing 4738. 快乐子数组"></a>AcWing 4738. 快乐子数组</h1><p><a href="https://www.acwing.com/problem/content/4741/">4738. 快乐子数组 - AcWing题库</a></p><p>我们将$F(B, L, R)$定义为整数数组$B$的索引从$L$到$R$（包括两者）的子数组的各个元素之和。</p><p>更具体的说，$F(B, L, R) = B_L +  B_{L + 1} + \cdots + B_R$。</p><p>如果一个长度为$K$的整数数组$C$满足其所有前缀和均为非负整数，则称数组$C$为快乐数组。</p><p>更具体的说，如果$F(C, 1, 1), F(C, 1, 2), \cdots, F(C, 1, K)$均为非负整数，则数组$C$为快乐数组。</p><p>给定一个包含$N$个整数的数组$A$，请你计算数组$A$中的所有快乐连续子数组的元素和相加的结果。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含整数$N$。</p><p>第二行包含$N$个整数$A_1, A_2, \cdots, A_N$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为所有快乐连续子数组的元素和相加的结果。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&1 \le T \le 100, \\&-800 \le A_i \le 800, \\&每个测试点最多30组数据满足1 \le N \le 4 \times 10^5, \\&其余数据满足1 \le N \le 200。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">5</span><br><span class="line">1 -2 3 -2 4</span><br><span class="line">3</span><br><span class="line">1 0 3</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 14</span><br><span class="line">Case #2: 12</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>在Case 1中，满足条件的快乐连续子数组有$[1], [3], [3, -2], [3, -2, 4], [4]$，它们的元素和分别为$1, 3, 1, 5, 4$，相加得到结果$14$。</p><p>在Case 2中，满足条件的快乐连续子数组有$[1], [1, 0], [1, 0, 3], [0], [0, 3], [3]$，它们的元素和分别为$1, 1, 4, 0, 3, 3$，相加得到结果$12$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><h5 id="前缀和"><a href="#前缀和" class="headerlink" title="前缀和"></a>前缀和</h5><p><a href="https://www.acwing.com/problem/content/797/">795. 前缀和 - AcWing题库</a></p><p>题目中提到了前缀和，所以先来复习一下前缀和。</p><p>前缀和的代码实现如下，前缀和存储在<code>q[N]</code>中：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> q[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n, m;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    q[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; q[i];</span><br><span class="line">        q[i] += q[i - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (m --)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> l, r;</span><br><span class="line">        cin &gt;&gt; l &gt;&gt; r;</span><br><span class="line">        cout &lt;&lt; q[r] - q[l - <span class="number">1</span>] &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>那么对于<code>q[N]</code>这个数组，若其原数组是<code>a[N]</code>，如果我们计算<code>q[j] - q[i - 1]</code>，求得的是从<code>a[i]</code>一直累加到<code>a[j]</code>的结果，即：</p><script type="math/tex; mode=display">q[j] - q[i - 1] = \sum_i^j a[i]</script><p>那么在<code>q[i - 1]</code>的右边，如果能找到第一个比<code>q[i - 1]</code>小的数<code>q[j]</code>，那么说明<code>q[j] - q[i - 1] &lt; 0</code>（注意<code>i - 1 &lt; j</code>），即从<code>a[i]</code>一直累加到<code>a[j]</code>的和是小于0的，即：</p><script type="math/tex; mode=display">\sum_i^j a[i] < 0</script><p>那么这可以说明什么呢？</p><p>这说明在<code>a[i]</code>到<code>a[j - 1]</code>之间的所有数都是非负数，因为只有这样才能够使得从<code>q[i] - q[i - 1]</code>到<code>q[j - 1] - q[i - 1]</code>都为正，即：</p><script type="math/tex; mode=display">q[k] - q[i - 1] > 0,\forall k \in [i, j - 1]</script><p>那么只要左端点为<code>i - 1</code>，在<code>[i - 1, j - 1]</code>这个区间内，任选一个右端点<code>k</code>，其都有<code>q[k] - q[i - 1] &gt; 0</code>，即：</p><script type="math/tex; mode=display">\sum_i^k a[i] > 0</script><p>那么接下来的任务是，如何快速的在<code>i - 1</code>的右侧，找到一个<code>q[j]</code>小于<code>q[i - 1]</code>。</p><p>这就需要单调栈了。</p><h5 id="单调栈"><a href="#单调栈" class="headerlink" title="单调栈"></a>单调栈</h5><p><a href="https://www.acwing.com/problem/content/832/">830. 单调栈 - AcWing题库</a></p><p>单调栈可以找到某个数左边第一个比它小的数。</p><p>大概思想就是，如果有一个数<code>c</code>，比先前在栈中的数<code>a b</code>（<code>a</code>先入栈）都小的话，那么对于数<code>c</code>右边的数，找比自己小的数时，肯定不会找到<code>a b</code>，所以可以将<code>a b</code>都出栈。</p><p>代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> a[N];</span><br><span class="line"><span class="type">int</span> p = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">push</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    a[p ++] = x;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">top</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(p &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> a[p - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">empty</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> !p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">pop</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(p &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        p --;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n, x;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    <span class="keyword">while</span> (n --)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; x;</span><br><span class="line">        <span class="comment">// 如果当前栈顶的数比输入的数大</span></span><br><span class="line">        <span class="keyword">while</span> (!<span class="built_in">empty</span>() &amp;&amp; <span class="built_in">top</span>() &gt;= x)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 出栈到栈顶的数比当前输入数小为止</span></span><br><span class="line">            <span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果栈为空，说明不存在比当前数更大的数</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">empty</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; <span class="number">-1</span> &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 否则直接输出栈顶数</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; <span class="built_in">top</span>() &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 最后将当前输入的数入栈</span></span><br><span class="line">        <span class="built_in">push</span>(x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>原本的单调栈求的是某个数左边第一个比它小的数，那么将数组翻转调用单调栈，就能够求出某个数右边第一个比它小的数。</p><p>分析结束，但是由于数据范围较大，达到了$10^7$，所以我们需要将时间复杂度控制在$O(n)$以内。</p><h5 id="求和"><a href="#求和" class="headerlink" title="求和"></a>求和</h5><p>数组元素为<code>a[i]</code>，区间为<code>[i, j - 1]</code>，其中有$j - i$个前缀和：</p><script type="math/tex; mode=display">\begin{matrix}数组a下标: & 0 & 1 & 2 & 3 & 4 & 5 \\数组a元素: & 0 & a & b & c & d & e\end{matrix}</script><script type="math/tex; mode=display">\begin{matrix}数组q下标: & 0 & 1 & 2 & 3 & 4 & 5\\数组q元素: & 0 & a & a + b & a + b + c & a + b + c + d & a + b + c + d + e\end{matrix}</script><p>那么<code>q[i] - q[i] = 0, q[i + 1] - q[i] = a[i + 1]</code>。</p><p>求的是$F(C, 1, 1), F(C, 1, 2), \cdots, F(C, 1, K)$的和。</p><p>令左边界为$i$，右边界为$j - 1$，那么变为：</p><script type="math/tex; mode=display">S =F(C, i, i) + F(C, i, i + 1) + \cdots + F(C, i, j - 1)</script><p>又：</p><script type="math/tex; mode=display">\begin{aligned}F(C, i, i) &= q[i] - q[i] \\F(C, i, i + 1) &= q[i + 1] - q[i] \\&\ \ \vdots \\F(C, i, j - 1) &= q[j - 1] - q[i]\end{aligned}</script><p>那么按照题意，求和为：</p><script type="math/tex; mode=display">S= (q[i] - q[i]) + (q[i + 1] - q[i]) + \cdots + (q[j - 1] - q[i])</script><p>其中<code>a[i + 1]</code>一共被求和了$j - 1 - (i + 1) + 1 = j - (i + 1)$次。</p><p>所以：</p><script type="math/tex; mode=display">S =0 + a[i + 1](j - (i + 1)) + a[i + 2](j - (i + 2)) + \cdots + a[j - 1](j - (j - 1))</script><p>令：</p><script type="math/tex; mode=display">B_i =a[1](n - 1) + a[2](n - 2) + \cdots + a[i - 1](n - (i - 1)) + a[i](n - i)</script><p>和：</p><script type="math/tex; mode=display">B_{j - 1} =a[1](n - 1) + a[2](n - 2) + \cdots + a[i](n - i) + a[i + 1](n - (i + 1)) + \cdots + a[j - 1](n - (j - 1))</script><p>$B_{j - 1}$和$B_i$两式做差得：</p><script type="math/tex; mode=display">B_{j - 1} - B_i =a[i + 1](n - (i + 1)) + a[i + 2](n - (i + 2)) + \cdots + a[j - 1](n - (j - 1))</script><p>那么$B_{j - 1} - B_i$和$S$的差是多少呢：</p><script type="math/tex; mode=display">\begin{aligned}B_{j - 1} - B_i - S&= a[i + 1](n - j) + a[i + 2](n - j) + \cdots + a[j - 1](n - j) \\&= (a[i + 1] + a[i + 2] + \cdots + a[j - 1]) \times (n - j) \\&= (q[j - 1] - q[i]) \times (n - j)\end{aligned}</script><p>那么$S$就能够通过下式直接求出：</p><script type="math/tex; mode=display">S =B_{j - 1} - B_i - (q[j - 1] - q[i]) \times (n - j)</script><p>分析结束。</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">400010</span>;</span><br><span class="line"><span class="comment">// q[N]是不加处理的前缀和数组</span></span><br><span class="line"><span class="comment">// B[N]是处理后的</span></span><br><span class="line"><span class="type">long</span> <span class="type">long</span> <span class="type">int</span> q[N], B[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> a;</span><br><span class="line">            <span class="comment">// cin &gt;&gt; a;</span></span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;a);</span><br><span class="line">            <span class="comment">// 构造q[i]和B[i]</span></span><br><span class="line">            q[i] = q[i - <span class="number">1</span>] + a;</span><br><span class="line">            B[i] = B[i - <span class="number">1</span>] + a * (n - i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 由于结果可能会爆int，所以需要开long long int</span></span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 单调栈</span></span><br><span class="line">        <span class="comment">// 注意栈中存储的都是下标</span></span><br><span class="line">        stack&lt;<span class="type">int</span>&gt; stk;</span><br><span class="line">        <span class="comment">// 在原前缀和数组的最后添加一个负无穷</span></span><br><span class="line">        <span class="comment">// 以便从原前缀和数组的最后一个元素开始处理</span></span><br><span class="line">        q[n + <span class="number">1</span>] = <span class="number">-2100000000</span>;</span><br><span class="line">        stk.<span class="built_in">push</span>(n + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = n; i &gt;= <span class="number">0</span>; i --)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 按照题意，前缀和数组中某数的右边一定存在一个比它小的数</span></span><br><span class="line">            <span class="keyword">while</span> (q[stk.<span class="built_in">top</span>()] &gt;= q[i])</span><br><span class="line">            &#123;</span><br><span class="line">                stk.<span class="built_in">pop</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 取出右侧第一个比自己小的数</span></span><br><span class="line">            <span class="type">int</span> j = stk.<span class="built_in">top</span>();</span><br><span class="line">            <span class="comment">// 将自己入栈</span></span><br><span class="line">            stk.<span class="built_in">push</span>(i);</span><br><span class="line">            <span class="comment">// 使用公式计算</span></span><br><span class="line">            res += B[j - <span class="number">1</span>] - B[i] - (q[j - <span class="number">1</span>] - q[i]) * (n - j);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lld\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4738. 快乐子数组。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4633. 学生和导师</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4633/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4633/</id>
    <published>2023-07-15T04:00:00.000Z</published>
    <updated>2023-07-15T05:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4633-学生和导师"><a href="#AcWing-4633-学生和导师" class="headerlink" title="AcWing 4633. 学生和导师"></a>AcWing 4633. 学生和导师</h1><p><a href="https://www.acwing.com/problem/content/4636/">4633. 学生和导师 - AcWing题库</a></p><p>有$N$个学生（编号$1 \sim N$）正在一起准备编程竞赛。</p><p>为了帮助彼此做好准备，每个学生都要选择一个其他学生作为他的导师，帮助其进步。</p><p>每个学生只能拥有一位导师，但是一个学生可以成为多个学生的导师。</p><p>第$i$个学生的实力评分为$R_i$。</p><p>我们认为，导师不能比其受指导者强太多，所以只有当$R_j \le 2 \times R_i$时，学生$j$才能成为学生$i$的导师。</p><p>请注意，导师的评分可以小于或等于其受指导者的评分。</p><p>毫不奇怪，每个学生都希望自己的导师尽可能强，所以对于每个学生，请你找出他们可以选择的导师的最高评分。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含整数$N$。</p><p>第二行包含$N$个整数$R_1, R_2, \cdots, R_N$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: M_1 M_2 ... M_N</code>，其中$x$为组别编号（从$1$开始），$M_i$为第$i$个学生可以选择的导师的最高评分，如果该学生无法选择任何导师，则输出$-1$。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\2 &\le N \le 10^5, \\1 &\le R_i \le 10^6。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">3</span><br><span class="line">2000 1500 1900</span><br><span class="line">5</span><br><span class="line">1000 600 1000 2300 1800</span><br><span class="line">2</span><br><span class="line">2500 1200</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 1900 2000 2000</span><br><span class="line">Case #2: 1800 1000 1800 1800 2300</span><br><span class="line">Case #3: 1200 -1</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>在Case 1中，三个学生的评分分别为$2000, 1500, 1900$。</p><p>每个学生都可以选择任何其他学生作为他们的导师，因此他们都会选择评分最高的导师。</p><p>所以，他们分别选择评分为$1900, 2000, 2000$的导师。</p><p>需要注意的是，评分$2000$的学生不能选择自己作为自己的导师，所以只能选择评分$1900$的学生作为导师。</p><p>在Case 2中，五个学生的评分分别为$1000, 600, 1000, 2300, 1800$（请注意，有些学生的评分可能相同）。</p><p>对于评分为$1000$的两个学生，他们能够选择的导师的最高评分为$1800$，他们不能选择评分为$2300$的导师，因为$2300 &gt; 2 \times 1000$。</p><p>对于评分为$600$的学生，他不能选择评分为$1800$或$2300$的导师，他能够选择的导师的最高评分为$1000$。</p><p>对于评分为$2300$的学生，他可以选择任何其他学生作为他的导师，因此他选择评分为$1800$的导师。</p><p>对于评分为$1800$的学生，他可以选择任何其他学生作为他的导师，因此他选择评分为$2300$的导师。</p><p>所以，五个学生分别选择评分为$1800, 1000, 1800, 1800, 2300$的导师。</p><p>在Case 3中，两个学生的评分分别为$2500, 1200$。</p><p>对于评分为$2500$的学生，他可以选择另一个评分为$1200$的学生作为导师。</p><p>对于评分为$1200$的学生，他无法选择评分为$2500$的学生作为导师，因为$2500 &gt; 2 \times 1200$。</p><p>所以，输出结果应该是$1200$和$-1$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>看完题之后，$T$为$100$，$N$为$10^5$，那么总共会有$10^7$的数据，那么如果使用$O(n)$的算法是肯定会TLE的。</p><p>所以需要控制在$O(n \log n)$内。</p><p>最终使用二分解决。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> a[N], b[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="comment">// 在a[i]上遍历，在b[i]上二分求值</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; a[i];</span><br><span class="line">            b[i] = a[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输出cases</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="comment">// 对b进行排序</span></span><br><span class="line">        <span class="built_in">sort</span>(b, b + n);</span><br><span class="line">        <span class="comment">// 对a中的每个元素进行二分</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 左右边界</span></span><br><span class="line">            <span class="type">int</span> l = <span class="number">0</span>, r = n - <span class="number">1</span>;</span><br><span class="line">            <span class="comment">// 如果没有找到</span></span><br><span class="line">            <span class="keyword">while</span> (l &lt; r)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 定义mid为两者中点</span></span><br><span class="line">                <span class="comment">// 查找的是不大于a[i] * 2的最后一个位置</span></span><br><span class="line">                <span class="type">int</span> mid = l + r + <span class="number">1</span> &gt;&gt; <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">if</span> (b[mid] &lt;= a[i] * <span class="number">2</span>)</span><br><span class="line">                    l = mid;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    r = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果找到的值和自己相等，说明可能是找到了自己</span></span><br><span class="line">            <span class="comment">// 那么直接将找到的位置推前一个位置</span></span><br><span class="line">            <span class="keyword">if</span> (a[i] == b[r])</span><br><span class="line">                r --;</span><br><span class="line">            <span class="comment">// 如果越界了，说明找不到</span></span><br><span class="line">            <span class="keyword">if</span> (r &lt; <span class="number">0</span>)</span><br><span class="line">                cout &lt;&lt; <span class="string">&quot;-1 &quot;</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                cout &lt;&lt; b[r] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4633. 学生和导师。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4672. 布料排序</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4672/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4672/</id>
    <published>2023-07-15T03:00:00.000Z</published>
    <updated>2023-07-15T04:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4672-布料排序"><a href="#AcWing-4672-布料排序" class="headerlink" title="AcWing 4672. 布料排序"></a>AcWing 4672. 布料排序</h1><p><a href="https://www.acwing.com/problem/content/4675/">4672. 布料排序 - AcWing题库</a></p><p>每块布料包含三种属性：</p><ul><li>颜色（$C$），一个由小写英文字母组成的字符串，表示布料的颜色。</li><li>耐久性（$D$），一个整数，表示布料的耐久性。</li><li><strong>唯一</strong>标识符（$U$），一个整数，表示布料的$ID$。</li></ul><p>给定$N$块布料，阿达和查尔斯需要对布料进行排序。</p><p>阿达按照颜色（$C$）字典序升序的顺序对布料进行排序，颜色相同的布料按唯一标识符（$U$）升序的顺序进行排序。</p><p>查尔斯按照耐久性（$D$）升序的顺序对布料进行排序，耐久性相同的布料按唯一标识符（$U$）升序的顺序进行排序。</p><p>请你计算，有多少块布料满足，无论是阿达还是查尔斯对布料进行排序，其最终顺位排名都相同。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含整数$N$。</p><p>接下来$N$行，每行包含一个字符串$C_i$，一个整数$D_i$，一个整数$U_i$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为满足条件的布料数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&1 \le T \le 100, \\&1 \le N \le 10^3, \\&1 \le \left | C_i \right | \le 10, \\&1 \le D_i \le 100, \\&1 \le U_i \le 10^3, \\&C_i只包含小写英文字母, \\&U_i两两不同。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">2</span><br><span class="line">blue 2 1</span><br><span class="line">yellow 1 2</span><br><span class="line">2</span><br><span class="line">blue 2 1</span><br><span class="line">brown 2 2</span><br><span class="line">1</span><br><span class="line">red 1 1</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 0</span><br><span class="line">Case #2: 2</span><br><span class="line">Case #3: 1</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>在Case 1中，按颜色排序时，布料（用唯一标识符表示）顺序为$1, 2$；按耐久性排序时，布料顺序为$2, 1$，所以$0$块布料具有相同的排名。</p><p>在Case 2中，按颜色排序时，布料（用唯一标识符表示）顺序为$1, 2$；按耐久性排序时，布料顺序为$1, 2$，所以$2$块布料具有相同的排名，这里需要注意，两块布料具有相同的耐久性，所以在查尔斯进行排序时，他将具有更小$ID$的$1$号布料排在前面。</p><p>在Case 3中，只有$1$块布料，所以无论如何其排名都不会有变化。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">5</span><br><span class="line">blue 1 2</span><br><span class="line">green 1 4</span><br><span class="line">orange 2 5</span><br><span class="line">red 3 6</span><br><span class="line">yellow 3 7</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 5</span><br></pre></td></tr></table></figure><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1010</span>;</span><br><span class="line">pair&lt;string, pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; a[N], c[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 阿达的排序</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">acompare</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> pair&lt;string, pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; &amp;a,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> pair&lt;string, pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.first == b.first)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> a.second.second &lt; b.second.second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> a.first &lt; b.first;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查尔斯的排序</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ccompare</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> pair&lt;string, pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; &amp;a,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> pair&lt;string, pair&lt;<span class="type">int</span>, <span class="type">int</span>&gt;&gt; &amp;b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.second.first == b.second.first)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> a.second.second &lt; b.second.second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> a.second.first &lt; b.second.first;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            string s;</span><br><span class="line">            <span class="type">int</span> d, u;</span><br><span class="line">            cin &gt;&gt; s &gt;&gt; d &gt;&gt; u;</span><br><span class="line">            a[i] = &#123;s, &#123;d, u&#125;&#125;;</span><br><span class="line">            c[i] = &#123;s, &#123;d, u&#125;&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">sort</span>(a, a + n, acompare);</span><br><span class="line">        <span class="built_in">sort</span>(c, c + n, ccompare);</span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 对应位置上如果相同</span></span><br><span class="line">            <span class="keyword">if</span> (</span><br><span class="line">                a[i].first == c[i].first</span><br><span class="line">                &amp;&amp; a[i].second.first == c[i].second.first</span><br><span class="line">                &amp;&amp; a[i].second.second == c[i].second.second)</span><br><span class="line">            &#123;</span><br><span class="line">                res ++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">1010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Data</span></span><br><span class="line">&#123;</span><br><span class="line">    string c;</span><br><span class="line">    <span class="type">int</span> d, u, idx;</span><br><span class="line">&#125;w[N];</span><br><span class="line"><span class="type">int</span> rk[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cmp1</span><span class="params">(Data&amp; a, Data&amp; b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.c != b.c) <span class="keyword">return</span> a.c &lt; b.c;</span><br><span class="line">    <span class="keyword">return</span> a.u &lt; b.u;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">cmp2</span><span class="params">(Data&amp; a, Data&amp; b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (a.d != b.d) <span class="keyword">return</span> a.d &lt; b.d;</span><br><span class="line">    <span class="keyword">return</span> a.u &lt; b.u;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">char</span> c[<span class="number">11</span>];</span><br><span class="line">            <span class="type">int</span> d, u;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%s%d%d&quot;</span>, c, &amp;d, &amp;u);</span><br><span class="line">            w[i] = &#123;c, d, u, i&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">sort</span>(w, w + n, cmp1);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">            rk[w[i].idx] = i;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">sort</span>(w, w + n, cmp2);</span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++ )</span><br><span class="line">            <span class="keyword">if</span> (rk[w[i].idx] == i)</span><br><span class="line">                res ++ ;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4672. 布料排序。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4737. 冰壶</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4737/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4737/</id>
    <published>2023-07-15T02:00:00.000Z</published>
    <updated>2023-07-15T03:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4737-冰壶"><a href="#AcWing-4737-冰壶" class="headerlink" title="AcWing 4737. 冰壶"></a>AcWing 4737. 冰壶</h1><p><a href="https://www.acwing.com/problem/content/4740/">4737. 冰壶 - AcWing题库</a></p><p>红队和黄队进行了一场冰壶比赛，比赛结束后，裁判正在计算两队的得分。</p><p>场地可以看作一个二维平面，得分区域可以看作一个以$(0, 0)$为圆心$R_h$为半径的圆。</p><p>场地上散落着$N$个红队的冰壶以及$M$个黄队的冰壶。</p><p>冰壶可以看作一个半径为$R_s$的圆。</p><p>每个冰壶的圆心坐标已知。</p><p>如果一个冰壶的任何部分位于得分区域的圆上或圆内（两者相切也算），则视为该冰壶位于得分区域内。</p><p>如果一个冰壶能够同时满足：</p><ol><li>它位于得分区域内。</li><li>不存在任何<strong>对方</strong>冰壶比它距离得分中心$(0, 0)$更近（欧几里得距离）。</li></ol><p>那么，这个冰壶就是一个得分冰壶。</p><p>一个队伍的最终得分等于该队伍的得分冰壶数量。</p><p>请你计算并输出两支队伍的最终得分。</p><p>一个冰壶与得分中心之间的距离等于其圆心点到点$(0, 0)$的距离。</p><p>数据保证不同冰壶与得分中心之间的距离不同，且冰壶两两之间不重叠（但可能相切）。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含两个整数$R_s, R_h$。</p><p>接下来一行包含一个整数$N$。</p><p>接下来$N$行，每行包含两个整数$X_i, Y_i$，表示一个红队冰壶的圆心坐标为$(X_i, Y_i)$。</p><p>接下来一行包含一个整数$M$。</p><p>接下来$M$行，每行包含两个整数$Z_i, W_i$，表示一个黄队冰壶的圆心坐标为$(Z_i, W_i)$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y z</code>，其中$x$为组别编号（从$1$开始），$y$为红队得分，$z$为黄队得分。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}&1 \le T \le 100, \\&1 \le R_s < R_h \le 10^4, \\&0 \le N, M \le 8, \\&-20000 \le X_i, Y_i, Z_i, W_i \le 20000。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">1 5</span><br><span class="line">4</span><br><span class="line">1 -1</span><br><span class="line">6 1</span><br><span class="line">0 6</span><br><span class="line">-5 0</span><br><span class="line">0</span><br><span class="line">10 100</span><br><span class="line">2</span><br><span class="line">-3 -4</span><br><span class="line">200 200</span><br><span class="line">0</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 3 0</span><br><span class="line">Case #2: 1 0</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>Case 1的冰壶分布情况如下图所示。</p><p><img src="https://img.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4737_case1.png" alt="acwing4737_case1"></p><p>在这种情况下，黄队没有冰壶在得分区域内，所以红队在得分区域内的每个冰壶都是得分冰壶。</p><p>红队除了以$(6, 1)$为圆心的那个冰壶以外，其他所有的冰壶都在得分区域内，所以红队得$3$分。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">1 5</span><br><span class="line">2</span><br><span class="line">1 0</span><br><span class="line">-3 0</span><br><span class="line">1</span><br><span class="line">0 2</span><br><span class="line">10 50</span><br><span class="line">2</span><br><span class="line">-40 -31</span><br><span class="line">-35 70</span><br><span class="line">3</span><br><span class="line">59 0</span><br><span class="line">-10 0</span><br><span class="line">30 40</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 1 0</span><br><span class="line">Case #2: 0 2</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>Case 1的冰壶分布情况如下图所示。</p><p><img src="https://img.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4737_case2.png" alt="acwing4737_case2"></p><p>在这种情况下，两支队伍都有冰壶在得分区域内。</p><p>红队圆心为$(1, 0)$的冰壶在得分区域内，且没有黄队的冰壶比它距离得分中心更近，所以它值$1$分。</p><p>红队圆心为$(-3, 0)$的冰壶在得分区域内，但是黄队圆心为$(0, 2)$的冰壶比它距离得分中心更近，所以它不能得分。</p><p>黄队圆心为$(0, 2)$的冰壶在得分区域内，但是红队圆心为$(1, 0)$的冰壶比它距离得分中心更近，所以它不能得分。</p><p>因此，红队得$1$分，黄队得$0$分。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 由于这里使用的是int来判断距离</span></span><br><span class="line"><span class="comment">// 所以使用平方来比较距离</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">sqr</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x * x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="comment">// 存储两队的距离</span></span><br><span class="line">    vector&lt;<span class="type">int</span>&gt; nd, md;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 清空</span></span><br><span class="line">        nd.<span class="built_in">clear</span>();</span><br><span class="line">        md.<span class="built_in">clear</span>();</span><br><span class="line">        <span class="comment">// s为冰壶半径</span></span><br><span class="line">        <span class="comment">// h为场地半径</span></span><br><span class="line">        <span class="type">int</span> s, h;</span><br><span class="line">        cin &gt;&gt; s &gt;&gt; h;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 首先是红队</span></span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="type">int</span> x, y;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; x &gt;&gt; y;</span><br><span class="line">            <span class="comment">// 如果这个冰壶在圈内</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">sqr</span>(x) + <span class="built_in">sqr</span>(y) &lt;= <span class="built_in">sqr</span>(s + h))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 将其存储，等待后续比较</span></span><br><span class="line">                nd.<span class="built_in">push_back</span>(<span class="built_in">sqr</span>(x) + <span class="built_in">sqr</span>(y));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 其次是黄队</span></span><br><span class="line">        <span class="type">int</span> m;</span><br><span class="line">        cin &gt;&gt; m;</span><br><span class="line">        <span class="type">int</span> z, w;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            cin &gt;&gt; z &gt;&gt; w;</span><br><span class="line">            <span class="comment">// 如果这个冰壶在圈内</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">sqr</span>(z) + <span class="built_in">sqr</span>(w) &lt;= <span class="built_in">sqr</span>(s + h))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 将其存储，等待后续比较</span></span><br><span class="line">                md.<span class="built_in">push_back</span>(<span class="built_in">sqr</span>(z) + <span class="built_in">sqr</span>(w));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 排序，要找出小于另一队的冰壶数</span></span><br><span class="line">        <span class="built_in">sort</span>(nd.<span class="built_in">begin</span>(), nd.<span class="built_in">end</span>());</span><br><span class="line">        <span class="built_in">sort</span>(md.<span class="built_in">begin</span>(), md.<span class="built_in">end</span>());</span><br><span class="line">        <span class="type">int</span> resn, resm;</span><br><span class="line">        resn = resm = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果双方都有冰壶在场内</span></span><br><span class="line">        <span class="keyword">if</span> (nd.<span class="built_in">size</span>() &amp;&amp; md.<span class="built_in">size</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果n红队的最近距离小于m黄队的最近距离</span></span><br><span class="line">            <span class="comment">// 那么黄队就不能得分</span></span><br><span class="line">            <span class="comment">// 接下来要统计红队的得分数</span></span><br><span class="line">            <span class="keyword">if</span> (nd[<span class="number">0</span>] &lt; md[<span class="number">0</span>])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">                <span class="comment">// 看n红队有多少个冰壶小于m黄队冰壶最小的距离</span></span><br><span class="line">                <span class="keyword">while</span> (nd[i] &lt;= md[<span class="number">0</span>] &amp;&amp; i &lt; nd.<span class="built_in">size</span>())</span><br><span class="line">                &#123;</span><br><span class="line">                    i ++;</span><br><span class="line">                &#125;</span><br><span class="line">                resn += i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (nd[<span class="number">0</span>] &gt; md[<span class="number">0</span>])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="type">int</span> i = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">while</span> (nd[<span class="number">0</span>] &gt;= md[i] &amp;&amp; i &lt; md.<span class="built_in">size</span>())</span><br><span class="line">                &#123;</span><br><span class="line">                    i ++;</span><br><span class="line">                &#125;</span><br><span class="line">                resm += i;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;&#125;</span><br><span class="line">            <span class="comment">// cout &lt;&lt; resn &lt;&lt; &quot; &quot; &lt;&lt; resm &lt;&lt; endl;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 如果有一方没有冰壶</span></span><br><span class="line">        <span class="comment">// 那么直接加上冰壶数</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果n红队没有冰壶</span></span><br><span class="line">            <span class="keyword">if</span> (nd.<span class="built_in">size</span>() == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 直接更新m黄队的得分</span></span><br><span class="line">                resm += md.<span class="built_in">size</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (md.<span class="built_in">size</span>() == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                resn += nd.<span class="built_in">size</span>();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;&#125;</span><br><span class="line">            <span class="comment">// cout &lt;&lt; resn &lt;&lt; &quot; &quot; &lt;&lt; resm &lt;&lt; endl;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d %d\n&quot;</span>, cases, resn, resm);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">20</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Curling</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> x, y, d, team;</span><br><span class="line">    <span class="type">bool</span> <span class="keyword">operator</span>&lt; (<span class="type">const</span> Curling&amp; t) <span class="type">const</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> d &lt; t.d;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;w[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">sqr</span><span class="params">(<span class="type">int</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x * x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n, m, rs, rh;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;rs, &amp;rh);</span><br><span class="line">        <span class="type">int</span> distance = <span class="built_in">sqr</span>(rh + rs);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">        <span class="keyword">while</span> (n -- )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> x, y;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;x, &amp;y);</span><br><span class="line">            <span class="type">int</span> d = <span class="built_in">sqr</span>(x) + <span class="built_in">sqr</span>(y);</span><br><span class="line">            <span class="keyword">if</span> (d &lt;= distance)</span><br><span class="line">                w[cnt ++ ] = &#123;x, y, d, <span class="number">0</span>&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;m);</span><br><span class="line">        <span class="keyword">while</span> (m -- )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> x, y;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;x, &amp;y);</span><br><span class="line">            <span class="type">int</span> d = <span class="built_in">sqr</span>(x) + <span class="built_in">sqr</span>(y);</span><br><span class="line">            <span class="keyword">if</span> (d &lt;= distance)</span><br><span class="line">                w[cnt ++ ] = &#123;x, y, d, <span class="number">1</span>&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">sort</span>(w, w + cnt);</span><br><span class="line">        <span class="type">int</span> res[<span class="number">2</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; cnt; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (i &amp;&amp; w[i].team != w[i - <span class="number">1</span>].team) <span class="keyword">break</span>;</span><br><span class="line">            res[w[i].team] ++ ;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d %d\n&quot;</span>, cases, res[<span class="number">0</span>], res[<span class="number">1</span>]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4737. 冰壶。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4741. 魔法百合井</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4741/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4741/</id>
    <published>2023-07-15T01:00:00.000Z</published>
    <updated>2023-07-15T02:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4741-魔法百合井"><a href="#AcWing-4741-魔法百合井" class="headerlink" title="AcWing 4741. 魔法百合井"></a>AcWing 4741. 魔法百合井</h1><p><a href="https://www.acwing.com/problem/content/4744/">4741. 魔法百合井 - AcWing题库</a></p><p>森林里有一口很深的魔法井，井中有$L$朵百合花。</p><p>你带着一个大空篮子和足够多的硬币来到了井边。</p><p>这个井有魔力，向里面投入硬币可以发生神奇的事情：</p><ul><li>如果你向井里一次性投入$1$个硬币，井就会发动魔法，将一朵百合花扔进你的篮子里。</li><li>如果你向井里一次性投入$4$个硬币，井就会发动魔法，统计并记录到目前为止，已经扔进你的篮子里的百合花的数量。</li><li>如果你向井里一次性投入$2$个硬币，井就会发动魔法，将等同于上次记录数量的百合花扔进你的篮子里。</li></ul><p>有一点需要特别注意，如果你向井里一次性投入$1$个或$2$个硬币后，井中已经没有足够的百合花扔给你了，那么井就不会发动任何魔法，也不会扔给你任何百合花（钱白花了）。</p><p>请你计算，为了将所有百合花都收入篮中，所需要花费的最少硬币数量。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据占一行，包含一个整数$L$，表示井中百合花的总数量。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为需要花费的最少硬币数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\1 &\le L \le 10^5。\end{aligned}</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">5</span><br><span class="line">20</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 5</span><br><span class="line">Case #2: 15</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>对于Case 1，井中一共有$5$朵百合花。</p><p>最佳方案是一个接一个的连续向井中投入$5$个硬币，这样我们可以一个接一个的得到$5$朵百合花。</p><p>一共需要花费$5$个硬币。</p><p>对于Case 1，井中一共有$15$朵百合花。</p><p>最佳方案为：</p><ul><li>首先，一个接一个的连续向井中投入$5$个硬币，这样我们可以一个接一个的得到$5$朵百合花。</li><li>然后，我们一次性向井中投入$4$个硬币，这样井会记录下到目前为止扔进我们篮中的百合花数量为$5$。</li><li>最后，我们重复三次，每次向井中投入$2$个硬币，这样每次都可以得到$5$朵百合花，从而得到剩余的全部$15$朵百合花。</li></ul><p>一共需要花费$15$个硬币。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>这是一个dp问题，根据闫氏dp分析法：</p><script type="math/tex; mode=display">\text{DP} =\begin{cases}状态表示f(i)\begin{cases}集合：所有可以取出i朵花的方案的集合 \\属性：最小值\end{cases} \\状态计算\end{cases}</script><p>$f(i)$表示的是所有取出$i$朵花的方案的集合中，硬币的最小值。</p><p>其中<strong>状态表示</strong>部分是比较简单的，属性就是题目要求我们求的属性，而集合则是所有可以取出$i$朵花的方案的集合。</p><p>那么对于<strong>状态计算</strong>部分，常用的方式是集合的划分，在划分时，一般是找到最后一个不同点进行划分，如果$f(i)$，其划分方式的分析如下：</p><p>题目中有三种操作，分别是投$1$、$4$、$2$个硬币，不妨将投$1$个硬币的动作标为$1$，投$4$个硬币的动作标为$4$，投$2$个硬币的动作标为$2$。</p><p>那么取出$i$朵花的方案一定是$124$的一个序列。我们找到序列中最后一个$4$，在这个$4$的后面会有若干个$2$，并且在$2$的中间还会夹杂着$1$，如下所示：</p><script type="math/tex; mode=display">4 \cdots 1 \cdots 2 \cdots 1 \cdots 2 \cdots 1 \cdots 2 \cdots 1 \cdots</script><p>在执行第一个$4$操作后，会统计此时已经扔进我们篮子中百合花的数量，假设这个数量为$x$，那么对于第一个$4$后面的每个$2$，每操作一次$2$，我们篮子中百合花的数量都会$+x$；操作一次$1$，篮子中的百合花数只会$+1$，如下所示：</p><script type="math/tex; mode=display">\begin{matrix}x & & +1 & & +x & & +1 & & +x & & +1 & & +x & & +1 \\4 & \cdots & 1 & \cdots & 2 & \cdots & 1 & \cdots & 2 & \cdots & 1 & \cdots & 2 & \cdots & 1 & \cdots\end{matrix}</script><p>那么可以发现，在$4$操作后，其余两个操作的顺序和最终取出来百合花的数量是无关的，所以可以将顺序调整如下：</p><script type="math/tex; mode=display">4 2 2 \cdots 2 1 \cdots 1</script><p>并且假设有$a$个$1$和$b$个$2$，那么取出花的数量为$a + bx$。</p><blockquote><p>能不能将$4$后面的所有$1$放到$4$的前面呢？这样不是后面的$2$每次取出来的$x$会更多吗。</p><p>这里不能把$4$后面的所有$1$放到$4$的前面，因为当$4$后面没有$1$时，每次执行$2$操作时取出来百合花的数量都是$x$，那么可能在某一次执行$2$时，井内没有足够的（没有$x$朵百合花）百合花了，依照题意，这样$1$朵都取不出来，这样是取不完井内的所有百合花的。</p></blockquote><p>那么对于调整后的序列$4 2 2 \cdots 2 1 \cdots 1$，后面$1$的个数可能为$0$，也有可能不为$0$，这样就分出了两种情况。</p><p>那么对$f(i)$初步的划分如下：</p><script type="math/tex; mode=display">f(i) =\begin{cases}最后一个操作是1 \\最后一段操作是4 2 \cdots 2\end{cases}</script><p>对于最后一个操作是$1$的这种情况，由于集合限定了最后一个操作是$1$，所以这是不变的部分，我们只能从前面变的部分求最小值，那么根据状态表示，其最小值为$f(i - 1) + 1$。</p><p>对于最后一段操作为$4 2 \cdots 2$的这种情况，$2$的数量是不定的，所以需要将这个划分继续进行根据$2$的数量进行划分。</p><p>如果只有$1$个$2$，并且假设$4$操作时统计出的百合花数量为$x$，那么总共就能够取出$2x$朵百合花，那么有$2x = i$；<br>如果有$2$个$2$，则有$3x = i$；<br>如果有$k - 1$个$2$，则有$kx = i$。</p><blockquote><p>$0$个$2$的情况不用考虑，因为当$4$后面没有$2$的时候，相当于是白花了$4$个硬币。</p></blockquote><p>那么对于$k - 1$个$2$的这种情况，其投的硬币分为两部分：</p><ol><li>在$4$之前取出$x$朵花需要用$f(x)$个硬币；</li><li>在$4$之后取出$(k - 1)x$朵花需要用$4 + 2(k - 1)$个硬币。</li></ol><p>且有$kx = i$，$x = \frac{i}{k}$。</p><p>所以当有$k - 1$个$2$时，使用的硬币数为$f(\frac{i}{k}) + 4 + 2(k - 1)$个。</p><p>所以$f(i)$最终的划分为：</p><script type="math/tex; mode=display">f(i) =\begin{cases}最后一个操作是1 \\最后一段操作是4 2 \cdots 2\begin{cases}1个2 \\2个2 \\\vdots \\k个2\end{cases}\end{cases}</script><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><h5 id="快速写法"><a href="#快速写法" class="headerlink" title="快速写法"></a>快速写法</h5><p>时间复杂度为$O(n \log n)$。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="comment">// f[i]是取出i朵花时，使用硬币数的最小值</span></span><br><span class="line"><span class="type">int</span> f[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 首先将数组置为inf</span></span><br><span class="line">    <span class="built_in">memset</span>(f, <span class="number">0x3f</span>, <span class="keyword">sizeof</span> f);</span><br><span class="line">    <span class="comment">// 取出0朵花时，使用的硬币数为0</span></span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 遍历取出1朵花到N朵花</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; N; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 最后一个操作是1</span></span><br><span class="line">        f[i] = <span class="built_in">min</span>(f[i], f[i - <span class="number">1</span>] + <span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 最后一段操作是42...2</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">2</span>; j * i &lt; N; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// f[i * j]即取出了i * j朵花</span></span><br><span class="line">            <span class="comment">// 是在取出i朵花的基础上执行了(j - 1)次取i朵的操作</span></span><br><span class="line">            <span class="comment">// 且上述操作需要的硬币数为f[i] + 4 + 2 * (j - 1)</span></span><br><span class="line">            <span class="comment">// 所以最终取出了i * j朵，且更新如下</span></span><br><span class="line">            f[i * j] = <span class="built_in">min</span>(f[i * j], f[i] + <span class="number">4</span> + <span class="number">2</span> * (j - <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, f[n]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="慢速写法"><a href="#慢速写法" class="headerlink" title="慢速写法"></a>慢速写法</h5><p>时间复杂度为$O(n \sqrt{n})$。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">int</span> f[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">memset</span>(f, <span class="number">0x3f</span>, <span class="keyword">sizeof</span> f);</span><br><span class="line">    f[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; N; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        f[i] = <span class="built_in">min</span>(f[i], f[i - <span class="number">1</span>] + <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">2</span>; j * j &lt;= i; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 当i % j == 0时</span></span><br><span class="line">            <span class="comment">// 说明i朵花可以通过先取i / j朵，然后取(j - 1) * (i / j)朵实现</span></span><br><span class="line">            <span class="comment">// 也可以通过先取j朵，再取(i / j - 1) * j朵实现</span></span><br><span class="line">            <span class="keyword">if</span> (i % j == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 同时求两个因数</span></span><br><span class="line">                f[i] = <span class="built_in">min</span>(f[i], f[i / j] + <span class="number">4</span> + <span class="number">2</span> * (j - <span class="number">1</span>));</span><br><span class="line">                f[i] = <span class="built_in">min</span>(f[i], f[j] + <span class="number">4</span> + <span class="number">2</span> * (i / j - <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> n;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, f[n]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4741. 魔法百合井。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4736. 步行者</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4736/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4736/</id>
    <published>2023-07-15T00:00:00.000Z</published>
    <updated>2023-07-15T01:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4736-步行者"><a href="#AcWing-4736-步行者" class="headerlink" title="AcWing 4736. 步行者"></a>AcWing 4736. 步行者</h1><p><a href="https://www.acwing.com/problem/content/4739/">4736. 步行者 - AcWing题库</a></p><p>约翰参加了一场步行比赛。</p><p>比赛为期$N$天，参赛者共$M$人（包括约翰）。</p><p>参赛者编号为$1 \sim M$，其中约翰的编号为$P$。</p><p>每个参赛者的每日步数都将被赛事方记录并公布。</p><p>每日步数最多的参赛者是当日的日冠军（可以有并列冠军）。</p><p>如果一名参赛者可以连续$N$天成为日冠军，那么他将成为创造历史的传奇冠军，这正是约翰的最终目标。</p><p>在比赛结束后，约翰拿到了所有选手的全部成绩，并试图分析自己在实现目标方面还差了多少步。</p><p>对于第$i$天，如果约翰是当日的冠军，那么他就会对当日的发挥表示满意，即当日不需要额外多走步数，如果约翰不是当日的冠军，那么他就会计算当日若要夺冠，还需要额外走的最小步数。</p><p>请你计算并输出，为了实现目标，约翰在这$N$天中需要额外走的最小总步数。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含三个整数$M, N, P$。</p><p>接下来$M$行，每行包含$N$个整数，其中第$i$行第$j$个数$S_{i, j}$表示第$i$个参赛者第$j$天的行走步数。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为实现目标需要额外走的最小总步数。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\2 &\le M \le 1000, \\1 &\le N \le 31, \\1 &\le P \le M, \\1 &\le S_{i, j} \le 60000。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2 3 1</span><br><span class="line">1000 2000 3000</span><br><span class="line">1500 1500 3000</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 500</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>在此样例中，比赛为期$3$天，共$2$人参赛，约翰是$1$号选手。</p><p>第$1$天，约翰若想夺冠，至少还需额外走$500$步。</p><p>后两天，约翰均为日冠军，无需额外多走步数。</p><p>所以，他一共需要额外走$500$步。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">3 2 3</span><br><span class="line">1000 2000</span><br><span class="line">1500 4000</span><br><span class="line">500 4000</span><br><span class="line">3 3 2</span><br><span class="line">1000 2000 1000</span><br><span class="line">1500 2000 1000</span><br><span class="line">500 4000 1500</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 1000</span><br><span class="line">Case #2: 2500</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>在Case 1中，比赛为期$2$天，共$3$人参赛，约翰是$3$号选手。</p><p>第$1$天，约翰若想夺冠，至少还需额外走$1000$步。</p><p>第$2$天，约翰为日冠军，无需额外多走步数。</p><p>所以，他一共需要额外走$1000$步。</p><p>在Case 2中，比赛为期$3$天，共$3$人参赛，约翰是$2$号选手。</p><p>第$1$天，约翰为日冠军，无需额外多走步数。</p><p>第$2$天，约翰若想夺冠，至少还需额外走$2000$步。</p><p>第$3$天，约翰若想夺冠，至少还需额外走$500$步。</p><p>所以，他一共需要额外走$2500$步。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> M = <span class="number">1010</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">40</span>;</span><br><span class="line"><span class="comment">// 存储所有的步数</span></span><br><span class="line"><span class="type">int</span> step[M][N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> m, n, p;</span><br><span class="line">        cin &gt;&gt; m &gt;&gt; n &gt;&gt; p;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i ++)</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j ++)</span><br><span class="line">                <span class="comment">// scanf还就那个快</span></span><br><span class="line">                <span class="comment">// cin还就那个TLE</span></span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;step[i][j]);</span><br><span class="line">                <span class="comment">// cin &gt;&gt; step[i][j];</span></span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; j ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 取出john的成绩</span></span><br><span class="line">            <span class="type">int</span> john = step[p - <span class="number">1</span>][j];</span><br><span class="line">            <span class="comment">// 求出当日的最大步数</span></span><br><span class="line">            <span class="type">int</span> curr_max = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; i ++)</span><br><span class="line">                curr_max = <span class="built_in">max</span>(curr_max, step[i][j]);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 将最大步数和john步数的差值</span></span><br><span class="line">            res += <span class="built_in">max</span>(<span class="number">0</span>, curr_max - john);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">35</span>, M = <span class="number">1010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> m, n, p;</span><br><span class="line"><span class="type">int</span> w[M][N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d%d&quot;</span>, &amp;m, &amp;n, &amp;p);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= m; i ++ )</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= n; j ++ )</span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;w[i][j]);</span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> t = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt;= m; j ++ )</span><br><span class="line">                t = <span class="built_in">max</span>(t, w[j][i]);</span><br><span class="line">            res += t - w[p][i];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4736. 步行者。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4740. 跑圈</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4740/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4740/</id>
    <published>2023-07-14T23:00:00.000Z</published>
    <updated>2023-07-15T00:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4740-跑圈"><a href="#AcWing-4740-跑圈" class="headerlink" title="AcWing 4740. 跑圈"></a>AcWing 4740. 跑圈</h1><p><a href="https://www.acwing.com/problem/content/4743/">4740. 跑圈 - AcWing题库</a></p><p>阿达正在一个长度为$L$的环形跑道上练习跑步。</p><p>为了更专注于跑步，阿达专门准备了一台机器来统计她跑的圈数。</p><p>机器放置在跑道的起跑线上，从$0$开始计数。</p><p>每当阿达离开起跑线时（直接越过起跑线或在起跑线位置处改变方向并离开起跑线），她的面朝方向就会被机器记录。</p><p>机器只会实时记录她最近一次离开起跑线时的面朝方向。</p><p>每当阿达到达起跑线位置时，只要其面朝方向与机器记录的上次离开起跑线时的面朝方向相同，机器计数就会加$1$。</p><p>阿达从起跑线处开始跑步。</p><p>她的耐力有限，无法将计划的训练量一口气完成。</p><p>因此，每跑一段距离，她都会原地休息一段时间，用来恢复体力。</p><p>不幸的是，阿达的记忆力并不是很好，每当她休息完再次开始跑步时，她都会忘了之前面朝的方向。</p><p>这时，她只能随意选择一个方向（顺时针或逆时针），并面朝该方向从她停下的位置开始继续跑步。</p><p>具体的说，她一共进行了$N$段跑步，其中第$i$段跑步的距离为$D_i$，跑步时的面朝方向为$C_i$。</p><p>请你计算，在阿达完成跑步后，机器最终记录的圈数。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含两个整数$L, N$。</p><p>接下来$N$行，每行包含一个整数$D_i$和一个字符$C_i$，分别表示阿达一段跑步的距离和面朝方向。$C_i$只可能是<code>C</code>（表示顺时针方向）或<code>A</code>（表示逆时针方向）。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为一个非负整数，表示机器最终记录的圈数。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\1 &\le L \le 10^9, \\1 &\le N \le 10^4, \\1 &\le D_i \le 10^9。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">5 3</span><br><span class="line">8 C</span><br><span class="line">3 C</span><br><span class="line">6 C</span><br><span class="line">8 4</span><br><span class="line">5 C</span><br><span class="line">9 C</span><br><span class="line">8 C</span><br><span class="line">20 C</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 3</span><br><span class="line">Case #2: 5</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>在Case 1中，环形跑道长度为$5$。</p><p>阿达的跑步过程如下：</p><ul><li>阿达朝向顺时针方向跑$8$单位长度，过程中触及起跑线，机器记录圈数加$1$。此时沿顺时针方向从起跑线到阿达的距离为$3$单位长度。</li><li>阿达朝向顺时针方向跑$3$单位长度，过程中触及起跑线，机器记录圈数加，此时沿顺时针方向从起跑线到阿达的距离为$1$单位长度。</li><li>阿达朝向顺时针方向跑$6$单位长度，过程中触及起跑线，机器记录圈数加$1$。</li></ul><p>最终机器记录圈数为$3$。</p><p>在Case 2中，环形跑道长度为$8$。</p><p>阿达的跑步过程如下：</p><ul><li>阿达朝向顺时针方向跑$5$单位长度。此时沿顺时针方向从起跑线到阿达的距离为$5$单位长度。</li><li>阿达朝向顺时针方向跑$9$单位长度，过程中触及起跑线，机器记录圈数加$1$。此时沿顺时针方向从起跑线到阿达的距离为$6$单位长度。</li><li>阿达朝向顺时针方向跑$8$单位长度，过程中触及起跑线，机器记录圈数加$1$。此时沿顺时针方向从起跑线到阿达的距离为$6$单位长度。</li><li>阿达朝向顺时针方向跑$20$单位长度，过程中$3$次触及起跑线，机器记录圈数加$3$。</li></ul><p>最终机器记录圈数为$5$。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">5 3</span><br><span class="line">8 C</span><br><span class="line">4 A</span><br><span class="line">5 C</span><br><span class="line">4 5</span><br><span class="line">2 C</span><br><span class="line">8 A</span><br><span class="line">3 A</span><br><span class="line">5 C</span><br><span class="line">8 A</span><br><span class="line">4 3</span><br><span class="line">3 C</span><br><span class="line">2 A</span><br><span class="line">5 C</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 1</span><br><span class="line">Case #2: 5</span><br><span class="line">Case #3: 1</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>在Case 1中，环形跑道长度为$5$。</p><p>阿达的跑步过程如下：</p><ul><li>阿达朝向顺时针方向跑$8$单位长度，过程中触及起跑线，机器记录圈数加$1$。此时沿顺时针方向从起跑线到阿达的距离为$3$单位长度。</li><li>阿达朝向逆时针方向跑$4$单位长度，过程中触及起跑线，但由于面朝方向与之前离开起跑线的面朝方向相反，所以不会增加记录圈数，此时沿逆时针方向从起跑线到阿达的距离为$1$单位长度。</li><li>阿达朝向顺时针方向跑$5$单位长度，过程中触及起跑线，但由于面朝方向与之前离开起跑线的面朝方向相反，所以不会增加记录圈数。</li></ul><p>最终机器记录圈数为$1$。</p><p>在Case 2中，环形跑道长度为$4$。</p><p>阿达的跑步过程如下：</p><ul><li>阿达朝向顺时针方向跑$2$单位长度。此时沿顺时针方向从起跑线到阿达的距离为$2$单位长度。</li><li>阿达朝向逆时针方向跑$8$单位长度，过程中$2$次触及起跑线，但由于第一次到达起跑线时的面朝方向与之前离开起跑线的面朝方向相反，所以记录圈数只增加$1$，此时沿逆时针方向从起跑线到阿达的距离为$2$。</li><li>阿达朝向逆时针方向跑$3$单位长度，过程中触及起跑线，机器记录圈数加$1$。此时沿逆时针方向从起跑线到阿达的距离为$1$单位长度。</li><li>阿达朝向顺时针方向跑$5$单位长度，过程中$2$次触及起跑线，但由于第一次到达起跑线时的面朝方向与之前离开起跑线的面朝方向相反，所以记录圈数只增加$1$。此时阿达恰好在起跑线上。</li><li>阿达朝向逆时针方向跑$8$单位长度，过程中$2$次触及起跑线，机器记录圈数加$2$。注意，此次跑步开始时，阿达在起跑线上改变方向，机器会直接将新方向（逆时针）记录为离开起跑线的面朝方向。</li></ul><p>最终机器记录圈数为$5$。</p><p>在Case 3中，环形跑道长度为$4$。</p><p>阿达的跑步过程如下：</p><ul><li>阿达朝向顺时针方向跑$3$单位长度。此时沿顺时针方向从起跑线到阿达的距离为$3$单位长度。</li><li>阿达朝向逆时针方向跑$2$单位长度，过程中没有触及起跑线，所以机器记录的最后一次离开起跑线的面朝方向仍然是顺时针方向。此时沿顺时针方向从起跑线到阿达的距离为$1$单位长度。</li><li>阿达朝向顺时针方向跑$5$单位长度，过程中触及起跑线，机器记录圈数加$1$。</li></ul><p>最终机器记录圈数为$1$。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> L, N;</span><br><span class="line">        cin &gt;&gt; L &gt;&gt; N;</span><br><span class="line">        <span class="comment">// 有一个10000000000000的示例，所以要long long</span></span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 现在的方向</span></span><br><span class="line">        <span class="type">char</span> curr_f = <span class="string">&#x27;B&#x27;</span>;</span><br><span class="line">        <span class="comment">// 在起点的方向</span></span><br><span class="line">        <span class="type">char</span> start_f = <span class="string">&#x27;B&#x27;</span>;</span><br><span class="line">        <span class="comment">// 现在跑的长度</span></span><br><span class="line">        <span class="type">int</span> curr_l = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> l;</span><br><span class="line">            <span class="type">char</span> f;</span><br><span class="line">            cin &gt;&gt; l &gt;&gt; f;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果是B，说明是刚开始跑</span></span><br><span class="line">            <span class="comment">// 那么要初始化现在方向和在起点的方向</span></span><br><span class="line">            <span class="keyword">if</span> (curr_f == <span class="string">&#x27;B&#x27;</span>)</span><br><span class="line">                curr_f = f;</span><br><span class="line">            <span class="keyword">if</span> (start_f == <span class="string">&#x27;B&#x27;</span>)</span><br><span class="line">                start_f = f;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果方向变了</span></span><br><span class="line">            <span class="keyword">if</span> (f != curr_f)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">// 首先距离会减少</span></span><br><span class="line">                curr_l -= l;</span><br><span class="line">                <span class="comment">// 现在的方向也会变</span></span><br><span class="line">                curr_f = f;</span><br><span class="line">                <span class="comment">// 此时分两种情况：</span></span><br><span class="line">                <span class="comment">// - 越过起点</span></span><br><span class="line">                <span class="comment">// - 没有越过起点</span></span><br><span class="line">                <span class="keyword">if</span> (curr_l &lt; <span class="number">0</span>) <span class="comment">// 越过起点</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 那么起点的方向要改变</span></span><br><span class="line">                    start_f = f;</span><br><span class="line">                    <span class="comment">// 跑的多出来的距离也要变为正</span></span><br><span class="line">                    curr_l = <span class="built_in">abs</span>(curr_l);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>            <span class="comment">// 没有越过起点</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// 直接将距离变负</span></span><br><span class="line">                    <span class="comment">// 这里是没问题的</span></span><br><span class="line">                    <span class="comment">// 在跑的方向变化时，现在跑的方向curr_f会变化</span></span><br><span class="line">                    <span class="comment">// 那么下一次跑时，要么f和curr_f相同，进入else，直接加距离l</span></span><br><span class="line">                    <span class="comment">// 要么f和curr_f不同，进入当前的if</span></span><br><span class="line">                    <span class="comment">// 并且假设这次越过起点(curr_l &lt; 0)</span></span><br><span class="line">                    <span class="comment">// 改变方向，将距离变正</span></span><br><span class="line">                    curr_l = -curr_l;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                curr_l += l;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 如果距离大于跑道长度，更新res和curr_l</span></span><br><span class="line">            <span class="keyword">if</span> (curr_l &gt;= L)</span><br><span class="line">            &#123;</span><br><span class="line">                res += curr_l / L;</span><br><span class="line">                curr_l %= L;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lld\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> LL;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">int</span> L, n;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d%d&quot;</span>, &amp;L, &amp;n);</span><br><span class="line"></span><br><span class="line">        LL res = <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (n -- )</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> d;</span><br><span class="line">            <span class="type">char</span> c;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d %c&quot;</span>, &amp;d, &amp;c);</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="string">&#x27;A&#x27;</span>) d = -d;</span><br><span class="line">            x += d;</span><br><span class="line">            res += <span class="built_in">abs</span>(x) / L;</span><br><span class="line">            x %= L;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %lld\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4740. 跑圈。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4742. 电</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4742/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4742/</id>
    <published>2023-07-14T22:00:00.000Z</published>
    <updated>2023-07-14T23:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4742-电"><a href="#AcWing-4742-电" class="headerlink" title="AcWing 4742. 电"></a>AcWing 4742. 电</h1><p><a href="https://www.acwing.com/problem/content/4745/">4742. 电 - AcWing题库</a></p><p>某城市有$N$个电力节点，编号$1 \sim N$。</p><p>这些电力节点形成的电力网络，可以看作一个$N$个节点$N - 1$条边的连通图。</p><p>每个电力节点都有一个固定的电容，其中第$i$个节点的电容为$A_i$。</p><p>现在，可以选择其中一个节点进行供电，其它节点也可以根据实际连接以及具体电容情况接收电力。</p><p>具体来说，如果第$i$个节点通电，那么它也可以将电力传输给其它所有与它<strong>直接连接</strong>且电容<strong>严格小于</strong>$A_i$的节点。</p><p>我们希望通过合理选择初始供电节点，从而使得尽可能多的节点能够通电。</p><p>请你计算并输出可以通电的最大节点数量。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据第一行包含整数$N$。</p><p>第二行包含$N$个整数$A_1, A_2, \cdots A_N$。</p><p>接下来$N - 1$行，每行包含两个整数$X_i, Y_i$，表示节点$X_i, Y_i$之间存在直接连接。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为可以通电的最大节点数量。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\1 &\le A_i \le 10^9, \\1 &\le X_i, Y_i \le N,\end{aligned}</script><p>一个测试点内最多$15$组数据满足$1 \le N \le 2 \times 10^5$，其余数据满足$1 \le N \le 10^3$。</p><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">5</span><br><span class="line">1 2 3 4 3</span><br><span class="line">1 3</span><br><span class="line">2 3</span><br><span class="line">4 3</span><br><span class="line">4 5</span><br><span class="line">6</span><br><span class="line">1 2 3 3 1 4</span><br><span class="line">3 1</span><br><span class="line">3 2</span><br><span class="line">3 4</span><br><span class="line">4 5</span><br><span class="line">1 6</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 5</span><br><span class="line">Case #2: 3</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p><img src="https://img.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4742_case1.png" alt="acwing4742_case1"></p><p>在Case 1中，最佳方案是给第$4$个节点供电，这样可以将电力传输到所有节点。</p><p>注意，如果给第$3$个节点供电，则电力只会传输至第$1, 2$个节点，而无法传输至第$4$个节点，这样只有三个节点可以通电。</p><p><img src="https://img.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4742_case2.png" alt="acwing4742_case2"></p><p>在Case 2中，最佳方案是给第$3$个节点供电，这样可以将电力传输至第$1, 2$个节点，但是无法传输至第$4$个节点，因为$A_4$并不严格小于$A_3$。</p><p>注意，如果给第$6$个节点供电，则电力只会传输至第$1$个节点，如果给第$4$个节点供电，则电力只会传输至第$5$个节点。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">200010</span>, M = N * <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n;</span><br><span class="line"><span class="comment">// 存储的全部是下标，值需要通过w[N]取出</span></span><br><span class="line"><span class="type">int</span> h[N], e[M], ne[M], idx;</span><br><span class="line"><span class="type">int</span> w[N];</span><br><span class="line"><span class="comment">// f[i]代表点i能够流向点的数量</span></span><br><span class="line"><span class="type">int</span> f[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 加边</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 首先将下标存储</span></span><br><span class="line">    <span class="comment">// 然后将新节点的下一个节点设为原来的头节点</span></span><br><span class="line">    <span class="comment">// 最后让头节点指向新的头节点，并位置++</span></span><br><span class="line">    e[idx] = b, ne[idx] = h[a], h[a] = idx ++;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 记忆化搜索</span></span><br><span class="line"><span class="comment">// 返回的是u这个点能够流向点的数量</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">dp</span><span class="params">(<span class="type">int</span> u)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 如果这个点已经被算过了</span></span><br><span class="line">    <span class="keyword">if</span> (f[u] != <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> f[u];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 至少能够传到自己</span></span><br><span class="line">    <span class="type">int</span> res = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 遍历点u能够到达的所有点</span></span><br><span class="line">    <span class="comment">// 中间循环结束条件写为~i也是可以的</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = h[u]; i != <span class="number">-1</span>; i = ne[i])</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 通过e[i]取出下标</span></span><br><span class="line">        <span class="type">int</span> j = e[i];</span><br><span class="line">        <span class="comment">// 比较电的大小</span></span><br><span class="line">        <span class="keyword">if</span> (w[u] &gt; w[j])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果u的电比j的多，那么u可以流向j流向的所有点</span></span><br><span class="line">            <span class="comment">// 包括j自己</span></span><br><span class="line">            res += <span class="built_in">dp</span>(j);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 更新f[u]</span></span><br><span class="line">    f[u] = res;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++)</span><br><span class="line">    &#123;</span><br><span class="line">        cin &gt;&gt; n;</span><br><span class="line">        <span class="comment">// 读入电容</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">            cin &gt;&gt; w[i];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化，清空邻接表</span></span><br><span class="line">        idx = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 这里将清空的范围设置为(n + 1) * 4会更快</span></span><br><span class="line">        <span class="comment">// 因为只有一小部分的数据用到了后面的空间</span></span><br><span class="line">        <span class="built_in">memset</span>(h, <span class="number">-1</span>, (n + <span class="number">1</span>) * <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加边，由于是无向图，每条边需要添加两次</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i ++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> a, b;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">&quot;%d %d&quot;</span>, &amp;a, &amp;b);</span><br><span class="line">            <span class="comment">// cin &gt;&gt; a &gt;&gt; b;</span></span><br><span class="line">            <span class="comment">// cin会TLE...</span></span><br><span class="line">            <span class="built_in">add</span>(a, b), <span class="built_in">add</span>(b, a);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 清空结果数组</span></span><br><span class="line">        <span class="built_in">memset</span>(f, <span class="number">-1</span>, (n + <span class="number">1</span>) * <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 在所有的结果中取一个最大值</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++)</span><br><span class="line">            res = <span class="built_in">max</span>(res, <span class="built_in">dp</span>(i));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输出</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: %d\n&quot;</span>, cases, res);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4742. 电。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4382. 快速打字</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4382/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4382/</id>
    <published>2023-07-14T21:00:00.000Z</published>
    <updated>2023-07-14T22:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4382-快速打字"><a href="#AcWing-4382-快速打字" class="headerlink" title="AcWing 4382. 快速打字"></a>AcWing 4382. 快速打字</h1><p><a href="https://www.acwing.com/problem/content/4385/">4382. 快速打字 - AcWing题库</a></p><p>芭芭拉是一个速度打字员。</p><p>为了检查她的打字速度，她进行了一个速度测试。</p><p>测试内容是给定她一个字符串$I$，她需要将字符串正确打出。</p><p>但是，芭芭拉作为一个速度打字员，在追求速度的同时，难免会发生一些错误，按错一些按键。</p><p>最终，芭芭拉打出的字符串为$P$。</p><p>现在，芭芭拉想知道，能否仅通过删除一些额外字母的方式，将字符串$P$变为字符串$I$。</p><p>如果可以，则输出需要删除的字母数量，如果不行，则输出<code>IMPOSSIBLE</code>。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含整数$T$，表示共有$T$组测试数据。</p><p>每组数据占两行，第一行包含字符串$I$，第二行包含字符串$P$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>每组数据输出一个结果，每个结果占一行。</p><p>结果表示为<code>Case #x: y</code>，其中$x$为组别编号（从$1$开始），$y$为需要删除的字母数量或<code>IMPOSSIBLE</code>。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le T \le 100, \\1 &\le \left| I \right|, \left| P \right| \le 10^5,\end{aligned}</script><p>字符串$I$和$P$均只包含大小写字母。</p><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">aaaa</span><br><span class="line">aaaaa</span><br><span class="line">bbbbb</span><br><span class="line">bbbbc</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 1</span><br><span class="line">Case #2: IMPOSSIBLE</span><br></pre></td></tr></table></figure><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">Ilovecoding</span><br><span class="line">IIllovecoding</span><br><span class="line">KickstartIsFun</span><br><span class="line">kkickstartiisfun</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Case #1: 2</span><br><span class="line">Case #2: IMPOSSIBLE</span><br></pre></td></tr></table></figure><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    string I, P;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">1</span>; k &lt;= T; k ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 输入两个字符串</span></span><br><span class="line">        cin &gt;&gt; I &gt;&gt; P;</span><br><span class="line">        <span class="type">int</span> i, j;</span><br><span class="line">        i = j = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 利用双指针算法遍历两个字符串</span></span><br><span class="line">        <span class="comment">// 如果匹配成功，则i会移动到I的末尾并退出循环</span></span><br><span class="line">        <span class="comment">// 如果匹配失败，则j会移动到P的末尾并退出循环</span></span><br><span class="line">        <span class="keyword">for</span> (; i &lt; I.<span class="built_in">size</span>() &amp;&amp; j &lt; P.<span class="built_in">size</span>();)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果当前位置I和P能够匹配，则i和j都向后移动</span></span><br><span class="line">            <span class="keyword">if</span> (I[i] == P[j])</span><br><span class="line">            &#123;</span><br><span class="line">                i ++;</span><br><span class="line">                j ++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 否则只有j移动，以寻找下一个匹配I[i]的字母</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                j ++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, k);</span><br><span class="line">        <span class="keyword">if</span> (i == I.<span class="built_in">size</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; P.<span class="built_in">size</span>() - I.<span class="built_in">size</span>() &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            cout &lt;&lt; <span class="string">&quot;IMPOSSIBLE&quot;</span> &lt;&lt; endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> n, m;</span><br><span class="line"><span class="type">char</span> a[N], b[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;T);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> cases = <span class="number">1</span>; cases &lt;= T; cases ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%s%s&quot;</span>, a, b);</span><br><span class="line">        n = <span class="built_in">strlen</span>(a), m = <span class="built_in">strlen</span>(b);</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt; n &amp;&amp; j &lt; m)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (a[i] == b[j]) i ++ ;</span><br><span class="line">            j ++ ;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Case #%d: &quot;</span>, cases);</span><br><span class="line">        <span class="keyword">if</span> (i == n) <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, m - n);</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">puts</span>(<span class="string">&quot;IMPOSSIBLE&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4382. 快速打字。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4440. 照相</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4440/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4440/</id>
    <published>2023-07-14T20:00:00.000Z</published>
    <updated>2023-07-14T21:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4440-照相"><a href="#AcWing-4440-照相" class="headerlink" title="AcWing 4440. 照相"></a>AcWing 4440. 照相</h1><p><a href="https://www.acwing.com/problem/content/4443/">4440. 照相 - AcWing题库</a></p><p>迫切希望在郡县集市上赢得最佳奶牛摄影师的农夫约翰正在尝试为他的$N$头奶牛拍摄一张完美的照片。</p><p>农夫约翰拥有两种品种的奶牛：更赛牛（Guernsey）和荷斯坦牛（Holstein）。</p><p>为了使他的照片尽可能地艺术，他想把他的奶牛排成一排，使得尽可能多的更赛牛处于队列中的偶数位置（队列中的第一个位置是奇数位置，下一个是偶数位置，以此类推）。</p><p>由于他与他的奶牛缺乏有效的沟通，他可以达到目的的唯一方法是让他的奶牛的偶数长的「前缀」进行反转（一个前缀指的是对于某个位置$j$，从第一头奶牛到第$j$头奶牛范围内的所有奶牛）。</p><p>请计算农夫约翰达到目的所需要的最小反转次数。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>输入的第一行包含$N$的值。</p><p>第二行包含一个长为$N$的的字符串，给出初始时所有奶牛从左到右的排列方式。每个<code>H</code>代表一头荷斯坦牛，每个<code>G</code>代表一头更赛牛。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>输出一行，包含达到目的所需要的最小反转次数。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">2 \le N \le 2 \times 10^5, N为偶数。</script><p><strong>输入样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">14</span><br><span class="line">GGGHGHHGHHHGHG</span><br></pre></td></tr></table></figure><p><strong>输出样例：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><p><strong>样例解释：</strong></p><p>在这个例子中，只需反转由前六头奶牛组成的前缀即可。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   GGGHGHHGHHHGHG （反转前）</span><br><span class="line">-&gt; HGHGGGHGHHHGHG （反转后）</span><br></pre></td></tr></table></figure><p>在反转之前，四头更赛牛处于偶数位置。</p><p>反转后，六头更赛牛处于偶数位置。不可能使得超过六头更赛牛处于偶数位置。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><p>解不了告辞。</p><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>题目给定了$N$为偶数，那么观察样例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GGGHGHHGHHHGHG</span><br></pre></td></tr></table></figure><p>从第一个开始，两个一组，那么可<strong>分组</strong>为<code>GG, GH, GH, HG, HH, HG, HG</code>，可以看出，总共只有四种模式，即：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GG</span><br><span class="line">HH</span><br><span class="line">GH</span><br><span class="line">HG</span><br></pre></td></tr></table></figure><p>显然我们是可以通过操作，改变任意一个分组中元素的顺序（意思是<code>GH</code>可以变成<code>HG</code>，<code>HG</code>也可以变成<code>GH</code>）。</p><p>显然，<code>GG</code>和<code>HH</code>的翻转不会对结果产生影响，那么我们可以忽略原串中的<code>GG</code>和<code>HH</code>分组，那么原串变为（空格是为了清楚地看出分组）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GH GH HG HG HG</span><br></pre></td></tr></table></figure><p>然后将<code>GH</code>分组编码为1，<code>HG</code>编码为0，串变为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 1 0 0 0</span><br></pre></td></tr></table></figure><p>那么0即<code>HG</code>是我们需要的，而1即<code>GH</code>经过一次翻转后可以变为0。</p><p>原题此时变为，在上述01串中（<code>11000</code>），随意选取前缀进行翻转，使得串中的0最多。</p><p>显然，全部变为0是可行的（将前两个1进行翻转，翻转后串变为<code>00000</code>），那么现在的任务是求出操作的最少次数。</p><p><strong>直接给出结论：最少交换次数是01串中01交替的次数，对于上面的<code>11000</code>，最少交换次数是1。</strong></p><p>由于只有<code>01</code>和<code>10</code>这两种情况，所以枚举证明如下：</p><ul><li>当为<code>01</code>时，即<code>HG GH</code>时，翻转<code>1</code>前面的所有数，变为<code>GH GH</code>，即<code>11</code>。</li><li>当为<code>10</code>时，即<code>GH HG</code>时，翻转<code>0</code>前面的所有数，变为<code>HG HG</code>，即<code>00</code>。</li></ul><p>由于是从第一位开始翻转的，所以从第一位翻转到最后一位之后，所有的1会变成0，串会变为一个全0串。</p><p>且当最后一位为1时，整个串会变为全1，所以需要额外再翻转一次。</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    string s, temp;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; s;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将GH串转为01串</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i += <span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 当相邻的两个字母不同时</span></span><br><span class="line">        <span class="keyword">if</span> (s[i] != s[i + <span class="number">1</span>])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 如果是GH</span></span><br><span class="line">            <span class="keyword">if</span> (s[i] == <span class="string">&#x27;G&#x27;</span>)</span><br><span class="line">                temp += <span class="string">&#x27;1&#x27;</span>;</span><br><span class="line">            <span class="comment">// 否则为HG</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                temp += <span class="string">&#x27;0&#x27;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// cout &lt;&lt; temp &lt;&lt; endl;</span></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; temp.<span class="built_in">size</span>() - <span class="number">1</span>; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 统计01串中01的交换次数</span></span><br><span class="line">        <span class="keyword">if</span> (temp[i] != temp[i + <span class="number">1</span>])</span><br><span class="line">        &#123;</span><br><span class="line">            res ++;</span><br><span class="line">            <span class="comment">// cout &lt;&lt; i &lt;&lt; &#x27; &#x27; &lt;&lt; res &lt;&lt; endl;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果01串的最后一位为1</span></span><br><span class="line">    <span class="keyword">if</span> (temp[temp.<span class="built_in">size</span>() - <span class="number">1</span>] == <span class="string">&#x27;1&#x27;</span>)</span><br><span class="line">        res ++;</span><br><span class="line"></span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4440. 照相。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>tmux配置文件</title>
    <link href="http://blog.karltan.com/environment/configuration-file-tmux/"/>
    <id>http://blog.karltan.com/environment/configuration-file-tmux/</id>
    <published>2023-07-14T19:00:00.000Z</published>
    <updated>2023-07-14T20:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="tmux配置文件"><a href="#tmux配置文件" class="headerlink" title="tmux配置文件"></a>tmux配置文件</h1><p>在家目录下，新建一个文件名为<code>.tmux.conf</code>的文件，将下面的内容写入文件，即可完成配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">set-option -g status-keys vi</span><br><span class="line">setw -g mode-keys vi</span><br><span class="line"></span><br><span class="line">setw -g monitor-activity on</span><br><span class="line"></span><br><span class="line"><span class="comment"># setw -g c0-change-trigger 10</span></span><br><span class="line"><span class="comment"># setw -g c0-change-interval 100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># setw -g c0-change-interval 50</span></span><br><span class="line"><span class="comment"># setw -g c0-change-trigger  75</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">set-window-option -g automatic-rename on</span><br><span class="line">set-option -g set-titles on</span><br><span class="line"><span class="built_in">set</span> -g history-limit 100000</span><br><span class="line"></span><br><span class="line"><span class="comment">#set-window-option -g utf8 on</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># set command prefix</span></span><br><span class="line">set-option -g prefix C-a</span><br><span class="line">unbind-key C-b</span><br><span class="line">bind-key C-a send-prefix</span><br><span class="line"></span><br><span class="line"><span class="built_in">bind</span> h select-pane -L</span><br><span class="line"><span class="built_in">bind</span> j select-pane -D</span><br><span class="line"><span class="built_in">bind</span> k select-pane -U</span><br><span class="line"><span class="built_in">bind</span> l select-pane -R</span><br><span class="line"></span><br><span class="line"><span class="built_in">bind</span> -n M-Left select-pane -L</span><br><span class="line"><span class="built_in">bind</span> -n M-Right select-pane -R</span><br><span class="line"><span class="built_in">bind</span> -n M-Up select-pane -U</span><br><span class="line"><span class="built_in">bind</span> -n M-Down select-pane -D</span><br><span class="line"></span><br><span class="line"><span class="built_in">bind</span> &lt; resize-pane -L 7</span><br><span class="line"><span class="built_in">bind</span> &gt; resize-pane -R 7</span><br><span class="line"><span class="built_in">bind</span> - resize-pane -D 7</span><br><span class="line"><span class="built_in">bind</span> + resize-pane -U 7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bind-key -n M-l next-window</span><br><span class="line">bind-key -n M-h previous-window</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g status-interval 1</span><br><span class="line"><span class="comment"># status bar</span></span><br><span class="line"><span class="built_in">set</span> -g status-bg black</span><br><span class="line"><span class="built_in">set</span> -g status-fg blue</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#set -g status-utf8 on</span></span><br><span class="line"><span class="built_in">set</span> -g status-justify centre</span><br><span class="line"><span class="built_in">set</span> -g status-bg default</span><br><span class="line"><span class="built_in">set</span> -g status-left <span class="string">&quot; #[fg=green]#S@#H #[default]&quot;</span></span><br><span class="line"><span class="built_in">set</span> -g status-left-length 20</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># mouse support</span></span><br><span class="line"><span class="comment"># for tmux 2.1</span></span><br><span class="line"><span class="comment"># set -g mouse-utf8 on</span></span><br><span class="line"><span class="built_in">set</span> -g mouse on</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># for previous version</span></span><br><span class="line"><span class="comment">#set -g mode-mouse on</span></span><br><span class="line"><span class="comment">#set -g mouse-resize-pane on</span></span><br><span class="line"><span class="comment">#set -g mouse-select-pane on</span></span><br><span class="line"><span class="comment">#set -g mouse-select-window on</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#set -g status-right-length 25</span></span><br><span class="line"><span class="built_in">set</span> -g status-right <span class="string">&quot;#[fg=green]%H:%M:%S #[fg=magenta]%a %m-%d #[default]&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fix for tmux 1.9</span></span><br><span class="line"><span class="built_in">bind</span> <span class="string">&#x27;&quot;&#x27;</span> split-window -vc <span class="string">&quot;#&#123;pane_current_path&#125;&quot;</span></span><br><span class="line"><span class="built_in">bind</span> <span class="string">&#x27;%&#x27;</span> split-window -hc <span class="string">&quot;#&#123;pane_current_path&#125;&quot;</span></span><br><span class="line"><span class="built_in">bind</span> <span class="string">&#x27;c&#x27;</span> new-window -c <span class="string">&quot;#&#123;pane_current_path&#125;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># run-shell &quot;powerline-daemon -q&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim: ft=conf</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">tmux的配置文件.tmux.conf记录。</summary>
    
    
    
    <category term="环境配置" scheme="http://blog.karltan.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="tmux" scheme="http://blog.karltan.com/tags/tmux/"/>
    
  </entry>
  
  <entry>
    <title>vim配置文件</title>
    <link href="http://blog.karltan.com/environment/configuration-file-vim/"/>
    <id>http://blog.karltan.com/environment/configuration-file-vim/</id>
    <published>2023-07-14T18:00:00.000Z</published>
    <updated>2023-07-14T19:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="vim配置文件"><a href="#vim配置文件" class="headerlink" title="vim配置文件"></a>vim配置文件</h1><p>在家目录下，新建一个文件名为<code>.vimrc</code>的文件，将下面的内容写入文件，即可完成配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot; An example for a vimrc file.</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot; To use it, copy it to</span></span><br><span class="line"><span class="string">&quot;</span>     <span class="keyword">for</span> Unix and OS/2:  ~/.vimrc</span><br><span class="line"><span class="string">&quot;      for Amiga:  s:.vimrc</span></span><br><span class="line"><span class="string">&quot;</span>  <span class="keyword">for</span> MS-DOS and Win32:  <span class="variable">$VIM</span>\_vimrc</span><br><span class="line"><span class="string">&quot;    for OpenVMS:  sys<span class="variable">$login</span>:.vimrc</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span> When started as <span class="string">&quot;evim&quot;</span>, evim.vim will already have <span class="keyword">done</span> these settings.</span><br><span class="line"><span class="keyword">if</span> v:progname =~? <span class="string">&quot;evim&quot;</span></span><br><span class="line">  finish</span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; Use Vim settings, rather then Vi settings (much better!).</span></span><br><span class="line"><span class="string">&quot;</span> This must be first, because it changes other options as a side effect.</span><br><span class="line"><span class="built_in">set</span> nocompatible</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; allow backspacing over everything in insert mode</span></span><br><span class="line"><span class="string">set backspace=indent,eol,start</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">if has(&quot;</span>vms<span class="string">&quot;)</span></span><br><span class="line"><span class="string">  set nobackup&quot;</span> <span class="keyword">do</span> not keep a backup file, use versions instead</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">set</span> backup<span class="string">&quot; keep a backup file</span></span><br><span class="line"><span class="string">endif</span></span><br><span class="line"><span class="string">set history=50&quot;</span> keep 50 lines of <span class="built_in">command</span> line <span class="built_in">history</span></span><br><span class="line"><span class="built_in">set</span> ruler<span class="string">&quot; show the cursor position all the time</span></span><br><span class="line"><span class="string">set showcmd&quot;</span> display incomplete commands</span><br><span class="line"><span class="built_in">set</span> incsearch<span class="string">&quot; do incremental searching</span></span><br><span class="line"><span class="string">&quot;</span>==========================================================================</span><br><span class="line"><span class="string">&quot;My Setting-sunshanlu</span></span><br><span class="line"><span class="string">&quot;</span>==========================================================================</span><br><span class="line">vmap &lt;leader&gt;y :w! /tmp/vitmp&lt;CR&gt;</span><br><span class="line">nmap &lt;leader&gt;p :r! <span class="built_in">cat</span> /tmp/vitmp&lt;CR&gt;</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;语法高亮</span></span><br><span class="line"><span class="string">syntax enable</span></span><br><span class="line"><span class="string">syntax on</span></span><br><span class="line"><span class="string">&quot;</span>显示行号</span><br><span class="line"><span class="built_in">set</span> nu</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;修改默认注释颜色</span></span><br><span class="line"><span class="string">&quot;</span>hi Comment ctermfg=DarkCyan</span><br><span class="line"><span class="string">&quot;允许退格键删除</span></span><br><span class="line"><span class="string">&quot;</span><span class="built_in">set</span> backspace=2</span><br><span class="line"><span class="string">&quot;启用鼠标</span></span><br><span class="line"><span class="string">set mouse=a</span></span><br><span class="line"><span class="string">set selection=exclusive</span></span><br><span class="line"><span class="string">set selectmode=mouse,key</span></span><br><span class="line"><span class="string">&quot;</span>按C语言格式缩进</span><br><span class="line"><span class="built_in">set</span> cindent</span><br><span class="line"><span class="built_in">set</span> autoindent</span><br><span class="line"><span class="built_in">set</span> smartindent</span><br><span class="line"><span class="built_in">set</span> shiftwidth=4</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; 允许在有未保存的修改时切换缓冲区</span></span><br><span class="line"><span class="string">&quot;</span><span class="built_in">set</span> hidden</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; 设置无备份文件</span></span><br><span class="line"><span class="string">set writebackup</span></span><br><span class="line"><span class="string">set nobackup</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span>显示括号匹配</span><br><span class="line"><span class="built_in">set</span> showmatch</span><br><span class="line"><span class="string">&quot;括号匹配显示时间为1(单位是十分之一秒)</span></span><br><span class="line"><span class="string">set matchtime=5</span></span><br><span class="line"><span class="string">&quot;</span>显示当前的行号列号：</span><br><span class="line"><span class="built_in">set</span> ruler</span><br><span class="line"><span class="string">&quot;在状态栏显示正在输入的命令</span></span><br><span class="line"><span class="string">set showcmd</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">set foldmethod=syntax</span></span><br><span class="line"><span class="string">&quot;</span>默认情况下不折叠</span><br><span class="line"><span class="built_in">set</span> foldlevel=100</span><br><span class="line"><span class="string">&quot; 开启状态栏信息</span></span><br><span class="line"><span class="string">set laststatus=2</span></span><br><span class="line"><span class="string">&quot;</span> 命令行的高度，默认为1，这里设为2</span><br><span class="line"><span class="built_in">set</span> cmdheight=2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot; 显示Tab符，使用一高亮竖线代替</span></span><br><span class="line"><span class="string">set list</span></span><br><span class="line"><span class="string">&quot;</span><span class="built_in">set</span> listchars=tab:\|\ ,</span><br><span class="line"><span class="built_in">set</span> listchars=tab:&gt;-,trail:-</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;侦测文件类型</span></span><br><span class="line"><span class="string">filetype on</span></span><br><span class="line"><span class="string">&quot;</span>载入文件类型插件</span><br><span class="line">filetype plugin on</span><br><span class="line"><span class="string">&quot;为特定文件类型载入相关缩进文件</span></span><br><span class="line"><span class="string">filetype indent on</span></span><br><span class="line"><span class="string">&quot;</span> 启用自动补全</span><br><span class="line">filetype plugin indent on </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;设置编码自动识别, 中文引号显示</span></span><br><span class="line"><span class="string">filetype on &quot;</span>打开文件类型检测</span><br><span class="line"><span class="string">&quot;set fileencodings=euc-cn,ucs-bom,utf-8,cp936,gb2312,gb18030,gbk,big5,euc-jp,euc-kr,latin1</span></span><br><span class="line"><span class="string">set fileencodings=utf-8,gb2312,gbk,gb18030</span></span><br><span class="line"><span class="string">&quot;</span>这个用能很给劲，不管encoding是什么编码，都能将文本显示汉字</span><br><span class="line"><span class="string">&quot;set termencoding=gb2312</span></span><br><span class="line"><span class="string">set termencoding=utf-8</span></span><br><span class="line"><span class="string">&quot;</span>新建文件使用的编码</span><br><span class="line"><span class="built_in">set</span> fileencoding=utf-8</span><br><span class="line"><span class="string">&quot;set fileencoding=gb2312</span></span><br><span class="line"><span class="string">&quot;</span>用于显示的编码，仅仅是显示</span><br><span class="line"><span class="built_in">set</span> encoding=utf-8</span><br><span class="line"><span class="string">&quot;set encoding=utf-8</span></span><br><span class="line"><span class="string">&quot;</span><span class="built_in">set</span> encoding=euc-cn</span><br><span class="line"><span class="string">&quot;set encoding=gbk</span></span><br><span class="line"><span class="string">&quot;</span><span class="built_in">set</span> encoding=gb2312</span><br><span class="line"><span class="string">&quot;set ambiwidth=double</span></span><br><span class="line"><span class="string">set fileformat=unix</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span>设置高亮搜索</span><br><span class="line"><span class="built_in">set</span> hlsearch</span><br><span class="line"><span class="string">&quot;在搜索时，输入的词句的逐字符高亮</span></span><br><span class="line"><span class="string">set incsearch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span> 着色模式</span><br><span class="line"><span class="built_in">set</span> t_Co=256</span><br><span class="line"><span class="string">&quot;colorscheme wombat256mod</span></span><br><span class="line"><span class="string">&quot;</span>colorscheme gardener</span><br><span class="line"><span class="string">&quot;colorscheme elflord</span></span><br><span class="line"><span class="string">colorscheme desert</span></span><br><span class="line"><span class="string">&quot;</span>colorscheme evening</span><br><span class="line"><span class="string">&quot;colorscheme darkblue</span></span><br><span class="line"><span class="string">&quot;</span>colorscheme torte</span><br><span class="line"><span class="string">&quot;colorscheme default</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span> 字体 &amp;&amp; 字号</span><br><span class="line"><span class="built_in">set</span> guifont=Monaco:h10</span><br><span class="line"><span class="string">&quot;set guifont=Consolas:h10</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span> :LoadTemplate       根据文件后缀自动加载模板</span><br><span class="line"><span class="string">&quot;let g:template_path=&#x27;/home/ruchee/.vim/template/&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span> :AuthorInfoDetect   自动添加作者、时间等信息，本质是NERD_commenter &amp;&amp; authorinfo的结合</span><br><span class="line"><span class="string">&quot;&quot;</span><span class="built_in">let</span> g:vimrc_author=<span class="string">&#x27;sunshanlu&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="built_in">let</span> g:vimrc_email=<span class="string">&#x27;sunshanlu@baidu.com&#x27;</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="built_in">let</span> g:vimrc_homepage=<span class="string">&#x27;http://www.sunshanlu.com&#x27;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot; Ctrl + E            一步加载语法模板和作者、时间信息</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;map &lt;c-e&gt; &lt;ESC&gt;:AuthorInfoDetect&lt;CR&gt;&lt;ESC&gt;Gi</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;imap &lt;c-e&gt; &lt;ESC&gt;:AuthorInfoDetect&lt;CR&gt;&lt;ESC&gt;Gi</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;vmap &lt;c-e&gt; &lt;ESC&gt;:AuthorInfoDetect&lt;CR&gt;&lt;ESC&gt;Gi</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span> ======= 引号 &amp;&amp; 括号自动匹配 ======= <span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;:inoremap ( ()&lt;ESC&gt;i</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span>:inoremap ) &lt;c-r&gt;=ClosePair(<span class="string">&#x27;)&#x27;</span>)&lt;CR&gt;</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span>:inoremap &#123; &#123;&#125;&lt;ESC&gt;i</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span>:inoremap &#125; &lt;c-r&gt;=ClosePair(<span class="string">&#x27;&#125;&#x27;</span>)&lt;CR&gt;</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span>:inoremap [ []&lt;ESC&gt;i</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span>:inoremap ] &lt;c-r&gt;=ClosePair(<span class="string">&#x27;]&#x27;</span>)&lt;CR&gt;</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span>:inoremap &lt; &lt;&gt;&lt;ESC&gt;i</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span>:inoremap &gt; &lt;c-r&gt;=ClosePair(<span class="string">&#x27;&gt;&#x27;</span>)&lt;CR&gt;</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;:inoremap &quot;</span> <span class="string">&quot;&quot;</span>&lt;ESC&gt;i</span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span>:inoremap <span class="string">&#x27; &#x27;</span><span class="string">&#x27;&lt;ESC&gt;i</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;:inoremap ` ``&lt;ESC&gt;i</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;:inoremap * **&lt;ESC&gt;i</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot; 每行超过80个的字符用下划线标示</span></span><br><span class="line"><span class="string">&quot;&quot;au BufRead,BufNewFile *.s,*.asm,*.h,*.c,*.cpp,*.java,*.cs,*.lisp,*.el,*.erl,*.tex,*.sh,*.lua,*.pl,*.php,*.tpl,*.py,*.rb,*.erb,*.vim,*.js,*.jade,*.coffee,*.css,*.xml,*.html,*.shtml,*.xhtml Underlined /.\%81v/</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot;</span></span><br><span class="line"><span class="string">&quot; For Win32 GUI: remove &#x27;</span>t<span class="string">&#x27; flag from &#x27;</span>guioptions<span class="string">&#x27;: no tearoff menu entries</span></span><br><span class="line"><span class="string">&quot; let &amp;guioptions = substitute(&amp;guioptions, &quot;t&quot;, &quot;&quot;, &quot;g&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot; Don&#x27;</span>t use Ex mode, use Q <span class="keyword">for</span> formatting</span><br><span class="line">map Q gq</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; This is an alternative that also works in block mode, but the deleted</span></span><br><span class="line"><span class="string">&quot;</span> text is lost and it only works <span class="keyword">for</span> putting the current register.</span><br><span class="line"><span class="string">&quot;vnoremap p &quot;</span>_dp</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; Switch syntax highlighting on, when the terminal has colors</span></span><br><span class="line"><span class="string">&quot;</span> Also switch on highlighting the last used search pattern.</span><br><span class="line"><span class="keyword">if</span> &amp;t_Co &gt; 2 || has(<span class="string">&quot;gui_running&quot;</span>)</span><br><span class="line">  syntax on</span><br><span class="line">  <span class="built_in">set</span> hlsearch</span><br><span class="line">endif</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; Only do this part when compiled with support for autocommands.</span></span><br><span class="line"><span class="string">if has(&quot;</span>autocmd<span class="string">&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;</span> Enable file <span class="built_in">type</span> detection.</span><br><span class="line">  <span class="string">&quot; Use the default filetype settings, so that mail gets &#x27;tw&#x27; set to 72,</span></span><br><span class="line"><span class="string">  &quot;</span> <span class="string">&#x27;cindent&#x27;</span> is on <span class="keyword">in</span> C files, etc.</span><br><span class="line">  <span class="string">&quot; Also load indent files, to automatically do language-dependent indenting.</span></span><br><span class="line"><span class="string">  filetype plugin indent on</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;</span> Put these <span class="keyword">in</span> an autocmd group, so that we can delete them easily.</span><br><span class="line">  augroup vimrcEx</span><br><span class="line">  au!</span><br><span class="line"></span><br><span class="line">  <span class="string">&quot; For all text files set &#x27;textwidth&#x27; to 80 characters.</span></span><br><span class="line"><span class="string">  autocmd FileType text setlocal textwidth=80</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  &quot;</span> When editing a file, always jump to the last known cursor position.</span><br><span class="line">  <span class="string">&quot; Don&#x27;t do it when the position is invalid or when inside an event handler</span></span><br><span class="line"><span class="string">  &quot;</span> (happens when dropping a file on gvim).</span><br><span class="line">  autocmd BufReadPost *</span><br><span class="line">    \ <span class="keyword">if</span> line(<span class="string">&quot;&#x27;\&quot;&quot;</span>) &gt; 0 &amp;&amp; line(<span class="string">&quot;&#x27;\&quot;&quot;</span>) &lt;= line(<span class="string">&quot;$&quot;</span>) |</span><br><span class="line">    \   exe <span class="string">&quot;normal g`\&quot;&quot;</span> |</span><br><span class="line">    \ endif</span><br><span class="line"></span><br><span class="line">  augroup END</span><br><span class="line"></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">set</span> autoindent<span class="string">&quot; always set autoindenting on</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">endif &quot;</span> has(<span class="string">&quot;autocmd&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; 增加鼠标行高亮</span></span><br><span class="line"><span class="string">set cursorline</span></span><br><span class="line"><span class="string">hi CursorLine  cterm=NONE   ctermbg=darkred ctermfg=white</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;</span> 设置tab是四个空格</span><br><span class="line"><span class="built_in">set</span> ts=4</span><br><span class="line"><span class="built_in">set</span> expandtab</span><br><span class="line"></span><br><span class="line"><span class="string">&quot; 主要给Tlist使用</span></span><br><span class="line"><span class="string">let Tlist_Exit_OnlyWindow = 1</span></span><br><span class="line"><span class="string">let Tlist_Auto_Open = 1</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">vim的配置文件.vimrc记录。</summary>
    
    
    
    <category term="环境配置" scheme="http://blog.karltan.com/categories/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="vim" scheme="http://blog.karltan.com/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>AcWing 4908. 饥饿的牛</title>
    <link href="http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4908/"/>
    <id>http://blog.karltan.com/ac-diary/acwing/summer-daily-question-2023/acwing4908/</id>
    <published>2023-07-14T17:00:00.000Z</published>
    <updated>2023-07-14T18:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AcWing-4908-饥饿的牛"><a href="#AcWing-4908-饥饿的牛" class="headerlink" title="AcWing 4908. 饥饿的牛"></a>AcWing 4908. 饥饿的牛</h1><p><a href="https://www.acwing.com/problem/content/4911/">4908. 饥饿的牛 - AcWing题库</a></p><p>贝茜是一头饥饿的牛。</p><p>每天晚上，如果牛棚中还有干草的话，贝茜都会吃掉其中的一捆。</p><p>初始时，牛棚中没有干草。</p><p>为了让贝茜不被饿死，农夫约翰制定了$N$个给贝茜送干草的计划。</p><p>其中第$i$个计划是在第$d_i$天的白天给贝茜送去$b_i$捆干草。</p><p>这些计划互不冲突，保证$1 \le d_1 &lt; d_2 &lt; \cdots &lt; d_N \le T$。</p><p>请你计算，贝茜在第$1 \sim T$天中有多少天有干草吃。</p><h2 id="输入格式"><a href="#输入格式" class="headerlink" title="输入格式"></a>输入格式</h2><p>第一行包含两个整数$N$和$T$。</p><p>接下来$N$行，每行包含两个整数$d_i, b_i$。</p><h2 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a>输出格式</h2><p>输出贝茜在第$1 \sim T$天中有干草吃的天数。</p><h2 id="数据范围"><a href="#数据范围" class="headerlink" title="数据范围"></a>数据范围</h2><script type="math/tex; mode=display">\begin{aligned}1 &\le N \le 10^5, \\1 &\le T \le 10^{14}, \\1 &\le d_i \le 10^{14}, \\1 &\le b_i \le 10^9。\end{aligned}</script><p><strong>输入样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 5</span><br><span class="line">1 2</span><br></pre></td></tr></table></figure><p><strong>输出样例1：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><p><strong>样例1解释：</strong></p><p>两捆干草在第$1$天早上被送到了牛棚，所以贝茜第$1, 2$天有干草吃。</p><p><strong>输入样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2 5</span><br><span class="line">1 2</span><br><span class="line">5 10</span><br></pre></td></tr></table></figure><p><strong>输出样例2：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><p><strong>样例2解释：</strong></p><p>两捆干草在第$1$天早上被送到了牛棚，所以贝茜第$1, 2$天有干草吃。</p><p>$10$捆干草在第$5$天早上被送到了牛棚，所以贝茜第$5$天有干草吃。</p><p><strong>输入样例3：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2 5</span><br><span class="line">1 10</span><br><span class="line">5 10</span><br></pre></td></tr></table></figure><p><strong>输出样例3：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5</span><br></pre></td></tr></table></figure><p><strong>样例3解释：</strong></p><p>$10$捆干草在第$1$天早上被送到了牛棚，所以贝茜第$1 \sim 5$天都有干草吃。</p><h2 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h2><h3 id="我的题解"><a href="#我的题解" class="headerlink" title="我的题解"></a>我的题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">100010</span>;</span><br><span class="line"><span class="type">long</span> <span class="type">long</span> n, T;</span><br><span class="line"><span class="type">long</span> <span class="type">long</span> d[N], b[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cin &gt;&gt; n &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">long</span> <span class="type">long</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">        cin &gt;&gt; d[i] &gt;&gt; b[i];</span><br><span class="line">    <span class="comment">// 避免边界问题，在数组的最后加上一组数据</span></span><br><span class="line">    d[n] = T;</span><br><span class="line">    b[n] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> res = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 遍历每一次送草</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">long</span> <span class="type">long</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 这一次送草为curr</span></span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> curr = i;</span><br><span class="line">        <span class="comment">// 下一次送草为next</span></span><br><span class="line">        <span class="type">long</span> <span class="type">long</span> next = i + <span class="number">1</span>;</span><br><span class="line">        <span class="comment">// 当前共有sum捆草</span></span><br><span class="line">        sum += b[curr];</span><br><span class="line">        <span class="comment">// cout &lt;&lt; sum &lt;&lt; endl;</span></span><br><span class="line">        <span class="comment">// 如果现有的草能够吃到下一次送草</span></span><br><span class="line">        <span class="keyword">if</span> (sum &gt;= d[next] - d[curr])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 加上天数</span></span><br><span class="line">            res += d[next] - d[curr];</span><br><span class="line">            sum -= d[next] - d[curr];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 否则不够，草会吃完，加上草的数量即可</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            res += sum;</span><br><span class="line">            <span class="comment">// 将草的数量清零</span></span><br><span class="line">            sum = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// cout &lt;&lt; d[i] &lt;&lt; &#x27; &#x27; &lt;&lt; b[i] &lt;&lt; endl;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果到了最后一天还有草</span></span><br><span class="line">    <span class="keyword">if</span> (sum)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 那么最后一天还有得吃</span></span><br><span class="line">        res ++;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; res &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="y总题解"><a href="#y总题解" class="headerlink" title="y总题解"></a>y总题解</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstring&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="type">long</span> <span class="type">long</span> LL;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> n;</span><br><span class="line">    LL T;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%d%lld&quot;</span>, &amp;n, &amp;T);</span><br><span class="line"></span><br><span class="line">    LL res = <span class="number">0</span>, cur = <span class="number">0</span>, last = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i ++ )</span><br><span class="line">    &#123;</span><br><span class="line">        LL d, b;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%lld%lld&quot;</span>, &amp;d, &amp;b);</span><br><span class="line">        LL len = d - <span class="number">1</span> - last;</span><br><span class="line">        LL days = <span class="built_in">min</span>(len, cur);</span><br><span class="line"></span><br><span class="line">        res += days;</span><br><span class="line">        cur = cur - days + b;</span><br><span class="line">        last = d - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    res += <span class="built_in">min</span>(cur, T - last);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%lld\n&quot;</span>, res);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">AcWing夏季每日一题2023——AcWing 4908. 饥饿的牛。</summary>
    
    
    
    <category term="AC日记" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/"/>
    
    <category term="AcWing" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/"/>
    
    <category term="夏季每日一题2023" scheme="http://blog.karltan.com/categories/AC%E6%97%A5%E8%AE%B0/AcWing/%E5%A4%8F%E5%AD%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%982023/"/>
    
    
    <category term="AcWing" scheme="http://blog.karltan.com/tags/AcWing/"/>
    
    <category term="算法题" scheme="http://blog.karltan.com/tags/%E7%AE%97%E6%B3%95%E9%A2%98/"/>
    
  </entry>
  
</feed>
