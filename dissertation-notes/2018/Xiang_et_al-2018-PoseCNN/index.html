<!-- ./node_modules/hexo-theme-anzhiyu/layout/includes/layout.pug--><!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》 | Karl的博客</title><meta name="keywords" content="论文笔记,RSS"><meta name="author" content="Karl"><meta name="copyright" content="Karl"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》"><meta name="application-name" content="论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》"><meta property="og:url" content="http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/index.html"><meta property="og:site_name" content="Karl的博客"><meta property="og:description" content="PoseCNN：基于卷积神经网络的6D目标姿态估计。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://img.karltan.com/covers/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN.png"><meta property="article:author" content="Karl"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://img.karltan.com/covers/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN.png"><meta name="description" content="PoseCNN：基于卷积神经网络的6D目标姿态估计。"><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/"><link rel="preconnect" href="//cdn.cbd.int"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//static.cloudflareinsights.com"/><link rel="preconnect" href="//www.clarity.ms"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="ItAgqL0bGv4TvuAzDl6nNhCuwt5RdBLYRkwae25qGzU"/><meta name="baidu-site-verification" content="codeva-Y7ehYLkADp"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@1.0.17/lib/assets/font-awesome-animation.min.css"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/swiper/swiper.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?0f2c96ab05c3c6d77e2d8fbf3240e404";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script defer="defer" data-pjax="data-pjax" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;758e166757ec488583caf56b7dd7f095&quot;}"></script><script>(function(c,l,a,r,i,t,y){
    c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
    t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
    y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
})(window, document, "clarity", "script", "i0moouu8jj");</script><script>const GLOBAL_CONFIG = {
  linkPageTop: {"enable":true,"title":"与数百名博主无限进步","addFriendPlaceholder":"昵称（请勿包含博客等字样）：\n网站地址（要求博客地址，请勿提交个人主页）：\n头像图片url（请提供尽可能清晰的图片）：\n描述：\n站点截图（可选）：\n"},
  peoplecanvas: undefined,
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"tianli","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"a93a787ee06b121647ed","Referer":"https://blog.karltan.com/"},
  diytitle: undefined,
  LA51: {"enable":true,"ck":"3Gbf0O9t4EtAOhf1","LingQueMonitorID":"3GbfAYtZPDEan7kS"},
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 宝贝","startTime":12,"endTime":13},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":17},{"greeting":"18点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":18,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: 'https://twikoo.karltan.com/.netlify/functions/twikoo',
  commentBarrageConfig:{"enable":true,"maxBarrage":1,"barrageTime":4000,"accessToken":"6f88301de4664d9d9a7cadf39a9f6e5a","mailMd5":""},
  root: '/',
  preloader: {"source":2},
  friends_vue_info: {"apiurl":"https://fcircle.karltan.com/"},
  navMusic: false,
  mainTone: undefined,
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: {"appId":"8C3UHN3LPN","apiKey":"e00dcc69cdc423d89a288bc784a14012","indexName":"blog-search","hits":{"per_page":6},"languages":{"input_placeholder":"输入关键词后按下回车查找","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: {"limitDay":1,"position":"top","messagePrev":"本文距上次修改已经","messageNext":"天，其中的内容可能已经不再适用。"},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Karl","link":"链接: ","source":"来源: Karl的博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: false,
  shortcutKey: {"enable":true,"delay":100,"shiftDelay":200},
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'Karl的博客',
  title: '论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》',
  postAI: 'true',
  pageFillDescription: 'PoseCNN A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes, Abstract, I. INTRODUCTION, II. RELATED WORK, III. POSECNN, A. Overview of the Network, B. Semantic Labeling, C. 3D Translation Estimation, D. 3D Rotation Regression, IV. THE YCB-VIDEO DATASET, A. 6D Pose Annotation, B. Dataset Characteristics, V. EXPERIMENTS, A. Datasets, B. Evaluation Metrics, C. Implementation Details, D. Baselines, E. Analysis on the Rotation Regress Losses, F. Results on the YCB-Video Dataset, G. Results on the OccludedLINEMOD Dataset, VI. CONCLUSIONS原文链接论文笔记的博客链接论文笔记博客论文链接代码链接项目链接对一个已知物体进行姿态估计对于机器人与现实世界的交互是非常重要的但是由于对象的多样性对象的杂乱以及对象之间的遮挡引起的复杂性这个问题是具有挑战性的所以这篇文章提出了一种新的卷积神经网络通过定位物体在图像中的中心并预测其到相机的距离来估计物体的平移通过回归到四元数表示来估计物体的旋转然后这里提出了一种现代的损失函数能够让有处理对称对象的能力应用场景机器人操作需要识别物体的位置和方向机器人可以从演示中学习挑战对象具有多样性对象有不同的形状对象在图像中的外观受光照物体之间的杂乱和物体之间的相互遮挡所影响在传统方法中姿态估计问题是通过模型与图像之间的特征点匹配来解决的然而这要求物体具有丰富的纹理这就导致了传统方法无法处理无纹理对象随着深度相机的出现人们提出了几种使用数据来识别无纹理物体的方法但是对于基于模板的方法遮挡显著降低了识别性能对于通过学习将图像像素回归到对象坐标以建立用于姿态估计的对应关系的方法不能处理对称对象背后的一个关键思想是将姿态估计任务解耦为不同的组件这使得网络能够显式地建模它们之间的依赖和独立执行三个相关任务如图所示首先它为输入图像中的每个像素预测一个对象标签做语义分割其次通过预测从每个像素到中心的单位向量来估计目标中心的像素坐标使用语义分割标签与对象相关联的图像像素会在图像中投票选出对象中心位置此外该网络还可以估算物体中心的距离假设已知相机特性估计对象中心和它的距离能够让我们还原它的平移最后通过将内提取的卷积特征回归到的四元数表示来估计旋转正如我们将展示的先进行中心投票然后进行旋转回归来估计和可以应用于纹理无纹理的物体并且因为网络被训练去给物体的中心投票所以对遮挡是鲁棒的处理对称对象是姿态估计的另一个挑战因为不同的对象方向可能产生相同的观察结果例如不可能唯一地估计红色碗第三行第三个或木块第三行第四个的方向如图所示虽然像数据集这样的基准数据集考虑对这些对象进行特殊的对称评估但在网络训练期间对称性通常会被忽略然而这可能会导致糟糕的训练性能因为网络接收到不一致的损失信号例如在目标方向上的高损失即使从网络中估计的对象的对称性是正确的受此启发我们引入了这是一个新的损失函数专注于匹配对象的形状我们将证明这个损失函数对具有形状对称性的物体产生了更好的估计我们在数据集上评估了我们的方法这是一个用于姿态估计的基准数据集在这个具有挑战性的数据集上实现了纯颜色和姿态估计的最新结果我们在迭代最近点算法中使用深度图像进行姿态优化综上所述我们的工作有以下主要贡献我们提出了一种用于目标位姿估计的卷积神经网络命名为我们的网络实现端到端的姿态估计并且对物体之间的遮挡非常鲁棒提出了一种用于对称目标位姿估计的训练损失函数我们为对象的姿态估计提供了一个大尺度视频数据集其中我们为个对象提供了姿态标注文献中的目标位姿估计方法大致可以分为基于模板的方法和基于特征的方法在基于模板的方法中构造一个刚体模板用于扫描输入图像中的不同位置在每个位置计算一个相似度得分通过比较这些相似度得分得到最佳匹配在位姿估计中通常通过渲染相应的模型来获得模板最近目标检测方法被用于模板匹配和姿态估计特别是基于深度学习的目标检测器基于模板的方法在检测无纹理对象方面很有用但是它们不能很好地处理对象之间的遮挡因为如果对象被遮挡模板的相似度分数会很低在基于特征的方法中从感兴趣的点或图像中的每个像素点提取局部特征并与模型上的特征进行匹配建立对应关系从而恢复姿态基于特征的方法能够处理物体之间的遮挡然而为了计算局部特征它们需要在对象上有足够的纹理为了处理无纹理对象提出了几种使用机器学习技术学习特征描述符的方法已经有若干种对于每个像素直接回归到对象坐标来建立对应关系的方法被提出了但是坐标回归在处理对称对象时遇到了歧义在这项工作中我们在深度学习框架中结合了基于模板的方法和基于特征的方法的优点其中网络结合了自底向上像素标记和自顶向下对象姿态回归最近由于亚马逊拾取挑战的竞争物体的姿态估计问题得到了更多的关注中针对特定的设置引入了几种数据集和方法只要提供适当的训练数据我们的网络有潜力应用于设置给定一幅输入图像目标位姿估计的任务是估计从目标坐标系到摄像机坐标系的刚性变换我们假设对象的模型是可用的并且对象的坐标系统定义在模型的空间中这里的刚性变换由一个包含旋转和平移的变换组成其中表示围绕对象坐标系的轴轴和轴的旋转角度是摄像机坐标系中的原点的坐标在成像过程中决定了物体在图像中的位置和比例而根据物体的形状和纹理影响物体的图像外观由于这两个参数有不同的视觉特性我们提出了一个卷积神经网络结构内部解耦和的估计图说明了我们用于目标姿态估计的网络结构网络包含两个阶段第一阶段由个卷积层和个层组成从输入图像中提取不同分辨率的特征图这个阶段是网络的主干因为所提取的特征在网络执行的所有任务中共享第二阶段包括一个嵌入步骤将第一阶段生成的高维特征映射嵌入到低维特定于任务的特征中然后网络执行姿态估计的三个不同任务即语义标记平移估计和旋转回归如下所述为了检测图像中的对象我们采用语义标记网络将每个图像像素分类成一类这个像素如果属于罐头那么这个像素就被分类为罐头类与最近借助于目标检测的位姿估计方法相比语义分割标记能够提供更丰富的对象信息并且能够更好的处理遮挡语义标注分支的嵌入步骤如图所示以特征提取阶段生成的两个通道尺寸为的特征映射作为输入两种特征图的分辨率分别为原始图像尺寸的和该网络首先使用两个卷积层将两个特征映射的信道维数降至然后用反卷积层将特征图的分辨率提高一倍然后将两个求和再用另一个反卷积层将分辨率提高倍得到与原始图像大小一致的最后卷积层对特征图进行操作生成像素的语义标记分数过程如下图这一层的输出有个通道和个语义类的数量在训练中采用交叉熵损失训练语义标注分支在测试过程中使用函数计算像素的类概率语义标注分支的设计灵感来自于中用于语义标注的全卷积网络平移是物体原点在摄像机坐标系中的坐标如图所示估计的一种简单方法是直接将图像特征回归到但是由于对象可以出现在图像中的任何位置所以这种方法是不可泛化的不能将图像的位置和物体的类型进行关联这是不对的此外它不能处理同一类别中的多个对象实例因此我们提出通过定位图像中的目标中心估计目标到摄像机的距离来估计平移量假设在图像上的投影为对于针孔摄像机来说如果网络能够在图像中定位并估计出深度那么我们可以根据下面的投影方程恢复和其中和表示相机的焦距是主点相机原点针孔相机模型下图将成像平面平移到了小孔处那么显然下图中的与是相似的假设图中点的坐标为点的坐标为那么有那么可得又小孔成像模型的原点在图像的中心位置数字图像处理的原点在图像的左上角所以需要一个偏移量即如果物体原点是物体的质心我们称为物体的中心即物体在图像上的位置一种直接定位目标中心注意这里说的是中心需要与后面的内点做区分的方法是像现有的关键点检测方法一样直接检测中心点然而如果对象中心被遮挡这些方法将不起作用在隐式形状模型中为了进行检测图像块会对物体中心进行投票受该模型的启发我们设计了我们的网络以回归预测出图像中每个像素的中心方向具体来说对于图像上的像素它回归到三个变量这里顺便回归出了内点的深度因为遮挡不能直接回归物体中心的深度注意我们没有直接回归到位移矢量而是将网络设计成回归到单位长度矢量即中心方向它是尺度不变的因此更容易训练正如我们通过实验验证的那样我们网络的中心回归分支图与语义标记分支采用相同的架构只是卷积层和反卷积层的通道维数不同我们将高维特征嵌入到维空间而不是维空间因为这个分支需要为每个对象类回归到三个变量该分支的最后一个卷积层的通道维数为对象类数为在训练中使用平滑的损失函数进行回归如为了找到一个对象的对象中心设计了一个投票层并集成到网络中投票层采用像素级语义标记结果和中心回归结果作为输入对于每个对象它首先计算图像中每个位置的投票分数投票分数表明该位置是对象中心的可能性有多大具体地说对象中的每一个像素都会在网络预测出的方向上投票见图在处理对象中的所有像素后我们获得所有图像位置的投票分数然后得分最高的位置就作为目标中心对于同一对象可能多次出现在图像中的情况我们对投票分数应用非极大值抑制然后选择分数大于某一阈值的位置在生成一组对象中心后我们认为投票给对象中心的像素是中心的内层然后将中心的预测深度简单地计算为内层预测深度的平均值最后利用公式我们可以估计出平移此外网络生成对象的作为约束所有内层的矩形该用于旋转回归图的最下方为旋转回归分支利用投票层预测的对象的我们可以用两个池化层对网络第一阶段生成的视觉特征进行裁剪和池化用于旋转回归合并的特征映射被加在一起并馈送到三个全连接层中前两个层的维数为最后一个层的维数为对象类数为对于每个类最后一个层输出一个四元数表示的旋转为了训练四元数回归我们提出了两个损失函数其中一个是专门设计用于处理对称对象第一个损失函数称为它在模型空间中运行并使用正确模型位姿和估计模型位姿两个位姿之间的对应点来测算平均平方距离定义为为模型点集为点个数和分别表示由估计的四元数和四元数计算得到的旋转矩阵当估计方向与方向相同时该损失有其唯一的最小值不幸的是不能适当地处理对称对象因为一个对称对象可以有多个正确的旋转在对称物体上使用这样的损失函数会对网络进行不必要的惩罚因为它会回归到一个可选的旋转中因此可能会给出不一致的训练信号虽然可以通过手动指定对象对称性然后考虑所有正确的方向作为选项来修改进而处理对称对象但我们在这里仍然要引入这是一个不需要指定对称性的损失函数定义为我们可以看到就像一样这个损失测量的是估计模型方向上的每个点与模型上最近的点之间的偏移当两个模型匹配时最小通过这种方式将不会惩罚相对于对象的形状对称等效的旋转因为标签是手动添加的所以以对象为中心的为对象位姿和或分割提供的数据集不会太大例如流行的数据集为数据集中的个对象中的每个对象提供了大约张图像的手动注释虽然这样的数据集对基于模型的姿态估计技术的评估是有用的但它比训练最先进的深度神经网络的典型数据集要小几个数量级这个问题的一个解决方案是用合成图像来扩充数据但是必须注意确保在真实场景和渲染场景之间的性能是通用的为了避免手动注释所有的视频帧我们只在每个视频的第一帧手动指定对象的姿势我们在第一个深度帧中细化每个对象的姿态使用每个对象的符号距离函数表示接下来通过固定物体相对于其他物体的姿态并通过深度视频跟踪物体的配置来初始化摄像机的轨迹最后对摄像机轨迹和相对目标位姿进行全局优化我们使用的对象是如图所示的个对象的一个子集选择这些对象是因为高质量的模型和良好的深度可见性视频采集使用华硕相机采用快速裁剪模式通过在本地设备上捕获图像并通过只传输中心区域以帧秒的速度提供分辨率的图像这以较低的视场为代价获得了更高的图像的有效分辨率但考虑到深度传感器的最小范围这是一个可以接受的折中完整的数据集包含幅图像比数据集大两个数量级关于数据集的更多统计信息请参见表图为我们数据集中的一个标注示例我们根据标注的位姿来渲染模型请注意我们的标注精度受到几个误差源的影响包括传感器的滚动快门对象模型中的不准确性和深度传感器之间的轻微异步以及相机内外参数的不确定性在我们的数据集中我们使用个视频进行训练并从剩下的个测试视频中提取个关键帧进行测试我们还在数据集上评估了我们的方法的作者从原始的数据集中选择了一个带有帧的视频并在该视频中标注了个物体的姿势和在这个视频序列中物体之间有明显的遮挡这使得这个数据集具有挑战性为了进行训练我们使用与这八个对象对应的原始数据集中的八个序列此外我们通过在场景中随机放置物体在两个数据集上生成张合成图像进行训练我们采用中提出的平均距离度量进行评估给定旋转和平移以及估计的旋转和平移平均距离计算根据姿态和估计姿态变换的模型点之间成对距离的平均值式中为模型点集为点个数如果平均距离小于预定义的阈值则认为姿势是正确的在数据集中阈值设置为模型直径的对于像和这样的对称对象对于某些视图点之间的匹配是不明确的因此利用最近点距离计算平均距离我们为旋转回归设计的损失函数受这两个评价指标所启发并且在计算位姿精度时使用固定的阈值不能揭示方法在这些不正确的位姿上的表现因此我们在评价时改变距离阈值在这种情况下我们可以绘制一条精度阈值曲线并计算曲线下的面积来进行姿态评估我们可以将变换后的点投影到图像上然后在图像空间中计算成对的距离而不是在空间中计算距离这个度量被称为重投影损失广泛用于仅使用彩色图像时的姿态估计是使用库实现的和一样投票层是在上实现的在训练中使用上训练的网络对特征提取阶段的前个卷积层和旋转回归分支的前两个层的参数进行初始化没有梯度通过投票层反向传播采用带动量的随机梯度下降法进行训练对象坐标回归网络由于目前最先进的姿态估计方法大多依赖于将图像像素回归到物体坐标因此我们实现了一种用于物体坐标回归的网络变体以进行比较在这个网络中我们不是像图那样回归到中心的方向和深度而是回归到每个像素在对象坐标系中的坐标我们可以使用相同的架构因为对于每个类每个像素仍然回归到三个变量然后我们删除旋转回归分支利用语义标记结果和物体坐标回归结果利用恢复姿态如所示意思是现有的方法是直接将坐标回归到坐标从而得到对象的坐标但是论文中的方法是先通过语义分割估计物体在图像中的中心坐标然后利用对内点的深度取平均来得到中心的深度从而根据公式算出那么为了将直接回归的方法和论文中的分步方法进行比较这里以论文网络架构为基础又编写了一套直接回归的算法以方便比较姿势细化当深度可用时我们可以通过网络估算出姿态我们使用迭代最近点算法来细化位姿具体地说我们采用具有投影数据关联和点平面残差项的每个像素的残差是中观测点到由中渲染点及其法线定义的平面的最小距离残差大于指定阈值的点被拒绝剩余残差使用梯度下降最小化利用网络中的语义标签对深度图像中的观察点进行裁剪由于对局部最小值不具有鲁棒性我们通过对网络中估计的位姿进行扰动来细化多个位姿然后利用中提出的对准度量来选择最优的细化位姿我们首先通过实验分析了在对称物体上两种损失函数对旋转回归的影响图为数据集木块和大夹子中两个对称对象使用这两个损失函数进行训练时的旋转损失直方图使用正确模型位姿和估计模型位姿两个位姿之间的对应点来测算平均平方距离测量的是估计模型方向上的每个点与模型上最近的点之间的偏移木块和大夹子的的旋转损失跨越了度到度这两个直方图表明网络被对称对象混淆了显然的损失直方图集中于木块的度以及大夹子的度和度因为它们围绕其坐标轴旋转度是对称的表和图对数据集中的所有个对象进行了详细的评估我们使用度量和度量来显示精度阈值曲线下的面积在这里我们改变平均距离的阈值然后计算姿态精度最大阈值设置为我们可以看到在仅使用彩色图像的情况下我们的网络在位姿估计方面明显优于结合算法的坐标回归网络当坐标回归结果存在误差时估计的位姿会偏离位姿很远而在我们的网络中即使物体被遮挡中心定位也有助于约束平移估计用来改进姿势可以显著提高表现与坐标回归网络相比基于的在使用深度图像时具有更好的性能的初始位姿是其收敛的关键为改进提供了更好的初始位姿我们可以看到有些物体更难处理比如金枪鱼罐头它很小而且质地不那么好这个网络也被大夹子和超大夹子弄混了因为它们有相同的外观坐标回归网络不能很好地处理对称对象如香蕉和碗图显示了数据集上的一些姿态估计结果我们可以看到即使中心被另一个物体遮挡中心的预测也是相当准确的我们的网络只有颜色已经能够提供良好的姿态估计随着的细化位姿的精度进一步提高数据集具有挑战性因为对象之间存在显著的遮挡我们首先只使用彩色图像进行实验图显示了数据集中个对象的重投影损失的精度阈值曲线其中我们使用彩色图像作为输入在该数据集上比较了和该达到了当前最先进的结果我们的方法明显优于特别是在重投影损失阈值较小的情况下这些结果表明即使在严重的遮挡下也能够正确地定位目标对象通过在中使用深度图像来优化姿势我们的方法也优于使用数据作为输入的最新方法表总结了数据集上的姿态估计精度最大的改进来自两个对称的对象和通过使用我们的进行训练能够正确估计出两个物体的位姿的对称性我们还在表三中展示了仅使用颜色的的结果由于这里的阈值通常小于所以精度要低得多当物体之间存在遮挡时基于颜色的方法很难在这么小的阈值内获得位姿图显示了数据集上的两个姿态估计结果示例在本文中我们引入了一种用于目标位姿估计的卷积神经网络解耦了旋转和平移的估计它通过定位目标中心和预测中心距离来估计平移量通过将每个像素向目标中心回归到一个单位向量可以独立于尺度稳健地估计目标中心更重要的是像素投票的对象中心即使它被其他对象遮挡旋转可通过回归到一个四元数表示来预测引入了两个新的损失函数用于旋转估计其中设计用于对称对象因此能够处理混乱场景中的遮挡和对称对象我们还介绍了一个用于目标姿态估计的大规模视频数据集我们的结果非常令人鼓舞因为他们表明在混乱的场景中仅使用视觉数据准确估计物体的姿态是可行的这为使用分辨率和视场远超目前使用的深度相机系统的相机打开了道路我们注意到有时会在位姿空间中产生类似于的局部最小值在未来的姿态估计中探索更有效地处理对称对象的方法将是一件有趣的事情',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-10-21 19:00:00',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/atom.xml" title="Karl的博客" type="application/atom+xml">
<link rel="alternate" href="/rss2.xml" title="Karl的博客" type="application/rss+xml">
</head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" href="https://blog.karltan.com/" title="Karl的博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/logo.png" alt="Karl的博客"/><span class="back-menu-item-text">Karl的博客</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://www.karltan.com/" title="Karl的导航"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://www.karltan.com/favicon.ico" alt="Karl的导航"/><span class="back-menu-item-text">Karl的导航</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">博客分流</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://v.karltan.com/" title="Vercel"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://assets.vercel.com/image/upload/front/favicon/vercel/favicon.ico" alt="Vercel"/><span class="back-menu-item-text">Vercel</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://c.karltan.com/" title="Cloudflare"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://dash.cloudflare.com/favicon-32x32.png" alt="Cloudflare"/><span class="back-menu-item-text">Cloudflare</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">工具</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://npmzjk.karltan.com/" title="NPM"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npmzjk.karltan.com/images/favicons/favicon-32x32.png" alt="NPM"/><span class="back-menu-item-text">NPM</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://paste.karltan.com/" title="Free-Bin"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://paste.karltan.com/static/favicon.ico" alt="Free-Bin"/><span class="back-menu-item-text">Free-Bin</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://tools.karltan.com/" title="IT-TOOLS"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://tools.karltan.com/android-chrome-192x192.png" alt="IT-TOOLS"/><span class="back-menu-item-text">IT-TOOLS</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://memos.karltan.com/" title="Memos"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://memos.karltan.com/logo.png" alt="Memos"/><span class="back-menu-item-text">Memos</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://wbo.karltan.com/" title="WBO"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://wbo.karltan.com/favicon.ico" alt="WBO"/><span class="back-menu-item-text">WBO</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://git.karltan.com/" title="Gitea"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://git.karltan.com/assets/img/logo.svg" alt="Gitea"/><span class="back-menu-item-text">Gitea</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">Karl的博客</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives"><i class="fa-solid fa-box-archive faa-tada"></i><span> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories"><i class="fa-solid fa-palette faa-tada"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags"><i class="fa-solid fa-tags faa-tada"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link"><i class="fa-solid fa-link faa-tada"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle"><i class="fa-brands fa-artstation faa-tada"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments"><i class="fa-solid fa-comments faa-tada"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/essay"><i class="fa-solid fa-comment-dots faa-tada"></i><span> 说说</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/music"><i class="fa-solid fa-music faa-tada"></i><span> 音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album"><i class="fa-solid fa-images faa-tada"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://status.karltan.com/status/1"><i class="fa-solid fa-chart-line faa-tada"></i><span> 网站监控</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="fa-solid fa-shoe-prints faa-tada"></i><span> 随便逛逛</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/about"><i class="fa-solid fa-heart faa-tada"></i><span> 关于我</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button only-home" id="travellings_button" title="随机前往一个开往项目网站"><a class="site-page" onclick="anzhiyu.totraveling()" title="随机前往一个开往项目网站" href="javascript:void(0);" rel="external nofollow" data-pjax-state="external"><i class="anzhiyufont anzhiyu-icon-train"></i></a></div><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" alt="微信" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/wechat.jpg"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" alt="支付宝" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/alipay.jpg"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/AcWing/" style="font-size: 1.05rem;">AcWing<sup>33</sup></a><a href="/tags/CUDA/" style="font-size: 1.05rem;">CUDA<sup>1</sup></a><a href="/tags/CVPR/" style="font-size: 1.05rem;">CVPR<sup>3</sup></a><a href="/tags/ICLR/" style="font-size: 1.05rem;">ICLR<sup>1</sup></a><a href="/tags/Linux/" style="font-size: 1.05rem;">Linux<sup>1</sup></a><a href="/tags/MAPL/" style="font-size: 1.05rem;">MAPL<sup>1</sup></a><a href="/tags/MySQL/" style="font-size: 1.05rem;">MySQL<sup>1</sup></a><a href="/tags/NIPS/" style="font-size: 1.05rem;">NIPS<sup>1</sup></a><a href="/tags/PyTorch/" style="font-size: 1.05rem;">PyTorch<sup>1</sup></a><a href="/tags/Python/" style="font-size: 1.05rem;">Python<sup>1</sup></a><a href="/tags/RSS/" style="font-size: 1.05rem;">RSS<sup>1</sup></a><a href="/tags/Ubuntu/" style="font-size: 1.05rem;">Ubuntu<sup>3</sup></a><a href="/tags/Windows/" style="font-size: 1.05rem;">Windows<sup>1</sup></a><a href="/tags/arXiv/" style="font-size: 1.05rem;">arXiv<sup>1</sup></a><a href="/tags/mAP/" style="font-size: 1.05rem;">mAP<sup>1</sup></a><a href="/tags/tmux/" style="font-size: 1.05rem;">tmux<sup>1</sup></a><a href="/tags/vim/" style="font-size: 1.05rem;">vim<sup>1</sup></a><a href="/tags/%E4%BB%A3%E7%A0%81/" style="font-size: 1.05rem;">代码<sup>1</sup></a><a href="/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/" style="font-size: 1.05rem;">位姿估计<sup>1</sup></a><a href="/tags/%E6%89%A9%E5%AE%B9/" style="font-size: 1.05rem;">扩容<sup>1</sup></a><a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 1.05rem;">数学<sup>1</sup></a><a href="/tags/%E6%97%8B%E8%BD%AC%E8%A1%A8%E7%A4%BA/" style="font-size: 1.05rem;">旋转表示<sup>1</sup></a><a href="/tags/%E6%9D%8E%E6%B2%90/" style="font-size: 1.05rem;">李沐<sup>76</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 1.05rem;">深度学习<sup>79</sup></a><a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 1.05rem;">目标检测<sup>1</sup></a><a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 1.05rem;">笔记<sup>77</sup></a><a href="/tags/%E7%AE%97%E6%B3%95%E9%A2%98/" style="font-size: 1.05rem;">算法题<sup>33</sup></a><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 1.05rem;">论文笔记<sup>9</sup></a><a href="/tags/%E9%98%BF%E9%87%8C%E4%BA%91/" style="font-size: 1.05rem;">阿里云<sup>1</sup></a><a href="/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/" style="font-size: 1.05rem;">高等数学<sup>1</sup></a><a href="/tags/%E9%AD%94%E6%B3%95/" style="font-size: 1.05rem;">魔法<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="anzhiyufont anzhiyu-icon-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">2024年04月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/03/"><span class="card-archive-list-date">2024年03月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/02/"><span class="card-archive-list-date">2024年02月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">2024年01月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">10</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">2023年12月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">48</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">2023年11月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">7</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">2023年10月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">32</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">2023年09月</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">6</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item on" id="consoleCommentBarrage" onclick="anzhiyu.switchCommentBarrage()" title="热评开关"><a class="commentBarrage"><i class="anzhiyufont anzhiyu-icon-message"></i></a></div><div class="console-btn-item" id="consoleKeyboard" onclick="anzhiyu.keyboardToggle()" title="快捷键开关"><a class="keyboard-switch"><i class="anzhiyufont anzhiyu-icon-keyboard"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url">论文笔记</a><i class="anzhiyufont anzhiyu-icon-angle-right post-meta-separator"></i><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2018/" itemprop="url">2018</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>论文笔记</span></a><a class="article-meta__tags" href="/tags/RSS/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>RSS</span></a></span></div></div><h1 class="post-title" itemprop="name headline">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2023-10-19T06:00:00.000Z" title="发表于 2023-10-19 14:00:00">2023-10-19</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2023-10-21T11:00:00.000Z" title="更新于 2023-10-21 19:00:00">2023-10-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">7.9k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span><span class="post-meta-separator"></span><span class="post-meta-commentcount"><i class="anzhiyufont anzhiyu-icon-comments post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/#post-comment" tabindex="-1"><span id="twikoo-count"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></a></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://img.karltan.com/covers/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN.png"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><div class="post-ai-description"><div class="ai-title"><i class="anzhiyufont anzhiyu-icon-bilibili"></i><div class="ai-title-text">AI-摘要</div><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i><i class="anzhiyufont anzhiyu-icon-circle-dot" title="朗读摘要"></i><div id="ai-tag">Tianli GPT</div></div><div class="ai-explanation">AI初始化中...</div><div class="ai-btn-box"><div class="ai-btn-item">介绍自己 🙈</div><div class="ai-btn-item">生成本文简介 👋</div><div class="ai-btn-item">推荐相关文章 📖</div><div class="ai-btn-item">前往主页 🏠</div><div class="ai-btn-item" id="go-tianli-blog">前往爱发电购买</div></div><script data-pjax src="/js/anzhiyu/ai_abstract.js"></script></div><article class="post-content" id="article-container" itemscope itemtype="http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/"><header><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url">论文笔记</a><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2018/" itemprop="url">2018</a><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" tabindex="-1" itemprop="url">论文笔记</a><a href="/tags/RSS/" tabindex="-1" itemprop="url">RSS</a><h1 id="CrawlerTitle" itemprop="name headline">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Karl</span><time itemprop="dateCreated datePublished" datetime="2023-10-19T06:00:00.000Z" title="发表于 2023-10-19 14:00:00">2023-10-19</time><time itemprop="dateCreated datePublished" datetime="2023-10-21T11:00:00.000Z" title="更新于 2023-10-21 19:00:00">2023-10-21</time></header><h1 id="PoseCNN-A-Convolutional-Neural-Network-for-6D-Object-Pose-Estimation-in-Cluttered-Scenes"><a href="#PoseCNN-A-Convolutional-Neural-Network-for-6D-Object-Pose-Estimation-in-Cluttered-Scenes" class="headerlink" title="PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes"></a>PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</h1><p>原文链接：<a href="https://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》 | Karl的博客 (karltan.com)</a></p>
<p>CSDN链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/karltan0328/article/details/134442631">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》-CSDN博客</a></p>
<p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00199">[1711.00199] PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes (arxiv.org)</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/yuxng/PoseCNN">yuxng/PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes (github.com)</a></p>
<p>项目链接：<a target="_blank" rel="noopener" href="https://rse-lab.cs.washington.edu/projects/posecnn/">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes – UW Robotics and State Estimation Lab (washington.edu)</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>对一个已知物体进行6D姿态估计对于机器人与现实世界的交互是非常重要的。</p>
<p>但是由于对象的多样性、对象的杂乱以及对象之间的遮挡引起的复杂性，这个问题是具有挑战性的。</p>
<p>所以这篇文章提出了PoseCNN，一种新的卷积神经网络。PoseCNN通过定位物体在图像中的中心并预测其到相机的距离来估计物体的3D平移。通过回归到四元数表示来估计物体的3D旋转。</p>
<p>然后这里提出了一种现代的损失函数，能够让PoseCNN有处理对称对象的能力。</p>
<h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>应用场景：</p>
<ol>
<li>机器人操作需要识别物体的3D位置和方向</li>
<li>机器人可以从演示中学习</li>
</ol>
<p>挑战：</p>
<ol>
<li>对象具有多样性</li>
<li>对象有不同的3D形状</li>
<li>对象在图像中的外观受光照、物体之间的杂乱和物体之间的相互遮挡所影响</li>
</ol>
<p>在传统方法中，6D姿态估计问题是通过3D模型与图像之间的特征点匹配来解决的[20, 25, 8]。然而，这要求物体具有丰富的纹理。这就导致了传统方法无法处理无纹理对象。</p>
<p>随着深度相机的出现，人们提出了几种使用RGB-D数据来识别无纹理物体的方法[13, 3, 2, 26, 15]。</p>
<p>但是对于基于模板的方法[13, 12]，遮挡显著降低了识别性能。</p>
<p>对于通过学习将图像像素回归到3D对象坐标以建立用于6D姿态估计的2D-3D对应关系的方法[3, 4]不能处理对称对象。</p>
<p>PoseCNN背后的一个关键思想是将姿态估计任务解耦为不同的组件，这使得网络能够显式地建模它们之间的依赖和独立。</p>
<p>PoseCNN执行三个相关任务，如图1所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019163316117.png" alt="image-20231019163316117"></p>
<ol>
<li><p>首先，它为输入图像中的每个像素预测一个对象标签，做语义分割。</p>
</li>
<li><p>其次，通过预测从每个像素到中心的单位向量来估计目标中心的2D像素坐标。</p>
<p> 使用语义分割标签，与对象相关联的图像像素会在图像中<strong>投票</strong>选出对象中心位置。</p>
<p> 此外，该网络还可以估算物体中心的距离。</p>
<p> 假设已知相机特性，估计2D对象中心和它的距离能够让我们还原它的3D平移$\mathbf{T}$。</p>
</li>
<li><p>最后，通过将bounding box内提取的卷积特征回归到$\mathbf{R}$的四元数表示来估计3D旋转$\mathbf{R}$。</p>
</li>
</ol>
<p>正如我们将展示的，先进行2D中心投票，然后进行旋转回归来估计$\mathbf{R}$和$\mathbf{T}$，可以应用于纹理/无纹理的物体。</p>
<p>并且因为网络被训练去给物体的中心投票，所以<strong>对遮挡是鲁棒的</strong>。</p>
<p>处理<strong>对称对象</strong>是姿态估计的另一个挑战，因为不同的对象方向可能产生相同的观察结果。例如，不可能唯一地估计红色碗（第三行第三个）或木块（第三行第四个）的方向，如图5所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019170722332.png" alt="image-20231019170722332"></p>
<p>虽然像OccludedLINEMOD数据集[17]这样的基准数据集考虑对这些对象进行特殊的对称评估，但在网络训练期间，对称性通常会被忽略。</p>
<p>然而，这可能会导致糟糕的训练性能，因为网络接收到不一致的损失信号，例如在目标方向上的高损失，即使从网络中估计的对象的对称性是正确的。</p>
<p>受此启发，我们引入了ShapeMatch-Loss，这是一个<strong>新的损失函数</strong>，专注于匹配对象的3D形状。</p>
<p>我们将证明这个损失函数对具有形状对称性的物体产生了更好的估计。</p>
<p>我们在OccludedLINEMOD数据集[17]上评估了我们的方法，这是一个用于6D姿态估计的基准数据集。在这个具有挑战性的数据集上，PoseCNN实现了纯颜色和RGB-D姿态估计的最新结果（我们在迭代最近点（ICP）算法中使用深度图像进行姿态优化）。</p>
<p>综上所述，我们的工作有以下主要贡献：</p>
<ul>
<li>我们提出了一种用于6D目标位姿估计的卷积神经网络，命名为PoseCNN。我们的网络实现端到端的6D姿态估计，并且对物体之间的遮挡非常鲁棒。</li>
<li>提出了一种用于对称目标位姿估计的训练损失函数ShapeMatch-Loss。</li>
<li>我们为6D对象的姿态估计提供了一个大尺度RGB-D视频数据集，其中我们为21个YCB对象提供了6D姿态标注。</li>
</ul>
<h2 id="II-RELATED-WORK"><a href="#II-RELATED-WORK" class="headerlink" title="II. RELATED WORK"></a>II. RELATED WORK</h2><p>文献中的6D目标位姿估计方法大致可以分为<strong>基于模板的方法</strong>和<strong>基于特征的方法</strong>。</p>
<p><strong>在基于模板的方法中</strong>，构造一个刚体模板，用于扫描输入图像中的不同位置。在每个位置，计算一个相似度得分，通过比较这些相似度得分得到最佳匹配[12, 13, 6]。在6D位姿估计中，通常通过渲染相应的3D模型来获得模板。最近，2D<strong>目标检测</strong>方法被用于模板匹配和6D姿态估计，特别是基于深度学习的目标检测器[28, 23, 16, 29]。基于模板的方法在检测无纹理对象方面很有用。但是，<strong>它们不能很好地处理对象之间的遮挡</strong>，因为如果对象被遮挡，模板的相似度分数会很低。</p>
<p><strong>在基于特征的方法中</strong>，从感兴趣的点或图像中的每个像素点提取局部特征，并与3D模型上的特征进行匹配，建立2D-3D对应关系，从而恢复6D姿态[20, 25, 30, 22]。基于特征的方法能够处理物体之间的遮挡。然而，为了计算局部特征，它们需要在对象上<strong>有足够的纹理</strong>。为了处理无纹理对象，提出了几种使用机器学习技术学习特征描述符的方法[32, 10]。已经有若干种对于每个像素直接回归到3D对象坐标来建立2D-3D对应关系的方法被提出了[3, 17, 4]。但是<strong>3D坐标回归在处理对称对象时遇到了歧义</strong>。</p>
<p>在这项工作中，我们在深度学习框架中结合了基于模板的方法和基于特征的方法的优点，其中网络结合了自底向上像素标记和自顶向下对象姿态回归。最近，由于亚马逊拾取挑战（APC）的竞争，6D物体的姿态估计问题得到了更多的关注。APC中针对特定的设置引入了几种数据集和方法[24, 35]。只要提供适当的训练数据，我们的网络有潜力应用于APC设置。</p>
<h2 id="III-POSECNN"><a href="#III-POSECNN" class="headerlink" title="III. POSECNN"></a>III. POSECNN</h2><p>给定一幅输入图像，6D目标位姿估计的任务是估计从目标坐标系$O$到摄像机坐标系$C$的刚性变换。我们假设对象的3D模型是可用的，并且对象的坐标系统定义在模型的3D空间中。这里的刚性变换由一个包含3D旋转R和3D平移T的SE(3)变换组成，其中$\mathbf{R}$表示围绕对象坐标系$O$的X轴、Y轴和Z轴的旋转角度，$\mathbf{T}$是摄像机坐标系$C$中$O$的原点的坐标。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019190812549.png" alt="image-20231019190812549"></p>
<p>在成像过程中，$\mathbf{T}$决定了物体在图像中的位置和比例，而$\mathbf{R}$根据物体的3D形状和纹理影响物体的图像外观。由于这两个参数有不同的视觉特性，我们提出了一个卷积神经网络结构，内部解耦$\mathbf{R}$和$\mathbf{T}$的估计。</p>
<h3 id="A-Overview-of-the-Network"><a href="#A-Overview-of-the-Network" class="headerlink" title="A. Overview of the Network"></a>A. Overview of the Network</h3><p>图2说明了我们用于6D目标姿态估计的网络结构：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019182446364.png" alt="image-20231019182446364"></p>
<p>网络包含<strong>两个阶段</strong>。</p>
<p><strong>第一阶段</strong>由13个卷积层和4个maxpooling层组成，从输入图像中提取不同分辨率的特征图。这个阶段是网络的主干，因为所提取的特征在网络执行的所有任务中共享。</p>
<p><strong>第二阶段</strong>包括一个嵌入步骤，将第一阶段生成的高维特征映射嵌入到低维、特定于任务的特征中。然后，网络执行6D姿态估计的三个不同任务，即语义标记、3D平移估计和3D旋转回归，如下所述。</p>
<h3 id="B-Semantic-Labeling"><a href="#B-Semantic-Labeling" class="headerlink" title="B. Semantic Labeling"></a>B. Semantic Labeling</h3><p>为了检测图像中的对象，我们采用语义标记，网络将每个图像像素分类成一类（这个像素如果属于罐头，那么这个像素就被分类为罐头类）。</p>
<p>与最近借助于bounding boxes目标检测的6D位姿估计方法[23, 16, 29]相比，语义分割标记能够提供更丰富的对象信息，并且能够更好的处理遮挡。</p>
<p>语义标注分支的嵌入步骤如图2所示，以特征提取阶段生成的两个通道尺寸为512的特征映射作为输入。两种特征图的分辨率分别为原始图像尺寸的$\frac{1}{8}$和$\frac{1}{16}$。该网络首先使用两个卷积层将两个特征映射的信道维数降至64。然后用反卷积层将$\frac{1}{16}$特征图的分辨率提高一倍。然后将两个feature map求和，再用另一个反卷积层将分辨率提高8倍，得到与原始图像大小一致的feature map。最后，卷积层对特征图进行操作，生成像素的语义标记分数。过程如下图：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019190153685.png" alt="image-20231019190153685"></p>
<p>这一层的输出有$n$个通道和$n$个语义类的数量。在训练中，采用softmax交叉熵损失训练语义标注分支。在测试过程中，使用softmax函数计算像素的类概率。语义标注分支的设计灵感来自于[19]中用于语义标注的全卷积网络。</p>
<h3 id="C-3D-Translation-Estimation"><a href="#C-3D-Translation-Estimation" class="headerlink" title="C. 3D Translation Estimation"></a>C. 3D Translation Estimation</h3><p>3D平移$\mathbf{T} = (T_x, T_y, T_z)^\mathrm{T}$是物体原点$O$在摄像机坐标系$C$中的坐标，如图3所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019190812549.png" alt="image-20231019190812549"></p>
<p>估计$\mathbf{T}$的一种简单方法是直接将图像特征回归到$\mathbf{T}$。但是，由于对象可以出现在图像中的任何位置，所以这种方法是不可泛化的（不能将图像的位置和物体的类型进行关联，这是不对的）。此外，它不能处理同一类别中的多个对象实例。</p>
<p>因此，我们提出通过定位图像中的2D目标中心，估计目标到摄像机的距离来估计3D平移量。</p>
<p>假设$\mathbf{T}$在图像上的投影为$\mathbf{c} = (c_x, c_y)^\mathrm{T}$。对于针孔摄像机来说，如果网络能够在图像中定位$\mathbf{c}$并估计出深度$T_z$，那么我们可以根据下面的投影方程恢复$T_x$和$T_y$：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
c_x \\
c_y
\end{bmatrix}
=
\begin{bmatrix}
f_x\frac{T_x}{T_z} + p_x \\
f_y\frac{T_y}{T_z} + p_y
\end{bmatrix}</script><p>其中$f_x$和$f_y$表示相机的焦距，$(p_x, p_y)^\mathrm{T}$是主点（相机原点）。</p>
<blockquote>
<p>针孔相机模型：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231021153356851.png" alt="image-20231021153356851"></p>
<p>下图将成像平面平移到了小孔处：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231021142854059.png" alt="针孔相机模型"></p>
<p>那么显然，下图中的$\bigtriangleup P^\prime OB$与$\bigtriangleup POA$是相似的：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231021191609785.png" alt="image-20231021191609785"></p>
<p>假设图中$P$点的坐标为$(T_x, T_y, T_z)$，$P^\prime$点的坐标为$(c_x, c_y)$，那么有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{c_x}{f} &= \frac{T_x}{T_z} \\
\frac{c_y}{f} &= \frac{T_y}{T_z}
\end{aligned}</script><p>那么可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
c_x &= f\frac{T_x}{T_z} \\
c_y &= f\frac{T_y}{T_z}
\end{aligned}</script><p>又小孔成像模型的原点在图像的中心位置，数字图像处理的原点在图像的左上角，所以需要一个偏移量$(p_x, p_y)$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
c_x &= f\frac{T_x}{T_z} + p_x \\
c_y &= f\frac{T_y}{T_z} + p_y
\end{aligned}</script><p>即：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
c_x \\
c_y
\end{bmatrix}
=
\begin{bmatrix}
f_x\frac{T_x}{T_z} + p_x \\
f_y\frac{T_y}{T_z} + p_y
\end{bmatrix}</script></blockquote>
<p>如果物体原点$O$是物体的质心，我们称$\mathbf{c}$为物体的2D中心（即物体在图像上的位置）。</p>
<p>一种直接定位2D目标<strong>中心</strong>（注意这里说的是中心，需要与后面的内点做区分）的方法是像现有的关键点检测方法一样直接检测中心点[22, 7]。然而，如果对象中心被遮挡，这些方法将不起作用。</p>
<p>在隐式形状模型（Implicit Shape Model, ISM）中，为了进行检测，图像块会对物体中心进行投票，受该模型的启发，我们设计了我们的网络以回归预测出图像中每个像素的中心<em>方向</em>。</p>
<p>具体来说，对于图像上的像素$\mathbf{p} = (x, y)^\mathrm{T}$，它回归到三个变量（这里顺便回归出了内点的深度，因为遮挡，不能直接回归物体中心的深度）：</p>
<script type="math/tex; mode=display">
(x, y) \to \left(n_x = \frac{c_x - x}{\Vert\mathbf{c} - \mathbf{p}\Vert}, n_y = \frac{c_y - y}{\Vert\mathbf{c} - \mathbf{p}\Vert}, T_z\right)</script><p>注意，我们没有直接回归到位移矢量$\mathbf{c} - \mathbf{p}$，而是将网络设计成回归到单位长度矢量$\mathbf{n} = (n_x, n_y)^\mathrm{T} = \frac{\mathbf{c} - \mathbf{p}}{\Vert\mathbf{c} - \mathbf{p}\Vert}$，即2D中心方向，它是尺度不变的，因此更容易训练（正如我们通过实验验证的那样）。</p>
<p>我们网络的中心回归分支（图2）与语义标记分支采用相同的架构，只是卷积层和反卷积层的通道维数不同。我们将高维特征嵌入到128维空间而不是64维空间，因为这个分支需要为每个对象类回归到三个变量。该分支的最后一个卷积层的通道维数为$3 \times n$，对象类数为$n$。在训练中，使用平滑的L1损失函数进行回归，如[11]。</p>
<p>为了找到一个对象的2D对象中心$\mathbf{c}$，设计了一个Hough投票层并集成到网络中。Hough投票层采用像素级语义标记结果和中心回归结果作为输入。对于每个对象，它首先计算图像中每个位置的投票分数。投票分数表明该位置是对象中心的可能性有多大。具体地说，对象中的每一个像素都会在网络预测出的方向上投票，见图4：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019195938437.png" alt="image-20231019195938437"></p>
<p>在处理对象中的所有像素后，我们获得所有图像位置的投票分数。然后得分最高的位置就作为目标中心。</p>
<p>对于同一对象可能多次出现在图像中的情况，我们对投票分数应用非极大值抑制，然后选择分数大于某一阈值的位置。</p>
<p>在生成一组对象中心后，我们认为投票给对象中心的像素是中心的<strong>内层</strong>。然后将中心$T_z$的预测深度简单地计算为内层预测深度的平均值。最后，利用公式$\begin{bmatrix}c_x \\ c_y\end{bmatrix} = \begin{bmatrix} f_x\frac{T_x}{T_z} + p_x \\ f_y\frac{T_y}{T_z} + p_y\end{bmatrix}$，我们可以估计出3D平移$\mathbf{T}$。</p>
<p>此外，网络生成对象的bounding box作为约束所有<strong>内层</strong>的2D矩形，该bounding box用于3D旋转回归。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019182446364.png" alt="image-20231019182446364"></p>
<h3 id="D-3D-Rotation-Regression"><a href="#D-3D-Rotation-Regression" class="headerlink" title="D. 3D Rotation Regression"></a>D. 3D Rotation Regression</h3><p>图2的最下方为3D旋转回归分支。</p>
<p>利用Hough投票层预测的对象的bounding boxes，我们可以用两个RoI池化层[11]对网络第一阶段生成的视觉特征进行“裁剪和池化”，用于3D旋转回归。合并的特征映射被加在一起，并馈送到三个全连接（FC）层中。前两个FC层的维数为4096，最后一个FC层的维数为$4 \times n$，对象类数为$n$。对于每个类，最后一个FC层输出一个四元数表示的3D旋转。</p>
<p>为了训练四元数回归，我们提出了两个损失函数，其中一个是专门设计用于处理对称对象。</p>
<p>第一个损失函数称为PoseLoss（PLOSS），它在3D模型空间中运行，并使用正确模型位姿和估计模型位姿两个位姿之间的对应点来测算平均平方距离。</p>
<p>PLOSS定义为：</p>
<script type="math/tex; mode=display">
\text{PLOSS}(\tilde{\mathbf{q}}, \mathbf{q}) = \frac{1}{2m}\sum_{\mathbf{x} \in \mathcal{M}}\Vert R(\tilde{\mathbf{q}})\mathbf{x} - R(\mathbf{q})\mathbf{x}\Vert^2</script><p>$\mathcal{M}$为3D模型点集，$m$为点个数。$R(\tilde{\mathbf{q}})$和$R(\mathbf{q})$分别表示由估计的四元数和GT四元数计算得到的旋转矩阵。当估计方向与GT方向相同时，该损失有其唯一的最小值。</p>
<p>不幸的是，PLOSS不能适当地处理对称对象，因为一个对称对象可以有多个正确的3D旋转。在对称物体上使用这样的损失函数会对网络进行不必要的惩罚，因为它会回归到一个可选的3D旋转中，因此可能会给出不一致的训练信号。</p>
<p>虽然可以通过手动指定对象对称性，然后考虑所有正确的方向作为GT选项来修改PLOSS，进而处理对称对象，但我们在这里仍然要引入ShapeMatch-Loss（SLOSS），这是一个不需要指定对称性的损失函数。</p>
<p>SLOSS定义为：</p>
<script type="math/tex; mode=display">
\text{SLOSS}(\tilde{\mathbf{q}}, \mathbf{q}) = \frac{1}{2m}\sum_{\mathbf{x}_1 \in \mathcal{M}}\min_{\mathbf{x}_2 \in \mathcal{M}}\Vert R(\tilde{\mathbf{q}})\mathbf{x}_1 - R(\mathbf{q})\mathbf{x}_2\Vert^2</script><p>我们可以看到，就像ICP一样，这个损失测量的是估计模型方向上的每个点与GT模型上最近的点之间的偏移。当两个3D模型匹配时，SLOSS最小。</p>
<p>通过这种方式，SLOSS将不会惩罚相对于对象的3D形状对称等效的旋转。</p>
<h2 id="IV-THE-YCB-VIDEO-DATASET"><a href="#IV-THE-YCB-VIDEO-DATASET" class="headerlink" title="IV. THE YCB-VIDEO DATASET"></a>IV. THE YCB-VIDEO DATASET</h2><p>因为标签是手动添加的，所以以对象为中心的为对象位姿和/或分割提供GT的数据集不会太大。例如，流行的LINEMOD数据集[13]为数据集中的15个对象中的每个对象提供了大约1000张图像的手动注释。虽然这样的数据集对基于模型的姿态估计技术的评估是有用的，但它比训练最先进的深度神经网络的典型数据集要小几个数量级。这个问题的一个解决方案是用合成图像来扩充数据。但是，必须注意确保在真实场景和渲染场景之间的性能是通用的。</p>
<h3 id="A-6D-Pose-Annotation"><a href="#A-6D-Pose-Annotation" class="headerlink" title="A. 6D Pose Annotation"></a>A. 6D Pose Annotation</h3><p>为了避免手动注释所有的视频帧，我们只在每个视频的第一帧手动指定对象的姿势。我们在第一个深度帧中细化每个对象的姿态，使用每个对象的符号距离函数（Signed Distance Function, SDF）表示。接下来，通过固定物体相对于其他物体的姿态并通过深度视频跟踪物体的配置来初始化摄像机的轨迹。最后，对摄像机轨迹和相对目标位姿进行全局优化。</p>
<h3 id="B-Dataset-Characteristics"><a href="#B-Dataset-Characteristics" class="headerlink" title="B. Dataset Characteristics"></a>B. Dataset Characteristics</h3><p>我们使用的对象是如图5所示的21个YCB对象[5]的一个子集：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019170722332.png" alt="image-20231019170722332"></p>
<p>选择这些对象是因为高质量的3D模型和良好的深度可见性。视频采集使用华硕Xtion Pro Live RGB-D相机，采用快速裁剪模式，通过在本地设备上捕获1280x960图像，并通过USB只传输中心区域，以30帧/秒的速度提供640x480分辨率的RGB图像。这以较低的视场为代价获得了更高的RGB图像的有效分辨率，但考虑到深度传感器的最小范围，这是一个可以接受的折中。</p>
<p>完整的数据集包含133,827幅图像，比LINEMOD数据集大两个数量级。关于数据集的更多统计信息请参见表1：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019210710064.png" alt="image-20231019210710064"></p>
<p>图6为我们数据集中的一个标注示例：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231019210728586.png" alt="image-20231019210728586"></p>
<p>我们根据标注的GT位姿来渲染3D模型。</p>
<p>请注意，我们的标注精度受到几个误差源的影响，包括RGB传感器的滚动快门、对象模型中的不准确性、RGB和深度传感器之间的轻微异步以及相机内外参数的不确定性。</p>
<h2 id="V-EXPERIMENTS"><a href="#V-EXPERIMENTS" class="headerlink" title="V. EXPERIMENTS"></a>V. EXPERIMENTS</h2><h3 id="A-Datasets"><a href="#A-Datasets" class="headerlink" title="A. Datasets"></a>A. Datasets</h3><p>在我们的YCB-Video数据集中，我们使用80个视频进行训练，并从剩下的12个测试视频中提取2949个关键帧进行测试。我们还在OccludedLINEMOD数据集[17]上评估了我们的方法。[17]的作者从原始的LINEMOD数据集[13]中选择了一个带有1214帧的视频，并在该视频中标注了8个物体的GT姿势：Ape、Can、Cat、Driller、Duck、Eggbox、Glue和Holepuncher。在这个视频序列中，物体之间有明显的遮挡，这使得这个数据集具有挑战性。为了进行训练，我们使用与这八个对象对应的原始LINEMOD数据集中的八个序列。此外，我们通过在场景中随机放置物体，在两个数据集上生成80,000张合成图像进行训练。</p>
<h3 id="B-Evaluation-Metrics"><a href="#B-Evaluation-Metrics" class="headerlink" title="B. Evaluation Metrics"></a>B. Evaluation Metrics</h3><p>我们采用[13]中提出的平均距离（ADD）度量进行评估。给定GT旋转$\mathbf{R}$和平移$\mathbf{T}$以及估计的旋转$\tilde{\mathbf{R}}$和平移$\tilde{\mathbf{T}}$，平均距离计算根据GT姿态和估计姿态变换的3D模型点之间成对距离的平均值：</p>
<script type="math/tex; mode=display">
\text{ADD} = \frac{1}{m}\sum_{\mathbf{x} \in \mathcal{M}}\Vert(\mathbf{Rx} + \mathbf{T}) - (\tilde{\mathbf{R}}\mathbf{x} + \tilde{\mathbf{T}})\Vert</script><p>式中，$\mathcal{M}$为3D模型点集，$m$为点个数。如果平均距离小于预定义的阈值，则认为6D姿势是正确的。</p>
<p>在OccludedLINEMOD数据集中，阈值设置为3D模型直径的10%。</p>
<p>对于像Eggbox和Glue这样的对称对象，对于某些视图，点之间的匹配是不明确的。因此，利用最近点距离计算平均距离：</p>
<script type="math/tex; mode=display">
\text{ADD-S} = \frac{1}{m}\sum_{\mathbf{x}_1 \in \mathcal{M}}\min_{\mathbf{x}_2 \in \mathcal{M}}\Vert(\mathbf{R}\mathbf{x}_1 + \mathbf{T}) - (\tilde{\mathbf{R}}\mathbf{x}_2 + \tilde{\mathbf{T}})\Vert</script><p>我们为旋转回归设计的损失函数受这两个评价指标所启发。并且在计算位姿精度时使用固定的阈值不能揭示方法在这些不正确的位姿上的表现。因此，我们在评价时改变距离阈值。在这种情况下，我们可以绘制一条精度阈值曲线，并计算曲线下的面积来进行姿态评估。</p>
<p>我们可以将变换后的点投影到图像上，然后在图像空间中计算成对的距离，而不是在3D空间中计算距离。这个度量被称为重投影损失，广泛用于仅使用彩色图像时的6D姿态估计。</p>
<h3 id="C-Implementation-Details"><a href="#C-Implementation-Details" class="headerlink" title="C. Implementation Details"></a>C. Implementation Details</h3><p>PoseCNN是使用TensorFlow库[1]实现的。和[31]一样，Hough投票层是在GPU上实现的。在训练中，使用ImageNet[9]上训练的VGG16网络[27]对特征提取阶段的前13个卷积层和3D旋转回归分支的前两个FC层的参数进行初始化。没有梯度通过Hough投票层反向传播。采用带动量的随机梯度下降法（SGD）进行训练。</p>
<h3 id="D-Baselines"><a href="#D-Baselines" class="headerlink" title="D. Baselines"></a>D. Baselines</h3><p><strong>3D对象坐标回归网络。</strong>由于目前最先进的6D姿态估计方法大多依赖于将图像像素回归到3D物体坐标[3, 4, 21]，因此我们实现了一种用于3D物体坐标回归的网络变体以进行比较。在这个网络中，我们不是像图2那样回归到中心的方向和深度，而是回归到每个像素在对象坐标系中的3D坐标。我们可以使用相同的架构，因为对于每个类，每个像素仍然回归到三个变量。然后我们删除3D旋转回归分支。利用语义标记结果和3D物体坐标回归结果，利用pre-emptive RANSAC恢复6D姿态，如[4]所示。</p>
<p>意思是现有的方法是直接将2D坐标回归到3D坐标，从而得到对象的3D坐标。但是论文中的方法是先通过语义分割估计物体在图像中的中心坐标，然后利用对内点的深度取平均来得到中心的深度，从而根据公式算出$\mathbf{T}$。那么为了将直接回归的方法和论文中的分步方法进行比较，这里以论文网络架构为基础，又编写了一套直接回归的算法以方便比较。</p>
<p><strong>姿势细化。</strong>当深度可用时，我们可以通过网络估算出6D姿态。我们使用迭代最近点（Iterative Closest Point, ICP）算法来细化6D位姿。具体地说，我们采用具有投影数据关联和点平面残差项的ICP。每个像素的残差是3D中观测点到由3D中渲染点及其法线定义的平面的最小距离。残差大于指定阈值的点被拒绝，剩余残差使用梯度下降最小化。利用网络中的语义标签对深度图像中的观察点进行裁剪。由于ICP对局部最小值不具有鲁棒性，我们通过对网络中估计的位姿进行扰动来细化多个位姿，然后利用[33]中提出的对准度量来选择最优的细化位姿。</p>
<h3 id="E-Analysis-on-the-Rotation-Regress-Losses"><a href="#E-Analysis-on-the-Rotation-Regress-Losses" class="headerlink" title="E. Analysis on the Rotation Regress Losses"></a>E. Analysis on the Rotation Regress Losses</h3><p>我们首先通过实验分析了在对称物体上两种损失函数对旋转回归的影响。图7为YCB-Video数据集（木块和大夹子）中两个对称对象使用这两个损失函数进行训练时的旋转损失直方图（PLoss使用正确模型位姿和估计模型位姿两个位姿之间的对应点来测算平均平方距离，SLoss测量的是估计模型方向上的每个点与GT模型上最近的点之间的偏移）：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020214447284.png" alt="image-20231020214447284"></p>
<p>木块和大夹子的PLOSS的旋转损失跨越了0度到180度。这两个直方图表明网络被对称对象混淆了。显然，SLOSS的损失直方图集中于木块的180度以及大夹子的0度和180度，因为它们围绕其坐标轴旋转180度是对称的。</p>
<h3 id="F-Results-on-the-YCB-Video-Dataset"><a href="#F-Results-on-the-YCB-Video-Dataset" class="headerlink" title="F. Results on the YCB-Video Dataset"></a>F. Results on the YCB-Video Dataset</h3><p>表II和图8(a)对YCB-Video数据集中的所有21个对象进行了详细的评估：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020215204360.png" alt="image-20231020215204360"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020215241179.png" alt="image-20231020215241179"></p>
<p>我们使用ADD度量和ADD-S度量来显示精度阈值曲线下的面积，在这里，我们改变平均距离的阈值，然后计算姿态精度。最大阈值设置为10cm。</p>
<p>我们可以看到：</p>
<ol>
<li>在仅使用彩色图像的情况下，我们的网络在6D位姿估计方面明显优于结合pre-emptive RANSAC算法的3D坐标回归网络。当3D坐标回归结果存在误差时，估计的6D位姿会偏离GT位姿很远。而在我们的网络中，即使物体被遮挡，中心定位也有助于约束3D平移估计。</li>
<li>用ICP来改进姿势，可以显著提高表现。与3D坐标回归网络相比，基于ICP的PoseCNN在使用深度图像时具有更好的性能。ICP的初始位姿是其收敛的关键。PoseCNN为ICP改进提供了更好的初始6D位姿。</li>
<li>我们可以看到，有些物体更难处理，比如金枪鱼罐头，它很小，而且质地不那么好。这个网络也被大夹子和超大夹子弄混了，因为它们有相同的外观。3D坐标回归网络不能很好地处理对称对象，如香蕉和碗。</li>
</ol>
<p>图9显示了YCB-Video数据集上的一些6D姿态估计结果：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020215756663.png" alt="image-20231020215756663"></p>
<p>我们可以看到，即使中心被另一个物体遮挡，中心的预测也是相当准确的。我们的网络只有颜色，已经能够提供良好的6D姿态估计。随着ICP的细化，6D位姿的精度进一步提高。</p>
<h3 id="G-Results-on-the-OccludedLINEMOD-Dataset"><a href="#G-Results-on-the-OccludedLINEMOD-Dataset" class="headerlink" title="G. Results on the OccludedLINEMOD Dataset"></a>G. Results on the OccludedLINEMOD Dataset</h3><p>OccludedLINEMOD数据集具有挑战性，因为对象之间存在显著的遮挡。我们首先只使用彩色图像进行实验。图8(b)显示了数据集中7个对象的重投影损失的精度阈值曲线，其中我们使用彩色图像作为输入，在该数据集上比较了PoseCNN和[29]，该[29]达到了当前最先进的结果：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020220102175.png" alt="image-20231020220102175"></p>
<p>我们的方法明显优于[29]，特别是在重投影损失阈值较小的情况下。这些结果表明，即使在严重的遮挡下，PoseCNN也能够正确地定位目标对象。</p>
<p>通过在ICP中使用深度图像来优化姿势，我们的方法也优于使用RGB-D数据作为输入的最新方法。表III总结了OccludedLINEMOD数据集上的姿态估计精度：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/image-20231020220226331.png" alt="image-20231020220226331"></p>
<p>最大的改进来自两个对称的对象“Eggbox”和“Glue”。通过使用我们的ShapeMatch-Loss进行训练，PoseCNN能够正确估计出两个物体的6D位姿的对称性。我们还在表三中展示了仅使用颜色的PoseCNN的结果。由于这里的阈值通常小于2cm，所以精度要低得多。当物体之间存在遮挡时，基于颜色的方法很难在这么小的阈值内获得6D位姿。图9显示了OccludedLINEMOD数据集上的两个6D姿态估计结果示例。</p>
<h2 id="VI-CONCLUSIONS"><a href="#VI-CONCLUSIONS" class="headerlink" title="VI. CONCLUSIONS"></a>VI. CONCLUSIONS</h2><p>在本文中，我们引入了一种用于6D目标位姿估计的卷积神经网络PoseCNN。PoseCNN解耦了3D旋转和3D平移的估计。它通过定位目标中心和预测中心距离来估计3D平移量。通过将每个像素向目标中心回归到一个单位向量，可以独立于尺度稳健地估计目标中心。更重要的是，像素投票的对象中心，即使它被其他对象遮挡。3D旋转可通过回归到一个四元数表示来预测。引入了两个新的损失函数用于旋转估计，其中ShapeMatch-Loss设计用于对称对象。因此，PoseCNN能够处理混乱场景中的遮挡和对称对象。我们还介绍了一个用于6D目标姿态估计的大规模视频数据集。我们的结果非常令人鼓舞，因为他们表明，在混乱的场景中，仅使用视觉数据准确估计物体的6D姿态是可行的。这为使用分辨率和视场远超目前使用的深度相机系统的相机打开了道路。我们注意到，SLOSS有时会在位姿空间中产生类似于ICP的局部最小值。在未来的6D姿态估计中，探索更有效地处理对称对象的方法将是一件有趣的事情。</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/profile.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/profile.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Karl</div><div class="post-copyright__author_desc">日拱一卒，功不唐捐</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/')">论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=论文笔记《PoseCNN：A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes》&amp;url=http://blog.karltan.com/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN/&amp;pic=https://img.karltan.com/covers/dissertation-notes/2018/Xiang_et_al-2018-PoseCNN.png" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://blog.karltan.com" target="_blank">Karl的博客</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__category-list"><a class="post-meta__box__categoryes" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="categoryes-punctuation"> <i class="anzhiyufont anzhiyu-icon-inbox"></i></span>论文笔记<span class="categoryesPageCount">9</span></a><a class="post-meta__box__categoryes" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2018/"><span class="categoryes-punctuation"> <i class="anzhiyufont anzhiyu-icon-inbox"></i></span>2018<span class="categoryesPageCount">1</span></a></div><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>论文笔记<span class="tagsPageCount">9</span></a><a class="post-meta__box__tags" href="/tags/RSS/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>RSS<span class="tagsPageCount">1</span></a></div></div><div class="post_share"><div class="social-share" data-image="https://img.karltan.com/covers/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/notes-out-class/d2l/19/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/notes-out-class/d2l/19.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">动手学深度学习v2 19 卷积层</div></div></a></div><div class="next-post pull-right"><a href="/notes-out-class/d2l/20/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/notes-out-class/d2l/20.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">动手学深度学习v2 20 卷积层里的填充和步幅</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/dissertation-notes/dissertation-summary/" title="论文总结"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/dissertation-summary.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-10-28</div><div class="title">论文总结</div></div></a></div><div><a href="/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/" title="论文笔记《Attention Is All You Need》"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-02-09</div><div class="title">论文笔记《Attention Is All You Need》</div></div></a></div><div><a href="/dissertation-notes/2019/Tillet_et_al-2019-Triton/" title="论文笔记《Triton：An Intermediate Language and Compiler for Tiled Neural Network Computations》"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2019/Tillet_et_al-2019-Triton.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-12-26</div><div class="title">论文笔记《Triton：An Intermediate Language and Compiler for Tiled Neural Network Computations》</div></div></a></div><div><a href="/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation/" title="论文笔记《Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation》"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2019/Wang_et_al-2019-Normalized_Object_Coordinate_Space_for_Category-Level_6D_Object_Pose_and_Size_Estimation.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-10-25</div><div class="title">论文笔记《Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation》</div></div></a></div><div><a href="/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/" title="论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2024-04-15</div><div class="title">论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》</div></div></a></div><div><a href="/dissertation-notes/2023/Fan_et_al-2023-POPE/" title="论文笔记《POPE：6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference》"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2023/Fan_et_al-2023-POPE.png" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2023-11-15</div><div class="title">论文笔记《POPE：6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference》</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> 评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div><div class="comment-tips" id="comment-tips"><span>✅ 你无需删除空行，直接评论以获取最佳展示效果</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/profile.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/chuoyichuo.gif" alt="status"/></div></div><div class="author-info__description"><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">直博狗一枚，日常会在博客上分享自己的学习笔记，希望可以帮到你。</div><div style="line-height:1.38;margin:0.6rem 0;text-align:justify;color:rgba(255, 255, 255, 0.8);">相信你可以在这里找到对你有用的<b style="color:#fff">知识</b>和<b style="color:#fff">教程</b>。</div></div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">Karl</h1><div class="author-info__desc">日拱一卒，功不唐捐</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/karltan0328" target="_blank" title="Github"><i class="fa-brands fa-github faa-tada"></i></a><a class="social-icon faa-parent animated-hover" href="mailto:admin@karltan.com" target="_blank" title="Email"><i class="fa-solid fa-envelope faa-tada"></i></a></div></div></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bullhorn anzhiyu-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来看我的博客鸭~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#PoseCNN-A-Convolutional-Neural-Network-for-6D-Object-Pose-Estimation-in-Cluttered-Scenes"><span class="toc-text">PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in Cluttered Scenes</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#I-INTRODUCTION"><span class="toc-text">I. INTRODUCTION</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#II-RELATED-WORK"><span class="toc-text">II. RELATED WORK</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#III-POSECNN"><span class="toc-text">III. POSECNN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Overview-of-the-Network"><span class="toc-text">A. Overview of the Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Semantic-Labeling"><span class="toc-text">B. Semantic Labeling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-3D-Translation-Estimation"><span class="toc-text">C. 3D Translation Estimation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#D-3D-Rotation-Regression"><span class="toc-text">D. 3D Rotation Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IV-THE-YCB-VIDEO-DATASET"><span class="toc-text">IV. THE YCB-VIDEO DATASET</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-6D-Pose-Annotation"><span class="toc-text">A. 6D Pose Annotation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Dataset-Characteristics"><span class="toc-text">B. Dataset Characteristics</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#V-EXPERIMENTS"><span class="toc-text">V. EXPERIMENTS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Datasets"><span class="toc-text">A. Datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Evaluation-Metrics"><span class="toc-text">B. Evaluation Metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C-Implementation-Details"><span class="toc-text">C. Implementation Details</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#D-Baselines"><span class="toc-text">D. Baselines</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#E-Analysis-on-the-Rotation-Regress-Losses"><span class="toc-text">E. Analysis on the Rotation Regress Losses</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#F-Results-on-the-YCB-Video-Dataset"><span class="toc-text">F. Results on the YCB-Video Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#G-Results-on-the-OccludedLINEMOD-Dataset"><span class="toc-text">G. Results on the OccludedLINEMOD Dataset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VI-CONCLUSIONS"><span class="toc-text">VI. CONCLUSIONS</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/" title="论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》"/></a><div class="content"><a class="title" href="/dissertation-notes/2021/Dosovitskiy_et_al-2021-An_Image_is_Worth_16x16_Words/" title="论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》">论文笔记《AN IMAGE IS WORTH 16X16 WORDS：TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE》</a><time datetime="2024-04-15T07:00:00.000Z" title="发表于 2024-04-15 15:00:00">2024-04-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/" title="论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》"/></a><div class="content"><a class="title" href="/dissertation-notes/2023/Woo_et_al-2023-ConvNeXt_V2/" title="论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》">论文笔记《ConvNeXt V2：Co-designing and Scaling ConvNets with Masked Autoencoders》</a><time datetime="2024-04-04T05:00:00.000Z" title="发表于 2024-04-04 13:00:00">2024-04-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/" title="论文笔记《NOPE：Novel Object Pose Estimation from a Single Image》"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2023/Nguyen_et_al-2023-NOPE.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文笔记《NOPE：Novel Object Pose Estimation from a Single Image》"/></a><div class="content"><a class="title" href="/dissertation-notes/2023/Nguyen_et_al-2023-NOPE/" title="论文笔记《NOPE：Novel Object Pose Estimation from a Single Image》">论文笔记《NOPE：Novel Object Pose Estimation from a Single Image》</a><time datetime="2024-03-28T04:00:00.000Z" title="发表于 2024-03-28 12:00:00">2024-03-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/" title="论文笔记《Attention Is All You Need》"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文笔记《Attention Is All You Need》"/></a><div class="content"><a class="title" href="/dissertation-notes/2017/Vaswani_et_al-2017-Attention_is_all_you_need/" title="论文笔记《Attention Is All You Need》">论文笔记《Attention Is All You Need》</a><time datetime="2024-02-09T02:00:00.000Z" title="发表于 2024-02-09 10:00:00">2024-02-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/ac-diary/acwing/autumn-daily-question-2023/acwing5199/" title="AcWing 5199. 现代艺术"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://img.karltan.com/covers/ac-diary/acwing/autumn-daily-question-2023.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="AcWing 5199. 现代艺术"/></a><div class="content"><a class="title" href="/ac-diary/acwing/autumn-daily-question-2023/acwing5199/" title="AcWing 5199. 现代艺术">AcWing 5199. 现代艺术</a><time datetime="2024-01-18T08:00:00.000Z" title="发表于 2024-01-18 16:00:00">2024-01-18</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="mailto:admin@karltan.com" title="email"><i class="fa-solid fa-envelope"></i></a><a class="deal_link" href="/atom.xml" title="RSS"><i class="fa-solid fa-rss"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/profile.jpg" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/karltan0328" title="Github"><i class="fa-brands fa-github"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://blog.csdn.net/karltan0328" title="CSDN"><i class="fa-solid fa-c"></i></a></div><div id="workboard"><img class="workSituationImg boardsign" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/label/work_img.svg" alt="paper快来，idea快来，我要毕业🥹" title="paper快来，idea快来，我要毕业🥹"/><div id="runtimeTextTip"></div></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="51la统计" target="_blank" rel="noopener" href="https://v6.51.la/">51la统计</a><a class="footer-item" title="十年之约" target="_blank" rel="noopener" href="https://www.foreverblog.cn/">十年之约</a><a class="footer-item" title="开往" target="_blank" rel="noopener" href="https://github.com/travellings-link/travellings">开往</a></div></div><div class="footer-group"><div class="footer-title">主题</div><div class="footer-links"><a class="footer-item" title="文档" target="_blank" rel="noopener" href="https://docs.anheyu.com/">文档</a><a class="footer-item" title="源码" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu">源码</a><a class="footer-item" title="更新日志" target="_blank" rel="noopener" href="https://blog.anheyu.com/update/">更新日志</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="即刻短文" href="/essay">即刻短文</a><a class="footer-item" title="友链文章" href="/fcircle">友链文章</a><a class="footer-item" title="留言板" href="/comments">留言板</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy">隐私协议</a><a class="footer-item" title="Cookies" href="/cookies">Cookies</a><a class="footer-item" title="版权协议" href="/copyright">版权协议</a></div></div><div class="footer-group"><div class="footer-title-group"><div class="footer-title">友链</div><a class="random-friends-btn" id="footer-random-friends-btn" href="javascript:addFriendLinksInFooter();" title="换一批友情链接"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i></a></div><div class="footer-links" id="friend-links-in-footer"></div></div></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo 7.0.0" title="博客框架为Hexo 7.0.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/label/frame-hexo.svg" alt="博客框架为Hexo 7.0.0"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/label/theme-anzhiyu.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://console.upyun.com/register/?invite=nVONH00RJ" style="margin-inline:5px" data-title="本网站由又拍云提供CDN加速/云储存服务" title="本网站由又拍云提供CDN加速/云储存服务"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/label/upyun.svg" alt="本网站由又拍云提供CDN加速/云储存服务"/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/label/copyright-by-nc-sa.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2023 - 2024 By <a class="footer-bar-link" href="/" title="Karl" target="_blank">Karl</a></div></div><div id="footer-type-tips"></div><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    window.typed = new Typed("#footer-type-tips", {
      strings: ["不积跬步无以至千里","今日事，今日毕","有善始者实繁，能克终者盖寡","穷且益坚，不坠青云之志","若无闲事挂心头，便是人间好时节"],
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("footer-type-tips").innerHTML = '不积跬步无以至千里'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.cbd.int/typed.js@2.0.15/dist/typed.umd.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" title="湘ICP备2023018619号-1">湘ICP备2023018619号-1</a><a class="footer-bar-link cc" href="/copyright" title="cc协议"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-by-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nc-line"></i><i class="anzhiyufont anzhiyu-icon-creative-commons-nd-line"></i></a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">130</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">16</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" href="https://blog.karltan.com/" title="Karl的博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/logo.png" alt="Karl的博客"/><span class="back-menu-item-text">Karl的博客</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://www.karltan.com/" title="Karl的导航"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://www.karltan.com/favicon.ico" alt="Karl的导航"/><span class="back-menu-item-text">Karl的导航</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">博客分流</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://v.karltan.com/" title="Vercel"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://assets.vercel.com/image/upload/front/favicon/vercel/favicon.ico" alt="Vercel"/><span class="back-menu-item-text">Vercel</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://c.karltan.com/" title="Cloudflare"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://dash.cloudflare.com/favicon-32x32.png" alt="Cloudflare"/><span class="back-menu-item-text">Cloudflare</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">工具</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://npmzjk.karltan.com/" title="NPM"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npmzjk.karltan.com/images/favicons/favicon-32x32.png" alt="NPM"/><span class="back-menu-item-text">NPM</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://paste.karltan.com/" title="Free-Bin"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://paste.karltan.com/static/favicon.ico" alt="Free-Bin"/><span class="back-menu-item-text">Free-Bin</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://tools.karltan.com/" title="IT-TOOLS"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://tools.karltan.com/android-chrome-192x192.png" alt="IT-TOOLS"/><span class="back-menu-item-text">IT-TOOLS</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://memos.karltan.com/" title="Memos"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://memos.karltan.com/logo.png" alt="Memos"/><span class="back-menu-item-text">Memos</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://wbo.karltan.com/" title="WBO"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://wbo.karltan.com/favicon.ico" alt="WBO"/><span class="back-menu-item-text">WBO</span></a><a class="back-menu-item" target="_blank" rel="noopener" href="https://git.karltan.com/" title="Gitea"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://git.karltan.com/assets/img/logo.svg" alt="Gitea"/><span class="back-menu-item-text">Gitea</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives"><i class="fa-solid fa-box-archive faa-tada"></i><span> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories"><i class="fa-solid fa-palette faa-tada"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags"><i class="fa-solid fa-tags faa-tada"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link"><i class="fa-solid fa-link faa-tada"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle"><i class="fa-brands fa-artstation faa-tada"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments"><i class="fa-solid fa-comments faa-tada"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/essay"><i class="fa-solid fa-comment-dots faa-tada"></i><span> 说说</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/music"><i class="fa-solid fa-music faa-tada"></i><span> 音乐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album"><i class="fa-solid fa-images faa-tada"></i><span> 相册</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://status.karltan.com/status/1"><i class="fa-solid fa-chart-line faa-tada"></i><span> 网站监控</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="fa-solid fa-shoe-prints faa-tada"></i><span> 随便逛逛</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/about"><i class="fa-solid fa-heart faa-tada"></i><span> 关于我</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/AcWing/" style="font-size: 0.88rem; color: rgb(85, 23, 91);">AcWing<sup>33</sup></a><a href="/tags/CUDA/" style="font-size: 0.88rem; color: rgb(12, 161, 160);">CUDA<sup>1</sup></a><a href="/tags/CVPR/" style="font-size: 0.88rem; color: rgb(147, 94, 197);">CVPR<sup>3</sup></a><a href="/tags/ICLR/" style="font-size: 0.88rem; color: rgb(43, 82, 180);">ICLR<sup>1</sup></a><a href="/tags/Linux/" style="font-size: 0.88rem; color: rgb(122, 113, 29);">Linux<sup>1</sup></a><a href="/tags/MAPL/" style="font-size: 0.88rem; color: rgb(36, 174, 170);">MAPL<sup>1</sup></a><a href="/tags/MySQL/" style="font-size: 0.88rem; color: rgb(181, 7, 113);">MySQL<sup>1</sup></a><a href="/tags/NIPS/" style="font-size: 0.88rem; color: rgb(7, 27, 3);">NIPS<sup>1</sup></a><a href="/tags/PyTorch/" style="font-size: 0.88rem; color: rgb(150, 37, 10);">PyTorch<sup>1</sup></a><a href="/tags/Python/" style="font-size: 0.88rem; color: rgb(141, 141, 197);">Python<sup>1</sup></a><a href="/tags/RSS/" style="font-size: 0.88rem; color: rgb(112, 22, 153);">RSS<sup>1</sup></a><a href="/tags/Ubuntu/" style="font-size: 0.88rem; color: rgb(19, 93, 120);">Ubuntu<sup>3</sup></a><a href="/tags/Windows/" style="font-size: 0.88rem; color: rgb(164, 101, 25);">Windows<sup>1</sup></a><a href="/tags/arXiv/" style="font-size: 0.88rem; color: rgb(9, 118, 154);">arXiv<sup>1</sup></a><a href="/tags/mAP/" style="font-size: 0.88rem; color: rgb(199, 124, 62);">mAP<sup>1</sup></a><a href="/tags/tmux/" style="font-size: 0.88rem; color: rgb(13, 185, 37);">tmux<sup>1</sup></a><a href="/tags/vim/" style="font-size: 0.88rem; color: rgb(4, 144, 16);">vim<sup>1</sup></a><a href="/tags/%E4%BB%A3%E7%A0%81/" style="font-size: 0.88rem; color: rgb(151, 15, 28);">代码<sup>1</sup></a><a href="/tags/%E4%BD%8D%E5%A7%BF%E4%BC%B0%E8%AE%A1/" style="font-size: 0.88rem; color: rgb(130, 27, 45);">位姿估计<sup>1</sup></a><a href="/tags/%E6%89%A9%E5%AE%B9/" style="font-size: 0.88rem; color: rgb(110, 175, 30);">扩容<sup>1</sup></a><a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 0.88rem; color: rgb(158, 42, 1);">数学<sup>1</sup></a><a href="/tags/%E6%97%8B%E8%BD%AC%E8%A1%A8%E7%A4%BA/" style="font-size: 0.88rem; color: rgb(11, 137, 41);">旋转表示<sup>1</sup></a><a href="/tags/%E6%9D%8E%E6%B2%90/" style="font-size: 0.88rem; color: rgb(109, 78, 54);">李沐<sup>76</sup></a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 0.88rem; color: rgb(99, 94, 68);">深度学习<sup>79</sup></a><a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 0.88rem; color: rgb(141, 161, 139);">目标检测<sup>1</sup></a><a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 0.88rem; color: rgb(57, 29, 124);">笔记<sup>77</sup></a><a href="/tags/%E7%AE%97%E6%B3%95%E9%A2%98/" style="font-size: 0.88rem; color: rgb(153, 5, 199);">算法题<sup>33</sup></a><a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" style="font-size: 0.88rem; color: rgb(186, 192, 126);">论文笔记<sup>9</sup></a><a href="/tags/%E9%98%BF%E9%87%8C%E4%BA%91/" style="font-size: 0.88rem; color: rgb(159, 79, 86);">阿里云<sup>1</sup></a><a href="/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/" style="font-size: 0.88rem; color: rgb(7, 133, 128);">高等数学<sup>1</sup></a><a href="/tags/%E9%AD%94%E6%B3%95/" style="font-size: 0.88rem; color: rgb(99, 91, 1);">魔法<sup>1</sup></a></div></div><hr/></div></div><div id="keyboard-tips"><div class="keyboardTitle">博客快捷键</div><div class="keybordList"><div class="keybordItem"><div class="keyGroup"><div class="key">shift K</div></div><div class="keyContent"><div class="content">关闭快捷键功能</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift A</div></div><div class="keyContent"><div class="content">打开/关闭中控台</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift M</div></div><div class="keyContent"><div class="content">播放/暂停音乐</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift D</div></div><div class="keyContent"><div class="content">深色/浅色显示模式</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift S</div></div><div class="keyContent"><div class="content">站内搜索</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift R</div></div><div class="keyContent"><div class="content">随机访问</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift H</div></div><div class="keyContent"><div class="content">返回首页</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift F</div></div><div class="keyContent"><div class="content">友链鱼塘</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift L</div></div><div class="keyContent"><div class="content">友链页面</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift P</div></div><div class="keyContent"><div class="content">关于本站</div></div></div><div class="keybordItem"><div class="keyGroup"><div class="key">shift I</div></div><div class="keyContent"><div class="content">原版/本站右键菜单</div></div></div></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="anzhiyufont anzhiyu-icon-comment-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><a id="switch-commentBarrage" href="javascript:anzhiyu.switchCommentBarrage();" title="开关弹幕"><i class="anzhiyufont anzhiyu-icon-danmu"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.20/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.4/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script async src="/anzhiyu/random.js"></script><script async="async">(function () {
  var grt = new Date("07/15/2023 00:00:00"); //设置网站上线时间
  var now = new Date();
  var dnum;
  var hnum;
  var mnum;
  var snum;
  var nowHour;

  // 计算并更新天数、小时数、分钟数和秒数
  function updateTime() {
    now = new Date(); // 更新 now 的值
    nowHour = now.getHours(); // 更新 nowHour 的值
    var days = (now - grt) / 1000 / 60 / 60 / 24;
    dnum = Math.floor(days);
    var hours = (now - grt) / 1000 / 60 / 60 - 24 * dnum;
    hnum = Math.floor(hours);
    if (String(hnum).length == 1) {
      hnum = "0" + hnum;
    }
    var minutes = (now - grt) / 1000 / 60 - 24 * 60 * dnum - 60 * hnum;
    mnum = Math.floor(minutes);
    if (String(mnum).length == 1) {
      mnum = "0" + mnum;
    }
    var seconds = (now - grt) / 1000 - 24 * 60 * 60 * dnum - 60 * 60 * hnum - 60 * mnum;
    snum = Math.round(seconds);
    if (String(snum).length == 1) {
      snum = "0" + snum;
    }
  }

  // 更新网页中显示的网站运行时间
  function updateHtml() {
    const footer = document.getElementById("footer");
    if (!footer) return
    let currentTimeHtml = "";
    if (nowHour < 18 && nowHour >= 9) {
      // 如果是上班时间，默认就是"安知鱼-上班摸鱼中.svg"图片，不需要更改
      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    } else {
      // 如果是下班时间，插入"安知鱼-下班啦.svg"图片
      let img = document.querySelector("#workboard .workSituationImg");
      if (img != null) {
        img.src = "/img/label/offduty_img.svg";
        img.title = "延毕就延毕，我先玩了再说🤡";
        img.alt = "延毕就延毕，我先玩了再说🤡";
      }

      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    }

    if (document.getElementById("runtimeTextTip")) {
      document.getElementById("runtimeTextTip").innerHTML = currentTimeHtml;
    }
  }

  setInterval(() => {
    updateTime();
    updateHtml();
  }, 1000);
})();</script><script src="https://cdn.cbd.int/algoliasearch@4.18.0/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.cbd.int/instantsearch.js@4.56.5/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.cbd.int/mathjax@3.2.2/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaid.length === 0) return
  const runMermaid = () => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    Array.from($mermaid).forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
      const mermaidID = 'mermaid-' + index
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)

      const renderV10 = () => {
        renderFn.then(({svg}) => {
          mermaidSrc.insertAdjacentHTML('afterend', svg)
        })
      }

      const renderV9 = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      typeof renderFn === 'string' ? renderV9(renderFn) : renderV10()
    })
  }

  const loadMermaid = () => {
    window.loadMermaid ? runMermaid() : getScript('https://cdn.cbd.int/mermaid@10.2.4/dist/mermaid.min.js').then(runMermaid)
  }

  anzhiyu.addGlobalFn('themeChange', runMermaid, 'mermaid')

  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.karltan.com/.netlify/functions/twikoo',
      region: '',
      onCommentLoaded: () => {
        anzhiyu.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(runFn,0)
    else getScript('https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js').then(runFn)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.karltan.com/.netlify/functions/twikoo',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const runFn = () => {
    init();
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) anzhiyu.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"><script async src="/js/anzhiyu/comment_barrage.js"></script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo.karltan.com/.netlify/functions/twikoo',
        region: '',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.cbd.int/twikoo@1.6.25/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'><div class='name'><span>${array[i].nick} </span></div></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <time datetime="${array[i].date}">${anzhiyu.diffDate(array[i].date, true)}</time></div>
        </div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script>var visitorMail = "visitor@karltan.com";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script>window.va = window.va || function () { (window.vaq = window.vaq || []).push(arguments); };</script><script defer src="/_vercel/insights/script.js"></script><script charset="UTF-8" id="LA_COLLECT" src="https://sdk.51.la/js-sdk-pro.min.js"></script><script src="https://sdk.51.la/perf/js-sdk-perf.min.js" crossorigin="anonymous"></script><script>LA.init({id:"{3GbfAYtZPDEan7kS}",ck:"{3Gbf0O9t4EtAOhf1}"})</script><script>new LingQue.Monitor().init({id:"3GbfAYtZPDEan7kS",sendSuspicious:true});</script><script>(() => {
  window.$crisp = [];
  window.CRISP_WEBSITE_ID = "99975f47-0cf6-4a4b-8088-49fd5cb33ab0";
  (function () {
    d = document;
    s = d.createElement("script");
    s.src = "https://client.crisp.chat/l.js";
    s.async = 1;
    d.getElementsByTagName("head")[0].appendChild(s);
  })();
  $crisp.push(["safe", true])

  const isChatBtn = true
  const isChatHideShow = false

  if (isChatBtn) {
    const open = () => {
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])
    }

    const close = () => {
      $crisp.push(["do", "chat:hide"])
    }

    close()
    $crisp.push(["on", "chat:closed", function() {
      close()
    }])

    window.chatBtnFn = () => {
      $crisp.is("chat:visible") ? close() : open()
    }
  } else if (isChatHideShow) {
    window.chatBtn = {
      hide: () => {
        $crisp.push(["do", "chat:hide"])
      },
      show: () => {
        $crisp.push(["do", "chat:show"])
      }
    }
  }
})()</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["meta[property=\"og:image\"]","meta[property=\"og:title\"]","meta[property=\"og:url\"]","meta[property=\"og:type\"]","meta[property=\"og:site_name\"]","meta[property=\"og:description\"]","head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>